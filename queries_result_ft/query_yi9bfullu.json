[
    {
        "question": {
            "question": "与大模型工具学习相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "81272",
                "标题": "Large Language Models as Tool Makers",
                "作者": " Tianle Cai,  Xuezhi Wang,  Tengyu Ma,  Xinyun Chen,  Denny Zhou",
                "发布日期": "2023-05-29",
                "摘要": "  Recent research shows the potential of enhancing the problem-solving ability\nof large language models (LLMs) through the use of external tools. However,\nprior work along this line depends on the availability of existing tools. In\nthis work, we take an initial step towards removing this dependency by\nproposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM),\nwhere LLMs create their own reusable tools for problem-solving. Our approach\nconsists of two key phases: 1) tool making: an LLM acts as the tool maker that\ncrafts tools for given tasks, where a tool is implemented as a Python utility\nfunction. 2) tool using: an LLM acts as the tool user, which applies the tool\nbuilt by the tool maker for problem-solving. The tool user can be either the\nsame or a different LLM from the tool maker. Tool-making enables an LLM to\ncontinually generate tools that can be applied to different requests so that\nfuture requests can call the corresponding APIs when beneficial for solving the\ntasks. Furthermore, the division of labor among LLMs for tool-making and\ntool-using phases introduces the opportunity to achieve cost effectiveness\nwithout degrading the quality of generated tools and problem solutions. For\nexample, recognizing that tool-making demands more sophisticated capabilities\nthan tool-using, we can apply a powerful yet resource-intensive model as the\ntool maker, and a lightweight while cost-effective model as the tool user. We\nvalidate the effectiveness of our approach across a variety of complex\nreasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and\nGPT-3.5 as the tool user, LATM can achieve performance that is on par with\nusing GPT-4 for both tool making and tool using, while the inference cost is\nsignificantly reduced.\n",
                "链接": "https://arxiv.org/abs/2305.17126"
            },
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "82016",
                "标题": "GPT4Tools: Teaching Large Language Model to Use Tools via\n  Self-instruction",
                "作者": " Rui Yang,  Lin Song,  Yanwei Li,  Sijie Zhao,  Yixiao Ge,  Xiu Li,  Ying Shan",
                "发布日期": "2023-05-31",
                "摘要": "  This paper aims to efficiently enable Large Language Models (LLMs) to use\nmultimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, have\nshown great potential for tool usage through sophisticated prompt engineering.\nNevertheless, these models typically rely on prohibitive computational costs\nand publicly inaccessible data. To address these challenges, we propose the\nGPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and\nOPT, to use tools. It generates an instruction-following dataset by prompting\nan advanced teacher with various multi-modal contexts. By using the Low-Rank\nAdaptation (LoRA) optimization, our approach facilitates the open-source LLMs\nto solve a range of visual problems, including visual comprehension and image\ngeneration. Moreover, we provide a benchmark to evaluate the ability of LLMs to\nuse tools, which is performed in both zero-shot and fine-tuning ways. Extensive\nexperiments demonstrate the effectiveness of our method on various language\nmodels, which not only significantly improves the accuracy of invoking seen\ntools, but also enables the zero-shot capacity for unseen tools. The code and\ndemo are available at https://github.com/StevenGrove/GPT4Tools.\n",
                "链接": "https://arxiv.org/abs/2305.18752"
            },
            {
                "文章ID": "91534",
                "标题": "Federated Large Language Model: A Position Paper",
                "作者": " Chaochao Chen,  Xiaohua Feng,  Jun Zhou,  Jianwei Yin,  Xiaolin Zheng",
                "发布日期": "2023-07-19",
                "摘要": "  Large scale language models (LLM) have received significant attention and\nfound diverse applications across various domains, but their development\nencounters challenges in real-world scenarios. These challenges arise due to\nthe scarcity of public domain data availability and the need to maintain\nprivacy with respect to private domain data. To address these issues, federated\nlearning (FL) has emerged as a promising technology that enables collaborative\ntraining of shared models while preserving decentralized data. We propose the\nconcept of federated LLM, which comprises three key components, i.e., federated\nLLM pre-training, federated LLM fine-tuning, and federated LLM prompt\nengineering. For each component, we discuss its advantage over traditional LLM\ntraining methods and propose specific engineering strategies for\nimplementation. Furthermore, we explore the novel challenges introduced by the\nintegration of FL and LLM. We analyze existing solutions and identify potential\nobstacles faced by these solutions within the context of federated LLM.\n",
                "链接": "https://arxiv.org/abs/2307.08925"
            },
            {
                "文章ID": "7583",
                "标题": "Paper Plain: Making Medical Research Papers Approachable to Healthcare\n  Consumers with Natural Language Processing",
                "作者": " Tal August,  Lucy Lu Wang,  Jonathan Bragg,  Marti A. Hearst,  Andrew Head,  Kyle Lo",
                "发布日期": "2022-03-02",
                "摘要": "  When seeking information not covered in patient-friendly documents, like\nmedical pamphlets, healthcare consumers may turn to the research literature.\nReading medical papers, however, can be a challenging experience. To improve\naccess to medical papers, we introduce a novel interactive interface-Paper\nPlain-with four features powered by natural language processing: definitions of\nunfamiliar terms, in-situ plain language section summaries, a collection of key\nquestions that guide readers to answering passages, and plain language\nsummaries of the answering passages. We evaluate Paper Plain, finding that\nparticipants who use Paper Plain have an easier time reading and understanding\nresearch papers without a loss in paper comprehension compared to those who use\na typical PDF reader. Altogether, the study results suggest that guiding\nreaders to relevant passages and providing plain language summaries, or\n\"gists,\" alongside the original paper content can make reading medical papers\neasier and give readers more confidence to approach these papers.\n",
                "链接": "https://arxiv.org/abs/2203.00130"
            },
            {
                "文章ID": "106425",
                "标题": "MetaTool Benchmark for Large Language Models: Deciding Whether to Use\n  Tools and Which to Use",
                "作者": " Yue Huang,  Jiawen Shi,  Yuan Li,  Chenrui Fan,  Siyuan Wu,  Qihui Zhang,  Yixin Liu,  Pan Zhou,  Yao Wan,  Neil Zhenqiang Gong,  Lichao Sun",
                "发布日期": "2023-10-25",
                "摘要": "  Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving nine popular\nLLMs and find that the majority of them still struggle to effectively select\ntools, highlighting the existing gaps between LLMs and genuine intelligent\nagents. However, through the error analysis, we found there is still\nsignificant room for improvement. Finally, we conclude with insights for tool\ndevelopers that follow ChatGPT to provide detailed descriptions that can\nenhance the tool selection performance of LLMs.\n",
                "链接": "https://arxiv.org/abs/2310.03128"
            },
            {
                "文章ID": "94111",
                "标题": "Structural Embeddings of Tools for Large Language Models",
                "作者": " Eren Unlu",
                "发布日期": "2023-08-02",
                "摘要": "  It is evident that the current state of Large Language Models (LLMs)\nnecessitates the incorporation of external tools. The lack of straightforward\nalgebraic and logical reasoning is well documented and prompted researchers to\ndevelop frameworks which allow LLMs to operate via external tools. The\nontological nature of tool utilization for a specific task can be well\nformulated with a Directed Acyclic Graph (DAG). The central aim of the paper is\nto highlight the importance of graph based approaches to LLM-tool interaction\nin near future. We propose an exemplary framework to guide the orchestration of\nexponentially increasing numbers of external tools with LLMs,where objectives\nand functionalities of tools are graph encoded hierarchically. Assuming that\ntextual segments of a Chain-of-Thought (CoT) can be imagined as a tool as\ndefined here, the graph based framework can pave new avenues in that particular\ndirection as well.\n",
                "链接": "https://arxiv.org/abs/2308.00447"
            },
            {
                "文章ID": "78693",
                "标题": "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via\n  Tool Embeddings",
                "作者": " Shibo Hao,  Tianyang Liu,  Zhen Wang,  Zhiting Hu",
                "发布日期": "2023-11-01",
                "摘要": "  Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to solving complex problems. However, traditional methods,\nwhich finetune LLMs with tool demonstration data, can be both costly and\nrestricted to a predefined set of tools. Recent in-context learning paradigm\nalleviates these issues, but the limited context length only allows for a few\nshots of demonstrations, leading to suboptimal understandings of the tools.\nMoreover, when there are numerous tools to choose from, in-context learning\ncould completely fail to work. In this paper, we propose an alternative\napproach, $\\textbf{ToolkenGPT}$, which combines the benefits of both sides. Our\napproach represents each $\\underline{tool}$ as a to$\\underline{ken}$\n($\\textit{toolken}$) and learns an embedding for it, enabling tool calls in the\nsame way as generating a regular word token. Once a toolken is triggered, the\nLLM is prompted to complete arguments for the tool to execute. ToolkenGPT\noffers the flexibility to plug in an arbitrary number of tools by expanding the\nset of toolkens on the fly. In addition, it improves tool use by allowing\nextensive demonstration data for learning the toolken embeddings. In diverse\ndomains, including numerical reasoning, knowledge-based question answering, and\nembodied plan generation, our approach effectively augments LLMs with tools and\nsubstantially outperforms various latest baselines. ToolkenGPT demonstrates the\npromising ability to use relevant tools from a large tool set in complex\nscenarios.\n",
                "链接": "https://arxiv.org/abs/2305.11554"
            },
            {
                "文章ID": "73161",
                "标题": "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved\n  Access to Biomedical Information",
                "作者": " Qiao Jin,  Yifan Yang,  Qingyu Chen,  Zhiyong Lu",
                "发布日期": "2023-05-17",
                "摘要": "  While large language models (LLMs) have been successfully applied to various\ntasks, they still face challenges with hallucinations. Augmenting LLMs with\ndomain-specific tools such as database utilities can facilitate easier and more\nprecise access to specialized knowledge. In this paper, we present GeneGPT, a\nnovel method for teaching LLMs to use the Web APIs of the National Center for\nBiotechnology Information (NCBI) for answering genomics questions.\nSpecifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs\nby in-context learning and an augmented decoding algorithm that can detect and\nexecute API calls. Experimental results show that GeneGPT achieves\nstate-of-the-art performance on eight tasks in the GeneTuring benchmark with an\naverage score of 0.83, largely surpassing retrieval-augmented LLMs such as the\nnew Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as\nwell as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1)\nAPI demonstrations have good cross-task generalizability and are more useful\nthan documentations for in-context learning; (2) GeneGPT can generalize to\nlonger chains of API calls and answer multi-hop questions in GeneHop, a novel\ndataset introduced in this work; (3) Different types of errors are enriched in\ndifferent tasks, providing valuable insights for future improvements.\n",
                "链接": "https://arxiv.org/abs/2304.09667"
            },
            {
                "文章ID": "107503",
                "标题": "An evolutionary model of personality traits related to cooperative\n  behavior using a large language model",
                "作者": " Reiji Suzuki,  Takaya Arita",
                "发布日期": "2023-10-11",
                "摘要": "  This paper aims to shed light on the evolutionary dynamics of diverse and\nsocial populations by introducing the rich expressiveness of generative models\ninto the trait expression of social agent-based evolutionary models.\nSpecifically, we focus on the evolution of personality traits in the context of\na game-theoretic relationship as a situation in which inter-individual\ninterests exert strong selection pressures. We construct an agent model in\nwhich linguistic descriptions of personality traits related to cooperative\nbehavior are used as genes. The deterministic strategies extracted from Large\nLanguage Model (LLM) that make behavioral decisions based on these personality\ntraits are used as behavioral traits. The population is evolved according to\nselection based on average payoff and mutation of genes by asking LLM to\nslightly modify the parent gene toward cooperative or selfish. Through\npreliminary experiments and analyses, we clarify that such a model can indeed\nexhibit the evolution of cooperative behavior based on the diverse and\nhigher-order representation of personality traits. We also observed the\nrepeated intrusion of cooperative and selfish personality traits through\nchanges in the expression of personality traits, and found that the emerging\nwords in the evolved gene well reflected the behavioral tendency of its\npersonality in terms of their semantics.\n",
                "链接": "https://arxiv.org/abs/2310.05976"
            }
        ]
    },
    {
        "question": {
            "question": "和大模型可解释相关的最新论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "116604",
                "标题": "RecExplainer: Aligning Large Language Models for Recommendation Model\n  Interpretability",
                "作者": " Yuxuan Lei,  Jianxun Lian,  Jing Yao,  Xu Huang,  Defu Lian,  Xing Xie",
                "发布日期": "2023-11-21",
                "摘要": "  Recommender systems are widely used in various online services, with\nembedding-based models being particularly popular due to their expressiveness\nin representing complex signals. However, these models often lack\ninterpretability, making them less reliable and transparent for both users and\ndevelopers. With the emergence of large language models (LLMs), we find that\ntheir capabilities in language expression, knowledge-aware reasoning, and\ninstruction following are exceptionally powerful. Based on this, we propose a\nnew model interpretation approach for recommender systems, by using LLMs as\nsurrogate models and learn to mimic and comprehend target recommender models.\nSpecifically, we introduce three alignment methods: behavior alignment,\nintention alignment, and hybrid alignment. Behavior alignment operates in the\nlanguage space, representing user preferences and item information as text to\nlearn the recommendation model's behavior; intention alignment works in the\nlatent space of the recommendation model, using user and item representations\nto understand the model's behavior; hybrid alignment combines both language and\nlatent spaces for alignment training. To demonstrate the effectiveness of our\nmethods, we conduct evaluation from two perspectives: alignment effect, and\nexplanation generation ability on three public datasets. Experimental results\nindicate that our approach effectively enables LLMs to comprehend the patterns\nof recommendation models and generate highly credible recommendation\nexplanations.\n",
                "链接": "https://arxiv.org/abs/2311.10947"
            },
            {
                "文章ID": "107503",
                "标题": "An evolutionary model of personality traits related to cooperative\n  behavior using a large language model",
                "作者": " Reiji Suzuki,  Takaya Arita",
                "发布日期": "2023-10-11",
                "摘要": "  This paper aims to shed light on the evolutionary dynamics of diverse and\nsocial populations by introducing the rich expressiveness of generative models\ninto the trait expression of social agent-based evolutionary models.\nSpecifically, we focus on the evolution of personality traits in the context of\na game-theoretic relationship as a situation in which inter-individual\ninterests exert strong selection pressures. We construct an agent model in\nwhich linguistic descriptions of personality traits related to cooperative\nbehavior are used as genes. The deterministic strategies extracted from Large\nLanguage Model (LLM) that make behavioral decisions based on these personality\ntraits are used as behavioral traits. The population is evolved according to\nselection based on average payoff and mutation of genes by asking LLM to\nslightly modify the parent gene toward cooperative or selfish. Through\npreliminary experiments and analyses, we clarify that such a model can indeed\nexhibit the evolution of cooperative behavior based on the diverse and\nhigher-order representation of personality traits. We also observed the\nrepeated intrusion of cooperative and selfish personality traits through\nchanges in the expression of personality traits, and found that the emerging\nwords in the evolved gene well reflected the behavioral tendency of its\npersonality in terms of their semantics.\n",
                "链接": "https://arxiv.org/abs/2310.05976"
            },
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "115222",
                "标题": "Assessing the Interpretability of Programmatic Policies with Large\n  Language Models",
                "作者": " Zahra Bashir,  Michael Bowling,  Levi H. S. Lelis",
                "发布日期": "2023-11-14",
                "摘要": "  Although the synthesis of programs encoding policies often carries the\npromise of interpretability, systematic evaluations to assess the\ninterpretability of these policies were never performed, likely because of the\ncomplexity of such an evaluation. In this paper, we introduce a novel metric\nthat uses large-language models (LLM) to assess the interpretability of\nprogrammatic policies. For our metric, an LLM is given both a program and a\ndescription of its associated programming language. The LLM then formulates a\nnatural language explanation of the program. This explanation is subsequently\nfed into a second LLM, which tries to reconstruct the program from the natural\nlanguage explanation. Our metric measures the behavioral similarity between the\nreconstructed program and the original. We validate our approach using\nobfuscated programs that are used to solve classic programming problems. We\nalso assess our metric with programmatic policies synthesized for playing a\nreal-time strategy game, comparing the interpretability scores of programmatic\npolicies synthesized by an existing system to lightly obfuscated versions of\nthe same programs. Our LLM-based interpretability score consistently ranks less\ninterpretable programs lower and more interpretable ones higher. These findings\nsuggest that our metric could serve as a reliable and inexpensive tool for\nevaluating the interpretability of programmatic policies.\n",
                "链接": "https://arxiv.org/abs/2311.06979"
            },
            {
                "文章ID": "120483",
                "标题": "FlexModel: A Framework for Interpretability of Distributed Large\n  Language Models",
                "作者": " Matthew Choi,  Muhammad Adil Asif,  John Willes,  David Emerson",
                "发布日期": "2023-12-07",
                "摘要": "  With the growth of large language models, now incorporating billions of\nparameters, the hardware prerequisites for their training and deployment have\nseen a corresponding increase. Although existing tools facilitate model\nparallelization and distributed training, deeper model interactions, crucial\nfor interpretability and responsible AI techniques, still demand thorough\nknowledge of distributed computing. This often hinders contributions from\nresearchers with machine learning expertise but limited distributed computing\nbackground. Addressing this challenge, we present FlexModel, a software package\nproviding a streamlined interface for engaging with models distributed across\nmulti-GPU and multi-node configurations. The library is compatible with\nexisting model distribution libraries and encapsulates PyTorch models. It\nexposes user-registerable HookFunctions to facilitate straightforward\ninteraction with distributed model internals, bridging the gap between\ndistributed and single-device model paradigms. Primarily, FlexModel enhances\naccessibility by democratizing model interactions and promotes more inclusive\nresearch in the domain of large-scale neural networks. The package is found at\nhttps://github.com/VectorInstitute/flex_model.\n",
                "链接": "https://arxiv.org/abs/2312.03140"
            },
            {
                "文章ID": "20302",
                "标题": "Scaling Laws and Interpretability of Learning from Repeated Data",
                "作者": " Danny Hernandez,  Tom Brown,  Tom Conerly,  Nova DasSarma,  Dawn Drain,  Sheer El-Showk,  Nelson Elhage,  Zac Hatfield-Dodds,  Tom Henighan,  Tristan Hume,  Scott Johnston,  Ben Mann,  Chris Olah,  Catherine Olsson,  Dario Amodei,  Nicholas Joseph,  Jared Kaplan,  Sam McCandlish",
                "发布日期": "2022-05-24",
                "摘要": "  Recent large language models have been trained on vast datasets, but also\noften on repeated data, either intentionally for the purpose of upweighting\nhigher quality data, or unintentionally because data deduplication is not\nperfect and the model is exposed to repeated data at the sentence, paragraph,\nor document level. Some works have reported substantial negative performance\neffects of this repeated data. In this paper we attempt to study repeated data\nsystematically and to understand its effects mechanistically. To do this, we\ntrain a family of models where most of the data is unique but a small fraction\nof it is repeated many times. We find a strong double descent phenomenon, in\nwhich repeated data can lead test loss to increase midway through training. A\npredictable range of repetition frequency leads to surprisingly severe\ndegradation in performance. For instance, performance of an 800M parameter\nmodel can be degraded to that of a 2x smaller model (400M params) by repeating\n0.1% of the data 100 times, despite the other 90% of the training tokens\nremaining unique. We suspect there is a range in the middle where the data can\nbe memorized and doing so consumes a large fraction of the model's capacity,\nand this may be where the peak of degradation occurs. Finally, we connect these\nobservations to recent mechanistic interpretability work - attempting to\nreverse engineer the detailed computations performed by the model - by showing\nthat data repetition disproportionately damages copying and internal structures\nassociated with generalization, such as induction heads, providing a possible\nmechanism for the shift from generalization to memorization. Taken together,\nthese results provide a hypothesis for why repeating a relatively small\nfraction of data in large language models could lead to disproportionately\nlarge harms to performance.\n",
                "链接": "https://arxiv.org/abs/2205.10487"
            },
            {
                "文章ID": "48567",
                "标题": "On interpretability and proper latent decomposition of autoencoders",
                "作者": " Luca Magri,  Anh Khoa Doan",
                "发布日期": "2022-12-05",
                "摘要": "  The dynamics of a turbulent flow tend to occupy only a portion of the phase\nspace at a statistically stationary regime. From a dynamical systems point of\nview, this portion is the attractor. The knowledge of the turbulent attractor\nis useful for two purposes, at least: (i) We can gain physical insight into\nturbulence (what is the shape and geometry of the attractor?), and (ii) it\nprovides the minimal number of degrees of freedom to accurately describe the\nturbulent dynamics. Autoencoders enable the computation of an optimal latent\nspace, which is a low-order representation of the dynamics. If properly trained\nand correctly designed, autoencoders can learn an approximation of the\nturbulent attractor, as shown by Doan, Racca and Magri (2022). In this paper,\nwe theoretically interpret the transformations of an autoencoder. First, we\nremark that the latent space is a curved manifold with curvilinear coordinates,\nwhich can be analyzed with simple tools from Riemann geometry. Second, we\ncharacterize the geometrical properties of the latent space. We mathematically\nderive the metric tensor, which provides a mathematical description of the\nmanifold. Third, we propose a method -- proper latent decomposition (PLD) --\nthat generalizes proper orthogonal decomposition of turbulent flows on the\nautoencoder latent space. This decomposition finds the dominant directions in\nthe curved latent space. This theoretical work opens up computational\nopportunities for interpreting autoencoders and creating reduced-order models\nof turbulent flows.\n",
                "链接": "https://arxiv.org/abs/2211.08345"
            },
            {
                "文章ID": "16113",
                "标题": "Sparsely-gated Mixture-of-Expert Layers for CNN Interpretability",
                "作者": " Svetlana Pavlitska,  Christian Hubschneider,  Lukas Struppek,  J. Marius Zöllner",
                "发布日期": "2023-04-28",
                "摘要": "  Sparsely-gated Mixture of Expert (MoE) layers have been recently successfully\napplied for scaling large transformers, especially for language modeling tasks.\nAn intriguing side effect of sparse MoE layers is that they convey inherent\ninterpretability to a model via natural expert specialization. In this work, we\napply sparse MoE layers to CNNs for computer vision tasks and analyze the\nresulting effect on model interpretability. To stabilize MoE training, we\npresent both soft and hard constraint-based approaches. With hard constraints,\nthe weights of certain experts are allowed to become zero, while soft\nconstraints balance the contribution of experts with an additional auxiliary\nloss. As a result, soft constraints handle expert utilization better and\nsupport the expert specialization process, while hard constraints maintain more\ngeneralized experts and increase overall model performance. Our findings\ndemonstrate that experts can implicitly focus on individual sub-domains of the\ninput space. For example, experts trained for CIFAR-100 image classification\nspecialize in recognizing different domains such as flowers or animals without\nprevious data clustering. Experiments with RetinaNet and the COCO dataset\nfurther indicate that object detection experts can also specialize in detecting\nobjects of distinct sizes.\n",
                "链接": "https://arxiv.org/abs/2204.10598"
            },
            {
                "文章ID": "125173",
                "标题": "A Large Language Model-based Computational Approach to Improve\n  Identity-Related Write-Ups",
                "作者": " Alex Doboli",
                "发布日期": "2023-12-29",
                "摘要": "  Creating written products is essential to modern life, including writings\nabout one's identity and personal experiences. However, writing is often a\ndifficult activity that requires extensive effort to frame the central ideas,\nthe pursued approach to communicate the central ideas, e.g., using analogies,\nmetaphors, or other possible means, the needed presentation structure, and the\nactual verbal expression. Large Language Models, a recently emerged approach in\nMachine Learning, can offer a significant help in reducing the effort and\nimproving the quality of written products. This paper proposes a new\ncomputational approach to explore prompts that given as inputs to a Large\nLanguage Models can generate cues to improve the considered written products.\nTwo case studies on improving write-ups, one based on an analogy and one on a\nmetaphor, are also presented in the paper.\n",
                "链接": "https://arxiv.org/abs/2312.16659"
            },
            {
                "文章ID": "70693",
                "标题": "Summary of ChatGPT-Related Research and Perspective Towards the Future\n  of Large Language Models",
                "作者": " Yiheng Liu,  Tianle Han,  Siyuan Ma,  Jiayue Zhang,  Yuanyuan Yang,  Jiaming Tian,  Hao He,  Antong Li,  Mengshen He,  Zhengliang Liu,  Zihao Wu,  Lin Zhao,  Dajiang Zhu,  Xiang Li,  Ning Qiang,  Dingang Shen,  Tianming Liu,  Bao Ge",
                "发布日期": "2023-08-25",
                "摘要": "  This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and\nGPT-4) research, state-of-the-art large language models (LLM) from the GPT\nseries, and their prospective applications across diverse domains. Indeed, key\ninnovations such as large-scale pre-training that captures knowledge across the\nentire world wide web, instruction fine-tuning and Reinforcement Learning from\nHuman Feedback (RLHF) have played significant roles in enhancing LLMs'\nadaptability and performance. We performed an in-depth analysis of 194 relevant\npapers on arXiv, encompassing trend analysis, word cloud representation, and\ndistribution analysis across various application domains. The findings reveal a\nsignificant and increasing interest in ChatGPT-related research, predominantly\ncentered on direct natural language processing applications, while also\ndemonstrating considerable potential in areas ranging from education and\nhistory to mathematics, medicine, and physics. This study endeavors to furnish\ninsights into ChatGPT's capabilities, potential implications, ethical concerns,\nand offer direction for future advancements in this field.\n",
                "链接": "https://arxiv.org/abs/2304.01852"
            }
        ]
    },
    {
        "question": {
            "question": "查找关于大模型Agent应用的最新发表论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "124998",
                "标题": "On the non-slippery slope: Some observations on a recent paper on\n  rolling bodies published in Nature",
                "作者": " Olaf Müller",
                "发布日期": "2023-12-27",
                "摘要": "  An interesting recent paper in Nature explores a new method to construct\nsolid bodies rolling along given curves on an inclined plane, based on the\nGauss Theorem. The present article complements those examinations with rigorous\nexistence theorems and connections to seemingly unrelated questions like\nmaritime rescue operations.\n",
                "链接": "https://arxiv.org/abs/2312.16128"
            },
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "44503",
                "标题": "On the Transformation of Latent Space in Fine-Tuned NLP Models",
                "作者": " Nadir Durrani,  Hassan Sajjad,  Fahim Dalvi,  Firoj Alam",
                "发布日期": "2022-10-25",
                "摘要": "  We study the evolution of latent space in fine-tuned NLP models. Different\nfrom the commonly used probing-framework, we opt for an unsupervised method to\nanalyze representations. More specifically, we discover latent concepts in the\nrepresentational space using hierarchical clustering. We then use an alignment\nfunction to gauge the similarity between the latent space of a pre-trained\nmodel and its fine-tuned version. We use traditional linguistic concepts to\nfacilitate our understanding and also study how the model space transforms\ntowards task-specific information. We perform a thorough analysis, comparing\npre-trained and fine-tuned models across three models and three downstream\ntasks. The notable findings of our work are: i) the latent space of the higher\nlayers evolve towards task-specific concepts, ii) whereas the lower layers\nretain generic concepts acquired in the pre-trained model, iii) we discovered\nthat some concepts in the higher layers acquire polarity towards the output\nclass, and iv) that these concepts can be used for generating adversarial\ntriggers.\n",
                "链接": "https://arxiv.org/abs/2210.12696"
            },
            {
                "文章ID": "105928",
                "标题": "Can large language models provide useful feedback on research papers? A\n  large-scale empirical analysis",
                "作者": " Weixin Liang,  Yuhui Zhang,  Hancheng Cao,  Binglu Wang,  Daisy Ding,  Xinyu Yang,  Kailas Vodrahalli,  Siyu He,  Daniel Smith,  Yian Yin,  Daniel McFarland,  James Zou",
                "发布日期": "2023-10-04",
                "摘要": "  Expert feedback lays the foundation of rigorous research. However, the rapid\ngrowth of scholarly production and intricate knowledge specialization challenge\nthe conventional scientific feedback mechanisms. High-quality peer reviews are\nincreasingly difficult to obtain. Researchers who are more junior or from\nunder-resourced settings have especially hard times getting timely feedback.\nWith the breakthrough of large language models (LLM) such as GPT-4, there is\ngrowing interest in using LLMs to generate scientific feedback on research\nmanuscripts. However, the utility of LLM-generated feedback has not been\nsystematically studied. To address this gap, we created an automated pipeline\nusing GPT-4 to provide comments on the full PDFs of scientific papers. We\nevaluated the quality of GPT-4's feedback through two large-scale studies. We\nfirst quantitatively compared GPT-4's generated feedback with human peer\nreviewer feedback in 15 Nature family journals (3,096 papers in total) and the\nICLR machine learning conference (1,709 papers). The overlap in the points\nraised by GPT-4 and by human reviewers (average overlap 30.85% for Nature\njournals, 39.23% for ICLR) is comparable to the overlap between two human\nreviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The\noverlap between GPT-4 and human reviewers is larger for the weaker papers. We\nthen conducted a prospective user study with 308 researchers from 110 US\ninstitutions in the field of AI and computational biology to understand how\nresearchers perceive feedback generated by our GPT-4 system on their own\npapers. Overall, more than half (57.4%) of the users found GPT-4 generated\nfeedback helpful/very helpful and 82.4% found it more beneficial than feedback\nfrom at least some human reviewers. While our findings show that LLM-generated\nfeedback can help researchers, we also identify several limitations.\n",
                "链接": "https://arxiv.org/abs/2310.01783"
            },
            {
                "文章ID": "102664",
                "标题": "Information based explanation methods for deep learning agents -- with\n  applications on large open-source chess models",
                "作者": " Patrik Hammersborg,  Inga Strümke",
                "发布日期": "2023-09-19",
                "摘要": "  With large chess-playing neural network models like AlphaZero contesting the\nstate of the art within the world of computerised chess, two challenges present\nthemselves: The question of how to explain the domain knowledge internalised by\nsuch models, and the problem that such models are not made openly available.\nThis work presents the re-implementation of the concept detection methodology\napplied to AlphaZero in McGrath et al. (2022), by using large, open-source\nchess models with comparable performance. We obtain results similar to those\nachieved on AlphaZero, while relying solely on open-source resources. We also\npresent a novel explainable AI (XAI) method, which is guaranteed to highlight\nexhaustively and exclusively the information used by the explained model. This\nmethod generates visual explanations tailored to domains characterised by\ndiscrete input spaces, as is the case for chess. Our presented method has the\ndesirable property of controlling the information flow between any input vector\nand the given model, which in turn provides strict guarantees regarding what\ninformation is used by the trained model during inference. We demonstrate the\nviability of our method by applying it to standard 8x8 chess, using large\nopen-source chess models.\n",
                "链接": "https://arxiv.org/abs/2309.09702"
            },
            {
                "文章ID": "86335",
                "标题": "Friend or Foe? Exploring the Implications of Large Language Models on\n  the Science System",
                "作者": " Benedikt Fecher,  Marcel Hebing,  Melissa Laufer,  Jörg Pohle,  Fabian Sofsky",
                "发布日期": "2023-06-19",
                "摘要": "  The advent of ChatGPT by OpenAI has prompted extensive discourse on its\npotential implications for science and higher education. While the impact on\neducation has been a primary focus, there is limited empirical research on the\neffects of large language models (LLMs) and LLM-based chatbots on science and\nscientific practice. To investigate this further, we conducted a Delphi study\ninvolving 72 experts specialising in research and AI. The study focused on\napplications and limitations of LLMs, their effects on the science system,\nethical and legal considerations, and the required competencies for their\neffective use. Our findings highlight the transformative potential of LLMs in\nscience, particularly in administrative, creative, and analytical tasks.\nHowever, risks related to bias, misinformation, and quality assurance need to\nbe addressed through proactive regulation and science education. This research\ncontributes to informed discussions on the impact of generative AI in science\nand helps identify areas for future action.\n",
                "链接": "https://arxiv.org/abs/2306.09928"
            },
            {
                "文章ID": "85994",
                "标题": "Exploring the Application of Large-scale Pre-trained Models on Adverse\n  Weather Removal",
                "作者": " Zhentao Tan,  Yue Wu,  Qiankun Liu,  Qi Chu,  Le Lu,  Jieping Ye,  Nenghai Yu",
                "发布日期": "2023-06-16",
                "摘要": "  Image restoration under adverse weather conditions (e.g., rain, snow and\nhaze) is a fundamental computer vision problem and has important indications\nfor various downstream applications. Different from early methods that are\nspecially designed for specific type of weather, most recent works tend to\nremove various adverse weather effects simultaneously through either spatial\nfeature representation learning or semantic information embedding. Inspired by\nthe various successful applications of large-scale pre-trained models (e.g,\nCLIP), in this paper, we explore the potential benefits of them for this task\nthrough both spatial feature representation learning and semantic information\nembedding aspects: 1) for spatial feature representation learning, we design a\nSpatially-Adaptive Residual (\\textbf{SAR}) Encoder to extract degraded areas\nadaptively. To facilitate its training, we propose a Soft Residual Distillation\n(\\textbf{CLIP-SRD}) strategy to transfer the spatial knowledge from CLIP\nbetween clean and adverse weather images; 2) for semantic information\nembedding, we propose a CLIP Weather Prior (\\textbf{CWP}) embedding module to\nmake the network handle different weather conditions adaptively. This module\nintegrates the sample specific weather prior extracted by CLIP image encoder\ntogether with the distribution specific information learned by a set of\nparameters, and embeds them through a cross attention mechanism. Extensive\nexperiments demonstrate that our proposed method can achieve state-of-the-art\nperformance under different and challenging adverse weather conditions. Code\nwill be made available.\n",
                "链接": "https://arxiv.org/abs/2306.09008"
            },
            {
                "文章ID": "91355",
                "标题": "On the application of Large Language Models for language teaching and\n  assessment technology",
                "作者": " Andrew Caines,  Luca Benedetto,  Shiva Taslimipoor,  Christopher Davis,  Yuan Gao,  Oeistein Andersen,  Zheng Yuan,  Mark Elliott,  Russell Moore,  Christopher Bryant,  Marek Rei,  Helen Yannakoudakis,  Andrew Mullooly,  Diane Nicholls,  Paula Buttery",
                "发布日期": "2023-07-18",
                "摘要": "  The recent release of very large language models such as PaLM and GPT-4 has\nmade an unprecedented impact in the popular media and public consciousness,\ngiving rise to a mixture of excitement and fear as to their capabilities and\npotential uses, and shining a light on natural language processing research\nwhich had not previously received so much attention. The developments offer\ngreat promise for education technology, and in this paper we look specifically\nat the potential for incorporating large language models in AI-driven language\nteaching and assessment systems. We consider several research areas and also\ndiscuss the risks and ethical considerations surrounding generative AI in\neducation technology for language learners. Overall we find that larger\nlanguage models offer improvements over previous models in text generation,\nopening up routes toward content generation which had not previously been\nplausible. For text generation they must be prompted carefully and their\noutputs may need to be reshaped before they are ready for use. For automated\ngrading and grammatical error correction, tasks whose progress is checked on\nwell-known benchmarks, early investigations indicate that large language models\non their own do not improve on state-of-the-art results according to standard\nevaluation metrics. For grading it appears that linguistic features established\nin the literature should still be used for best performance, and for error\ncorrection it may be that the models can offer alternative feedback styles\nwhich are not measured sensitively with existing methods. In all cases, there\nis work to be done to experiment with the inclusion of large language models in\neducation technology for language learners, in order to properly understand and\nreport on their capacities and limitations, and to ensure that foreseeable\nrisks such as misinformation and harmful bias are mitigated.\n",
                "链接": "https://arxiv.org/abs/2307.08393"
            },
            {
                "文章ID": "109250",
                "标题": "Theory of Mind for Multi-Agent Collaboration via Large Language Models",
                "作者": " Huao Li,  Yu Quan Chong,  Simon Stepputtis,  Joseph Campbell,  Dana Hughes,  Michael Lewis,  Katia Sycara",
                "发布日期": "2023-10-24",
                "摘要": "  While Large Language Models (LLMs) have demonstrated impressive\naccomplishments in both reasoning and planning, their abilities in multi-agent\ncollaborations remains largely unexplored. This study evaluates LLM-based\nagents in a multi-agent cooperative text game with Theory of Mind (ToM)\ninference tasks, comparing their performance with Multi-Agent Reinforcement\nLearning (MARL) and planning-based baselines. We observed evidence of emergent\ncollaborative behaviors and high-order Theory of Mind capabilities among\nLLM-based agents. Our results reveal limitations in LLM-based agents' planning\noptimization due to systematic failures in managing long-horizon contexts and\nhallucination about the task state. We explore the use of explicit belief state\nrepresentations to mitigate these issues, finding that it enhances task\nperformance and the accuracy of ToM inferences for LLM-based agents.\n",
                "链接": "https://arxiv.org/abs/2310.10701"
            },
            {
                "文章ID": "91534",
                "标题": "Federated Large Language Model: A Position Paper",
                "作者": " Chaochao Chen,  Xiaohua Feng,  Jun Zhou,  Jianwei Yin,  Xiaolin Zheng",
                "发布日期": "2023-07-19",
                "摘要": "  Large scale language models (LLM) have received significant attention and\nfound diverse applications across various domains, but their development\nencounters challenges in real-world scenarios. These challenges arise due to\nthe scarcity of public domain data availability and the need to maintain\nprivacy with respect to private domain data. To address these issues, federated\nlearning (FL) has emerged as a promising technology that enables collaborative\ntraining of shared models while preserving decentralized data. We propose the\nconcept of federated LLM, which comprises three key components, i.e., federated\nLLM pre-training, federated LLM fine-tuning, and federated LLM prompt\nengineering. For each component, we discuss its advantage over traditional LLM\ntraining methods and propose specific engineering strategies for\nimplementation. Furthermore, we explore the novel challenges introduced by the\nintegration of FL and LLM. We analyze existing solutions and identify potential\nobstacles faced by these solutions within the context of federated LLM.\n",
                "链接": "https://arxiv.org/abs/2307.08925"
            }
        ]
    },
    {
        "question": {
            "question": "查询近一年模型推理加速相关的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "80551",
                "标题": "SPRING: Studying the Paper and Reasoning to Play Games",
                "作者": " Yue Wu,  Shrimai Prabhumoye,  So Yeon Min,  Yonatan Bisk,  Ruslan Salakhutdinov,  Amos Azaria,  Tom Mitchell,  Yuanzhi Li",
                "发布日期": "2023-12-13",
                "摘要": "  Open-world survival games pose significant challenges for AI algorithms due\nto their multi-tasking, deep exploration, and goal prioritization requirements.\nDespite reinforcement learning (RL) being popular for solving games, its high\nsample complexity limits its effectiveness in complex open-world games like\nCrafter or Minecraft. We propose a novel approach, SPRING, to read the game's\noriginal academic paper and use the knowledge learned to reason and play the\ngame through a large language model (LLM). Prompted with the LaTeX source as\ngame context and a description of the agent's current observation, our SPRING\nframework employs a directed acyclic graph (DAG) with game-related questions as\nnodes and dependencies as edges. We identify the optimal action to take in the\nenvironment by traversing the DAG and calculating LLM responses for each node\nin topological order, with the LLM's answer to final node directly translating\nto environment actions. In our experiments, we study the quality of in-context\n\"reasoning\" induced by different forms of prompts under the setting of the\nCrafter open-world environment. Our experiments suggest that LLMs, when\nprompted with consistent chain-of-thought, have great potential in completing\nsophisticated high-level trajectories. Quantitatively, SPRING with GPT-4\noutperforms all state-of-the-art RL baselines, trained for 1M steps, without\nany training. Finally, we show the potential of games as a test bed for LLMs.\n",
                "链接": "https://arxiv.org/abs/2305.15486"
            },
            {
                "文章ID": "117018",
                "标题": "Explaining Deep Learning Models for Age-related Gait Classification\n  based on time series acceleration",
                "作者": " Xiaoping Zheng,  Bert Otten,  Michiel F Reneman,  Claudine JC Lamoth",
                "发布日期": "2023-11-29",
                "摘要": "  Gait analysis holds significant importance in monitoring daily health,\nparticularly among older adults. Advancements in sensor technology enable the\ncapture of movement in real-life environments and generate big data. Machine\nlearning, notably deep learning (DL), shows promise to use these big data in\ngait analysis. However, the inherent black-box nature of these models poses\nchallenges for their clinical application. This study aims to enhance\ntransparency in DL-based gait classification for aged-related gait patterns\nusing Explainable Artificial Intelligence, such as SHAP.\n  A total of 244 subjects, comprising 129 adults and 115 older adults (age>65),\nwere included. They performed a 3-minute walking task while accelerometers were\naffixed to the lumbar segment L3. DL models, convolutional neural network (CNN)\nand gated recurrent unit (GRU), were trained using 1-stride and 8-stride\naccelerations, respectively, to classify adult and older adult groups. SHAP was\nemployed to explain the models' predictions.\n  CNN achieved a satisfactory performance with an accuracy of 81.4% and an AUC\nof 0.89, and GRU demonstrated promising results with an accuracy of 84.5% and\nan AUC of 0.94. SHAP analysis revealed that both CNN and GRU assigned higher\nSHAP values to the data from vertical and walking directions, particularly\nemphasizing data around heel contact, spanning from the terminal swing to\nloading response phases. Furthermore, SHAP values indicated that GRU did not\ntreat every stride equally.\n  CNN accurately distinguished between adults and older adults based on the\ncharacteristics of a single stride's data. GRU achieved accurate classification\nby considering the relationships and subtle differences between strides. In\nboth models, data around heel contact emerged as most critical, suggesting\ndifferences in acceleration and deceleration patterns during walking between\ndifferent age groups.\n",
                "链接": "https://arxiv.org/abs/2311.12089"
            },
            {
                "文章ID": "78490",
                "标题": "Lightweight Online Learning for Sets of Related Problems in Automated\n  Reasoning",
                "作者": " Haoze Wu,  Christopher Hahn,  Florian Lonsing,  Makai Mann,  Raghuram Ramanujan,  Clark Barrett",
                "发布日期": "2023-08-16",
                "摘要": "  We present Self-Driven Strategy Learning ($\\textit{sdsl}$), a lightweight\nonline learning methodology for automated reasoning tasks that involve solving\na set of related problems. $\\textit{sdsl}$ does not require offline training,\nbut instead automatically constructs a dataset while solving earlier problems.\nIt fits a machine learning model to this data which is then used to adjust the\nsolving strategy for later problems. We formally define the approach as a set\nof abstract transition rules. We describe a concrete instance of the sdsl\ncalculus which uses conditional sampling for generating data and random forests\nas the underlying machine learning model. We implement the approach on top of\nthe Kissat solver and show that the combination of Kissat+$\\textit{sdsl}$\ncertifies larger bounds and finds more counter-examples than other\nstate-of-the-art bounded model checking approaches on benchmarks obtained from\nthe latest Hardware Model Checking Competition.\n",
                "链接": "https://arxiv.org/abs/2305.11087"
            },
            {
                "文章ID": "104449",
                "标题": "The Role of Document Embedding in Research Paper Recommender Systems: To\n  Breakdown or to Bolster Disciplinary Borders?",
                "作者": " Eoghan Cunningham,  Derek Greene,  Barry Smyth",
                "发布日期": "2023-09-27",
                "摘要": "  In the extensive recommender systems literature, novelty and diversity have\nbeen identified as key properties of useful recommendations. However, these\nproperties have received limited attention in the specific sub-field of\nresearch paper recommender systems. In this work, we argue for the importance\nof offering novel and diverse research paper recommendations to scientists.\nThis approach aims to reduce siloed reading, break down filter bubbles, and\npromote interdisciplinary research. We propose a novel framework for evaluating\nthe novelty and diversity of research paper recommendations that leverages\nmethods from network analysis and natural language processing. Using this\nframework, we show that the choice of representational method within a larger\nresearch paper recommendation system can have a measurable impact on the nature\nof downstream recommendations, specifically on their novelty and diversity. We\nintroduce a novel paper embedding method, which we demonstrate offers more\ninnovative and diverse recommendations without sacrificing precision, compared\nto other state-of-the-art baselines.\n",
                "链接": "https://arxiv.org/abs/2309.14984"
            },
            {
                "文章ID": "65053",
                "标题": "Population-based Evaluation in Repeated Rock-Paper-Scissors as a\n  Benchmark for Multiagent Reinforcement Learning",
                "作者": " Marc Lanctot,  John Schultz,  Neil Burch,  Max Olan Smith,  Daniel Hennes,  Thomas Anthony,  Julien Perolat",
                "发布日期": "2023-11-02",
                "摘要": "  Progress in fields of machine learning and adversarial planning has benefited\nsignificantly from benchmark domains, from checkers and the classic UCI data\nsets to Go and Diplomacy. In sequential decision-making, agent evaluation has\nlargely been restricted to few interactions against experts, with the aim to\nreach some desired level of performance (e.g. beating a human professional\nplayer). We propose a benchmark for multiagent learning based on repeated play\nof the simple game Rock, Paper, Scissors along with a population of forty-three\ntournament entries, some of which are intentionally sub-optimal. We describe\nmetrics to measure the quality of agents based both on average returns and\nexploitability. We then show that several RL, online learning, and language\nmodel approaches can learn good counter-strategies and generalize well, but\nultimately lose to the top-performing bots, creating an opportunity for\nresearch in multiagent learning.\n",
                "链接": "https://arxiv.org/abs/2303.03196"
            },
            {
                "文章ID": "91534",
                "标题": "Federated Large Language Model: A Position Paper",
                "作者": " Chaochao Chen,  Xiaohua Feng,  Jun Zhou,  Jianwei Yin,  Xiaolin Zheng",
                "发布日期": "2023-07-19",
                "摘要": "  Large scale language models (LLM) have received significant attention and\nfound diverse applications across various domains, but their development\nencounters challenges in real-world scenarios. These challenges arise due to\nthe scarcity of public domain data availability and the need to maintain\nprivacy with respect to private domain data. To address these issues, federated\nlearning (FL) has emerged as a promising technology that enables collaborative\ntraining of shared models while preserving decentralized data. We propose the\nconcept of federated LLM, which comprises three key components, i.e., federated\nLLM pre-training, federated LLM fine-tuning, and federated LLM prompt\nengineering. For each component, we discuss its advantage over traditional LLM\ntraining methods and propose specific engineering strategies for\nimplementation. Furthermore, we explore the novel challenges introduced by the\nintegration of FL and LLM. We analyze existing solutions and identify potential\nobstacles faced by these solutions within the context of federated LLM.\n",
                "链接": "https://arxiv.org/abs/2307.08925"
            },
            {
                "文章ID": "121615",
                "标题": "Team-related Features in Code Review Prediction Models",
                "作者": " Eduardo Witter,  Ingrid Nunes,  Dietmar Jannach",
                "发布日期": "2023-12-12",
                "摘要": "  Modern Code Review (MCR) is an informal tool-assisted quality assurance\npractice. It relies on the asynchronous communication among the authors of code\nchanges and reviewers, who are developers that provide feedback. However, from\ncandidate developers, some are able to provide better feedback than others\ngiven a particular context. The selection of reviewers is thus an important\ntask, which can benefit from automated support. Many approaches have been\nproposed in this direction, using for example data from code review\nrepositories to recommend reviewers. In this paper, we propose the use of\nteam-related features to improve the performance of predictions that are\nhelpful to build code reviewer recommenders, with our target predictions being\nthe identification of reviewers that would participate in a review and the\nprovided amount of feedback. We evaluate the prediction power of these\nfeatures, which are related to code ownership, workload, and team relationship.\nThis evaluation was done by carefully addressing challenges imposed by the MCR\ndomain, such as temporal aspects of the dataset and unbalanced classes.\nMoreover, given that it is currently unknown how much past data is needed for\nbuilding MCR prediction models with acceptable performance, we explore the\namount of past data used to build prediction models. Our results show that,\nindividually, features related to code ownership have the best prediction\npower. However, based on feature selection, we conclude that all proposed\nfeatures together with lines of code can make the best predictions for both\nreviewer participation and amount of feedback. Regarding the amount of past\ndata, the timeframes of 3, 6, 9, and 12 months of data produce similar results.\nTherefore, models can be trained considering short timeframes, thus reducing\nthe computational costs with negligible impact in the prediction performance\n...\n",
                "链接": "https://arxiv.org/abs/2312.06244"
            },
            {
                "文章ID": "91604",
                "标题": "Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related\n  Rewards",
                "作者": " Saeed Ghoorchian,  Setareh Maghsudi",
                "发布日期": "2023-07-19",
                "摘要": "  Sequential decision-making under uncertainty is often associated with long\nfeedback delays. Such delays degrade the performance of the learning agent in\nidentifying a subset of arms with the optimal collective reward in the long\nrun. This problem becomes significantly challenging in a non-stationary\nenvironment with structural dependencies amongst the reward distributions\nassociated with the arms. Therefore, besides adapting to delays and\nenvironmental changes, learning the causal relations alleviates the adverse\neffects of feedback delay on the decision-making process. We formalize the\ndescribed setting as a non-stationary and delayed combinatorial semi-bandit\nproblem with causally related rewards. We model the causal relations by a\ndirected graph in a stationary structural equation model. The agent maximizes\nthe long-term average payoff, defined as a linear function of the base arms'\nrewards. We develop a policy that learns the structural dependencies from\ndelayed feedback and utilizes that to optimize the decision-making while\nadapting to drifts. We prove a regret bound for the performance of the proposed\nalgorithm. Besides, we evaluate our method via numerical analysis using\nsynthetic and real-world datasets to detect the regions that contribute the\nmost to the spread of Covid-19 in Italy.\n",
                "链接": "https://arxiv.org/abs/2307.09093"
            },
            {
                "文章ID": "108512",
                "标题": "CompA: Addressing the Gap in Compositional Reasoning in Audio-Language\n  Models",
                "作者": " Sreyan Ghosh,  Ashish Seth,  Sonal Kumar,  Utkarsh Tyagi,  Chandra Kiran Evuru,  S. Ramaneswaran,  S. Sakshi,  Oriol Nieto,  Ramani Duraiswami,  Dinesh Manocha",
                "发布日期": "2023-10-16",
                "摘要": "  A fundamental characteristic of audio is its compositional nature.\nAudio-language models (ALMs) trained using a contrastive approach (e.g., CLAP)\nthat learns a shared representation between audio and language modalities have\nimproved performance in many downstream applications, including zero-shot audio\nclassification, audio retrieval, etc. However, the ability of these models to\neffectively perform compositional reasoning remains largely unexplored and\nnecessitates additional research. In this paper, we propose CompA, a collection\nof two expert-annotated benchmarks with a majority of real-world audio samples,\nto evaluate compositional reasoning in ALMs. Our proposed CompA-order evaluates\nhow well an ALM understands the order or occurrence of acoustic events in\naudio, and CompA-attribute evaluates attribute binding of acoustic events. An\ninstance from either benchmark consists of two audio-caption pairs, where both\naudios have the same acoustic events but with different compositions. An ALM is\nevaluated on how well it matches the right audio to the right caption. Using\nthis benchmark, we first show that current ALMs perform only marginally better\nthan random chance, thereby struggling with compositional reasoning. Next, we\npropose CompA-CLAP, where we fine-tune CLAP using a novel learning method to\nimprove its compositional reasoning abilities. To train CompA-CLAP, we first\npropose improvements to contrastive training with composition-aware hard\nnegatives, allowing for more focused training. Next, we propose a novel modular\ncontrastive loss that helps the model learn fine-grained compositional\nunderstanding and overcomes the acute scarcity of openly available\ncompositional audios. CompA-CLAP significantly improves over all our baseline\nmodels on the CompA benchmark, indicating its superior compositional reasoning\ncapabilities.\n",
                "链接": "https://arxiv.org/abs/2310.08753"
            },
            {
                "文章ID": "83569",
                "标题": "Learning to Relate to Previous Turns in Conversational Search",
                "作者": " Fengran Mo,  Jian-Yun Nie,  Kaiyu Huang,  Kelong Mao,  Yutao Zhu,  Peng Li,  Yang Liu",
                "发布日期": "2023-06-06",
                "摘要": "  Conversational search allows a user to interact with a search system in\nmultiple turns. A query is strongly dependent on the conversation context. An\neffective way to improve retrieval effectiveness is to expand the current query\nwith historical queries. However, not all the previous queries are related to,\nand useful for expanding the current query. In this paper, we propose a new\nmethod to select relevant historical queries that are useful for the current\nquery. To cope with the lack of labeled training data, we use a pseudo-labeling\napproach to annotate useful historical queries based on their impact on the\nretrieval results. The pseudo-labeled data are used to train a selection model.\nWe further propose a multi-task learning framework to jointly train the\nselector and the retriever during fine-tuning, allowing us to mitigate the\npossible inconsistency between the pseudo labels and the changed retriever.\nExtensive experiments on four conversational search datasets demonstrate the\neffectiveness and broad applicability of our method compared with several\nstrong baselines.\n",
                "链接": "https://arxiv.org/abs/2306.02553"
            }
        ]
    },
    {
        "question": {
            "question": "查找论文中包含指令微调细节描述的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "66434",
                "标题": "HCI Papers Cite HCI Papers, Increasingly So",
                "作者": " Xiang 'Anthony' Chen",
                "发布日期": "2023-03-15",
                "摘要": "  We propose X-index -- the proportion of papers' citations coming from outside\ntheir research field -- and use this metric to analyze citations of CHI, UIST,\nand CSCW papers between 2010 and 2022. We found an overall decreasing X-index\nby several measures, indicating that HCI papers have been more and more likely\nto be cited by HCI papers rather than by non-HCI papers.\n",
                "链接": "https://arxiv.org/abs/2303.07539"
            },
            {
                "文章ID": "109092",
                "标题": "Legal NLP Meets MiCAR: Advancing the Analysis of Crypto White Papers",
                "作者": " Carolina Camassa",
                "发布日期": "2023-10-26",
                "摘要": "  In the rapidly evolving field of crypto assets, white papers are essential\ndocuments for investor guidance, and are now subject to unprecedented content\nrequirements under the European Union's Markets in Crypto-Assets Regulation\n(MiCAR). Natural Language Processing (NLP) can serve as a powerful tool for\nboth analyzing these documents and assisting in regulatory compliance. This\npaper delivers two contributions to the topic. First, we survey existing\napplications of textual analysis to unregulated crypto asset white papers,\nuncovering a research gap that could be bridged with interdisciplinary\ncollaboration. We then conduct an analysis of the changes introduced by MiCAR,\nhighlighting the opportunities and challenges of integrating NLP within the new\nregulatory framework. The findings set the stage for further research, with the\npotential to benefit regulators, crypto asset issuers, and investors.\n",
                "链接": "https://arxiv.org/abs/2310.10333"
            },
            {
                "文章ID": "24473",
                "标题": "An analysis of retracted papers in Computer Science",
                "作者": " Martin Shepperd,  Leila Yousefi",
                "发布日期": "2023-07-19",
                "摘要": "  Context: The retraction of research papers, for whatever reason, is a growing\nphenomenon. However, although retracted paper information is publicly available\nvia publishers, it is somewhat distributed and inconsistent. Objective: The aim\nis to assess: (i) the extent and nature of retracted research in Computer\nScience (CS) (ii) the post-retraction citation behaviour of retracted works and\n(iii) the potential impact on systematic reviews and mapping studies. Method:\nWe analyse the Retraction Watch database and take citation information from the\nWeb of Science and Google scholar. Results: We find that of the 33,955 entries\nin the Retraction watch database (16 May 2022), 2,816 are classified as CS,\ni.e., approximately 8.3%. For CS, 56% of retracted papers, provide little or no\ninformation as to the reasons. This contrasts with 26% for other disciplines.\nThere is also a remarkable disparity between different publishers, a tendency\nfor multiple versions of a retracted paper over and above the Version of Record\n(VoR), and for new citations long after a paper is officially retracted.\nConclusions: Unfortunately retraction seems to be a sufficiently common outcome\nfor a scientific paper that we as a research community need to take it more\nseriously, e.g., standardising procedures and taxonomies across publishers and\nthe provision of appropriate research tools. Finally, we recommend particular\ncaution when undertaking secondary analyses and meta-analyses which are at risk\nof becoming contaminated by these problem primary studies.\n",
                "链接": "https://arxiv.org/abs/2206.06706"
            },
            {
                "文章ID": "113764",
                "标题": "Citance-Contextualized Summarization of Scientific Papers",
                "作者": " Shahbaz Syed,  Ahmad Dawar Hakimi,  Khalid Al-Khatib,  Martin Potthast",
                "发布日期": "2023-11-14",
                "摘要": "  Current approaches to automatic summarization of scientific papers generate\ninformative summaries in the form of abstracts. However, abstracts are not\nintended to show the relationship between a paper and the references cited in\nit. We propose a new contextualized summarization approach that can generate an\ninformative summary conditioned on a given sentence containing the citation of\na reference (a so-called \"citance\"). This summary outlines the content of the\ncited paper relevant to the citation location. Thus, our approach extracts and\nmodels the citances of a paper, retrieves relevant passages from cited papers,\nand generates abstractive summaries tailored to each citance. We evaluate our\napproach using $\\textbf{Webis-Context-SciSumm-2023}$, a new dataset containing\n540K~computer science papers and 4.6M~citances therein.\n",
                "链接": "https://arxiv.org/abs/2311.02408"
            },
            {
                "文章ID": "116000",
                "标题": "Disentangling the Potential Impacts of Papers into Diffusion,\n  Conformity, and Contribution Values",
                "作者": " Zhikai Xue,  Guoxiu He,  Zhuoren Jiang,  Yangyang Kang,  Star Zhao,  Wei Lu",
                "发布日期": "2023-11-17",
                "摘要": "  The potential impact of an academic paper is determined by various factors,\nincluding its popularity and contribution. Existing models usually estimate\noriginal citation counts based on static graphs and fail to differentiate\nvalues from nuanced perspectives. In this study, we propose a novel graph\nneural network to Disentangle the Potential impacts of Papers into Diffusion,\nConformity, and Contribution values (called DPPDCC). Given a target paper,\nDPPDCC encodes temporal and structural features within the constructed dynamic\nheterogeneous graph. Particularly, to capture the knowledge flow, we emphasize\nthe importance of comparative and co-cited/citing information between papers\nand aggregate snapshots evolutionarily. To unravel popularity, we contrast\naugmented graphs to extract the essence of diffusion and predict the\naccumulated citation binning to model conformity. We further apply orthogonal\nconstraints to encourage distinct modeling of each perspective and preserve the\ninherent value of contribution. To evaluate models' generalization for papers\npublished at various times, we reformulate the problem by partitioning data\nbased on specific time points to mirror real-world conditions. Extensive\nexperimental results on three datasets demonstrate that DPPDCC significantly\noutperforms baselines for previously, freshly, and immediately published\npapers. Further analyses confirm its robust capabilities. We will make our\ndatasets and codes publicly available.\n",
                "链接": "https://arxiv.org/abs/2311.09262"
            },
            {
                "文章ID": "50978",
                "标题": "On the Effectiveness of Parameter-Efficient Fine-Tuning",
                "作者": " Zihao Fu,  Haoran Yang,  Anthony Man-Cho So,  Wai Lam,  Lidong Bing,  Nigel Collier",
                "发布日期": "2022-11-29",
                "摘要": "  Fine-tuning pre-trained models has been ubiquitously proven to be effective\nin a wide range of NLP tasks. However, fine-tuning the whole model is parameter\ninefficient as it always yields an entirely new model for each task. Currently,\nmany research works propose to only fine-tune a small portion of the parameters\nwhile keeping most of the parameters shared across different tasks. These\nmethods achieve surprisingly good performance and are shown to be more stable\nthan their corresponding fully fine-tuned counterparts. However, such kind of\nmethods is still not well understood. Some natural questions arise: How does\nthe parameter sparsity lead to promising performance? Why is the model more\nstable than the fully fine-tuned models? How to choose the tunable parameters?\nIn this paper, we first categorize the existing methods into random approaches,\nrule-based approaches, and projection-based approaches based on how they choose\nwhich parameters to tune. Then, we show that all of the methods are actually\nsparse fine-tuned models and conduct a novel theoretical analysis of them. We\nindicate that the sparsity is actually imposing a regularization on the\noriginal model by controlling the upper bound of the stability. Such stability\nleads to better generalization capability which has been empirically observed\nin a lot of recent research works. Despite the effectiveness of sparsity\ngrounded by our theory, it still remains an open problem of how to choose the\ntunable parameters. To better choose the tunable parameters, we propose a novel\nSecond-order Approximation Method (SAM) which approximates the original problem\nwith an analytically solvable optimization function. The tunable parameters are\ndetermined by directly optimizing the approximation function. The experimental\nresults show that our proposed SAM model outperforms many strong baseline\nmodels and it also verifies our theoretical analysis.\n",
                "链接": "https://arxiv.org/abs/2211.15583"
            },
            {
                "文章ID": "114514",
                "标题": "Training CLIP models on Data from Scientific Papers",
                "作者": " Calvin Metzger",
                "发布日期": "2023-11-09",
                "摘要": "  Contrastive Language-Image Pretraining (CLIP) models are able to capture the\nsemantic relationship of images and texts and have enabled a wide range of\napplications, from image retrieval to classification. These models are trained\nwith datasets extracted from web crawls, which are of large quantity but\nlimited quality. This paper explores whether limited amounts higher quality\ndata in a specific domain improve the general performance of CLIP models. To\nthis purpose, we extract text-image data from scientific papers hosted in the\narXiv and PubMed Central repositories. Experiments on small-scale CLIP models\n(ViT B/32) show that model performance increases on average, but only\nmoderately. This result indicates that using the data sources considered in the\npaper to train large-scale CLIP models is a worthwile research direction.\n",
                "链接": "https://arxiv.org/abs/2311.04711"
            },
            {
                "文章ID": "58620",
                "标题": "Call for Papers -- The BabyLM Challenge: Sample-efficient pretraining on\n  a developmentally plausible corpus",
                "作者": " Alex Warstadt,  Leshem Choshen,  Aaron Mueller,  Adina Williams,  Ethan Wilcox,  Chengxu Zhuang",
                "发布日期": "2023-01-30",
                "摘要": "  We present the call for papers for the BabyLM Challenge: Sample-efficient\npretraining on a developmentally plausible corpus. This shared task is intended\nfor participants with an interest in small scale language modeling, human\nlanguage acquisition, low-resource NLP, and cognitive modeling. In partnership\nwith CoNLL and CMCL, we provide a platform for approaches to pretraining with a\nlimited-size corpus sourced from data inspired by the input to children. The\ntask has three tracks, two of which restrict the training data to pre-released\ndatasets of 10M and 100M words and are dedicated to explorations of approaches\nsuch as architectural variations, self-supervised objectives, or curriculum\nlearning. The final track only restricts the amount of text used, allowing\ninnovation in the choice of the data, its domain, and even its modality (i.e.,\ndata from sources other than text is welcome). We will release a shared\nevaluation pipeline which scores models on a variety of benchmarks and tasks,\nincluding targeted syntactic evaluations and natural language understanding.\n",
                "链接": "https://arxiv.org/abs/2301.11796"
            },
            {
                "文章ID": "42172",
                "标题": "Predicting the clinical citation count of biomedical papers using\n  multilayer perceptron neural network",
                "作者": " Xin Li,  Xuli Tang,  Qikai Cheng",
                "发布日期": "2022-10-24",
                "摘要": "  The number of clinical citations received from clinical guidelines or\nclinical trials has been considered as one of the most appropriate indicators\nfor quantifying the clinical impact of biomedical papers. Therefore, the early\nprediction of the clinical citation count of biomedical papers is critical to\nscientific activities in biomedicine, such as research evaluation, resource\nallocation, and clinical translation. In this study, we designed a four-layer\nmultilayer perceptron neural network (MPNN) model to predict the clinical\ncitation count of biomedical papers in the future by using 9,822,620 biomedical\npapers published from 1985 to 2005. We extracted ninety-one paper features from\nthree dimensions as the input of the model, including twenty-one features in\nthe paper dimension, thirty-five in the reference dimension, and thirty-five in\nthe citing paper dimension. In each dimension, the features can be classified\ninto three categories, i.e., the citation-related features, the clinical\ntranslation-related features, and the topic-related features. Besides, in the\npaper dimension, we also considered the features that have previously been\ndemonstrated to be related to the citation counts of research papers. The\nresults showed that the proposed MPNN model outperformed the other five\nbaseline models, and the features in the reference dimension were the most\nimportant.\n",
                "链接": "https://arxiv.org/abs/2210.06346"
            },
            {
                "文章ID": "85250",
                "标题": "The Devil is in the Details: On the Pitfalls of Event Extraction\n  Evaluation",
                "作者": " Hao Peng,  Xiaozhi Wang,  Feng Yao,  Kaisheng Zeng,  Lei Hou,  Juanzi Li,  Zhiyuan Liu,  Weixing Shen",
                "发布日期": "2023-06-16",
                "摘要": "  Event extraction (EE) is a crucial task aiming at extracting events from\ntexts, which includes two subtasks: event detection (ED) and event argument\nextraction (EAE). In this paper, we check the reliability of EE evaluations and\nidentify three major pitfalls: (1) The data preprocessing discrepancy makes the\nevaluation results on the same dataset not directly comparable, but the data\npreprocessing details are not widely noted and specified in papers. (2) The\noutput space discrepancy of different model paradigms makes different-paradigm\nEE models lack grounds for comparison and also leads to unclear mapping issues\nbetween predictions and annotations. (3) The absence of pipeline evaluation of\nmany EAE-only works makes them hard to be directly compared with EE works and\nmay not well reflect the model performance in real-world pipeline scenarios. We\ndemonstrate the significant influence of these pitfalls through comprehensive\nmeta-analyses of recent papers and empirical experiments. To avoid these\npitfalls, we suggest a series of remedies, including specifying data\npreprocessing, standardizing outputs, and providing pipeline evaluation\nresults. To help implement these remedies, we develop a consistent evaluation\nframework OMNIEVENT, which can be obtained from\nhttps://github.com/THU-KEG/OmniEvent.\n",
                "链接": "https://arxiv.org/abs/2306.06918"
            }
        ]
    },
    {
        "question": {
            "question": "查找OCR文本检测最新进展",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "53038",
                "标题": "Algorithmic progress in computer vision",
                "作者": " Ege Erdil,  Tamay Besiroglu",
                "发布日期": "2023-08-25",
                "摘要": "  We investigate algorithmic progress in image classification on ImageNet,\nperhaps the most well-known test bed for computer vision. We estimate a model,\ninformed by work on neural scaling laws, and infer a decomposition of progress\ninto the scaling of compute, data, and algorithms. Using Shapley values to\nattribute performance improvements, we find that algorithmic improvements have\nbeen roughly as important as the scaling of compute for progress computer\nvision. Our estimates indicate that algorithmic innovations mostly take the\nform of compute-augmenting algorithmic advances (which enable researchers to\nget better performance from less compute), not data-augmenting algorithmic\nadvances. We find that compute-augmenting algorithmic advances are made at a\npace more than twice as fast as the rate usually associated with Moore's law.\nIn particular, we estimate that compute-augmenting innovations halve compute\nrequirements every nine months (95\\% confidence interval: 4 to 25 months).\n",
                "链接": "https://arxiv.org/abs/2212.05153"
            },
            {
                "文章ID": "18001",
                "标题": "Text Detection on Technical Drawings for the Digitization of Brown-field\n  Processes",
                "作者": " Tobias Schlagenhauf,  Markus Netzer,  Jan Hillinger",
                "发布日期": "2022-05-06",
                "摘要": "  This paper addresses the issue of autonomously detecting text on technical\ndrawings. The detection of text on technical drawings is a critical step\ntowards autonomous production machines, especially for brown-field processes,\nwhere no closed CAD-CAM solutions are available yet. Automating the process of\nreading and detecting text on technical drawings reduces the effort for\nhandling inefficient media interruptions due to paper-based processes, which\nare often todays quasi-standard in brown-field processes. However, there are no\nreliable methods available yet to solve the issue of automatically detecting\ntext on technical drawings. The unreliable detection of the contents on\ntechnical drawings using classical detection and object character recognition\n(OCR) tools is mainly due to the limited number of technical drawings and the\ncaptcha-like structure of the contents. Text is often combined with unknown\nsymbols and interruptions by lines. Additionally, due to intellectual property\nrights and technical know-how issues, there are no out-of-the box training\ndatasets available in the literature to train such models. This paper combines\na domain knowledge-based generator to generate realistic technical drawings\nwith a state-of-the-art object detection model to solve the issue of detecting\ntext on technical drawings. The generator yields artificial technical drawings\nin a large variety and can be considered as a data augmentation generator.\nThese artificial drawings are used for training, while the model is tested on\nreal data. The authors show that artificially generated data of technical\ndrawings improve the detection quality with an increasing number of drawings.\n",
                "链接": "https://arxiv.org/abs/2205.02659"
            },
            {
                "文章ID": "80395",
                "标题": "Computer Vision for Construction Progress Monitoring: A Real-Time Object\n  Detection Approach",
                "作者": " Jiesheng Yang,  Andreas Wilde,  Karsten Menzel,  Md Zubair Sheikh,  Boris Kuznetsov",
                "发布日期": "2023-05-25",
                "摘要": "  Construction progress monitoring (CPM) is essential for effective project\nmanagement, ensuring on-time and on-budget delivery. Traditional CPM methods\noften rely on manual inspection and reporting, which are time-consuming and\nprone to errors. This paper proposes a novel approach for automated CPM using\nstate-of-the-art object detection algorithms. The proposed method leverages\ne.g. YOLOv8's real-time capabilities and high accuracy to identify and track\nconstruction elements within site images and videos. A dataset was created,\nconsisting of various building elements and annotated with relevant objects for\ntraining and validation. The performance of the proposed approach was evaluated\nusing standard metrics, such as precision, recall, and F1-score, demonstrating\nsignificant improvement over existing methods. The integration of Computer\nVision into CPM provides stakeholders with reliable, efficient, and\ncost-effective means to monitor project progress, facilitating timely\ndecision-making and ultimately contributing to the successful completion of\nconstruction projects.\n",
                "链接": "https://arxiv.org/abs/2305.15097"
            },
            {
                "文章ID": "53845",
                "标题": "Backdoor Attack Detection in Computer Vision by Applying Matrix\n  Factorization on the Weights of Deep Networks",
                "作者": " Khondoker Murad Hossain,  Tim Oates",
                "发布日期": "2022-12-19",
                "摘要": "  The increasing importance of both deep neural networks (DNNs) and cloud\nservices for training them means that bad actors have more incentive and\nopportunity to insert backdoors to alter the behavior of trained models. In\nthis paper, we introduce a novel method for backdoor detection that extracts\nfeatures from pre-trained DNN's weights using independent vector analysis (IVA)\nfollowed by a machine learning classifier. In comparison to other detection\ntechniques, this has a number of benefits, such as not requiring any training\ndata, being applicable across domains, operating with a wide range of network\narchitectures, not assuming the nature of the triggers used to change network\nbehavior, and being highly scalable. We discuss the detection pipeline, and\nthen demonstrate the results on two computer vision datasets regarding image\nclassification and object detection. Our method outperforms the competing\nalgorithms in terms of efficiency and is more accurate, helping to ensure the\nsafe application of deep learning and AI.\n",
                "链接": "https://arxiv.org/abs/2212.08121"
            },
            {
                "文章ID": "46442",
                "标题": "Deep Learning Computer Vision Algorithms for Real-time UAVs On-board\n  Camera Image Processing",
                "作者": " Alessandro Palmas,  Pietro Andronico",
                "发布日期": "2022-11-03",
                "摘要": "  This paper describes how advanced deep learning based computer vision\nalgorithms are applied to enable real-time on-board sensor processing for small\nUAVs. Four use cases are considered: target detection, classification and\nlocalization, road segmentation for autonomous navigation in GNSS-denied zones,\nhuman body segmentation, and human action recognition. All algorithms have been\ndeveloped using state-of-the-art image processing methods based on deep neural\nnetworks. Acquisition campaigns have been carried out to collect custom\ndatasets reflecting typical operational scenarios, where the peculiar point of\nview of a multi-rotor UAV is replicated. Algorithms architectures and trained\nmodels performances are reported, showing high levels of both accuracy and\ninference speed. Output examples and on-field videos are presented,\ndemonstrating models operation when deployed on a GPU-powered commercial\nembedded device (NVIDIA Jetson Xavier) mounted on board of a custom quad-rotor,\npaving the way to enabling high level autonomy.\n",
                "链接": "https://arxiv.org/abs/2211.01037"
            },
            {
                "文章ID": "77352",
                "标题": "On the Hidden Mystery of OCR in Large Multimodal Models",
                "作者": " Yuliang Liu,  Zhang Li,  Hongliang Li,  Wenwen Yu,  Yang Liu,  Biao Yang,  Mingxin Huang,  Dezhi Peng,  Mingyu Liu,  Mingrui Chen,  Chunyuan Li,  Xucheng Yin,  Cheng-lin Liu,  Lianwen Jin,  Xiang Bai",
                "发布日期": "2023-06-21",
                "摘要": "  Large models have recently played a dominant role in natural language\nprocessing and multimodal vision-language learning. It remains less explored\nabout their efficacy in text-related visual tasks. We conducted a comprehensive\nstudy of existing publicly available multimodal models, evaluating their\nperformance in text recognition (document text, artistic text, handwritten\ntext, scene text), text-based visual question answering (document text, scene\ntext, and bilingual text), key information extraction (receipts, documents, and\nnutrition facts) and handwritten mathematical expression recognition. Our\nfindings reveal strengths and weaknesses in these models, which primarily rely\non semantic understanding for word recognition and exhibit inferior perception\nof individual character shapes. They also display indifference towards text\nlength and have limited capabilities in detecting finegrained features in\nimages. Consequently, these results demonstrate that even the current most\npowerful large multimodal models cannot match domain-specific methods in\ntraditional text tasks and face greater challenges in more complex tasks. Most\nimportantly, the baseline results showcased in this study could provide a\nfoundational framework for the conception and assessment of innovative\nstrategies targeted at enhancing zero-shot multimodal techniques. Evaluation\npipeline is available at https://github.com/Yuliang-Liu/MultimodalOCR.\n",
                "链接": "https://arxiv.org/abs/2305.07895"
            },
            {
                "文章ID": "87564",
                "标题": "Resume Information Extraction via Post-OCR Text Processing",
                "作者": " Selahattin Serdar Helli,  Senem Tanberk,  Sena Nur Cavsak",
                "发布日期": "2023-06-27",
                "摘要": "  Information extraction (IE), one of the main tasks of natural language\nprocessing (NLP), has recently increased importance in the use of resumes. In\nstudies on the text to extract information from the CV, sentence classification\nwas generally made using NLP models. In this study, it is aimed to extract\ninformation by classifying all of the text groups after pre-processing such as\nOptical Character Recognition (OCT) and object recognition with the YOLOv8\nmodel of the resumes. The text dataset consists of 286 resumes collected for 5\ndifferent (education, experience, talent, personal and language) job\ndescriptions in the IT industry. The dataset created for object recognition\nconsists of 1198 resumes, which were collected from the open-source internet\nand labeled as sets of text. BERT, BERT-t, DistilBERT, RoBERTa and XLNet were\nused as models. F1 score variances were used to compare the model results. In\naddition, the YOLOv8 model has also been reported comparatively in itself. As a\nresult of the comparison, DistilBERT was showed better results despite having a\nlower number of parameters than other models.\n",
                "链接": "https://arxiv.org/abs/2306.13775"
            },
            {
                "文章ID": "85288",
                "标题": "When Vision Fails: Text Attacks Against ViT and OCR",
                "作者": " Nicholas Boucher,  Jenny Blessing,  Ilia Shumailov,  Ross Anderson,  Nicolas Papernot",
                "发布日期": "2023-06-13",
                "摘要": "  While text-based machine learning models that operate on visual inputs of\nrendered text have become robust against a wide range of existing attacks, we\nshow that they are still vulnerable to visual adversarial examples encoded as\ntext. We use the Unicode functionality of combining diacritical marks to\nmanipulate encoded text so that small visual perturbations appear when the text\nis rendered. We show how a genetic algorithm can be used to generate visual\nadversarial examples in a black-box setting, and conduct a user study to\nestablish that the model-fooling adversarial examples do not affect human\ncomprehension. We demonstrate the effectiveness of these attacks in the real\nworld by creating adversarial examples against production models published by\nFacebook, Microsoft, IBM, and Google.\n",
                "链接": "https://arxiv.org/abs/2306.07033"
            },
            {
                "文章ID": "32360",
                "标题": "Bias and Fairness in Computer Vision Applications of the Criminal\n  Justice System",
                "作者": " Sophie Noiret,  Jennifer Lumetzberger,  Martin Kampel",
                "发布日期": "2022-08-08",
                "摘要": "  Discriminatory practices involving AI-driven police work have been the\nsubject of much controversies in the past few years, with algorithms such as\nCOMPAS, PredPol and ShotSpotter being accused of unfairly impacting minority\ngroups. At the same time, the issues of fairness in machine learning, and in\nparticular in computer vision, have been the subject of a growing number of\nacademic works. In this paper, we examine how these area intersect. We provide\ninformation on how these practices have come to exist and the difficulties in\nalleviating them. We then examine three applications currently in development\nto understand what risks they pose to fairness and how those risks can be\nmitigated.\n",
                "链接": "https://arxiv.org/abs/2208.03209"
            },
            {
                "文章ID": "15863",
                "标题": "Making the Most of Text Semantics to Improve Biomedical Vision--Language\n  Processing",
                "作者": " Benedikt Boecking,  Naoto Usuyama,  Shruthi Bannur,  Daniel C. Castro,  Anton Schwaighofer,  Stephanie Hyland,  Maria Wetscherek,  Tristan Naumann,  Aditya Nori,  Javier Alvarez-Valle,  Hoifung Poon,  Ozan Oktay",
                "发布日期": "2022-12-08",
                "摘要": "  Multi-modal data abounds in biomedicine, such as radiology images and\nreports. Interpreting this data at scale is essential for improving clinical\ncare and accelerating clinical research. Biomedical text with its complex\nsemantics poses additional challenges in vision--language modelling compared to\nthe general domain, and previous work has used insufficiently adapted models\nthat lack domain-specific language understanding. In this paper, we show that\nprincipled textual semantic modelling can substantially improve contrastive\nlearning in self-supervised vision--language processing. We release a language\nmodel that achieves state-of-the-art results in radiology natural language\ninference through its improved vocabulary and novel language pretraining\nobjective leveraging semantics and discourse characteristics in radiology\nreports. Further, we propose a self-supervised joint vision--language approach\nwith a focus on better text modelling. It establishes new state of the art\nresults on a wide range of publicly available benchmarks, in part by leveraging\nour new domain-specific language model. We release a new dataset with\nlocally-aligned phrase grounding annotations by radiologists to facilitate the\nstudy of complex semantic modelling in biomedical vision--language processing.\nA broad evaluation, including on this new dataset, shows that our contrastive\nlearning approach, aided by textual-semantic modelling, outperforms prior\nmethods in segmentation tasks, despite only using a global-alignment objective.\n",
                "链接": "https://arxiv.org/abs/2204.09817"
            }
        ]
    },
    {
        "question": {
            "question": "查找OCR文本识别最新进展。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "90438",
                "标题": "Handwritten Text Recognition Using Convolutional Neural Network",
                "作者": " Atman Mishra,  A. Sharath Ram,  Kavyashree C",
                "发布日期": "2023-07-12",
                "摘要": "  OCR (Optical Character Recognition) is a technology that offers comprehensive\nalphanumeric recognition of handwritten and printed characters at electronic\nspeed by merely scanning the document. Recently, the understanding of visual\ndata has been termed Intelligent Character Recognition (ICR). Intelligent\nCharacter Recognition (ICR) is the OCR module that can convert scans of\nhandwritten or printed characters into ASCII text. ASCII data is the standard\nformat for data encoding in electronic communication. ASCII assigns standard\nnumeric values to letters, numeral, symbols, white-spaces and other characters.\nIn more technical terms, OCR is the process of using an electronic device to\ntransform 2-Dimensional textual information into machine-encoded text. Anything\nthat contains text both machine written or handwritten can be scanned either\nthrough a scanner or just simply a picture of the text is enough for the\nrecognition system to distinguish the text. The goal of this papers is to show\nthe results of a Convolutional Neural Network model which has been trained on\nNational Institute of Science and Technology (NIST) dataset containing over a\n100,000 images. The network learns from the features extracted from the images\nand use it to generate the probability of each class to which the picture\nbelongs to. We have achieved an accuracy of 90.54% with a loss of 2.53%.\n",
                "链接": "https://arxiv.org/abs/2307.05396"
            },
            {
                "文章ID": "90096",
                "标题": "A Novel Pipeline for Improving Optical Character Recognition through\n  Post-processing Using Natural Language Processing",
                "作者": " Aishik Rakshit,  Samyak Mehta,  Anirban Dasgupta",
                "发布日期": "2023-07-11",
                "摘要": "  Optical Character Recognition (OCR) technology finds applications in\ndigitizing books and unstructured documents, along with applications in other\ndomains such as mobility statistics, law enforcement, traffic, security\nsystems, etc. The state-of-the-art methods work well with the OCR with printed\ntext on license plates, shop names, etc. However, applications such as printed\ntextbooks and handwritten texts have limited accuracy with existing techniques.\nThe reason may be attributed to similar-looking characters and variations in\nhandwritten characters. Since these issues are challenging to address with OCR\ntechnologies exclusively, we propose a post-processing approach using Natural\nLanguage Processing (NLP) tools. This work presents an end-to-end pipeline that\nfirst performs OCR on the handwritten or printed text and then improves its\naccuracy using NLP.\n",
                "链接": "https://arxiv.org/abs/2307.04245"
            },
            {
                "文章ID": "46214",
                "标题": "Self-supervised Character-to-Character Distillation for Text Recognition",
                "作者": " Tongkun Guan,  Wei Shen,  Xue Yang,  Qi Feng,  Zekun Jiang,  Xiaokang Yang",
                "发布日期": "2023-08-21",
                "摘要": "  When handling complicated text images (e.g., irregular structures, low\nresolution, heavy occlusion, and uneven illumination), existing supervised text\nrecognition methods are data-hungry. Although these methods employ large-scale\nsynthetic text images to reduce the dependence on annotated real images, the\ndomain gap still limits the recognition performance. Therefore, exploring the\nrobust text feature representations on unlabeled real images by self-supervised\nlearning is a good solution. However, existing self-supervised text recognition\nmethods conduct sequence-to-sequence representation learning by roughly\nsplitting the visual features along the horizontal axis, which limits the\nflexibility of the augmentations, as large geometric-based augmentations may\nlead to sequence-to-sequence feature inconsistency. Motivated by this, we\npropose a novel self-supervised Character-to-Character Distillation method,\nCCD, which enables versatile augmentations to facilitate general text\nrepresentation learning. Specifically, we delineate the character structures of\nunlabeled real images by designing a self-supervised character segmentation\nmodule. Following this, CCD easily enriches the diversity of local characters\nwhile keeping their pairwise alignment under flexible augmentations, using the\ntransformation matrix between two augmented views from images. Experiments\ndemonstrate that CCD achieves state-of-the-art results, with average\nperformance gains of 1.38% in text recognition, 1.7% in text segmentation, 0.24\ndB (PSNR) and 0.0321 (SSIM) in text super-resolution. Code is available at\nhttps://github.com/TongkunGuan/CCD.\n",
                "链接": "https://arxiv.org/abs/2211.00288"
            },
            {
                "文章ID": "94665",
                "标题": "Universal Defensive Underpainting Patch: Making Your Text Invisible to\n  Optical Character Recognition",
                "作者": " JiaCheng Deng,  Li Dong,  Jiahao Chen,  Diqun Yan,  Rangding Wang,  Dengpan Ye,  Lingchen Zhao,  Jinyu Tian",
                "发布日期": "2023-08-07",
                "摘要": "  Optical Character Recognition (OCR) enables automatic text extraction from\nscanned or digitized text images, but it also makes it easy to pirate valuable\nor sensitive text from these images. Previous methods to prevent OCR piracy by\ndistorting characters in text images are impractical in real-world scenarios,\nas pirates can capture arbitrary portions of the text images, rendering the\ndefenses ineffective. In this work, we propose a novel and effective defense\nmechanism termed the Universal Defensive Underpainting Patch (UDUP) that\nmodifies the underpainting of text images instead of the characters. UDUP is\ncreated through an iterative optimization process to craft a small, fixed-size\ndefensive patch that can generate non-overlapping underpainting for text images\nof any size. Experimental results show that UDUP effectively defends against\nunauthorized OCR under the setting of any screenshot range or complex image\nbackground. It is agnostic to the content, size, colors, and languages of\ncharacters, and is robust to typical image operations such as scaling and\ncompressing. In addition, the transferability of UDUP is demonstrated by\nevading several off-the-shelf OCRs. The code is available at\nhttps://github.com/QRICKDD/UDUP.\n",
                "链接": "https://arxiv.org/abs/2308.02369"
            },
            {
                "文章ID": "10262",
                "标题": "Optimal Rejection Function Meets Character Recognition Tasks",
                "作者": " Xiaotong Ji,  Yuchen Zheng,  Daiki Suehiro,  Seiichi Uchida",
                "发布日期": "2022-03-18",
                "摘要": "  In this paper, we propose an optimal rejection method for rejecting ambiguous\nsamples by a rejection function. This rejection function is trained together\nwith a classification function under the framework of Learning-with-Rejection\n(LwR). The highlights of LwR are: (1) the rejection strategy is not heuristic\nbut has a strong background from a machine learning theory, and (2) the\nrejection function can be trained on an arbitrary feature space which is\ndifferent from the feature space for classification. The latter suggests we can\nchoose a feature space that is more suitable for rejection. Although the past\nresearch on LwR focused only on its theoretical aspect, we propose to utilize\nLwR for practical pattern classification tasks. Moreover, we propose to use\nfeatures from different CNN layers for classification and rejection. Our\nextensive experiments of notMNIST classification and character/non-character\nclassification demonstrate that the proposed method achieves better performance\nthan traditional rejection strategies.\n",
                "链接": "https://arxiv.org/abs/2203.09151"
            },
            {
                "文章ID": "39690",
                "标题": "3D Rendering Framework for Data Augmentation in Optical Character\n  Recognition",
                "作者": " Andreas Spruck,  Maximiliane Hawesch,  Anatol Maier,  Christian Riess,  Jürgen Seiler,  André Kaup",
                "发布日期": "2022-09-30",
                "摘要": "  In this paper, we propose a data augmentation framework for Optical Character\nRecognition (OCR). The proposed framework is able to synthesize new viewing\nangles and illumination scenarios, effectively enriching any available OCR\ndataset. Its modular structure allows to be modified to match individual user\nrequirements. The framework enables to comfortably scale the enlargement factor\nof the available dataset. Furthermore, the proposed method is not restricted to\nsingle frame OCR but can also be applied to video OCR. We demonstrate the\nperformance of our framework by augmenting a 15% subset of the common Brno\nMobile OCR dataset. Our proposed framework is capable of leveraging the\nperformance of OCR applications especially for small datasets. Applying the\nproposed method, improvements of up to 2.79 percentage points in terms of\nCharacter Error Rate (CER), and up to 7.88 percentage points in terms of Word\nError Rate (WER) are achieved on the subset. Especially the recognition of\nchallenging text lines can be improved. The CER may be decreased by up to 14.92\npercentage points and the WER by up to 18.19 percentage points for this class.\nMoreover, we are able to achieve smaller error rates when training on the 15%\nsubset augmented with the proposed method than on the original non-augmented\nfull dataset.\n",
                "链接": "https://arxiv.org/abs/2209.14970"
            },
            {
                "文章ID": "99425",
                "标题": "DTrOCR: Decoder-only Transformer for Optical Character Recognition",
                "作者": " Masato Fujitake",
                "发布日期": "2023-08-31",
                "摘要": "  Typical text recognition methods rely on an encoder-decoder structure, in\nwhich the encoder extracts features from an image, and the decoder produces\nrecognized text from these features. In this study, we propose a simpler and\nmore effective method for text recognition, known as the Decoder-only\nTransformer for Optical Character Recognition (DTrOCR). This method uses a\ndecoder-only Transformer to take advantage of a generative language model that\nis pre-trained on a large corpus. We examined whether a generative language\nmodel that has been successful in natural language processing can also be\neffective for text recognition in computer vision. Our experiments demonstrated\nthat DTrOCR outperforms current state-of-the-art methods by a large margin in\nthe recognition of printed, handwritten, and scene text in both English and\nChinese.\n",
                "链接": "https://arxiv.org/abs/2308.15996"
            },
            {
                "文章ID": "108340",
                "标题": "Structural analysis of Hindi online handwritten characters for character\n  recognition",
                "作者": "MIET, Meerut  Anand Sharma, IISc, Bengaluru  A. G. Ramakrishnan",
                "发布日期": "2023-10-13",
                "摘要": "  Direction properties of online strokes are used to analyze them in terms of\nhomogeneous regions or sub-strokes with points satisfying common geometric\nproperties. Such sub-strokes are called sub-units. These properties are used to\nextract sub-units from Hindi ideal online characters. These properties along\nwith some heuristics are used to extract sub-units from Hindi online\nhandwritten characters.\\\\ A method is developed to extract point stroke,\nclockwise curve stroke, counter-clockwise curve stroke and loop stroke segments\nas sub-units from Hindi online handwritten characters. These extracted\nsub-units are close in structure to the sub-units of the corresponding Hindi\nonline ideal characters.\\\\ Importance of local representation of online\nhandwritten characters in terms of sub-units is assessed by training a\nclassifier with sub-unit level local and character level global features\nextracted from characters for character recognition. The classifier has the\nrecognition accuracy of 93.5\\% on the testing set. This accuracy is the highest\nwhen compared with that of the classifiers trained only with global features\nextracted from characters in the same training set and evaluated on the same\ntesting set.\\\\ Sub-unit extraction algorithm and the sub-unit based character\nclassifier are tested on Hindi online handwritten character dataset. This\ndataset consists of samples from 96 different characters. There are 12832 and\n2821 samples in the training and testing sets, respectively.\n",
                "链接": "https://arxiv.org/abs/2310.08222"
            },
            {
                "文章ID": "123651",
                "标题": "Advancements and Challenges in Arabic Optical Character Recognition: A\n  Comprehensive Survey",
                "作者": " Mahmoud SalahEldin Kasem,  Mohamed Mahmoud,  Hyun-Soo Kang",
                "发布日期": "2023-12-20",
                "摘要": "  Optical character recognition (OCR) is a vital process that involves the\nextraction of handwritten or printed text from scanned or printed images,\nconverting it into a format that can be understood and processed by machines.\nThis enables further data processing activities such as searching and editing.\nThe automatic extraction of text through OCR plays a crucial role in digitizing\ndocuments, enhancing productivity, improving accessibility, and preserving\nhistorical records. This paper seeks to offer an exhaustive review of\ncontemporary applications, methodologies, and challenges associated with Arabic\nOptical Character Recognition (OCR). A thorough analysis is conducted on\nprevailing techniques utilized throughout the OCR process, with a dedicated\neffort to discern the most efficacious approaches that demonstrate enhanced\noutcomes. To ensure a thorough evaluation, a meticulous keyword-search\nmethodology is adopted, encompassing a comprehensive analysis of articles\nrelevant to Arabic OCR, including both backward and forward citation reviews.\nIn addition to presenting cutting-edge techniques and methods, this paper\ncritically identifies research gaps within the realm of Arabic OCR. By\nhighlighting these gaps, we shed light on potential areas for future\nexploration and development, thereby guiding researchers toward promising\navenues in the field of Arabic OCR. The outcomes of this study provide valuable\ninsights for researchers, practitioners, and stakeholders involved in Arabic\nOCR, ultimately fostering advancements in the field and facilitating the\ncreation of more accurate and efficient OCR systems for the Arabic language.\n",
                "链接": "https://arxiv.org/abs/2312.11812"
            },
            {
                "文章ID": "35422",
                "标题": "A Black-Box Attack on Optical Character Recognition Systems",
                "作者": " Samet Bayram,  Kenneth Barner",
                "发布日期": "2022-08-31",
                "摘要": "  Adversarial machine learning is an emerging area showing the vulnerability of\ndeep learning models. Exploring attack methods to challenge state of the art\nartificial intelligence (A.I.) models is an area of critical concern. The\nreliability and robustness of such A.I. models are one of the major concerns\nwith an increasing number of effective adversarial attack methods.\nClassification tasks are a major vulnerable area for adversarial attacks. The\nmajority of attack strategies are developed for colored or gray-scaled images.\nConsequently, adversarial attacks on binary image recognition systems have not\nbeen sufficiently studied. Binary images are simple two possible pixel-valued\nsignals with a single channel. The simplicity of binary images has a\nsignificant advantage compared to colored and gray scaled images, namely\ncomputation efficiency. Moreover, most optical character recognition systems\n(O.C.R.s), such as handwritten character recognition, plate number\nidentification, and bank check recognition systems, use binary images or\nbinarization in their processing steps. In this paper, we propose a simple yet\nefficient attack method, Efficient Combinatorial Black-box Adversarial Attack,\non binary image classifiers. We validate the efficiency of the attack technique\non two different data sets and three classification networks, demonstrating its\nperformance. Furthermore, we compare our proposed method with state-of-the-art\nmethods regarding advantages and disadvantages as well as applicability.\n",
                "链接": "https://arxiv.org/abs/2208.14302"
            }
        ]
    },
    {
        "question": {
            "question": "近几个月agent系列决策文章。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "75128",
                "标题": "On the Complexity of Multi-Agent Decision Making: From Learning in Games\n  to Partial Monitoring",
                "作者": " Dylan J. Foster,  Dean P. Foster,  Noah Golowich,  Alexander Rakhlin",
                "发布日期": "2023-05-02",
                "摘要": "  A central problem in the theory of multi-agent reinforcement learning (MARL)\nis to understand what structural conditions and algorithmic principles lead to\nsample-efficient learning guarantees, and how these considerations change as we\nmove from few to many agents. We study this question in a general framework for\ninteractive decision making with multiple agents, encompassing Markov games\nwith function approximation and normal-form games with bandit feedback. We\nfocus on equilibrium computation, in which a centralized learning algorithm\naims to compute an equilibrium by controlling multiple agents that interact\nwith an unknown environment. Our main contributions are:\n  - We provide upper and lower bounds on the optimal sample complexity for\nmulti-agent decision making based on a multi-agent generalization of the\nDecision-Estimation Coefficient, a complexity measure introduced by Foster et\nal. (2021) in the single-agent counterpart to our setting. Compared to the best\nresults for the single-agent setting, our bounds have additional gaps. We show\nthat no \"reasonable\" complexity measure can close these gaps, highlighting a\nstriking separation between single and multiple agents.\n  - We show that characterizing the statistical complexity for multi-agent\ndecision making is equivalent to characterizing the statistical complexity of\nsingle-agent decision making, but with hidden (unobserved) rewards, a framework\nthat subsumes variants of the partial monitoring problem. As a consequence, we\ncharacterize the statistical complexity for hidden-reward interactive decision\nmaking to the best extent possible.\n  Building on this development, we provide several new structural results,\nincluding 1) conditions under which the statistical complexity of multi-agent\ndecision making can be reduced to that of single-agent, and 2) conditions under\nwhich the so-called curse of multiple agents can be avoided.\n",
                "链接": "https://arxiv.org/abs/2305.00684"
            },
            {
                "文章ID": "121479",
                "标题": "Using deep neural networks to improve the precision of fast-sampled\n  particle timing detectors",
                "作者": " Mateusz Kocot,  Krzysztof Misan,  Valentina Avati,  Edoardo Bossini,  Leszek Grzanka,  Nicola Minafra",
                "发布日期": "2023-12-12",
                "摘要": "  Measurements from particle timing detectors are often affected by the time\nwalk effect caused by statistical fluctuations in the charge deposited by\npassing particles. The constant fraction discriminator (CFD) algorithm is\nfrequently used to mitigate this effect both in test setups and in running\nexperiments, such as the CMS-PPS system at the CERN's LHC. The CFD is simple\nand effective but does not leverage all voltage samples in a time series. Its\nperformance could be enhanced with deep neural networks, which are commonly\nused for time series analysis, including computing the particle arrival time.\nWe evaluated various neural network architectures using data acquired at the\ntest beam facility in the DESY-II synchrotron, where a precise MCP\n(MicroChannel Plate) detector was installed in addition to PPS diamond timing\ndetectors. MCP measurements were used as a reference to train the networks and\ncompare the results with the standard CFD method. Ultimately, we improved the\ntiming precision by 8% to 23%, depending on the detector's readout channel. The\nbest results were obtained using a UNet-based model, which outperformed\nclassical convolutional networks and the multilayer perceptron.\n",
                "链接": "https://arxiv.org/abs/2312.05883"
            },
            {
                "文章ID": "1984",
                "标题": "Belief Revision in Sentential Decision Diagrams",
                "作者": " Lilith Mattei,  Alessandro Facchini,  Alessandro Antonucci",
                "发布日期": "2022-01-21",
                "摘要": "  Belief revision is the task of modifying a knowledge base when new\ninformation becomes available, while also respecting a number of desirable\nproperties. Classical belief revision schemes have been already specialised to\n\\emph{binary decision diagrams} (BDDs), the classical formalism to compactly\nrepresent propositional knowledge. These results also apply to \\emph{ordered}\nBDDs (OBDDs), a special class of BDDs, designed to guarantee canonicity. Yet,\nthose revisions cannot be applied to \\emph{sentential decision diagrams}\n(SDDs), a typically more compact but still canonical class of Boolean circuits,\nwhich generalizes OBDDs, while not being a subclass of BDDs. Here we fill this\ngap by deriving a general revision algorithm for SDDs based on a syntactic\ncharacterisation of Dalal revision. A specialised procedure for DNFs is also\npresented. Preliminary experiments performed with randomly generated knowledge\nbases show the advantages of directly perform revision within SDD formalism.\n",
                "链接": "https://arxiv.org/abs/2201.08112"
            },
            {
                "文章ID": "729",
                "标题": "Zero-Shot and Few-Shot Classification of Biomedical Articles in Context\n  of the COVID-19 Pandemic",
                "作者": " Simon Lupart,  Benoit Favre,  Vassilina Nikoulina,  Salah Ait-Mokhtar",
                "发布日期": "2022-01-12",
                "摘要": "  MeSH (Medical Subject Headings) is a large thesaurus created by the National\nLibrary of Medicine and used for fine-grained indexing of publications in the\nbiomedical domain. In the context of the COVID-19 pandemic, MeSH descriptors\nhave emerged in relation to articles published on the corresponding topic.\nZero-shot classification is an adequate response for timely labeling of the\nstream of papers with MeSH categories. In this work, we hypothesise that rich\nsemantic information available in MeSH has potential to improve BioBERT\nrepresentations and make them more suitable for zero-shot/few-shot tasks. We\nframe the problem as determining if MeSH term definitions, concatenated with\npaper abstracts are valid instances or not, and leverage multi-task learning to\ninduce the MeSH hierarchy in the representations thanks to a seq2seq task.\nResults establish a baseline on the MedLine and LitCovid datasets, and probing\nshows that the resulting representations convey the hierarchical relations\npresent in MeSH.\n",
                "链接": "https://arxiv.org/abs/2201.03017"
            },
            {
                "文章ID": "91476",
                "标题": "Where Did the President Visit Last Week? Detecting Celebrity Trips from\n  News Articles",
                "作者": " Kai Peng,  Ying Zhang,  Shuai Ling,  Zhaoru Ke,  Haipeng Zhang",
                "发布日期": "2023-10-10",
                "摘要": "  Celebrities' whereabouts are of pervasive importance. For instance, where\npoliticians go, how often they visit, and who they meet, come with profound\ngeopolitical and economic implications. Although news articles contain travel\ninformation of celebrities, it is not possible to perform large-scale and\nnetwork-wise analysis due to the lack of automatic itinerary detection tools.\nTo design such tools, we have to overcome difficulties from the heterogeneity\namong news articles: 1)One single article can be noisy, with irrelevant people\nand locations, especially when the articles are long. 2)Though it may be\nhelpful if we consider multiple articles together to determine a particular\ntrip, the key semantics are still scattered across different articles\nintertwined with various noises, making it hard to aggregate them effectively.\n3)Over 20% of the articles refer to the celebrities' trips indirectly, instead\nof using the exact celebrity names or location names, leading to large portions\nof trips escaping regular detecting algorithms. We model text content across\narticles related to each candidate location as a graph to better associate\nessential information and cancel out the noises. Besides, we design a special\npooling layer based on attention mechanism and node similarity, reducing\nirrelevant information from longer articles. To make up the missing information\nresulted from indirect mentions, we construct knowledge sub-graphs for named\nentities (person, organization, facility, etc.). Specifically, we dynamically\nupdate embeddings of event entities like the G7 summit from news descriptions\nsince the properties (date and location) of the event change each time, which\nis not captured by the pre-trained event representations. The proposed CeleTrip\njointly trains these modules, which outperforms all baseline models and\nachieves 82.53% in the F1 metric.\n",
                "链接": "https://arxiv.org/abs/2307.08721"
            },
            {
                "文章ID": "888",
                "标题": "Pavlovian Signalling with General Value Functions in Agent-Agent\n  Temporal Decision Making",
                "作者": " Andrew Butcher,  Michael Bradley Johanson,  Elnaz Davoodi,  Dylan J. A. Brenneis,  Leslie Acker,  Adam S. R. Parker,  Adam White,  Joseph Modayil,  Patrick M. Pilarski",
                "发布日期": "2022-01-12",
                "摘要": "  In this paper, we contribute a multi-faceted study into Pavlovian signalling\n-- a process by which learned, temporally extended predictions made by one\nagent inform decision-making by another agent. Signalling is intimately\nconnected to time and timing. In service of generating and receiving signals,\nhumans and other animals are known to represent time, determine time since past\nevents, predict the time until a future stimulus, and both recognize and\ngenerate patterns that unfold in time. We investigate how different temporal\nprocesses impact coordination and signalling between learning agents by\nintroducing a partially observable decision-making domain we call the Frost\nHollow. In this domain, a prediction learning agent and a reinforcement\nlearning agent are coupled into a two-part decision-making system that works to\nacquire sparse reward while avoiding time-conditional hazards. We evaluate two\ndomain variations: machine agents interacting in a seven-state linear walk, and\nhuman-machine interaction in a virtual-reality environment. Our results\nshowcase the speed of learning for Pavlovian signalling, the impact that\ndifferent temporal representations do (and do not) have on agent-agent\ncoordination, and how temporal aliasing impacts agent-agent and human-agent\ninteractions differently. As a main contribution, we establish Pavlovian\nsignalling as a natural bridge between fixed signalling paradigms and fully\nadaptive communication learning between two agents. We further show how to\ncomputationally build this adaptive signalling process out of a fixed\nsignalling process, characterized by fast continual prediction learning and\nminimal constraints on the nature of the agent receiving signals. Our results\ntherefore suggest an actionable, constructivist path towards communication\nlearning between reinforcement learning agents.\n",
                "链接": "https://arxiv.org/abs/2201.03709"
            },
            {
                "文章ID": "69612",
                "标题": "A Hierarchical Game-Theoretic Decision-Making for Cooperative\n  Multi-Agent Systems Under the Presence of Adversarial Agents",
                "作者": " Qin Yang,  Ramviyas Parasuraman",
                "发布日期": "2023-03-30",
                "摘要": "  Underlying relationships among Multi-Agent Systems (MAS) in hazardous\nscenarios can be represented as Game-theoretic models. This paper proposes a\nnew hierarchical network-based model called Game-theoretic Utility Tree (GUT),\nwhich decomposes high-level strategies into executable low-level actions for\ncooperative MAS decisions. It combines with a new payoff measure based on agent\nneeds for real-time strategy games. We present an Explore game domain, where we\nmeasure the performance of MAS achieving tasks from the perspective of\nbalancing the success probability and system costs. We evaluate the GUT\napproach against state-of-the-art methods that greedily rely on rewards of the\ncomposite actions. Conclusive results on extensive numerical simulations\nindicate that GUT can organize more complex relationships among MAS\ncooperation, helping the group achieve challenging tasks with lower costs and\nhigher winning rates. Furthermore, we demonstrated the applicability of the GUT\nusing the simulator-hardware testbed - Robotarium. The performances verified\nthe effectiveness of the GUT in the real robot application and validated that\nthe GUT could effectively organize MAS cooperation strategies, helping the\ngroup with fewer advantages achieve higher performance.\n",
                "链接": "https://arxiv.org/abs/2303.16641"
            },
            {
                "文章ID": "32744",
                "标题": "An Agent-Based Fleet Management Model for First- and Last-Mile Services",
                "作者": " Saumya Bhatnagar,  Tarun Rambha,  Gitakrishnan Ramadurai",
                "发布日期": "2022-12-06",
                "摘要": "  With the growth of cars and car-sharing applications, commuters in many\ncities, particularly developing countries, are shifting away from public\ntransport. These shifts have affected two key stakeholders: transit operators\nand first- and last-mile (FLM) services. Although most cities continue to\ninvest heavily in bus and metro projects to make public transit attractive,\nridership in these systems has often failed to reach targeted levels. FLM\nservice providers also experience lower demand and revenues in the wake of\nshifts to other means of transport. Effective FLM options are required to\nprevent this phenomenon and make public transport attractive for commuters. One\npossible solution is to forge partnerships between public transport and FLM\nproviders that offer competitive joint mobility options. Such solutions require\nprudent allocation of supply and optimised strategies for FLM operations and\nride-sharing. To this end, we build an agent- and event-based simulation model\nwhich captures interactions between passengers and FLM services using\nstatecharts, vehicle routing models, and other trip matching rules. An\noptimisation model for allocating FLM vehicles at different transit stations is\nproposed to reduce unserved requests. Using real-world metro transit demand\ndata from Bengaluru, India, the effectiveness of our approach in improving FLM\nconnectivity and quantifying the benefits of sharing trips is demonstrated.\n",
                "链接": "https://arxiv.org/abs/2208.04563"
            },
            {
                "文章ID": "113498",
                "标题": "Content Significance Distribution of Sub-Text Blocks in Articles and Its\n  Application to Article-Organization Assessment",
                "作者": " You Zhou,  Jie Wang",
                "发布日期": "2023-11-06",
                "摘要": "  We explore how to capture the significance of a sub-text block in an article\nand how it may be used for text mining tasks. A sub-text block is a\nsub-sequence of sentences in the article. We formulate the notion of content\nsignificance distribution (CSD) of sub-text blocks, referred to as CSD of the\nfirst kind and denoted by CSD-1. In particular, we leverage Hugging Face's\nSentenceTransformer to generate contextual sentence embeddings, and use\nMoverScore over text embeddings to measure how similar a sub-text block is to\nthe entire text. To overcome the exponential blowup on the number of sub-text\nblocks, we present an approximation algorithm and show that the approximated\nCSD-1 is almost identical to the exact CSD-1. Under this approximation, we show\nthat the average and median CSD-1's for news, scholarly research, argument, and\nnarrative articles share the same pattern. We also show that under a certain\nlinear transformation, the complement of the cumulative distribution function\nof the beta distribution with certain values of $\\alpha$ and $\\beta$ resembles\na CSD-1 curve. We then use CSD-1's to extract linguistic features to train an\nSVC classifier for assessing how well an article is organized. Through\nexperiments, we show that this method achieves high accuracy for assessing\nstudent essays. Moreover, we study CSD of sentence locations, referred to as\nCSD of the second kind and denoted by CSD-2, and show that average CSD-2's for\ndifferent types of articles possess distinctive patterns, which either conform\ncommon perceptions of article structures or provide rectification with minor\ndeviation.\n",
                "链接": "https://arxiv.org/abs/2311.01673"
            },
            {
                "文章ID": "8551",
                "标题": "Automated Few-Shot Time Series Forecasting based on Bi-level Programming",
                "作者": " Jiangjiao Xu,  Ke Li",
                "发布日期": "2022-03-08",
                "摘要": "  New micro-grid design with renewable energy sources and battery storage\nsystems can help improve greenhouse gas emissions and reduce the operational\ncost. To provide an effective short-/long-term forecasting of both energy\ngeneration and load demand, time series predictive modeling has been one of the\nkey tools to guide the optimal decision-making for planning and operation. One\nof the critical challenges of time series renewable energy forecasting is the\nlack of historical data to train an adequate predictive model. Moreover, the\nperformance of a machine learning model is sensitive to the choice of its\ncorresponding hyperparameters. Bearing these considerations in mind, this paper\ndevelops a BiLO-Auto-TSF/ML framework that automates the optimal design of a\nfew-shot learning pipeline from a bi-level programming perspective.\nSpecifically, the lower-level meta-learning helps boost the base-learner to\nmitigate the small data challenge while the hyperparameter optimization at the\nupper level proactively searches for the optimal hyperparameter configurations\nfor both base- and meta-learners. Note that the proposed framework is so\ngeneral that any off-the-shelf machine learning method can be used in a plug-in\nmanner. Comprehensive experiments fully demonstrate the effectiveness of our\nproposed BiLO-Auto-TSF/ML framework to search for a high-performance few-shot\nlearning pipeline for various energy sources.\n",
                "链接": "https://arxiv.org/abs/2203.03328"
            }
        ]
    },
    {
        "question": {
            "question": "查找多模态优化训练方式的相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "67955",
                "标题": "Clustering US Counties to Find Patterns Related to the COVID-19 Pandemic",
                "作者": " Cora Brown,  Sarah Milstein,  Tianyi Sun,  Cooper Zhao",
                "发布日期": "2023-03-22",
                "摘要": "  When COVID-19 first started spreading and quarantine was implemented, the\nSociety for Industrial and Applied Mathematics (SIAM) Student Chapter at the\nUniversity of Minnesota-Twin Cities began a collaboration with Ecolab to use\nour skills as data scientists and mathematicians to extract useful insights\nfrom relevant data relating to the pandemic. This collaboration consisted of\nmultiple groups working on different projects. In this write-up we focus on\nusing clustering techniques to help us find groups of similar counties in the\nUS and use that to help us understand the pandemic. Our team for this project\nconsisted of University of Minnesota students Cora Brown, Sarah Milstein,\nTianyi Sun, and Cooper Zhao, with help from Ecolab Data Scientist Jimmy\nBroomfield and University of Minnesota student Skye Ke. In the sections below\nwe describe all of the work done for this project. In Section 2, we list the\ndata we gathered, as well as the feature engineering we performed. In Section\n3, we describe the metrics we used for evaluating our models. In Section 4, we\nexplain the methods we used for interpreting the results of our various\nclustering approaches. In Section 5, we describe the different clustering\nmethods we implemented. In Section 6, we present the results of our clustering\ntechniques and provide relevant interpretation. Finally, in Section 7, we\nprovide some concluding remarks comparing the different clustering methods.\n",
                "链接": "https://arxiv.org/abs/2303.11936"
            },
            {
                "文章ID": "56117",
                "标题": "Exploring Machine Learning Techniques to Identify Important Factors\n  Leading to Injury in Curve Related Crashes",
                "作者": " Mehdi Moeinaddini,  Mozhgan Pourmoradnasseri,  Amnir Hadachi,  Mario Cools",
                "发布日期": "2023-01-06",
                "摘要": "  Different factors have effects on traffic crashes and crash-related injuries.\nThese factors include segment characteristics, crash-level characteristics,\noccupant level characteristics, environment characteristics, and vehicle level\ncharacteristics. There are several studies regarding these factors' effects on\ncrash injuries. However, limited studies have examined the effects of pre-crash\nevents on injuries, especially for curve-related crashes. The majority of\nprevious studies for curve-related crashes focused on the impact of geometric\nfeatures or street design factors. The current study tries to eliminate the\naforementioned shortcomings by considering important pre-crash events related\nfactors as selected variables and the number of vehicles with or without injury\nas the predicted variable. This research used CRSS data from the National\nHighway Traffic Safety Administration (NHTSA), which includes traffic\ncrash-related data for different states in the USA. The relationships are\nexplored using different machine learning algorithms like the random forest,\nC5.0, CHAID, Bayesian Network, Neural Network, C\\&R Tree, Quest, etc. The\nrandom forest and SHAP values are used to identify the most effective\nvariables. The C5.0 algorithm, which has the highest accuracy rate among the\nother algorithms, is used to develop the final model. Analysis results revealed\nthat the extent of the damage, critical pre-crash event, pre-impact location,\nthe trafficway description, roadway surface condition, the month of the crash,\nthe first harmful event, number of motor vehicles, attempted avoidance\nmaneuver, and roadway grade affect the number of vehicles with or without\ninjury in curve-related crashes.\n",
                "链接": "https://arxiv.org/abs/2301.01771"
            },
            {
                "文章ID": "87122",
                "标题": "Fantastic Weights and How to Find Them: Where to Prune in Dynamic Sparse\n  Training",
                "作者": " Aleksandra I. Nowak,  Bram Grooten,  Decebal Constantin Mocanu,  Jacek Tabor",
                "发布日期": "2023-12-01",
                "摘要": "  Dynamic Sparse Training (DST) is a rapidly evolving area of research that\nseeks to optimize the sparse initialization of a neural network by adapting its\ntopology during training. It has been shown that under specific conditions, DST\nis able to outperform dense models. The key components of this framework are\nthe pruning and growing criteria, which are repeatedly applied during the\ntraining process to adjust the network's sparse connectivity. While the growing\ncriterion's impact on DST performance is relatively well studied, the influence\nof the pruning criterion remains overlooked. To address this issue, we design\nand perform an extensive empirical analysis of various pruning criteria to\nbetter understand their impact on the dynamics of DST solutions. Surprisingly,\nwe find that most of the studied methods yield similar results. The differences\nbecome more significant in the low-density regime, where the best performance\nis predominantly given by the simplest technique: magnitude-based pruning. The\ncode is provided at https://github.com/alooow/fantastic_weights_paper\n",
                "链接": "https://arxiv.org/abs/2306.12230"
            },
            {
                "文章ID": "51062",
                "标题": "Instance-Specific Image Goal Navigation: Training Embodied Agents to\n  Find Object Instances",
                "作者": " Jacob Krantz,  Stefan Lee,  Jitendra Malik,  Dhruv Batra,  Devendra Singh Chaplot",
                "发布日期": "2022-11-30",
                "摘要": "  We consider the problem of embodied visual navigation given an image-goal\n(ImageNav) where an agent is initialized in an unfamiliar environment and\ntasked with navigating to a location 'described' by an image. Unlike related\nnavigation tasks, ImageNav does not have a standardized task definition which\nmakes comparison across methods difficult. Further, existing formulations have\ntwo problematic properties; (1) image-goals are sampled from random locations\nwhich can lead to ambiguity (e.g., looking at walls), and (2) image-goals match\nthe camera specification and embodiment of the agent; this rigidity is limiting\nwhen considering user-driven downstream applications. We present the\nInstance-specific ImageNav task (InstanceImageNav) to address these\nlimitations. Specifically, the goal image is 'focused' on some particular\nobject instance in the scene and is taken with camera parameters independent of\nthe agent. We instantiate InstanceImageNav in the Habitat Simulator using\nscenes from the Habitat-Matterport3D dataset (HM3D) and release a standardized\nbenchmark to measure community progress.\n",
                "链接": "https://arxiv.org/abs/2211.15876"
            },
            {
                "文章ID": "48380",
                "标题": "Detection of fraudulent financial papers by picking a collection of\n  characteristics using optimization algorithms and classification techniques\n  based on squirrels",
                "作者": " Peyman Mohammadzadeh germi,  Mohsen Najarbashi",
                "发布日期": "2022-11-28",
                "摘要": "  To produce important investment decisions, investors require financial\nrecords and economic information. However, most companies manipulate investors\nand financial institutions by inflating their financial statements. Fraudulent\nFinancial Activities exist in any monetary or financial transaction scenario,\nwhether physical or electronic. A challenging problem that arises in this\ndomain is the issue that affects and troubles individuals and institutions.\nThis problem has attracted more attention in the field in part owing to the\nprevalence of financial fraud and the paucity of previous research. For this\npurpose, in this study, the main approach to solve this problem, an anomaly\ndetection-based approach based on a combination of feature selection based on\nsquirrel optimization pattern and classification methods have been used. The\naim is to develop this method to provide a model for detecting anomalies in\nfinancial statements using a combination of selected features with the nearest\nneighbor classifications, neural networks, support vector machine, and\nBayesian. Anomaly samples are then analyzed and compared to recommended\ntechniques using assessment criteria. Squirrel optimization's meta-exploratory\ncapability, along with the approach's ability to identify abnormalities in\nfinancial data, has been shown to be effective in implementing the suggested\nstrategy. They discovered fake financial statements because of their expertise.\n",
                "链接": "https://arxiv.org/abs/2211.07747"
            },
            {
                "文章ID": "114514",
                "标题": "Training CLIP models on Data from Scientific Papers",
                "作者": " Calvin Metzger",
                "发布日期": "2023-11-09",
                "摘要": "  Contrastive Language-Image Pretraining (CLIP) models are able to capture the\nsemantic relationship of images and texts and have enabled a wide range of\napplications, from image retrieval to classification. These models are trained\nwith datasets extracted from web crawls, which are of large quantity but\nlimited quality. This paper explores whether limited amounts higher quality\ndata in a specific domain improve the general performance of CLIP models. To\nthis purpose, we extract text-image data from scientific papers hosted in the\narXiv and PubMed Central repositories. Experiments on small-scale CLIP models\n(ViT B/32) show that model performance increases on average, but only\nmoderately. This result indicates that using the data sources considered in the\npaper to train large-scale CLIP models is a worthwile research direction.\n",
                "链接": "https://arxiv.org/abs/2311.04711"
            },
            {
                "文章ID": "75053",
                "标题": "Predictability of Machine Learning Algorithms and Related Feature\n  Extraction Techniques",
                "作者": " Yunbo Dong",
                "发布日期": "2023-05-02",
                "摘要": "  This thesis designs a prediction system based on matrix factorization to\npredict the classification accuracy of a specific model on a particular\ndataset. In this thesis, we conduct comprehensive empirical research on more\nthan fifty datasets that we collected from the openml website. We study the\nperformance prediction of three fundamental machine learning algorithms,\nnamely, random forest, XGBoost, and MultiLayer Perceptron(MLP). In particular,\nwe obtain the following results: 1. Predictability of fine-tuned models using\ncoarse-tuned variants. 2. Predictability of MLP using feature extraction\ntechniques. 3. Predict model performance using implicit feedback.\n",
                "链接": "https://arxiv.org/abs/2305.00449"
            },
            {
                "文章ID": "66434",
                "标题": "HCI Papers Cite HCI Papers, Increasingly So",
                "作者": " Xiang 'Anthony' Chen",
                "发布日期": "2023-03-15",
                "摘要": "  We propose X-index -- the proportion of papers' citations coming from outside\ntheir research field -- and use this metric to analyze citations of CHI, UIST,\nand CSCW papers between 2010 and 2022. We found an overall decreasing X-index\nby several measures, indicating that HCI papers have been more and more likely\nto be cited by HCI papers rather than by non-HCI papers.\n",
                "链接": "https://arxiv.org/abs/2303.07539"
            },
            {
                "文章ID": "97726",
                "标题": "Spurious Correlations and Where to Find Them",
                "作者": " Gautam Sreekumar,  Vishnu Naresh Boddeti",
                "发布日期": "2023-08-23",
                "摘要": "  Spurious correlations occur when a model learns unreliable features from the\ndata and are a well-known drawback of data-driven learning. Although there are\nseveral algorithms proposed to mitigate it, we are yet to jointly derive the\nindicators of spurious correlations. As a result, the solutions built upon\nstandalone hypotheses fail to beat simple ERM baselines. We collect some of the\ncommonly studied hypotheses behind the occurrence of spurious correlations and\ninvestigate their influence on standard ERM baselines using synthetic datasets\ngenerated from causal graphs. Subsequently, we observe patterns connecting\nthese hypotheses and model design choices.\n",
                "链接": "https://arxiv.org/abs/2308.11043"
            },
            {
                "文章ID": "110484",
                "标题": "Improved Techniques for Training Consistency Models",
                "作者": " Yang Song,  Prafulla Dhariwal",
                "发布日期": "2023-10-24",
                "摘要": "  Consistency models are a nascent family of generative models that can sample\nhigh quality data in one step without the need for adversarial training.\nCurrent consistency models achieve optimal sample quality by distilling from\npre-trained diffusion models and employing learned metrics such as LPIPS.\nHowever, distillation limits the quality of consistency models to that of the\npre-trained diffusion model, and LPIPS causes undesirable bias in evaluation.\nTo tackle these challenges, we present improved techniques for consistency\ntraining, where consistency models learn directly from data without\ndistillation. We delve into the theory behind consistency training and identify\na previously overlooked flaw, which we address by eliminating Exponential\nMoving Average from the teacher consistency model. To replace learned metrics\nlike LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally,\nwe introduce a lognormal noise schedule for the consistency training objective,\nand propose to double total discretization steps every set number of training\niterations. Combined with better hyperparameter tuning, these modifications\nenable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10\nand ImageNet $64\\times 64$ respectively in a single sampling step. These scores\nmark a 3.5$\\times$ and 4$\\times$ improvement compared to prior consistency\ntraining approaches. Through two-step sampling, we further reduce FID scores to\n2.24 and 2.77 on these two datasets, surpassing those obtained via distillation\nin both one-step and two-step settings, while narrowing the gap between\nconsistency models and other state-of-the-art generative models.\n",
                "链接": "https://arxiv.org/abs/2310.14189"
            }
        ]
    },
    {
        "question": {
            "question": "与大模型安全相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "102947",
                "标题": "Love or Hate? Share or Split? Privacy-Preserving Training Using Split\n  Learning and Homomorphic Encryption",
                "作者": " Tanveer Khan,  Khoa Nguyen,  Antonis Michalas,  Alexandros Bakas",
                "发布日期": "2023-09-20",
                "摘要": "  Split learning (SL) is a new collaborative learning technique that allows\nparticipants, e.g. a client and a server, to train machine learning models\nwithout the client sharing raw data. In this setting, the client initially\napplies its part of the machine learning model on the raw data to generate\nactivation maps and then sends them to the server to continue the training\nprocess. Previous works in the field demonstrated that reconstructing\nactivation maps could result in privacy leakage of client data. In addition to\nthat, existing mitigation techniques that overcome the privacy leakage of SL\nprove to be significantly worse in terms of accuracy. In this paper, we improve\nupon previous works by constructing a protocol based on U-shaped SL that can\noperate on homomorphically encrypted data. More precisely, in our approach, the\nclient applies homomorphic encryption on the activation maps before sending\nthem to the server, thus protecting user privacy. This is an important\nimprovement that reduces privacy leakage in comparison to other SL-based works.\nFinally, our results show that, with the optimum set of parameters, training\nwith HE data in the U-shaped SL setting only reduces accuracy by 2.65% compared\nto training on plaintext. In addition, raw training data privacy is preserved.\n",
                "链接": "https://arxiv.org/abs/2309.10517"
            },
            {
                "文章ID": "112490",
                "标题": "Security Challenges for Cloud or Fog Computing-Based AI Applications",
                "作者": " Amir Pakmehr,  Andreas Aßmuth,  Christoph P. Neumann,  Gerald Pirkl",
                "发布日期": "2023-12-22",
                "摘要": "  Security challenges for Cloud or Fog-based machine learning services pose\nseveral concerns. Securing the underlying Cloud or Fog services is essential,\nas successful attacks against these services, on which machine learning\napplications rely, can lead to significant impairments of these applications.\nBecause the requirements for AI applications can also be different, we\ndifferentiate according to whether they are used in the Cloud or in a Fog\nComputing network. This then also results in different threats or attack\npossibilities. For Cloud platforms, the responsibility for security can be\ndivided between different parties. Security deficiencies at a lower level can\nhave a direct impact on the higher level where user data is stored. While\nresponsibilities are simpler for Fog Computing networks, by moving services to\nthe edge of the network, we have to secure them against physical access to the\ndevices. We conclude by outlining specific information security requirements\nfor AI applications.\n",
                "链接": "https://arxiv.org/abs/2310.19459"
            },
            {
                "文章ID": "92148",
                "标题": "LLM Censorship: A Machine Learning Challenge or a Computer Security\n  Problem?",
                "作者": " David Glukhov,  Ilia Shumailov,  Yarin Gal,  Nicolas Papernot,  Vardan Papyan",
                "发布日期": "2023-07-25",
                "摘要": "  Large language models (LLMs) have exhibited impressive capabilities in\ncomprehending complex instructions. However, their blind adherence to provided\ninstructions has led to concerns regarding risks of malicious use. Existing\ndefence mechanisms, such as model fine-tuning or output censorship using LLMs,\nhave proven to be fallible, as LLMs can still generate problematic responses.\nCommonly employed censorship approaches treat the issue as a machine learning\nproblem and rely on another LM to detect undesirable content in LLM outputs. In\nthis paper, we present the theoretical limitations of such semantic censorship\napproaches. Specifically, we demonstrate that semantic censorship can be\nperceived as an undecidable problem, highlighting the inherent challenges in\ncensorship that arise due to LLMs' programmatic and instruction-following\ncapabilities. Furthermore, we argue that the challenges extend beyond semantic\ncensorship, as knowledgeable attackers can reconstruct impermissible outputs\nfrom a collection of permissible ones. As a result, we propose that the problem\nof censorship needs to be reevaluated; it should be treated as a security\nproblem which warrants the adaptation of security-based approaches to mitigate\npotential risks.\n",
                "链接": "https://arxiv.org/abs/2307.10719"
            },
            {
                "文章ID": "2957",
                "标题": "Speckle-based optical cryptosystem and its application for human face\n  recognition via deep learning",
                "作者": " Qi Zhao,  Huanhao Li,  Zhipeng Yu,  Chi Man Woo,  Tianting Zhong,  Shengfu Cheng,  Yuanjin Zheng,  Honglin Liu,  Jie Tian,  Puxiang Lai",
                "发布日期": "2022-01-31",
                "摘要": "  Face recognition has recently become ubiquitous in many scenes for\nauthentication or security purposes. Meanwhile, there are increasing concerns\nabout the privacy of face images, which are sensitive biometric data that\nshould be carefully protected. Software-based cryptosystems are widely adopted\nnowadays to encrypt face images, but the security level is limited by\ninsufficient digital secret key length or computing power. Hardware-based\noptical cryptosystems can generate enormously longer secret keys and enable\nencryption at light speed, but most reported optical methods, such as double\nrandom phase encryption, are less compatible with other systems due to system\ncomplexity. In this study, a plain yet high-efficient speckle-based optical\ncryptosystem is proposed and implemented. A scattering ground glass is\nexploited to generate physical secret keys of gigabit length and encrypt face\nimages via seemingly random optical speckles at light speed. Face images can\nthen be decrypted from the random speckles by a well-trained decryption neural\nnetwork, such that face recognition can be realized with up to 98% accuracy.\nThe proposed cryptosystem has wide applicability, and it may open a new avenue\nfor high-security complex information encryption and decryption by utilizing\noptical speckles.\n",
                "链接": "https://arxiv.org/abs/2201.11844"
            },
            {
                "文章ID": "95553",
                "标题": "Enhancing Mobile Privacy and Security: A Face Skin Patch-Based\n  Anti-Spoofing Approach",
                "作者": " Qiushi Guo",
                "发布日期": "2023-08-10",
                "摘要": "  As Facial Recognition System(FRS) is widely applied in areas such as access\ncontrol and mobile payments due to its convenience and high accuracy. The\nsecurity of facial recognition is also highly regarded. The Face anti-spoofing\nsystem(FAS) for face recognition is an important component used to enhance the\nsecurity of face recognition systems. Traditional FAS used images containing\nidentity information to detect spoofing traces, however there is a risk of\nprivacy leakage during the transmission and storage of these images. Besides,\nthe encryption and decryption of these privacy-sensitive data takes too long\ncompared to inference time by FAS model. To address the above issues, we\npropose a face anti-spoofing algorithm based on facial skin patches leveraging\npure facial skin patch images as input, which contain no privacy information,\nno encryption or decryption is needed for these images. We conduct experiments\non several public datasets, the results prove that our algorithm has\ndemonstrated superiority in both accuracy and speed.\n",
                "链接": "https://arxiv.org/abs/2308.04798"
            },
            {
                "文章ID": "11176",
                "标题": "4D-OR: Semantic Scene Graphs for OR Domain Modeling",
                "作者": " Ege Özsoy,  Evin Pınar Örnek,  Ulrich Eck,  Tobias Czempiel,  Federico Tombari,  Nassir Navab",
                "发布日期": "2022-03-23",
                "摘要": "  Surgical procedures are conducted in highly complex operating rooms (OR),\ncomprising different actors, devices, and interactions. To date, only medically\ntrained human experts are capable of understanding all the links and\ninteractions in such a demanding environment. This paper aims to bring the\ncommunity one step closer to automated, holistic and semantic understanding and\nmodeling of OR domain. Towards this goal, for the first time, we propose using\nsemantic scene graphs (SSG) to describe and summarize the surgical scene. The\nnodes of the scene graphs represent different actors and objects in the room,\nsuch as medical staff, patients, and medical equipment, whereas edges are the\nrelationships between them. To validate the possibilities of the proposed\nrepresentation, we create the first publicly available 4D surgical SSG dataset,\n4D-OR, containing ten simulated total knee replacement surgeries recorded with\nsix RGB-D sensors in a realistic OR simulation center. 4D-OR includes 6734\nframes and is richly annotated with SSGs, human and object poses, and clinical\nroles. We propose an end-to-end neural network-based SSG generation pipeline,\nwith a rate of success of 0.75 macro F1, indeed being able to infer semantic\nreasoning in the OR. We further demonstrate the representation power of our\nscene graphs by using it for the problem of clinical role prediction, where we\nachieve 0.85 macro F1. The code and dataset will be made available upon\nacceptance.\n",
                "链接": "https://arxiv.org/abs/2203.11937"
            },
            {
                "文章ID": "85982",
                "标题": "An Efficient and Multi-private Key Secure Aggregation for Federated\n  Learning",
                "作者": " Xue Yang,  Zifeng Liu,  Xiaohu Tang,  Rongxing Lu,  Bo Liu",
                "发布日期": "2023-06-16",
                "摘要": "  With the emergence of privacy leaks in federated learning, secure aggregation\nprotocols that mainly adopt either homomorphic encryption or threshold secret\nsharing have been widely developed for federated learning to protect the\nprivacy of the local training data of each client. However, these existing\nprotocols suffer from many shortcomings, such as the dependence on a trusted\nthird party, the vulnerability to clients being corrupted, low efficiency, the\ntrade-off between security and fault tolerance, etc. To solve these\ndisadvantages, we propose an efficient and multi-private key secure aggregation\nscheme for federated learning. Specifically, we skillfully modify the variant\nElGamal encryption technique to achieve homomorphic addition operation, which\nhas two important advantages: 1) The server and each client can freely select\npublic and private keys without introducing a trust third party and 2) Compared\nto the variant ElGamal encryption, the plaintext space is relatively large,\nwhich is more suitable for the deep model. Besides, for the high dimensional\ndeep model parameter, we introduce a super-increasing sequence to compress\nmulti-dimensional data into 1-D, which can greatly reduce encryption and\ndecryption times as well as communication for ciphertext transmission. Detailed\nsecurity analyses show that our proposed scheme achieves the semantic security\nof both individual local gradients and the aggregated result while achieving\noptimal robustness in tolerating both client collusion and dropped clients.\nExtensive simulations demonstrate that the accuracy of our scheme is almost the\nsame as the non-private approach, while the efficiency of our scheme is much\nbetter than the state-of-the-art homomorphic encryption-based secure\naggregation schemes. More importantly, the efficiency advantages of our scheme\nwill become increasingly prominent as the number of model parameters increases.\n",
                "链接": "https://arxiv.org/abs/2306.08970"
            },
            {
                "文章ID": "115941",
                "标题": "Grounding or Guesswork? Large Language Models are Presumptive Grounders",
                "作者": " Omar Shaikh,  Kristina Gligorić,  Ashna Khetan,  Matthias Gerstgrasser,  Diyi Yang,  Dan Jurafsky",
                "发布日期": "2023-11-16",
                "摘要": "  Effective conversation requires common ground: a shared understanding between\nthe participants. Common ground, however, does not emerge spontaneously in\nconversation. Speakers and listeners work together to both identify and\nconstruct a shared basis while avoiding misunderstanding. To accomplish\ngrounding, humans rely on a range of dialogue acts, like clarification (What do\nyou mean?) and acknowledgment (I understand.). In domains like teaching and\nemotional support, carefully constructing grounding prevents misunderstanding.\nHowever, it is unclear whether large language models (LLMs) leverage these\ndialogue acts in constructing common ground. To this end, we curate a set of\ngrounding acts and propose corresponding metrics that quantify attempted\ngrounding. We study whether LLMs use these grounding acts, simulating them\ntaking turns from several dialogue datasets, and comparing the results to\nhumans. We find that current LLMs are presumptive grounders, biased towards\nassuming common ground without using grounding acts. To understand the roots of\nthis behavior, we examine the role of instruction tuning and reinforcement\nlearning with human feedback (RLHF), finding that RLHF leads to less grounding.\nAltogether, our work highlights the need for more research investigating\ngrounding in human-AI interaction.\n",
                "链接": "https://arxiv.org/abs/2311.09144"
            },
            {
                "文章ID": "117487",
                "标题": "A Comparative Analysis Between SciTokens, Verifiable Credentials, and\n  Smart Contracts: Novel Approaches for Authentication and Secure Access to\n  Scientific Data",
                "作者": " Md Jobair Hossain Faruk,  Bilash Saha,  Jim Basney",
                "发布日期": "2023-11-23",
                "摘要": "  Managing and exchanging sensitive information securely is a paramount concern\nfor the scientific and cybersecurity community. The increasing reliance on\ncomputing workflows and digital data transactions requires ensuring that\nsensitive information is protected from unauthorized access, tampering, or\nmisuse. This research paper presents a comparative analysis of three novel\napproaches for authenticating and securing access to scientific data:\nSciTokens, Verifiable Credentials, and Smart Contracts. The aim of this study\nis to investigate the strengths and weaknesses of each approach from trust,\nrevocation, privacy, and security perspectives. We examine the technical\nfeatures and privacy and security mechanisms of each technology and provide a\ncomparative synthesis with the proposed model. Through our analysis, we\ndemonstrate that each technology offers unique advantages and limitations, and\nthe integration of these technologies can lead to more secure and efficient\nsolutions for authentication and access to scientific data.\n",
                "链接": "https://arxiv.org/abs/2311.13422"
            },
            {
                "文章ID": "1501",
                "标题": "EFMVFL: An Efficient and Flexible Multi-party Vertical Federated\n  Learning without a Third Party",
                "作者": " Yimin Huang,  Xinyu Feng,  Wanwan Wang,  Hao He,  Yukun Wang,  Ming Yao",
                "发布日期": "2023-10-19",
                "摘要": "  Federated learning allows multiple participants to conduct joint modeling\nwithout disclosing their local data. Vertical federated learning (VFL) handles\nthe situation where participants share the same ID space and different feature\nspaces. In most VFL frameworks, to protect the security and privacy of the\nparticipants' local data, a third party is needed to generate homomorphic\nencryption key pairs and perform decryption operations. In this way, the third\nparty is granted the right to decrypt information related to model parameters.\nHowever, it isn't easy to find such a credible entity in the real world.\nExisting methods for solving this problem are either communication-intensive or\nunsuitable for multi-party scenarios. By combining secret sharing and\nhomomorphic encryption, we propose a novel VFL framework without a third party\ncalled EFMVFL, which supports flexible expansion to multiple participants with\nlow communication overhead and is applicable to generalized linear models. We\ngive instantiations of our framework under logistic regression and Poisson\nregression. Theoretical analysis and experiments show that our framework is\nsecure, more efficient, and easy to be extended to multiple participants.\n",
                "链接": "https://arxiv.org/abs/2201.06244"
            }
        ]
    },
    {
        "question": {
            "question": "查找大模型推理的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "105661",
                "标题": "Reasoning on Graphs: Faithful and Interpretable Large Language Model\n  Reasoning",
                "作者": " Linhao Luo,  Yuan-Fang Li,  Gholamreza Haffari,  Shirui Pan",
                "发布日期": "2023-10-03",
                "摘要": "  Large language models (LLMs) have demonstrated impressive reasoning abilities\nin complex tasks. However, they lack up-to-date knowledge and experience\nhallucinations during reasoning, which can lead to incorrect reasoning\nprocesses and diminish their performance and trustworthiness. Knowledge graphs\n(KGs), which capture vast amounts of facts in a structured format, offer a\nreliable source of knowledge for reasoning. Nevertheless, existing KG-based LLM\nreasoning methods only treat KGs as factual knowledge bases and overlook the\nimportance of their structural information for reasoning. In this paper, we\npropose a novel method called reasoning on graphs (RoG) that synergizes LLMs\nwith KGs to enable faithful and interpretable reasoning. Specifically, we\npresent a planning-retrieval-reasoning framework, where RoG first generates\nrelation paths grounded by KGs as faithful plans. These plans are then used to\nretrieve valid reasoning paths from the KGs for LLMs to conduct faithful\nreasoning. Furthermore, RoG not only distills knowledge from KGs to improve the\nreasoning ability of LLMs through training but also allows seamless integration\nwith any arbitrary LLMs during inference. Extensive experiments on two\nbenchmark KGQA datasets demonstrate that RoG achieves state-of-the-art\nperformance on KG reasoning tasks and generates faithful and interpretable\nreasoning results.\n",
                "链接": "https://arxiv.org/abs/2310.01061"
            },
            {
                "文章ID": "115724",
                "标题": "LLMs cannot find reasoning errors, but can correct them!",
                "作者": " Gladys Tyen,  Hassan Mansoor,  Peter Chen,  Tony Mak,  Victor Cărbune",
                "发布日期": "2023-11-16",
                "摘要": "  While self-correction has shown promise in improving LLM outputs in terms of\nstyle and quality (e.g. Chen et al., 2023; Madaan et al., 2023), recent\nattempts to self-correct logical or reasoning errors often cause correct\nanswers to become incorrect, resulting in worse performances overall (Huang et\nal., 2023). In this paper, we break down the self-correction process into two\ncore components: mistake finding and output correction. For mistake finding, we\nrelease BIG-Bench Mistake, a dataset of logical mistakes in Chain-of-Thought\nreasoning traces. We provide benchmark numbers for several state-of-the-art\nLLMs, and demonstrate that LLMs generally struggle with finding logical\nmistakes. For output correction, we propose a backtracking method which\nprovides large improvements when given information on mistake location. We\nconstrue backtracking as a lightweight alternative to reinforcement learning\nmethods, and show that it remains effective with a reward model at 60-70%\naccuracy.\n",
                "链接": "https://arxiv.org/abs/2311.08516"
            },
            {
                "文章ID": "94177",
                "标题": "LISA: Reasoning Segmentation via Large Language Model",
                "作者": " Xin Lai,  Zhuotao Tian,  Yukang Chen,  Yanwei Li,  Yuhui Yuan,  Shu Liu,  Jiaya Jia",
                "发布日期": "2023-08-04",
                "摘要": "  Although perception systems have made remarkable advancements in recent\nyears, they still rely on explicit human instruction to identify the target\nobjects or categories before executing visual recognition tasks. Such systems\nlack the ability to actively reason and comprehend implicit user intentions. In\nthis work, we propose a new segmentation task -- reasoning segmentation. The\ntask is designed to output a segmentation mask given a complex and implicit\nquery text. Furthermore, we establish a benchmark comprising over one thousand\nimage-instruction pairs, incorporating intricate reasoning and world knowledge\nfor evaluation purposes. Finally, we present LISA: large Language Instructed\nSegmentation Assistant, which inherits the language generation capabilities of\nthe multi-modal Large Language Model (LLM) while also possessing the ability to\nproduce segmentation masks. We expand the original vocabulary with a <SEG>\ntoken and propose the embedding-as-mask paradigm to unlock the segmentation\ncapability. Remarkably, LISA can handle cases involving: 1) complex reasoning;\n2) world knowledge; 3) explanatory answers; 4) multi-turn conversation. Also,\nit demonstrates robust zero-shot capability when trained exclusively on\nreasoning-free datasets. In addition, fine-tuning the model with merely 239\nreasoning segmentation image-instruction pairs results in further performance\nenhancement. Experiments show our method not only unlocks new reasoning\nsegmentation capabilities but also proves effective in both complex reasoning\nsegmentation and standard referring segmentation tasks. Code, models, and demo\nare at https://github.com/dvlab-research/LISA.\n",
                "链接": "https://arxiv.org/abs/2308.00692"
            },
            {
                "文章ID": "120164",
                "标题": "PixelLM: Pixel Reasoning with Large Multimodal Model",
                "作者": " Zhongwei Ren,  Zhicheng Huang,  Yunchao Wei,  Yao Zhao,  Dongmei Fu,  Jiashi Feng,  Xiaojie Jin",
                "发布日期": "2023-12-06",
                "摘要": "  While large multimodal models (LMMs) have achieved remarkable progress,\ngenerating pixel-level masks for image reasoning tasks involving multiple\nopen-world targets remains a challenge. To bridge this gap, we introduce\nPixelLM, an effective and efficient LMM for pixel-level reasoning and\nunderstanding. Central to PixelLM is a novel, lightweight pixel decoder and a\ncomprehensive segmentation codebook. The decoder efficiently produces masks\nfrom the hidden embeddings of the codebook tokens, which encode detailed\ntarget-relevant information. With this design, PixelLM harmonizes with the\nstructure of popular LMMs and avoids the need for additional costly\nsegmentation models. Furthermore, we propose a target refinement loss to\nenhance the model's ability to differentiate between multiple targets, leading\nto substantially improved mask quality. To advance research in this area, we\nconstruct MUSE, a high-quality multi-target reasoning segmentation benchmark.\nPixelLM excels across various pixel-level image reasoning and understanding\ntasks, outperforming well-established methods in multiple benchmarks, including\nMUSE, single- and multi-referring segmentation. Comprehensive ablations confirm\nthe efficacy of each proposed component. All code, models, and datasets will be\npublicly available.\n",
                "链接": "https://arxiv.org/abs/2312.02228"
            },
            {
                "文章ID": "6997",
                "标题": "Matching Papers and Reviewers at Large Conferences",
                "作者": " Kevin Leyton-Brown,   Mausam,  Yatin Nandwani,  Hedayat Zarkoob,  Chris Cameron,  Neil Newman,  Dinesh Raghu",
                "发布日期": "2022-08-08",
                "摘要": "  Peer-reviewed conferences, the main publication venues in CS, rely critically\non matching highly qualified reviewers for each paper. Because of the growing\nscale of these conferences, the tight timelines on which they operate, and a\nrecent surge in explicitly dishonest behavior, there is now no alternative to\nperforming this matching in an automated way. This paper studies a novel\nreviewer-paper matching approach that was recently deployed in the 35th AAAI\nConference on Artificial Intelligence (AAAI 2021), and has since been adopted\n(wholly or partially) by other conferences including ICML 2022, AAAI 2022, and\nIJCAI 2022. This approach has three main elements: (1) collecting and\nprocessing input data to identify problematic matches and generate\nreviewer-paper scores; (2) formulating and solving an optimization problem to\nfind good reviewer-paper matchings; and (3) a two-phase reviewing process that\nshifts reviewing resources away from papers likely to be rejected and towards\npapers closer to the decision boundary. This paper also describes an evaluation\nof these innovations based on an extensive post-hoc analysis on real data --\nincluding a comparison with the matching algorithm used in AAAI's previous\n(2020) iteration -- and supplements this with additional numerical\nexperimentation.\n",
                "链接": "https://arxiv.org/abs/2202.12273"
            },
            {
                "文章ID": "79948",
                "标题": "Automatic Model Selection with Large Language Models for Reasoning",
                "作者": " James Xu Zhao,  Yuxi Xie,  Kenji Kawaguchi,  Junxian He,  Michael Qizhe Xie",
                "发布日期": "2023-10-24",
                "摘要": "  Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two\ndistinct reasoning methods, each with its own strengths. CoT employs natural\nlanguage, offering flexibility and interpretability, while PAL utilizes\nprogramming language, yielding more structured and rigorous logic. We introduce\na model selection method to combine the best of both worlds by employing a\nlarge language model (LLM) to dynamically select between them. Our theoretical\nanalysis underscores the feasibility of this method, which is further\ncorroborated by empirical results. Our proposed method demonstrates significant\nperformance improvements across eight reasoning datasets with Codex, ChatGPT,\nand GPT-4. Additionally, our method is complementary to self-consistency; when\nintegrated, it can further enhance performance while significantly reducing\ncomputation costs. Moreover, we achieve new state-of-the-art results on GSM8K\nand SVAMP, with respective accuracies of 96.8% and 93.7%. Our code, data and\nprompts are available at https://github.com/XuZhao0/Model-Selection-Reasoning\n",
                "链接": "https://arxiv.org/abs/2305.14333"
            },
            {
                "文章ID": "97658",
                "标题": "Enhancing Recommender Systems with Large Language Model Reasoning Graphs",
                "作者": " Yan Wang,  Zhixuan Chu,  Xin Ouyang,  Simeng Wang,  Hongyan Hao,  Yue Shen,  Jinjie Gu,  Siqiao Xue,  James Y Zhang,  Qing Cui,  Longfei Li,  Jun Zhou,  Sheng Li",
                "发布日期": "2023-08-22",
                "摘要": "  Recommendation systems aim to provide users with relevant suggestions, but\noften lack interpretability and fail to capture higher-level semantic\nrelationships between user behaviors and profiles. In this paper, we propose a\nnovel approach that leverages large language models (LLMs) to construct\npersonalized reasoning graphs. These graphs link a user's profile and\nbehavioral sequences through causal and logical inferences, representing the\nuser's interests in an interpretable way. Our approach, LLM reasoning graphs\n(LLMRG), has four components: chained graph reasoning, divergent extension,\nself-verification and scoring, and knowledge base self-improvement. The\nresulting reasoning graph is encoded using graph neural networks, which serves\nas additional input to improve conventional recommender systems, without\nrequiring extra user or item information. Our approach demonstrates how LLMs\ncan enable more logical and interpretable recommender systems through\npersonalized reasoning graphs. LLMRG allows recommendations to benefit from\nboth engineered recommendation systems and LLM-derived reasoning graphs. We\ndemonstrate the effectiveness of LLMRG on benchmarks and real-world scenarios\nin enhancing base recommendation models.\n",
                "链接": "https://arxiv.org/abs/2308.10835"
            },
            {
                "文章ID": "107444",
                "标题": "GraphLLM: Boosting Graph Reasoning Ability of Large Language Model",
                "作者": " Ziwei Chai,  Tianjie Zhang,  Liang Wu,  Kaiqiao Han,  Xiaohai Hu,  Xuanwen Huang,  Yang Yang",
                "发布日期": "2023-10-10",
                "摘要": "  The advancement of Large Language Models (LLMs) has remarkably pushed the\nboundaries towards artificial general intelligence (AGI), with their\nexceptional ability on understanding diverse types of information, including\nbut not limited to images and audio. Despite this progress, a critical gap\nremains in empowering LLMs to proficiently understand and reason on graph data.\nRecent studies underscore LLMs' underwhelming performance on fundamental graph\nreasoning tasks. In this paper, we endeavor to unearth the obstacles that\nimpede LLMs in graph reasoning, pinpointing the common practice of converting\ngraphs into natural language descriptions (Graph2Text) as a fundamental\nbottleneck. To overcome this impediment, we introduce GraphLLM, a pioneering\nend-to-end approach that synergistically integrates graph learning models with\nLLMs. This synergy equips LLMs with the ability to proficiently interpret and\nreason on graph data, harnessing the superior expressive power of graph\nlearning models. Our empirical evaluations across four fundamental graph\nreasoning tasks validate the effectiveness of GraphLLM. The results exhibit a\nsubstantial average accuracy enhancement of 54.44%, alongside a noteworthy\ncontext reduction of 96.45% across various graph reasoning tasks.\n",
                "链接": "https://arxiv.org/abs/2310.05845"
            },
            {
                "文章ID": "110171",
                "标题": "Democratizing Reasoning Ability: Tailored Learning from Large Language\n  Model",
                "作者": " Zhaoyang Wang,  Shaohan Huang,  Yuxuan Liu,  Jiahai Wang,  Minghui Song,  Zihan Zhang,  Haizhen Huang,  Furu Wei,  Weiwei Deng,  Feng Sun,  Qi Zhang",
                "发布日期": "2023-10-23",
                "摘要": "  Large language models (LLMs) exhibit impressive emergent abilities in natural\nlanguage processing, but their democratization is hindered due to huge\ncomputation requirements and closed-source nature. Recent research on advancing\nopen-source smaller LMs by distilling knowledge from black-box LLMs has\nobtained promising results in the instruction-following ability. However, the\nreasoning ability which is more challenging to foster, is relatively rarely\nexplored. In this paper, we propose a tailored learning approach to distill\nsuch reasoning ability to smaller LMs to facilitate the democratization of the\nexclusive reasoning ability. In contrast to merely employing LLM as a data\nannotator, we exploit the potential of LLM as a reasoning teacher by building\nan interactive multi-round learning paradigm. This paradigm enables the student\nto expose its deficiencies to the black-box teacher who then can provide\ncustomized training data in return. Further, to exploit the reasoning potential\nof the smaller LM, we propose self-reflection learning to motivate the student\nto learn from self-made mistakes. The learning from self-reflection and LLM are\nall tailored to the student's learning status, thanks to the seamless\nintegration with the multi-round learning paradigm. Comprehensive experiments\nand analysis on mathematical and commonsense reasoning tasks demonstrate the\neffectiveness of our method. The code will be available at\nhttps://github.com/Raibows/Learn-to-Reason.\n",
                "链接": "https://arxiv.org/abs/2310.13332"
            },
            {
                "文章ID": "66434",
                "标题": "HCI Papers Cite HCI Papers, Increasingly So",
                "作者": " Xiang 'Anthony' Chen",
                "发布日期": "2023-03-15",
                "摘要": "  We propose X-index -- the proportion of papers' citations coming from outside\ntheir research field -- and use this metric to analyze citations of CHI, UIST,\nand CSCW papers between 2010 and 2022. We found an overall decreasing X-index\nby several measures, indicating that HCI papers have been more and more likely\nto be cited by HCI papers rather than by non-HCI papers.\n",
                "链接": "https://arxiv.org/abs/2303.07539"
            }
        ]
    },
    {
        "question": {
            "question": "近几个月自然语言处理相关的文章。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "197",
                "标题": "Robust Natural Language Processing: Recent Advances, Challenges, and\n  Future Directions",
                "作者": " Marwan Omar,  Soohyeon Choi,  DaeHun Nyang,  David Mohaisen",
                "发布日期": "2022-10-24",
                "摘要": "  Recent natural language processing (NLP) techniques have accomplished high\nperformance on benchmark datasets, primarily due to the significant improvement\nin the performance of deep learning. The advances in the research community\nhave led to great enhancements in state-of-the-art production systems for NLP\ntasks, such as virtual assistants, speech recognition, and sentiment analysis.\nHowever, such NLP systems still often fail when tested with adversarial\nattacks. The initial lack of robustness exposed troubling gaps in current\nmodels' language understanding capabilities, creating problems when NLP systems\nare deployed in real life. In this paper, we present a structured overview of\nNLP robustness research by summarizing the literature in a systemic way across\nvarious dimensions. We then take a deep-dive into the various dimensions of\nrobustness, across techniques, metrics, embeddings, and benchmarks. Finally, we\nargue that robustness should be multi-dimensional, provide insights into\ncurrent research, identify gaps in the literature to suggest directions worth\npursuing to address these gaps.\n",
                "链接": "https://arxiv.org/abs/2201.00768"
            },
            {
                "文章ID": "44791",
                "标题": "Classification of Misinformation in New Articles using Natural Language\n  Processing and a Recurrent Neural Network",
                "作者": " Brendan Cunha,  Lydia Manikonda",
                "发布日期": "2022-10-26",
                "摘要": "  This paper seeks to address the classification of misinformation in news\narticles using a Long Short Term Memory Recurrent Neural Network. Articles were\ntaken from 2018; a year that was filled with reporters writing about President\nDonald Trump, Special Counsel Robert Mueller, the Fifa World Cup, and Russia.\nThe model presented successfully classifies these articles with an accuracy\nscore of 0.779944. We consider this to be successful because the model was\ntrained on articles that included languages other than English as well as\nincomplete, or fragmented, articles.\n",
                "链接": "https://arxiv.org/abs/2210.13534"
            },
            {
                "文章ID": "76320",
                "标题": "Putting Natural in Natural Language Processing",
                "作者": " Grzegorz Chrupała",
                "发布日期": "2023-05-24",
                "摘要": "  Human language is firstly spoken and only secondarily written. Text, however,\nis a very convenient and efficient representation of language, and modern\ncivilization has made it ubiquitous. Thus the field of NLP has overwhelmingly\nfocused on processing written rather than spoken language. Work on spoken\nlanguage, on the other hand, has been siloed off within the largely separate\nspeech processing community which has been inordinately preoccupied with\ntranscribing speech into text. Recent advances in deep learning have led to a\nfortuitous convergence in methods between speech processing and mainstream NLP.\nArguably, the time is ripe for a unification of these two fields, and for\nstarting to take spoken language seriously as the primary mode of human\ncommunication. Truly natural language processing could lead to better\nintegration with the rest of language science and could lead to systems which\nare more data-efficient and more human-like, and which can communicate beyond\nthe textual modality.\n",
                "链接": "https://arxiv.org/abs/2305.04572"
            },
            {
                "文章ID": "79436",
                "标题": "Interactive Natural Language Processing",
                "作者": " Zekun Wang,  Ge Zhang,  Kexin Yang,  Ning Shi,  Wangchunshu Zhou,  Shaochun Hao,  Guangzheng Xiong,  Yizhi Li,  Mong Yuan Sim,  Xiuying Chen,  Qingqing Zhu,  Zhenzhu Yang,  Adam Nik,  Qi Liu,  Chenghua Lin,  Shi Wang,  Ruibo Liu,  Wenhu Chen,  Ke Xu,  Dayiheng Liu,  Yike Guo,  Jie Fu",
                "发布日期": "2023-05-23",
                "摘要": "  Interactive Natural Language Processing (iNLP) has emerged as a novel\nparadigm within the field of NLP, aimed at addressing limitations in existing\nframeworks while aligning with the ultimate goals of artificial intelligence.\nThis paradigm considers language models as agents capable of observing, acting,\nand receiving feedback iteratively from external entities. Specifically,\nlanguage models in this context can: (1) interact with humans for better\nunderstanding and addressing user needs, personalizing responses, aligning with\nhuman values, and improving the overall user experience; (2) interact with\nknowledge bases for enriching language representations with factual knowledge,\nenhancing the contextual relevance of responses, and dynamically leveraging\nexternal information to generate more accurate and informed responses; (3)\ninteract with models and tools for effectively decomposing and addressing\ncomplex tasks, leveraging specialized expertise for specific subtasks, and\nfostering the simulation of social behaviors; and (4) interact with\nenvironments for learning grounded representations of language, and effectively\ntackling embodied tasks such as reasoning, planning, and decision-making in\nresponse to environmental observations. This paper offers a comprehensive\nsurvey of iNLP, starting by proposing a unified definition and framework of the\nconcept. We then provide a systematic classification of iNLP, dissecting its\nvarious components, including interactive objects, interaction interfaces, and\ninteraction methods. We proceed to delve into the evaluation methodologies used\nin the field, explore its diverse applications, scrutinize its ethical and\nsafety issues, and discuss prospective research directions. This survey serves\nas an entry point for researchers who are interested in this rapidly evolving\narea and offers a broad view of the current landscape and future trajectory of\niNLP.\n",
                "链接": "https://arxiv.org/abs/2305.13246"
            },
            {
                "文章ID": "60462",
                "标题": "Natural Language Processing for Policymaking",
                "作者": " Zhijing Jin,  Rada Mihalcea",
                "发布日期": "2023-02-08",
                "摘要": "  Language is the medium for many political activities, from campaigns to news\nreports. Natural language processing (NLP) uses computational tools to parse\ntext into key information that is needed for policymaking. In this chapter, we\nintroduce common methods of NLP, including text classification, topic modeling,\nevent extraction, and text scaling. We then overview how these methods can be\nused for policymaking through four major applications including data collection\nfor evidence-based policymaking, interpretation of political decisions, policy\ncommunication, and investigation of policy effects. Finally, we highlight some\npotential limitations and ethical concerns when using NLP for policymaking.\n  This text is from Chapter 7 (pages 141-162) of the Handbook of Computational\nSocial Science for Policy (2023). Open Access on Springer:\nhttps://doi.org/10.1007/978-3-031-16624-2\n",
                "链接": "https://arxiv.org/abs/2302.03490"
            },
            {
                "文章ID": "24559",
                "标题": "Computational linguistics and Natural Language Processing",
                "作者": " Saturnino Luz",
                "发布日期": "2022-06-15",
                "摘要": "  This chapter provides an introduction to computational linguistics methods,\nwith focus on their applications to the practice and study of translation. It\ncovers computational models, methods and tools for collection, storage,\nindexing and analysis of linguistic data in the context of translation, and\ndiscusses the main methodological issues and challenges in this field. While an\nexhaustive review of existing computational linguistics methods and tools is\nbeyond the scope of this chapter, we describe the most representative\napproaches, and illustrate them with descriptions of typical applications.\n",
                "链接": "https://arxiv.org/abs/2206.07026"
            },
            {
                "文章ID": "53444",
                "标题": "Categorical Tools for Natural Language Processing",
                "作者": " Giovanni de Felice",
                "发布日期": "2022-12-14",
                "摘要": "  This thesis develops the translation between category theory and\ncomputational linguistics as a foundation for natural language processing. The\nthree chapters deal with syntax, semantics and pragmatics. First, string\ndiagrams provide a unified model of syntactic structures in formal grammars.\nSecond, functors compute semantics by turning diagrams into logical, tensor,\nneural or quantum computation. Third, the resulting functorial models can be\ncomposed to form games where equilibria are the solutions of language\nprocessing tasks. This framework is implemented as part of DisCoPy, the Python\nlibrary for computing with string diagrams. We describe the correspondence\nbetween categorical, linguistic and computational structures, and demonstrate\ntheir applications in compositional natural language processing.\n",
                "链接": "https://arxiv.org/abs/2212.06636"
            },
            {
                "文章ID": "68255",
                "标题": "Features matching using natural language processing",
                "作者": " Muhammad Danial Khilji",
                "发布日期": "2023-03-24",
                "摘要": "  The feature matching is a basic step in matching different datasets. This\narticle proposes shows a new hybrid model of a pretrained Natural Language\nProcessing (NLP) based model called BERT used in parallel with a statistical\nmodel based on Jaccard similarity to measure the similarity between list of\nfeatures from two different datasets. This reduces the time required to search\nfor correlations or manually match each feature from one dataset to another.\n",
                "链接": "https://arxiv.org/abs/2303.12804"
            },
            {
                "文章ID": "85699",
                "标题": "Operationalising Representation in Natural Language Processing",
                "作者": " Jacqueline Harding",
                "发布日期": "2023-11-21",
                "摘要": "  Despite its centrality in the philosophy of cognitive science, there has been\nlittle prior philosophical work engaging with the notion of representation in\ncontemporary NLP practice. This paper attempts to fill that lacuna: drawing on\nideas from cognitive science, I introduce a framework for evaluating the\nrepresentational claims made about components of neural NLP models, proposing\nthree criteria with which to evaluate whether a component of a model represents\na property and operationalising these criteria using probing classifiers, a\npopular analysis technique in NLP (and deep learning more broadly).\n  The project of operationalising a philosophically-informed notion of\nrepresentation should be of interest to both philosophers of science and NLP\npractitioners. It affords philosophers a novel testing-ground for claims about\nthe nature of representation, and helps NLPers organise the large literature on\nprobing experiments, suggesting novel avenues for empirical research.\n",
                "链接": "https://arxiv.org/abs/2306.08193"
            },
            {
                "文章ID": "115729",
                "标题": "Natural Language Processing for Financial Regulation",
                "作者": " Ixandra Achitouv,  Dragos Gorduza,  Antoine Jacquier",
                "发布日期": "2023-11-16",
                "摘要": "  This article provides an understanding of Natural Language Processing\ntechniques in the framework of financial regulation, more specifically in order\nto perform semantic matching search between rules and policy when no dataset is\navailable for supervised learning. We outline how to outperform simple\npre-trained sentences-transformer models using freely available resources and\nexplain the mathematical concepts behind the key building blocks of Natural\nLanguage Processing.\n",
                "链接": "https://arxiv.org/abs/2311.08533"
            }
        ]
    },
    {
        "question": {
            "question": "查找多语言情感分析的最新论文。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "2025",
                "标题": "NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual\n  Sentiment Analysis",
                "作者": " Shamsuddeen Hassan Muhammad,  David Ifeoluwa Adelani,  Sebastian Ruder,  Ibrahim Said Ahmad,  Idris Abdulmumin,  Bello Shehu Bello,  Monojit Choudhury,  Chris Chinenye Emezue,  Saheed Salahudeen Abdullahi,  Anuoluwapo Aremu,  Alipio Jeorge,  Pavel Brazdil",
                "发布日期": "2022-06-22",
                "摘要": "  Sentiment analysis is one of the most widely studied applications in NLP, but\nmost work focuses on languages with large amounts of data. We introduce the\nfirst large-scale human-annotated Twitter sentiment dataset for the four most\nwidely spoken languages in Nigeria (Hausa, Igbo, Nigerian-Pidgin, and\nYor\\`ub\\'a ) consisting of around 30,000 annotated tweets per language (and\n14,000 for Nigerian-Pidgin), including a significant fraction of code-mixed\ntweets. We propose text collection, filtering, processing and labeling methods\nthat enable us to create datasets for these low-resource languages. We evaluate\na rangeof pre-trained models and transfer strategies on the dataset. We find\nthat language-specific models and language-adaptivefine-tuning generally\nperform best. We release the datasets, trained models, sentiment lexicons, and\ncode to incentivizeresearch on sentiment analysis in under-represented\nlanguages.\n",
                "链接": "https://arxiv.org/abs/2201.08277"
            },
            {
                "文章ID": "122117",
                "标题": "Multimodal Sentiment Analysis: Perceived vs Induced Sentiments",
                "作者": " Aditi Aggarwal,  Deepika Varshney,  Saurabh Patel",
                "发布日期": "2023-12-14",
                "摘要": "  Social media has created a global network where people can easily access and\nexchange vast information. This information gives rise to a variety of\nopinions, reflecting both positive and negative viewpoints. GIFs stand out as a\nmultimedia format offering a visually engaging way for users to communicate. In\nthis research, we propose a multimodal framework that integrates visual and\ntextual features to predict the GIF sentiment. It also incorporates attributes\nincluding face emotion detection and OCR generated captions to capture the\nsemantic aspects of the GIF. The developed classifier achieves an accuracy of\n82.7% on Twitter GIFs, which is an improvement over state-of-the-art models.\nMoreover, we have based our research on the ReactionGIF dataset, analysing the\nvariance in sentiment perceived by the author and sentiment induced in the\nreader\n",
                "链接": "https://arxiv.org/abs/2312.07627"
            },
            {
                "文章ID": "3562",
                "标题": "Negativity Spreads Faster: A Large-Scale Multilingual Twitter Analysis\n  on the Role of Sentiment in Political Communication",
                "作者": " Dimosthenis Antypas,  Alun Preece,  Jose Camacho-Collados",
                "发布日期": "2023-04-05",
                "摘要": "  Social media has become extremely influential when it comes to policy making\nin modern societies, especially in the western world, where platforms such as\nTwitter allow users to follow politicians, thus making citizens more involved\nin political discussion. In the same vein, politicians use Twitter to express\ntheir opinions, debate among others on current topics and promote their\npolitical agendas aiming to influence voter behaviour. In this paper, we\nattempt to analyse tweets of politicians from three European countries and\nexplore the virality of their tweets. Previous studies have shown that tweets\nconveying negative sentiment are likely to be retweeted more frequently. By\nutilising state-of-the-art pre-trained language models, we performed sentiment\nanalysis on hundreds of thousands of tweets collected from members of\nparliament in Greece, Spain and the United Kingdom, including devolved\nadministrations. We achieved this by systematically exploring and analysing the\ndifferences between influential and less popular tweets. Our analysis indicates\nthat politicians' negatively charged tweets spread more widely, especially in\nmore recent times, and highlights interesting differences between political\nparties as well as between politicians and the general population.\n",
                "链接": "https://arxiv.org/abs/2202.00396"
            },
            {
                "文章ID": "118618",
                "标题": "Entity-Aspect-Opinion-Sentiment Quadruple Extraction for Fine-grained\n  Sentiment Analysis",
                "作者": " Dan Ma,  Jun Xu,  Zongyu Wang,  Xuezhi Cao,  Yunsen Xian",
                "发布日期": "2023-11-29",
                "摘要": "  Product reviews often contain a large number of implicit aspects and\nobject-attribute co-existence cases. Unfortunately, many existing studies in\nAspect-Based Sentiment Analysis (ABSA) have overlooked this issue, which can\nmake it difficult to extract opinions comprehensively and fairly. In this\npaper, we propose a new task called Entity-Aspect-Opinion-Sentiment Quadruple\nExtraction (EASQE), which aims to hierarchically decompose aspect terms into\nentities and aspects to avoid information loss, non-exclusive annotations, and\nopinion misunderstandings in ABSA tasks. To facilitate research in this new\ntask, we have constructed four datasets (Res14-EASQE, Res15-EASQE, Res16-EASQE,\nand Lap14-EASQE) based on the SemEval Restaurant and Laptop datasets. We have\nalso proposed a novel two-stage sequence-tagging based Trigger-Opinion\nframework as the baseline for the EASQE task. Empirical evaluations show that\nour Trigger-Opinion framework can generate satisfactory EASQE results and can\nalso be applied to other ABSA tasks, significantly outperforming\nstate-of-the-art methods. We have made the four datasets and source code of\nTrigger-Opinion publicly available to facilitate further research in this area.\n",
                "链接": "https://arxiv.org/abs/2311.16678"
            },
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "23956",
                "标题": "Sentiment analysis on electricity twitter posts",
                "作者": " Pardeep Kaur,  Maryam Edalati",
                "发布日期": "2022-06-13",
                "摘要": "  In today's world, everyone is expressive in some way, and the focus of this\nproject is on people's opinions about rising electricity prices in United\nKingdom and India using data from Twitter, a micro-blogging platform on which\npeople post messages, known as tweets. Because many people's incomes are not\ngood and they have to pay so many taxes and bills, maintaining a home has\nbecome a disputed issue these days. Despite the fact that Government offered\nsubsidy schemes to compensate people electricity bills but it is not welcomed\nby people. In this project, the aim is to perform sentiment analysis on\npeople's expressions and opinions expressed on Twitter. In order to grasp the\nelectricity prices opinion, it is necessary to carry out sentiment analysis for\nthe government and consumers in energy market. Furthermore, text present on\nthese medias are unstructured in nature, so to process them we firstly need to\npre-process the data. There are so many feature extraction techniques such as\nBag of Words, TF-IDF (Term Frequency-Inverse Document Frequency), word\nembedding, NLP based features like word count. In this project, we analysed the\nimpact of feature TF-IDF word level on electricity bills dataset of sentiment\nanalysis. We found that by using TF-IDF word level performance of sentiment\nanalysis is 3-4 higher than using N-gram features. Analysis is done using four\nclassification algorithms including Naive Bayes, Decision Tree, Random Forest,\nand Logistic Regression and considering F-Score, Accuracy, Precision, and\nRecall performance parameters.\n",
                "链接": "https://arxiv.org/abs/2206.05042"
            },
            {
                "文章ID": "13930",
                "标题": "Mapping the Multilingual Margins: Intersectional Biases of Sentiment\n  Analysis Systems in English, Spanish, and Arabic",
                "作者": " António Câmara,  Nina Taneja,  Tamjeed Azad,  Emily Allaway,  Richard Zemel",
                "发布日期": "2022-04-08",
                "摘要": "  As natural language processing systems become more widespread, it is\nnecessary to address fairness issues in their implementation and deployment to\nensure that their negative impacts on society are understood and minimized.\nHowever, there is limited work that studies fairness using a multilingual and\nintersectional framework or on downstream tasks. In this paper, we introduce\nfour multilingual Equity Evaluation Corpora, supplementary test sets designed\nto measure social biases, and a novel statistical framework for studying\nunisectional and intersectional social biases in natural language processing.\nWe use these tools to measure gender, racial, ethnic, and intersectional social\nbiases across five models trained on emotion regression tasks in English,\nSpanish, and Arabic. We find that many systems demonstrate statistically\nsignificant unisectional and intersectional social biases.\n",
                "链接": "https://arxiv.org/abs/2204.03558"
            },
            {
                "文章ID": "42411",
                "标题": "On the Evaluation of the Plausibility and Faithfulness of Sentiment\n  Analysis Explanations",
                "作者": " Julia El Zini,  Mohamad Mansour,  Basel Mousi,  Mariette Awad",
                "发布日期": "2022-10-14",
                "摘要": "  Current Explainable AI (ExAI) methods, especially in the NLP field, are\nconducted on various datasets by employing different metrics to evaluate\nseveral aspects. The lack of a common evaluation framework is hindering the\nprogress tracking of such methods and their wider adoption. In this work,\ninspired by offline information retrieval, we propose different metrics and\ntechniques to evaluate the explainability of SA models from two angles. First,\nwe evaluate the strength of the extracted \"rationales\" in faithfully explaining\nthe predicted outcome. Second, we measure the agreement between ExAI methods\nand human judgment on a homegrown dataset1 to reflect on the rationales\nplausibility. Our conducted experiments comprise four dimensions: (1) the\nunderlying architectures of SA models, (2) the approach followed by the ExAI\nmethod, (3) the reasoning difficulty, and (4) the homogeneity of the\nground-truth rationales. We empirically demonstrate that anchors explanations\nare more aligned with the human judgment and can be more confident in\nextracting supporting rationales. As can be foreseen, the reasoning complexity\nof sentiment is shown to thwart ExAI methods from extracting supporting\nevidence. Moreover, a remarkable discrepancy is discerned between the results\nof different explainability methods on the various architectures suggesting the\nneed for consolidation to observe enhanced performance. Predominantly,\ntransformers are shown to exhibit better explainability than convolutional and\nrecurrent architectures. Our work paves the way towards designing more\ninterpretable NLP models and enabling a common evaluation ground for their\nrelative strengths and robustness.\n",
                "链接": "https://arxiv.org/abs/2210.06916"
            },
            {
                "文章ID": "109794",
                "标题": "The Sentiment Problem: A Critical Survey towards Deconstructing\n  Sentiment Analysis",
                "作者": " Pranav Narayanan Venkit,  Mukund Srinath,  Sanjana Gautam,  Saranya Venkatraman,  Vipul Gupta,  Rebecca J. Passonneau,  Shomir Wilson",
                "发布日期": "2023-10-20",
                "摘要": "  We conduct an inquiry into the sociotechnical aspects of sentiment analysis\n(SA) by critically examining 189 peer-reviewed papers on their applications,\nmodels, and datasets. Our investigation stems from the recognition that SA has\nbecome an integral component of diverse sociotechnical systems, exerting\ninfluence on both social and technical users. By delving into sociological and\ntechnological literature on sentiment, we unveil distinct conceptualizations of\nthis term in domains such as finance, government, and medicine. Our study\nexposes a lack of explicit definitions and frameworks for characterizing\nsentiment, resulting in potential challenges and biases. To tackle this issue,\nwe propose an ethics sheet encompassing critical inquiries to guide\npractitioners in ensuring equitable utilization of SA. Our findings underscore\nthe significance of adopting an interdisciplinary approach to defining\nsentiment in SA and offer a pragmatic solution for its implementation.\n",
                "链接": "https://arxiv.org/abs/2310.12318"
            },
            {
                "文章ID": "26253",
                "标题": "Emoji-based Fine-grained Attention Network for Sentiment Analysis in the\n  Microblog Comments",
                "作者": " Deng Yang,  Liu Kejian,  Yang Cheng,  Feng Yuanyuan,  Li Weihao",
                "发布日期": "2022-06-27",
                "摘要": "  Microblogs have become a social platform for people to express their emotions\nin real-time, and it is a trend to analyze user emotional tendencies from the\ninformation on Microblogs. The dynamic features of emojis can affect the\nsentiment polarity of microblog texts. Since existing models seldom consider\nthe diversity of emoji sentiment polarity,the paper propose a microblog\nsentiment classification model based on ALBERT-FAET. We obtain text embedding\nvia ALBERT pretraining model and learn the inter-emoji embedding with an\nattention-based LSTM network. In addition, a fine-grained attention mechanism\nis proposed to capture the word-level interactions between plain text and\nemoji. Finally, we concatenate these features and feed them into a CNN\nclassifier to predict the sentiment labels of the microblogs. To verify the\neffectiveness of the model and the fine-grained attention network, we conduct\ncomparison experiments and ablation experiments. The comparison experiments\nshow that the model outperforms previous methods in three evaluation indicators\n(accuracy, precision, and recall) and the model can significantly improve\nsentiment classification. The ablation experiments show that compared with\nALBERT-AET, the proposed model ALBERT-FAET is better in the metrics, indicating\nthat the fine-grained attention network can understand the diversified\ninformation of emoticons.\n",
                "链接": "https://arxiv.org/abs/2206.12262"
            }
        ]
    },
    {
        "question": {
            "question": "2023年以后关于NLP领域的持续性学习论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "49978",
                "标题": "Continual Learning of Natural Language Processing Tasks: A Survey",
                "作者": " Zixuan Ke,  Bing Liu",
                "发布日期": "2023-05-12",
                "摘要": "  Continual learning (CL) is a learning paradigm that emulates the human\ncapability of learning and accumulating knowledge continually without\nforgetting the previously learned knowledge and also transferring the learned\nknowledge to help learn new tasks better. This survey presents a comprehensive\nreview and analysis of the recent progress of CL in NLP, which has significant\ndifferences from CL in computer vision and machine learning. It covers (1) all\nCL settings with a taxonomy of existing techniques; (2) catastrophic forgetting\n(CF) prevention, (3) knowledge transfer (KT), which is particularly important\nfor NLP tasks; and (4) some theory and the hidden challenge of inter-task class\nseparation (ICS). (1), (3) and (4) have not been included in the existing\nsurvey. Finally, a list of future directions is discussed.\n",
                "链接": "https://arxiv.org/abs/2211.12701"
            },
            {
                "文章ID": "37115",
                "标题": "The Role of Explanatory Value in Natural Language Processing",
                "作者": " Kees van Deemter",
                "发布日期": "2022-09-14",
                "摘要": "  A key aim of science is explanation, yet the idea of explaining language\nphenomena has taken a backseat in mainstream Natural Language Processing (NLP)\nand many other areas of Artificial Intelligence. I argue that explanation of\nlinguistic behaviour should be a main goal of NLP, and that this is not the\nsame as making NLP models explainable. To illustrate these ideas, some recent\nmodels of human language production are compared with each other. I conclude by\nasking what it would mean for NLP research and institutional policies if our\ncommunity took explanatory value seriously, while heeding some possible\npitfalls.\n",
                "链接": "https://arxiv.org/abs/2209.06169"
            },
            {
                "文章ID": "42672",
                "标题": "The State of Profanity Obfuscation in Natural Language Processing",
                "作者": " Debora Nozza,  Dirk Hovy",
                "发布日期": "2022-10-17",
                "摘要": "  Work on hate speech has made the consideration of rude and harmful examples\nin scientific publications inevitable. This raises various problems, such as\nwhether or not to obscure profanities. While science must accurately disclose\nwhat it does, the unwarranted spread of hate speech is harmful to readers, and\nincreases its internet frequency. While maintaining publications' professional\nappearance, obfuscating profanities makes it challenging to evaluate the\ncontent, especially for non-native speakers. Surveying 150 ACL papers, we\ndiscovered that obfuscation is usually employed for English but not other\nlanguages, and even so quite uneven. We discuss the problems with obfuscation\nand suggest a multilingual community resource called PrOf that has a Python\nmodule to standardize profanity obfuscation processes. We believe PrOf can help\nscientific publication policies to make hate speech work accessible and\ncomparable, irrespective of language.\n",
                "链接": "https://arxiv.org/abs/2210.07595"
            },
            {
                "文章ID": "63180",
                "标题": "Natural Language Processing in the Legal Domain",
                "作者": " Daniel Martin Katz,  Dirk Hartung,  Lauritz Gerlach,  Abhik Jana, II Michael J. Bommarito",
                "发布日期": "2023-02-24",
                "摘要": "  In this paper, we summarize the current state of the field of NLP & Law with\na specific focus on recent technical and substantive developments. To support\nour analysis, we construct and analyze a nearly complete corpus of more than\nsix hundred NLP & Law related papers published over the past decade. Our\nanalysis highlights several major trends. Namely, we document an increasing\nnumber of papers written, tasks undertaken, and languages covered over the\ncourse of the past decade. We observe an increase in the sophistication of the\nmethods which researchers deployed in this applied context. Slowly but surely,\nLegal NLP is beginning to match not only the methodological sophistication of\ngeneral NLP but also the professional standards of data availability and code\nreproducibility observed within the broader scientific community. We believe\nall of these trends bode well for the future of the field, but many questions\nin both the academic and commercial sphere still remain open.\n",
                "链接": "https://arxiv.org/abs/2302.12039"
            },
            {
                "文章ID": "76320",
                "标题": "Putting Natural in Natural Language Processing",
                "作者": " Grzegorz Chrupała",
                "发布日期": "2023-05-24",
                "摘要": "  Human language is firstly spoken and only secondarily written. Text, however,\nis a very convenient and efficient representation of language, and modern\ncivilization has made it ubiquitous. Thus the field of NLP has overwhelmingly\nfocused on processing written rather than spoken language. Work on spoken\nlanguage, on the other hand, has been siloed off within the largely separate\nspeech processing community which has been inordinately preoccupied with\ntranscribing speech into text. Recent advances in deep learning have led to a\nfortuitous convergence in methods between speech processing and mainstream NLP.\nArguably, the time is ripe for a unification of these two fields, and for\nstarting to take spoken language seriously as the primary mode of human\ncommunication. Truly natural language processing could lead to better\nintegration with the rest of language science and could lead to systems which\nare more data-efficient and more human-like, and which can communicate beyond\nthe textual modality.\n",
                "链接": "https://arxiv.org/abs/2305.04572"
            },
            {
                "文章ID": "34310",
                "标题": "Review of Natural Language Processing in Pharmacology",
                "作者": " Dimitar Trajanov,  Vangel Trajkovski,  Makedonka Dimitrieva,  Jovana Dobreva,  Milos Jovanovik,  Matej Klemen,  Aleš Žagar,  Marko Robnik-Šikonja",
                "发布日期": "2023-01-27",
                "摘要": "  Natural language processing (NLP) is an area of artificial intelligence that\napplies information technologies to process the human language, understand it\nto a certain degree, and use it in various applications. This area has rapidly\ndeveloped in the last few years and now employs modern variants of deep neural\nnetworks to extract relevant patterns from large text corpora. The main\nobjective of this work is to survey the recent use of NLP in the field of\npharmacology. As our work shows, NLP is a highly relevant information\nextraction and processing approach for pharmacology. It has been used\nextensively, from intelligent searches through thousands of medical documents\nto finding traces of adversarial drug interactions in social media. We split\nour coverage into five categories to survey modern NLP methodology, commonly\naddressed tasks, relevant textual data, knowledge bases, and useful programming\nlibraries. We split each of the five categories into appropriate subcategories,\ndescribe their main properties and ideas, and summarize them in a tabular form.\nThe resulting survey presents a comprehensive overview of the area, useful to\npractitioners and interested observers.\n",
                "链接": "https://arxiv.org/abs/2208.10228"
            },
            {
                "文章ID": "92120",
                "标题": "Exploring the Landscape of Natural Language Processing Research",
                "作者": " Tim Schopf,  Karim Arabi,  Florian Matthes",
                "发布日期": "2023-09-26",
                "摘要": "  As an efficient approach to understand, generate, and process natural\nlanguage texts, research in natural language processing (NLP) has exhibited a\nrapid spread and wide adoption in recent years. Given the increasing research\nwork in this area, several NLP-related approaches have been surveyed in the\nresearch community. However, a comprehensive study that categorizes established\ntopics, identifies trends, and outlines areas for future research remains\nabsent. Contributing to closing this gap, we have systematically classified and\nanalyzed research papers in the ACL Anthology. As a result, we present a\nstructured overview of the research landscape, provide a taxonomy of fields of\nstudy in NLP, analyze recent developments in NLP, summarize our findings, and\nhighlight directions for future work.\n",
                "链接": "https://arxiv.org/abs/2307.10652"
            },
            {
                "文章ID": "7583",
                "标题": "Paper Plain: Making Medical Research Papers Approachable to Healthcare\n  Consumers with Natural Language Processing",
                "作者": " Tal August,  Lucy Lu Wang,  Jonathan Bragg,  Marti A. Hearst,  Andrew Head,  Kyle Lo",
                "发布日期": "2022-03-02",
                "摘要": "  When seeking information not covered in patient-friendly documents, like\nmedical pamphlets, healthcare consumers may turn to the research literature.\nReading medical papers, however, can be a challenging experience. To improve\naccess to medical papers, we introduce a novel interactive interface-Paper\nPlain-with four features powered by natural language processing: definitions of\nunfamiliar terms, in-situ plain language section summaries, a collection of key\nquestions that guide readers to answering passages, and plain language\nsummaries of the answering passages. We evaluate Paper Plain, finding that\nparticipants who use Paper Plain have an easier time reading and understanding\nresearch papers without a loss in paper comprehension compared to those who use\na typical PDF reader. Altogether, the study results suggest that guiding\nreaders to relevant passages and providing plain language summaries, or\n\"gists,\" alongside the original paper content can make reading medical papers\neasier and give readers more confidence to approach these papers.\n",
                "链接": "https://arxiv.org/abs/2203.00130"
            },
            {
                "文章ID": "109576",
                "标题": "Field-testing items using artificial intelligence: Natural language\n  processing with transformers",
                "作者": " Hotaka Maeda",
                "发布日期": "2023-10-19",
                "摘要": "  Five thousand variations of the RoBERTa model, an artificially intelligent\n\"transformer\" that can understand text language, completed an English literacy\nexam with 29 multiple-choice questions. Data were used to calculate the\npsychometric properties of the items, which showed some degree of agreement to\nthose obtained from human examinee data.\n",
                "链接": "https://arxiv.org/abs/2310.11655"
            },
            {
                "文章ID": "85699",
                "标题": "Operationalising Representation in Natural Language Processing",
                "作者": " Jacqueline Harding",
                "发布日期": "2023-11-21",
                "摘要": "  Despite its centrality in the philosophy of cognitive science, there has been\nlittle prior philosophical work engaging with the notion of representation in\ncontemporary NLP practice. This paper attempts to fill that lacuna: drawing on\nideas from cognitive science, I introduce a framework for evaluating the\nrepresentational claims made about components of neural NLP models, proposing\nthree criteria with which to evaluate whether a component of a model represents\na property and operationalising these criteria using probing classifiers, a\npopular analysis technique in NLP (and deep learning more broadly).\n  The project of operationalising a philosophically-informed notion of\nrepresentation should be of interest to both philosophers of science and NLP\npractitioners. It affords philosophers a novel testing-ground for claims about\nthe nature of representation, and helps NLPers organise the large literature on\nprobing experiments, suggesting novel avenues for empirical research.\n",
                "链接": "https://arxiv.org/abs/2306.08193"
            }
        ]
    },
    {
        "question": {
            "question": "找一下最近关于大模型评测的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "72313",
                "标题": "Evaluation of Social Biases in Recent Large Pre-Trained Models",
                "作者": " Swapnil Sharma,  Nikita Anand,  Kranthi Kiran G. V.,  Alind Jain",
                "发布日期": "2023-04-17",
                "摘要": "  Large pre-trained language models are widely used in the community. These\nmodels are usually trained on unmoderated and unfiltered data from open sources\nlike the Internet. Due to this, biases that we see in platforms online which\nare a reflection of those in society are in turn captured and learned by these\nmodels. These models are deployed in applications that affect millions of\npeople and their inherent biases are harmful to the targeted social groups. In\nthis work, we study the general trend in bias reduction as newer pre-trained\nmodels are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT)\nare chosen and evaluated against two bias benchmarks, StereoSet and\nCrowS-Pairs. They are compared to the baseline of BERT using the associated\nmetrics. We explore whether as advancements are made and newer, faster, lighter\nmodels are released: are they being developed responsibly such that their\ninherent social biases have been reduced compared to their older counterparts?\nThe results are compiled and we find that all the models under study do exhibit\nbiases but have generally improved as compared to BERT.\n",
                "链接": "https://arxiv.org/abs/2304.06861"
            },
            {
                "文章ID": "69100",
                "标题": "Large Language Models are Diverse Role-Players for Summarization\n  Evaluation",
                "作者": " Ning Wu,  Ming Gong,  Linjun Shou,  Shining Liang,  Daxin Jiang",
                "发布日期": "2023-09-20",
                "摘要": "  Text summarization has a wide range of applications in many scenarios. The\nevaluation of the quality of the generated text is a complex problem. A big\nchallenge to language evaluation is that there is a clear divergence between\nexisting metrics and human evaluation. A document summary's quality can be\nassessed by human annotators on various criteria, both objective ones like\ngrammar and correctness, and subjective ones like informativeness,\nsuccinctness, and appeal. Most of the automatic evaluation methods like\nBLUE/ROUGE may be not able to adequately capture the above dimensions. In this\npaper, we propose a new evaluation framework based on LLMs, which provides a\ncomprehensive evaluation framework by comparing generated text and reference\ntext from both objective and subjective aspects. First, we propose to model\nobjective and subjective dimensions of generated text based on roleplayers\nprompting mechanism. Furthermore, we introduce a context-based prompting\nmechanism that is able to generate dynamic roleplayer profiles based on input\ncontext. Finally, we design a multi-roleplayer prompting technology based on\nbatch prompting and integrate multiple outputs into the final evaluation\nresults. Experimental results on three real datasets for summarization show\nthat our model is highly competitive and has a very high consistency with human\nannotators.\n",
                "链接": "https://arxiv.org/abs/2303.15078"
            },
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "105928",
                "标题": "Can large language models provide useful feedback on research papers? A\n  large-scale empirical analysis",
                "作者": " Weixin Liang,  Yuhui Zhang,  Hancheng Cao,  Binglu Wang,  Daisy Ding,  Xinyu Yang,  Kailas Vodrahalli,  Siyu He,  Daniel Smith,  Yian Yin,  Daniel McFarland,  James Zou",
                "发布日期": "2023-10-04",
                "摘要": "  Expert feedback lays the foundation of rigorous research. However, the rapid\ngrowth of scholarly production and intricate knowledge specialization challenge\nthe conventional scientific feedback mechanisms. High-quality peer reviews are\nincreasingly difficult to obtain. Researchers who are more junior or from\nunder-resourced settings have especially hard times getting timely feedback.\nWith the breakthrough of large language models (LLM) such as GPT-4, there is\ngrowing interest in using LLMs to generate scientific feedback on research\nmanuscripts. However, the utility of LLM-generated feedback has not been\nsystematically studied. To address this gap, we created an automated pipeline\nusing GPT-4 to provide comments on the full PDFs of scientific papers. We\nevaluated the quality of GPT-4's feedback through two large-scale studies. We\nfirst quantitatively compared GPT-4's generated feedback with human peer\nreviewer feedback in 15 Nature family journals (3,096 papers in total) and the\nICLR machine learning conference (1,709 papers). The overlap in the points\nraised by GPT-4 and by human reviewers (average overlap 30.85% for Nature\njournals, 39.23% for ICLR) is comparable to the overlap between two human\nreviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The\noverlap between GPT-4 and human reviewers is larger for the weaker papers. We\nthen conducted a prospective user study with 308 researchers from 110 US\ninstitutions in the field of AI and computational biology to understand how\nresearchers perceive feedback generated by our GPT-4 system on their own\npapers. Overall, more than half (57.4%) of the users found GPT-4 generated\nfeedback helpful/very helpful and 82.4% found it more beneficial than feedback\nfrom at least some human reviewers. While our findings show that LLM-generated\nfeedback can help researchers, we also identify several limitations.\n",
                "链接": "https://arxiv.org/abs/2310.01783"
            },
            {
                "文章ID": "89744",
                "标题": "A Survey on Evaluation of Large Language Models",
                "作者": " Yupeng Chang,  Xu Wang,  Jindong Wang,  Yuan Wu,  Linyi Yang,  Kaijie Zhu,  Hao Chen,  Xiaoyuan Yi,  Cunxiang Wang,  Yidong Wang,  Wei Ye,  Yue Zhang,  Yi Chang,  Philip S. Yu,  Qiang Yang,  Xing Xie",
                "发布日期": "2023-10-18",
                "摘要": "  Large language models (LLMs) are gaining increasing popularity in both\nacademia and industry, owing to their unprecedented performance in various\napplications. As LLMs continue to play a vital role in both research and daily\nuse, their evaluation becomes increasingly critical, not only at the task\nlevel, but also at the society level for better understanding of their\npotential risks. Over the past years, significant efforts have been made to\nexamine LLMs from various perspectives. This paper presents a comprehensive\nreview of these evaluation methods for LLMs, focusing on three key dimensions:\nwhat to evaluate, where to evaluate, and how to evaluate. Firstly, we provide\nan overview from the perspective of evaluation tasks, encompassing general\nnatural language processing tasks, reasoning, medical usage, ethics,\neducations, natural and social sciences, agent applications, and other areas.\nSecondly, we answer the `where' and `how' questions by diving into the\nevaluation methods and benchmarks, which serve as crucial components in\nassessing performance of LLMs. Then, we summarize the success and failure cases\nof LLMs in different tasks. Finally, we shed light on several future challenges\nthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to\nresearchers in the realm of LLMs evaluation, thereby aiding the development of\nmore proficient LLMs. Our key point is that evaluation should be treated as an\nessential discipline to better assist the development of LLMs. We consistently\nmaintain the related open-source materials at:\nhttps://github.com/MLGroupJLU/LLM-eval-survey.\n",
                "链接": "https://arxiv.org/abs/2307.03109"
            },
            {
                "文章ID": "91534",
                "标题": "Federated Large Language Model: A Position Paper",
                "作者": " Chaochao Chen,  Xiaohua Feng,  Jun Zhou,  Jianwei Yin,  Xiaolin Zheng",
                "发布日期": "2023-07-19",
                "摘要": "  Large scale language models (LLM) have received significant attention and\nfound diverse applications across various domains, but their development\nencounters challenges in real-world scenarios. These challenges arise due to\nthe scarcity of public domain data availability and the need to maintain\nprivacy with respect to private domain data. To address these issues, federated\nlearning (FL) has emerged as a promising technology that enables collaborative\ntraining of shared models while preserving decentralized data. We propose the\nconcept of federated LLM, which comprises three key components, i.e., federated\nLLM pre-training, federated LLM fine-tuning, and federated LLM prompt\nengineering. For each component, we discuss its advantage over traditional LLM\ntraining methods and propose specific engineering strategies for\nimplementation. Furthermore, we explore the novel challenges introduced by the\nintegration of FL and LLM. We analyze existing solutions and identify potential\nobstacles faced by these solutions within the context of federated LLM.\n",
                "链接": "https://arxiv.org/abs/2307.08925"
            },
            {
                "文章ID": "78952",
                "标题": "Revisiting Automated Topic Model Evaluation with Large Language Models",
                "作者": " Dominik Stammbach,  Vilém Zouhar,  Alexander Hoyle,  Mrinmaya Sachan,  Elliott Ash",
                "发布日期": "2023-10-24",
                "摘要": "  Topic models are used to make sense of large text collections. However,\nautomatically evaluating topic model output and determining the optimal number\nof topics both have been longstanding challenges, with no effective automated\nsolutions to date. This paper proposes using large language models to evaluate\nsuch output. We find that large language models appropriately assess the\nresulting topics, correlating more strongly with human judgments than existing\nautomated metrics. We then investigate whether we can use large language models\nto automatically determine the optimal number of topics. We automatically\nassign labels to documents and choosing configurations with the most pure\nlabels returns reasonable values for the optimal number of topics.\n",
                "链接": "https://arxiv.org/abs/2305.12152"
            },
            {
                "文章ID": "72848",
                "标题": "An Evaluation on Large Language Model Outputs: Discourse and\n  Memorization",
                "作者": " Adrian de Wynter,  Xun Wang,  Alex Sokolov,  Qilong Gu,  Si-Qing Chen",
                "发布日期": "2023-07-06",
                "摘要": "  We present an empirical evaluation of various outputs generated by nine of\nthe most widely-available large language models (LLMs). Our analysis is done\nwith off-the-shelf, readily-available tools. We find a correlation between\npercentage of memorized text, percentage of unique text, and overall output\nquality, when measured with respect to output pathologies such as\ncounterfactual and logically-flawed statements, and general failures like not\nstaying on topic. Overall, 80.0% of the outputs evaluated contained memorized\ndata, but outputs containing the most memorized content were also more likely\nto be considered of high quality. We discuss and evaluate mitigation\nstrategies, showing that, in the models evaluated, the rate of memorized text\nbeing output is reduced. We conclude with a discussion on potential\nimplications around what it means to learn, to memorize, and to evaluate\nquality text.\n",
                "链接": "https://arxiv.org/abs/2304.08637"
            },
            {
                "文章ID": "115944",
                "标题": "CLEAN-EVAL: Clean Evaluation on Contaminated Large Language Models",
                "作者": " Wenhong Zhu,  Hongkun Hao,  Zhiwei He,  Yunze Song,  Yumeng Zhang,  Hanxu Hu,  Yiran Wei,  Rui Wang,  Hongyuan Lu",
                "发布日期": "2023-11-16",
                "摘要": "  We are currently in an era of fierce competition among various large language\nmodels (LLMs) continuously pushing the boundaries of benchmark performance.\nHowever, genuinely assessing the capabilities of these LLMs has become a\nchallenging and critical issue due to potential data contamination, and it\nwastes dozens of time and effort for researchers and engineers to download and\ntry those contaminated models. To save our precious time, we propose a novel\nand useful method, Clean-Eval, which mitigates the issue of data contamination\nand evaluates the LLMs in a cleaner manner. Clean-Eval employs an LLM to\nparaphrase and back-translate the contaminated data into a candidate set,\ngenerating expressions with the same meaning but in different surface forms. A\nsemantic detector is then used to filter the generated low-quality samples to\nnarrow down this candidate set. The best candidate is finally selected from\nthis set based on the BLEURT score. According to human assessment, this best\ncandidate is semantically similar to the original contamination data but\nexpressed differently. All candidates can form a new benchmark to evaluate the\nmodel. Our experiments illustrate that Clean-Eval substantially restores the\nactual evaluation results on contaminated LLMs under both few-shot learning and\nfine-tuning scenarios.\n",
                "链接": "https://arxiv.org/abs/2311.09154"
            },
            {
                "文章ID": "25181",
                "标题": "Evaluating the Impact of Source Code Parsers on ML4SE Models",
                "作者": " Ilya Utkin,  Egor Spirin,  Egor Bogomolov,  Timofey Bryksin",
                "发布日期": "2022-06-20",
                "摘要": "  As researchers and practitioners apply Machine Learning to increasingly more\nsoftware engineering problems, the approaches they use become more\nsophisticated. A lot of modern approaches utilize internal code structure in\nthe form of an abstract syntax tree (AST) or its extensions: path-based\nrepresentation, complex graph combining AST with additional edges. Even though\nthe process of extracting ASTs from code can be done with different parsers,\nthe impact of choosing a parser on the final model quality remains unstudied.\nMoreover, researchers often omit the exact details of extracting particular\ncode representations.\n  In this work, we evaluate two models, namely Code2Seq and TreeLSTM, in the\nmethod name prediction task backed by eight different parsers for the Java\nlanguage. To unify the process of data preparation with different parsers, we\ndevelop SuperParser, a multi-language parser-agnostic library based on\nPathMiner. SuperParser facilitates the end-to-end creation of datasets suitable\nfor training and evaluation of ML models that work with structural information\nfrom source code. Our results demonstrate that trees built by different parsers\nvary in their structure and content. We then analyze how this diversity affects\nthe models' quality and show that the quality gap between the most and least\nsuitable parsers for both models turns out to be significant. Finally, we\ndiscuss other features of the parsers that researchers and practitioners should\ntake into account when selecting a parser along with the impact on the models'\nquality.\n  The code of SuperParser is publicly available at\nhttps://doi.org/10.5281/zenodo.6366591. We also publish Java-norm, the dataset\nwe use to evaluate the models: https://doi.org/10.5281/zenodo.6366599.\n",
                "链接": "https://arxiv.org/abs/2206.08713"
            }
        ]
    },
    {
        "question": {
            "question": "近一个月与多模态大模型相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "91534",
                "标题": "Federated Large Language Model: A Position Paper",
                "作者": " Chaochao Chen,  Xiaohua Feng,  Jun Zhou,  Jianwei Yin,  Xiaolin Zheng",
                "发布日期": "2023-07-19",
                "摘要": "  Large scale language models (LLM) have received significant attention and\nfound diverse applications across various domains, but their development\nencounters challenges in real-world scenarios. These challenges arise due to\nthe scarcity of public domain data availability and the need to maintain\nprivacy with respect to private domain data. To address these issues, federated\nlearning (FL) has emerged as a promising technology that enables collaborative\ntraining of shared models while preserving decentralized data. We propose the\nconcept of federated LLM, which comprises three key components, i.e., federated\nLLM pre-training, federated LLM fine-tuning, and federated LLM prompt\nengineering. For each component, we discuss its advantage over traditional LLM\ntraining methods and propose specific engineering strategies for\nimplementation. Furthermore, we explore the novel challenges introduced by the\nintegration of FL and LLM. We analyze existing solutions and identify potential\nobstacles faced by these solutions within the context of federated LLM.\n",
                "链接": "https://arxiv.org/abs/2307.08925"
            },
            {
                "文章ID": "114068",
                "标题": "GLaMM: Pixel Grounding Large Multimodal Model",
                "作者": " Hanoona Rasheed,  Muhammad Maaz,  Sahal Shaji,  Abdelrahman Shaker,  Salman Khan,  Hisham Cholakkal,  Rao M. Anwer,  Erix Xing,  Ming-Hsuan Yang,  Fahad S. Khan",
                "发布日期": "2023-11-07",
                "摘要": "  Large Multimodal Models (LMMs) extend Large Language Models to the vision\ndomain. Initial efforts towards LMMs used holistic images and text prompts to\ngenerate ungrounded textual responses. Very recently, region-level LMMs have\nbeen used to generate visually grounded responses. However, they are limited to\nonly referring a single object category at a time, require users to specify the\nregions in inputs, or cannot offer dense pixel-wise object grounding. In this\nwork, we present Grounding LMM (GLaMM), the first model that can generate\nnatural language responses seamlessly intertwined with corresponding object\nsegmentation masks. GLaMM not only grounds objects appearing in the\nconversations but is flexible enough to accept both textual and optional visual\nprompts (region of interest) as input. This empowers users to interact with the\nmodel at various levels of granularity, both in textual and visual domains. Due\nto the lack of standard benchmarks for the novel setting of generating visually\ngrounded detailed conversations, we introduce a comprehensive evaluation\nprotocol with our curated grounded conversations. Our proposed Grounded\nConversation Generation (GCG) task requires densely grounded concepts in\nnatural scenes at a large-scale. To this end, we propose a densely annotated\nGrounding-anything Dataset (GranD) using our proposed automated annotation\npipeline that encompasses 7.5M unique concepts grounded in a total of 810M\nregions available with segmentation masks. Besides GCG, GLaMM also performs\neffectively on several downstream tasks e.g., referring expression\nsegmentation, image and region-level captioning and vision-language\nconversations. Project Page: https://mbzuai-oryx.github.io/groundingLMM.\n",
                "链接": "https://arxiv.org/abs/2311.03356"
            },
            {
                "文章ID": "100082",
                "标题": "Large AI Model Empowered Multimodal Semantic Communications",
                "作者": " Feibo Jiang,  Yubo Peng,  Li Dong,  Kezhi Wang,  Kun Yang,  Cunhua Pan,  Xiaohu You",
                "发布日期": "2023-09-06",
                "摘要": "  Multimodal signals, including text, audio, image and video, can be integrated\ninto Semantic Communication (SC) for providing an immersive experience with low\nlatency and high quality at the semantic level. However, the multimodal SC has\nseveral challenges, including data heterogeneity, semantic ambiguity, and\nsignal fading. Recent advancements in large AI models, particularly in\nMultimodal Language Model (MLM) and Large Language Model (LLM), offer potential\nsolutions for these issues. To this end, we propose a Large AI Model-based\nMultimodal SC (LAM-MSC) framework, in which we first present the MLM-based\nMultimodal Alignment (MMA) that utilizes the MLM to enable the transformation\nbetween multimodal and unimodal data while preserving semantic consistency.\nThen, a personalized LLM-based Knowledge Base (LKB) is proposed, which allows\nusers to perform personalized semantic extraction or recovery through the LLM.\nThis effectively addresses the semantic ambiguity. Finally, we apply the\nConditional Generative adversarial networks-based channel Estimation (CGE) to\nobtain Channel State Information (CSI). This approach effectively mitigates the\nimpact of fading channels in SC. Finally, we conduct simulations that\ndemonstrate the superior performance of the LAM-MSC framework.\n",
                "链接": "https://arxiv.org/abs/2309.01249"
            },
            {
                "文章ID": "108480",
                "标题": "Multimodal Large Language Model for Visual Navigation",
                "作者": " Yao-Hung Hubert Tsai,  Vansh Dhar,  Jialu Li,  Bowen Zhang,  Jian Zhang",
                "发布日期": "2023-11-07",
                "摘要": "  Recent efforts to enable visual navigation using large language models have\nmainly focused on developing complex prompt systems. These systems incorporate\ninstructions, observations, and history into massive text prompts, which are\nthen combined with pre-trained large language models to facilitate visual\nnavigation. In contrast, our approach aims to fine-tune large language models\nfor visual navigation without extensive prompt engineering. Our design involves\na simple text prompt, current observations, and a history collector model that\ngathers information from previous observations as input. For output, our design\nprovides a probability distribution of possible actions that the agent can take\nduring navigation. We train our model using human demonstrations and collision\nsignals from the Habitat-Matterport 3D Dataset (HM3D). Experimental results\ndemonstrate that our method outperforms state-of-the-art behavior cloning\nmethods and effectively reduces collision rates.\n",
                "链接": "https://arxiv.org/abs/2310.08669"
            },
            {
                "文章ID": "120164",
                "标题": "PixelLM: Pixel Reasoning with Large Multimodal Model",
                "作者": " Zhongwei Ren,  Zhicheng Huang,  Yunchao Wei,  Yao Zhao,  Dongmei Fu,  Jiashi Feng,  Xiaojie Jin",
                "发布日期": "2023-12-06",
                "摘要": "  While large multimodal models (LMMs) have achieved remarkable progress,\ngenerating pixel-level masks for image reasoning tasks involving multiple\nopen-world targets remains a challenge. To bridge this gap, we introduce\nPixelLM, an effective and efficient LMM for pixel-level reasoning and\nunderstanding. Central to PixelLM is a novel, lightweight pixel decoder and a\ncomprehensive segmentation codebook. The decoder efficiently produces masks\nfrom the hidden embeddings of the codebook tokens, which encode detailed\ntarget-relevant information. With this design, PixelLM harmonizes with the\nstructure of popular LMMs and avoids the need for additional costly\nsegmentation models. Furthermore, we propose a target refinement loss to\nenhance the model's ability to differentiate between multiple targets, leading\nto substantially improved mask quality. To advance research in this area, we\nconstruct MUSE, a high-quality multi-target reasoning segmentation benchmark.\nPixelLM excels across various pixel-level image reasoning and understanding\ntasks, outperforming well-established methods in multiple benchmarks, including\nMUSE, single- and multi-referring segmentation. Comprehensive ablations confirm\nthe efficacy of each proposed component. All code, models, and datasets will be\npublicly available.\n",
                "链接": "https://arxiv.org/abs/2312.02228"
            },
            {
                "文章ID": "107503",
                "标题": "An evolutionary model of personality traits related to cooperative\n  behavior using a large language model",
                "作者": " Reiji Suzuki,  Takaya Arita",
                "发布日期": "2023-10-11",
                "摘要": "  This paper aims to shed light on the evolutionary dynamics of diverse and\nsocial populations by introducing the rich expressiveness of generative models\ninto the trait expression of social agent-based evolutionary models.\nSpecifically, we focus on the evolution of personality traits in the context of\na game-theoretic relationship as a situation in which inter-individual\ninterests exert strong selection pressures. We construct an agent model in\nwhich linguistic descriptions of personality traits related to cooperative\nbehavior are used as genes. The deterministic strategies extracted from Large\nLanguage Model (LLM) that make behavioral decisions based on these personality\ntraits are used as behavioral traits. The population is evolved according to\nselection based on average payoff and mutation of genes by asking LLM to\nslightly modify the parent gene toward cooperative or selfish. Through\npreliminary experiments and analyses, we clarify that such a model can indeed\nexhibit the evolution of cooperative behavior based on the diverse and\nhigher-order representation of personality traits. We also observed the\nrepeated intrusion of cooperative and selfish personality traits through\nchanges in the expression of personality traits, and found that the emerging\nwords in the evolved gene well reflected the behavioral tendency of its\npersonality in terms of their semantics.\n",
                "链接": "https://arxiv.org/abs/2310.05976"
            },
            {
                "文章ID": "118536",
                "标题": "LLMGA: Multimodal Large Language Model based Generation Assistant",
                "作者": " Bin Xia,  Shiyin Wang,  Yingfan Tao,  Yitong Wang,  Jiaya Jia",
                "发布日期": "2023-12-13",
                "摘要": "  In this paper, we introduce a Multimodal Large Language Model-based\nGeneration Assistant (LLMGA), leveraging the vast reservoir of knowledge and\nproficiency in reasoning, comprehension, and response inherent in Large\nLanguage Models (LLMs) to assist users in image generation and editing.\nDiverging from existing approaches where Multimodal Large Language Models\n(MLLMs) generate fixed-size embeddings to control Stable Diffusion (SD), our\nLLMGA provides a detailed language generation prompt for precise control over\nSD. This not only augments LLM context understanding but also reduces noise in\ngeneration prompts, yields images with more intricate and precise content, and\nelevates the interpretability of the network. To this end, we curate a\ncomprehensive dataset comprising prompt refinement, similar image generation,\ninpainting $\\&$ outpainting, and visual question answering. Moreover, we\npropose a two-stage training scheme. In the first stage, we train the MLLM to\ngrasp the properties of image generation and editing, enabling it to generate\ndetailed prompts. In the second stage, we optimize SD to align with the MLLM's\ngeneration prompts. Additionally, we propose a reference-based restoration\nnetwork to alleviate texture, brightness, and contrast disparities between\ngenerated and preserved regions during image editing. Extensive results show\nthat LLMGA has promising generative capabilities and can enable wider\napplications in an interactive manner.\n",
                "链接": "https://arxiv.org/abs/2311.16500"
            },
            {
                "文章ID": "125173",
                "标题": "A Large Language Model-based Computational Approach to Improve\n  Identity-Related Write-Ups",
                "作者": " Alex Doboli",
                "发布日期": "2023-12-29",
                "摘要": "  Creating written products is essential to modern life, including writings\nabout one's identity and personal experiences. However, writing is often a\ndifficult activity that requires extensive effort to frame the central ideas,\nthe pursued approach to communicate the central ideas, e.g., using analogies,\nmetaphors, or other possible means, the needed presentation structure, and the\nactual verbal expression. Large Language Models, a recently emerged approach in\nMachine Learning, can offer a significant help in reducing the effort and\nimproving the quality of written products. This paper proposes a new\ncomputational approach to explore prompts that given as inputs to a Large\nLanguage Models can generate cues to improve the considered written products.\nTwo case studies on improving write-ups, one based on an analogy and one on a\nmetaphor, are also presented in the paper.\n",
                "链接": "https://arxiv.org/abs/2312.16659"
            },
            {
                "文章ID": "77518",
                "标题": "Parameter-efficient Tuning of Large-scale Multimodal Foundation Model",
                "作者": " Haixin Wang,  Xinlong Yang,  Jianlong Chang,  Dian Jin,  Jinan Sun,  Shikun Zhang,  Xiao Luo,  Qi Tian",
                "发布日期": "2023-10-31",
                "摘要": "  Driven by the progress of large-scale pre-training, parameter-efficient\ntransfer learning has gained immense popularity across different subfields of\nArtificial Intelligence. The core is to adapt the model to downstream tasks\nwith only a small set of parameters. Recently, researchers have leveraged such\nproven techniques in multimodal tasks and achieve promising results. However,\ntwo critical issues remain unresolved: how to further reduce the complexity\nwith lightweight design and how to boost alignment between modalities under\nextremely low parameters. In this paper, we propose A graceful prompt framework\nfor cross-modal transfer (Aurora) to overcome these challenges. Considering the\nredundancy in existing architectures, we first utilize the mode approximation\nto generate 0.1M trainable parameters to implement the multimodal prompt\ntuning, which explores the low intrinsic dimension with only 0.04% parameters\nof the pre-trained model. Then, for better modality alignment, we propose the\nInformative Context Enhancement and Gated Query Transformation module under\nextremely few parameters scenes. A thorough evaluation on six cross-modal\nbenchmarks shows that it not only outperforms the state-of-the-art but even\noutperforms the full fine-tuning approach. Our code is available at:\nhttps://github.com/WillDreamer/Aurora.\n",
                "链接": "https://arxiv.org/abs/2305.08381"
            },
            {
                "文章ID": "121884",
                "标题": "Hallucination Augmented Contrastive Learning for Multimodal Large\n  Language Model",
                "作者": " Chaoya Jiang,  Haiyang Xu,  Mengfan Dong,  Jiaxing Chen,  Wei Ye,  Ming Yan,  Qinghao Ye,  Ji Zhang,  Fei Huang,  Shikun Zhang",
                "发布日期": "2023-12-14",
                "摘要": "  Multi-modal large language models (MLLMs) have been shown to efficiently\nintegrate natural language with visual information to handle multi-modal tasks.\nHowever, MLLMs still face a fundamental limitation of hallucinations, where\nthey tend to generate erroneous or fabricated information. In this paper, we\naddress hallucinations in MLLMs from a novel perspective of representation\nlearning. We first analyzed the representation distribution of textual and\nvisual tokens in MLLM, revealing two important findings: 1) there is a\nsignificant gap between textual and visual representations, indicating\nunsatisfactory cross-modal representation alignment; 2) representations of\ntexts that contain and do not contain hallucinations are entangled, making it\nchallenging to distinguish them. These two observations inspire us with a\nsimple yet effective method to mitigate hallucinations. Specifically, we\nintroduce contrastive learning into MLLMs and use text with hallucination as\nhard negative examples, naturally bringing representations of non-hallucinative\ntext and visual samples closer while pushing way representations of\nnon-hallucinating and hallucinative text. We evaluate our method quantitatively\nand qualitatively, showing its effectiveness in reducing hallucination\noccurrences and improving performance across multiple benchmarks. On the\nMMhal-Bench benchmark, our method obtains a 34.66% /29.5% improvement over the\nbaseline MiniGPT-4/LLaVA.\n",
                "链接": "https://arxiv.org/abs/2312.06968"
            }
        ]
    },
    {
        "question": {
            "question": "查找近六个月工具学习评测数据集的论文",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找一下工具评测相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "24473",
                "标题": "An analysis of retracted papers in Computer Science",
                "作者": " Martin Shepperd,  Leila Yousefi",
                "发布日期": "2023-07-19",
                "摘要": "  Context: The retraction of research papers, for whatever reason, is a growing\nphenomenon. However, although retracted paper information is publicly available\nvia publishers, it is somewhat distributed and inconsistent. Objective: The aim\nis to assess: (i) the extent and nature of retracted research in Computer\nScience (CS) (ii) the post-retraction citation behaviour of retracted works and\n(iii) the potential impact on systematic reviews and mapping studies. Method:\nWe analyse the Retraction Watch database and take citation information from the\nWeb of Science and Google scholar. Results: We find that of the 33,955 entries\nin the Retraction watch database (16 May 2022), 2,816 are classified as CS,\ni.e., approximately 8.3%. For CS, 56% of retracted papers, provide little or no\ninformation as to the reasons. This contrasts with 26% for other disciplines.\nThere is also a remarkable disparity between different publishers, a tendency\nfor multiple versions of a retracted paper over and above the Version of Record\n(VoR), and for new citations long after a paper is officially retracted.\nConclusions: Unfortunately retraction seems to be a sufficiently common outcome\nfor a scientific paper that we as a research community need to take it more\nseriously, e.g., standardising procedures and taxonomies across publishers and\nthe provision of appropriate research tools. Finally, we recommend particular\ncaution when undertaking secondary analyses and meta-analyses which are at risk\nof becoming contaminated by these problem primary studies.\n",
                "链接": "https://arxiv.org/abs/2206.06706"
            },
            {
                "文章ID": "73689",
                "标题": "Exploring the Use of ChatGPT as a Tool for Learning and Assessment in\n  Undergraduate Computer Science Curriculum: Opportunities and Challenges",
                "作者": " Basit Qureshi",
                "发布日期": "2023-04-25",
                "摘要": "  The application of Artificial intelligence for teaching and learning in the\nacademic sphere is a trending subject of interest in the computing education.\nChatGPT, as an AI-based tool, provides various advantages, such as heightened\nstudent involvement, cooperation, accessibility and availability. This paper\naddresses the prospects and obstacles associated with utilizing ChatGPT as a\ntool for learning and assessment in undergraduate Computer Science curriculum\nin particular to teaching and learning fundamental programming courses.\nStudents having completed the course work for a Data Structures and Algorithms\n(a sophomore level course) participated in this study. Two groups of students\nwere given programming challenges to solve within a short period of time. The\ncontrol group (group A) had access to text books and notes of programming\ncourses, however no Internet access was provided. Group B students were given\naccess to ChatGPT and were encouraged to use it to help solve the programming\nchallenges. The challenge was conducted in a computer lab environment using PC2\nenvironment. Each team of students address the problem by writing executable\ncode that satisfies certain number of test cases. Student teams were scored\nbased on their performance in terms of number of successful passed testcases.\nResults show that students using ChatGPT had an advantage in terms of earned\nscores, however there were inconsistencies and inaccuracies in the submitted\ncode consequently affecting the overall performance. After a thorough analysis,\nthe paper's findings indicate that incorporating AI in higher education brings\nabout various opportunities and challenges.\n",
                "链接": "https://arxiv.org/abs/2304.11214"
            },
            {
                "文章ID": "26762",
                "标题": "A Theoretical Computer Science Perspective on Free Will",
                "作者": " Lenore Blum,  Manuel Blum",
                "发布日期": "2022-12-27",
                "摘要": "  We consider the paradoxical concept of free will from the perspective of\nTheoretical Computer Science (TCS), a branch of mathematics concerned with\nunderstanding the underlying principles of computation and complexity,\nincluding the implications and surprising consequences of resource limitations.\n",
                "链接": "https://arxiv.org/abs/2206.13942"
            },
            {
                "文章ID": "86386",
                "标题": "TOBY: A Tool for Exploring Data in Academic Survey Papers",
                "作者": " Tathagata Chakraborti,  Jungkoo Kang,  Christian Muise,  Sarath Sreedharan,  Michael Walker,  Daniel Szafir,  Tom Williams",
                "发布日期": "2023-06-21",
                "摘要": "  This paper describes TOBY, a visualization tool that helps a user explore the\ncontents of an academic survey paper. The visualization consists of four\ncomponents: a hierarchical view of taxonomic data in the survey, a document\nsimilarity view in the space of taxonomic classes, a network view of citations,\nand a new paper recommendation tool. In this paper, we will discuss these\nfeatures in the context of three separate deployments of the tool.\n",
                "链接": "https://arxiv.org/abs/2306.10051"
            },
            {
                "文章ID": "67095",
                "标题": "ChatGPT Participates in a Computer Science Exam",
                "作者": " Sebastian Bordt,  Ulrike von Luxburg",
                "发布日期": "2023-03-23",
                "摘要": "  We asked ChatGPT to participate in an undergraduate computer science exam on\n''Algorithms and Data Structures''. The program was evaluated on the entire\nexam as posed to the students. We hand-copied its answers onto an exam sheet,\nwhich was subsequently graded in a blind setup alongside those of 200\nparticipating students. We find that ChatGPT narrowly passed the exam,\nobtaining 20.5 out of 40 points. This impressive performance indicates that\nChatGPT can indeed succeed in challenging tasks like university exams. At the\nsame time, the questions in our exam are structurally similar to those of other\nexams, solved homework problems, and teaching materials that can be found\nonline and might have been part of ChatGPT's training data. Therefore, it would\nbe inadequate to conclude from this experiment that ChatGPT has any\nunderstanding of computer science. We also assess the improvements brought by\nGPT-4. We find that GPT-4 would have obtained about 17\\% more exam points than\nGPT-3.5, reaching the performance of the average student. The transcripts of\nour conversations with ChatGPT are available at\n\\url{https://github.com/tml-tuebingen/chatgpt-algorithm-exam}, and the entire\ngraded exam is in the appendix of this paper.\n",
                "链接": "https://arxiv.org/abs/2303.09461"
            },
            {
                "文章ID": "37491",
                "标题": "A Survey on the application of Data Science And Analytics in the field\n  of Organised Sports",
                "作者": " Sachin Kumar S,  Prithvi HV,  C Nandini",
                "发布日期": "2022-09-19",
                "摘要": "  The application of Data Science and Analytics to optimize or predict outcomes\nis Ubiquitous in the Modern World. Data Science and Analytics have optimized\nalmost every domain that exists in the market. In our survey, we focus on how\nthe field of Analytics has been adopted in the field of sports, and how it has\ncontributed to the transformation of the game right from the assessment of\non-field players and their selection to the prediction of winning team and to\nthe marketing of tickets and business aspects of big sports tournaments. We\nwill present the analytical tools, algorithms, and methodologies adopted in the\nfield of Sports Analytics for different sports and also present our views on\nthe same and we will also compare and contrast these existing approaches. By\ndoing so, we will also present the best tools, algorithms, and analytical\nmethodologies to be considered by anyone who is looking to experiment with\nsports data and analyze various aspects of the game.\n",
                "链接": "https://arxiv.org/abs/2209.07528"
            },
            {
                "文章ID": "12012",
                "标题": "Computer Science Named Entity Recognition in the Open Research Knowledge\n  Graph",
                "作者": " Jennifer D'Souza,  Sören Auer",
                "发布日期": "2022-11-15",
                "摘要": "  Domain-specific named entity recognition (NER) on Computer Science (CS)\nscholarly articles is an information extraction task that is arguably more\nchallenging for the various annotation aims that can beset the task and has\nbeen less studied than NER in the general domain. Given that significant\nprogress has been made on NER, we believe that scholarly domain-specific NER\nwill receive increasing attention in the years to come. Currently, progress on\nCS NER -- the focus of this work -- is hampered in part by its recency and the\nlack of a standardized annotation aim for scientific entities/terms. This work\nproposes a standardized task by defining a set of seven contribution-centric\nscholarly entities for CS NER viz., research problem, solution, resource,\nlanguage, tool, method, and dataset. Following which, its main contributions\nare: combines existing CS NER resources that maintain their annotation focus on\nthe set or subset of contribution-centric scholarly entities we consider;\nfurther, noting the need for big data to train neural NER models, this work\nadditionally supplies thousands of contribution-centric entity annotations from\narticle titles and abstracts, thus releasing a cumulative large novel resource\nfor CS NER; and, finally, trains a sequence labeling CS NER model inspired\nafter state-of-the-art neural architectures from the general domain NER task.\nThroughout the work, several practical considerations are made which can be\nuseful to information technology designers of the digital libraries.\n",
                "链接": "https://arxiv.org/abs/2203.14579"
            },
            {
                "文章ID": "51683",
                "标题": "Analyzing the State of Computer Science Research with the DBLP Discovery\n  Dataset",
                "作者": " Lennart Küll",
                "发布日期": "2022-12-02",
                "摘要": "  The number of scientific publications continues to rise exponentially,\nespecially in Computer Science (CS). However, current solutions to analyze\nthose publications restrict access behind a paywall, offer no features for\nvisual analysis, limit access to their data, only focus on niches or\nsub-fields, and/or are not flexible and modular enough to be transferred to\nother datasets. In this thesis, we conduct a scientometric analysis to uncover\nthe implicit patterns hidden in CS metadata and to determine the state of CS\nresearch. Specifically, we investigate trends of the quantity, impact, and\ntopics for authors, venues, document types (conferences vs. journals), and\nfields of study (compared to, e.g., medicine). To achieve this we introduce the\nCS-Insights system, an interactive web application to analyze CS publications\nwith various dashboards, filters, and visualizations. The data underlying this\nsystem is the DBLP Discovery Dataset (D3), which contains metadata from 5\nmillion CS publications. Both D3 and CS-Insights are open-access, and\nCS-Insights can be easily adapted to other datasets in the future. The most\ninteresting findings of our scientometric analysis include that i) there has\nbeen a stark increase in publications, authors, and venues in the last two\ndecades, ii) many authors only recently joined the field, iii) the most cited\nauthors and venues focus on computer vision and pattern recognition, while the\nmost productive prefer engineering-related topics, iv) the preference of\nresearchers to publish in conferences over journals dwindles, v) on average,\njournal articles receive twice as many citations compared to conference papers,\nbut the contrast is much smaller for the most cited conferences and journals,\nand vi) journals also get more citations in all other investigated fields of\nstudy, while only CS and engineering publish more in conferences than journals.\n",
                "链接": "https://arxiv.org/abs/2212.00629"
            },
            {
                "文章ID": "19420",
                "标题": "CurFi: An automated tool to find the best regression analysis model\n  using curve fitting",
                "作者": " Ayon Roy,  Tausif Al Zubayer,  Nafisa Tabassum,  Muhammad Nazrul Islam,  Md. Abdus Sattar",
                "发布日期": "2022-05-17",
                "摘要": "  Regression analysis is a well known quantitative research method that\nprimarily explores the relationship between one or more independent variables\nand a dependent variable. Conducting regression analysis manually on large\ndatasets with multiple independent variables can be tedious. An automated\nsystem for regression analysis will be of great help for researchers as well as\nnon-expert users. Thus, the objective of this research is to design and develop\nan automated curve fitting system. As outcome, a curve fitting system named\n\"CurFi\" was developed that uses linear regression models to fit a curve to a\ndataset and to find out the best fit model. The system facilitates to upload a\ndataset, split the dataset into training set and test set, select relevant\nfeatures and label from the dataset; and the system will return the best fit\nlinear regression model after training is completed. The developed tool would\nbe a great resource for the users having limited technical knowledge who will\nalso be able to find the best fit regression model for a dataset using the\ndeveloped \"CurFi\" system.\n",
                "链接": "https://arxiv.org/abs/2205.07804"
            },
            {
                "文章ID": "103091",
                "标题": "Field evaluation of a mobile app for assisting blind and visually\n  impaired travelers to find bus stops",
                "作者": " Shrinivas Pundlik,  Prerana Shivshanker,  Tim Traut-Savino,  Gang Luo",
                "发布日期": "2023-09-21",
                "摘要": "  Purpose: It is reported that there can be considerable gaps due to GPS\ninaccuracy and mapping errors if blind and visually impaired (BVI) travelers\nrely on digital maps to go to their desired bus stops. We evaluated the ability\nof a mobile app, All_Aboard, to guide BVI travelers precisely to the bus-stops.\nMethods: The All_Aboard app detected bus-stop signs in real-time via smartphone\ncamera using a neural network model, and provided distance coded audio feedback\nto help localize the detected sign. BVI individuals used the All_Aboard and\nGoogle Maps app to localize 10 bus-stop locations in Boston downtown and\nanother 10 in a sub-urban area. For each bus stop, the subjects used the apps\nto navigate as close as possible to the physical bus-stop sign, starting from\n30 to 50 meters away. The outcome measures were success rate and gap distance\nbetween the app-indicated location and the actual physical location of the bus\nstop. Results: The study was conducted with 24 legally blind participants (mean\nage [SD]: 51[14] years; 11 (46%) Female). The success rate of the All_Aboard\napp (91%) was significantly higher than the Google Maps (52%, p<0.001). The gap\ndistance when using the All_Aboard app was significantly lower (mean [95%CI]:\n1.8 [1.2-2.3] meters) compared to the Google Maps (7 [6.5-7.5] meters;\np<0.001). Conclusion: The All_Aboard app localizes bus stops more accurately\nand reliably than GPS-based smartphone navigation options in real-world\nenvironments.\n",
                "链接": "https://arxiv.org/abs/2309.10940"
            }
        ]
    },
    {
        "question": {
            "question": "大模型在游戏方面的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "6997",
                "标题": "Matching Papers and Reviewers at Large Conferences",
                "作者": " Kevin Leyton-Brown,   Mausam,  Yatin Nandwani,  Hedayat Zarkoob,  Chris Cameron,  Neil Newman,  Dinesh Raghu",
                "发布日期": "2022-08-08",
                "摘要": "  Peer-reviewed conferences, the main publication venues in CS, rely critically\non matching highly qualified reviewers for each paper. Because of the growing\nscale of these conferences, the tight timelines on which they operate, and a\nrecent surge in explicitly dishonest behavior, there is now no alternative to\nperforming this matching in an automated way. This paper studies a novel\nreviewer-paper matching approach that was recently deployed in the 35th AAAI\nConference on Artificial Intelligence (AAAI 2021), and has since been adopted\n(wholly or partially) by other conferences including ICML 2022, AAAI 2022, and\nIJCAI 2022. This approach has three main elements: (1) collecting and\nprocessing input data to identify problematic matches and generate\nreviewer-paper scores; (2) formulating and solving an optimization problem to\nfind good reviewer-paper matchings; and (3) a two-phase reviewing process that\nshifts reviewing resources away from papers likely to be rejected and towards\npapers closer to the decision boundary. This paper also describes an evaluation\nof these innovations based on an extensive post-hoc analysis on real data --\nincluding a comparison with the matching algorithm used in AAAI's previous\n(2020) iteration -- and supplements this with additional numerical\nexperimentation.\n",
                "链接": "https://arxiv.org/abs/2202.12273"
            },
            {
                "文章ID": "93368",
                "标题": "ArcGPT: A Large Language Model Tailored for Real-world Archival\n  Applications",
                "作者": " Shitou Zhang,  Jingrui Hou,  Siyuan Peng,  Zuchao Li,  Qibiao Hu,  Ping Wang",
                "发布日期": "2023-07-28",
                "摘要": "  Archives play a crucial role in preserving information and knowledge, and the\nexponential growth of such data necessitates efficient and automated tools for\nmanaging and utilizing archive information resources. Archival applications\ninvolve managing massive data that are challenging to process and analyze.\nAlthough LLMs have made remarkable progress in diverse domains, there are no\npublicly available archives tailored LLM. Addressing this gap, we introduce\nArcGPT, to our knowledge, the first general-purpose LLM tailored to the\narchival field. To enhance model performance on real-world archival tasks,\nArcGPT has been pre-trained on massive and extensive archival domain data.\nAlongside ArcGPT, we release AMBLE, a benchmark comprising four real-world\narchival tasks. Evaluation on AMBLE shows that ArcGPT outperforms existing\nstate-of-the-art models, marking a substantial step forward in effective\narchival data management. Ultimately, ArcGPT aims to better serve the archival\ncommunity, aiding archivists in their crucial role of preserving and harnessing\nour collective information and knowledge.\n",
                "链接": "https://arxiv.org/abs/2307.14852"
            },
            {
                "文章ID": "66434",
                "标题": "HCI Papers Cite HCI Papers, Increasingly So",
                "作者": " Xiang 'Anthony' Chen",
                "发布日期": "2023-03-15",
                "摘要": "  We propose X-index -- the proportion of papers' citations coming from outside\ntheir research field -- and use this metric to analyze citations of CHI, UIST,\nand CSCW papers between 2010 and 2022. We found an overall decreasing X-index\nby several measures, indicating that HCI papers have been more and more likely\nto be cited by HCI papers rather than by non-HCI papers.\n",
                "链接": "https://arxiv.org/abs/2303.07539"
            },
            {
                "文章ID": "103034",
                "标题": "Interactive Distillation of Large Single-Topic Corpora of Scientific\n  Papers",
                "作者": " Nicholas Solovyev,  Ryan Barron,  Manish Bhattarai,  Maksim E. Eren,  Kim O. Rasmussen,  Boian S. Alexandrov",
                "发布日期": "2023-09-20",
                "摘要": "  Highly specific datasets of scientific literature are important for both\nresearch and education. However, it is difficult to build such datasets at\nscale. A common approach is to build these datasets reductively by applying\ntopic modeling on an established corpus and selecting specific topics. A more\nrobust but time-consuming approach is to build the dataset constructively in\nwhich a subject matter expert (SME) handpicks documents. This method does not\nscale and is prone to error as the dataset grows. Here we showcase a new tool,\nbased on machine learning, for constructively generating targeted datasets of\nscientific literature. Given a small initial \"core\" corpus of papers, we build\na citation network of documents. At each step of the citation network, we\ngenerate text embeddings and visualize the embeddings through dimensionality\nreduction. Papers are kept in the dataset if they are \"similar\" to the core or\nare otherwise pruned through human-in-the-loop selection. Additional insight\ninto the papers is gained through sub-topic modeling using SeNMFk. We\ndemonstrate our new tool for literature review by applying it to two different\nfields in machine learning.\n",
                "链接": "https://arxiv.org/abs/2309.10772"
            },
            {
                "文章ID": "92138",
                "标题": "Topics, Authors, and Networks in Large Language Model Research: Trends\n  from a Survey of 17K arXiv Papers",
                "作者": " Rajiv Movva,  Sidhika Balachandar,  Kenny Peng,  Gabriel Agostini,  Nikhil Garg,  Emma Pierson",
                "发布日期": "2023-10-24",
                "摘要": "  Large language model (LLM) research is dramatically impacting society, making\nit essential to understand the topics and values it prioritizes, the authors\nand institutions driving it, and its networks of collaboration. Due to the\nrecent growth of the field, many of these fundamental attributes lack\nsystematic description. We gather, annotate, and analyze a new dataset of\n16,979 LLM-related arXiv papers, focusing on changes in 2023 vs. 2018-2022. We\nshow that LLM research increasingly focuses on societal impacts: the Computers\nand Society sub-arXiv has seen 20x growth in its proportion of LLM-related\npapers in 2023. This change is driven in part by an influx of new authors: a\nmajority of 2023 papers are first-authored by researchers who have not\npreviously written an LLM-related paper, and these papers focus particularly on\napplications and societal considerations. While a handful of companies hold\noutsize influence, academia publishes a much larger fraction of papers than\nindustry overall, and this gap widens in 2023. LLM research is also being\nshaped by social dynamics: there are gender and academic/industry differences\nin the topics authors prioritize, and a stark U.S./China schism in the\ncollaboration network. Overall, our analysis documents how LLM research both\nshapes and is shaped by society, attesting to the necessity of sociotechnical\nlenses; we discuss implications for researchers and policymakers.\n",
                "链接": "https://arxiv.org/abs/2307.10700"
            },
            {
                "文章ID": "103222",
                "标题": "Dynamic Pricing of Applications in Cloud Marketplaces using Game Theory",
                "作者": " Safiye Ghasemi,  Mohammad Reza Meybodi,  Mehdi Dehghan Takht-Fooladi,  Amir Masoud Rahmani",
                "发布日期": "2023-09-21",
                "摘要": "  The competitive nature of Cloud marketplaces as new concerns in delivery of\nservices makes the pricing policies a crucial task for firms. so that, pricing\nstrategies has recently attracted many researchers. Since game theory can\nhandle such competing well this concern is addressed by designing a normal form\ngame between providers in current research. A committee is considered in which\nproviders register for improving their competition based pricing policies. The\nfunctionality of game theory is applied to design dynamic pricing policies. The\nusage of the committee makes the game a complete information one, in which each\nplayer is aware of every others payoff functions. The players enhance their\npricing policies to maximize their profits. The contribution of this paper is\nthe quantitative modeling of Cloud marketplaces in form of a game to provide\nnovel dynamic pricing strategies; the model is validated by proving the\nexistence and the uniqueness of Nash equilibrium of the game.\n",
                "链接": "https://arxiv.org/abs/2309.11316"
            },
            {
                "文章ID": "106983",
                "标题": "Kawaii Game Vocalics: A Preliminary Model",
                "作者": " Katie Seaborn,  Katja Rogers,  Somang Name,  Miu Kojima",
                "发布日期": "2023-10-10",
                "摘要": "  Kawaii is the Japanese concept of cute++, a global export with local\ncharacteristics. Recent work has explored kawaii as a feature of user\nexperience (UX) with social robots, virtual characters, and voice assistants,\ni.e., kawaii vocalics. Games have a long history of incorporating characters\nthat use voice as a means of expressing kawaii. Nevertheless, no work to date\nhas evaluated kawaii game voices or mapped out a model of kawaii game vocalics.\nIn this work, we explored whether and how a model of kawaii vocalics maps onto\ngame character voices. We conducted an online perceptions study (N=157) using\n18 voices from kawaii characters in Japanese games. We replicated the results\nfor computer voice and discovered nuanced relationships between gender and age,\nespecially youthfulness, agelessness, gender ambiguity, and gender neutrality.\nWe provide our initial model and advocate for future work on character visuals\nand within play contexts.\n",
                "链接": "https://arxiv.org/abs/2310.04731"
            },
            {
                "文章ID": "105928",
                "标题": "Can large language models provide useful feedback on research papers? A\n  large-scale empirical analysis",
                "作者": " Weixin Liang,  Yuhui Zhang,  Hancheng Cao,  Binglu Wang,  Daisy Ding,  Xinyu Yang,  Kailas Vodrahalli,  Siyu He,  Daniel Smith,  Yian Yin,  Daniel McFarland,  James Zou",
                "发布日期": "2023-10-04",
                "摘要": "  Expert feedback lays the foundation of rigorous research. However, the rapid\ngrowth of scholarly production and intricate knowledge specialization challenge\nthe conventional scientific feedback mechanisms. High-quality peer reviews are\nincreasingly difficult to obtain. Researchers who are more junior or from\nunder-resourced settings have especially hard times getting timely feedback.\nWith the breakthrough of large language models (LLM) such as GPT-4, there is\ngrowing interest in using LLMs to generate scientific feedback on research\nmanuscripts. However, the utility of LLM-generated feedback has not been\nsystematically studied. To address this gap, we created an automated pipeline\nusing GPT-4 to provide comments on the full PDFs of scientific papers. We\nevaluated the quality of GPT-4's feedback through two large-scale studies. We\nfirst quantitatively compared GPT-4's generated feedback with human peer\nreviewer feedback in 15 Nature family journals (3,096 papers in total) and the\nICLR machine learning conference (1,709 papers). The overlap in the points\nraised by GPT-4 and by human reviewers (average overlap 30.85% for Nature\njournals, 39.23% for ICLR) is comparable to the overlap between two human\nreviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The\noverlap between GPT-4 and human reviewers is larger for the weaker papers. We\nthen conducted a prospective user study with 308 researchers from 110 US\ninstitutions in the field of AI and computational biology to understand how\nresearchers perceive feedback generated by our GPT-4 system on their own\npapers. Overall, more than half (57.4%) of the users found GPT-4 generated\nfeedback helpful/very helpful and 82.4% found it more beneficial than feedback\nfrom at least some human reviewers. While our findings show that LLM-generated\nfeedback can help researchers, we also identify several limitations.\n",
                "链接": "https://arxiv.org/abs/2310.01783"
            },
            {
                "文章ID": "104212",
                "标题": "Prediction Model For Wordle Game Results With High Robustness",
                "作者": " Jiaqi Weng,  Chunlin Feng",
                "发布日期": "2023-09-26",
                "摘要": "  In this study, we delve into the dynamics of Wordle using data analysis and\nmachine learning. Our analysis initially focused on the correlation between the\ndate and the number of submitted results. Due to initial popularity bias, we\nmodeled stable data using an ARIMAX model with coefficient values of 9, 0, 2,\nand weekdays/weekends as the exogenous variable. We found no significant\nrelationship between word attributes and hard mode results.\n  To predict word difficulty, we employed a Backpropagation Neural Network,\novercoming overfitting via feature engineering. We also used K-means\nclustering, optimized at five clusters, to categorize word difficulty\nnumerically. Our findings indicate that on March 1st, 2023, around 12,884\nresults will be submitted and the word \"eerie\" averages 4.8 attempts, falling\ninto the hardest difficulty cluster.\n  We further examined the percentage of loyal players and their propensity to\nundertake daily challenges. Our models underwent rigorous sensitivity analyses,\nincluding ADF, ACF, PACF tests, and cross-validation, confirming their\nrobustness. Overall, our study provides a predictive framework for Wordle\ngameplay based on date or a given five-letter word. Results have been\nsummarized and submitted to the Puzzle Editor of the New York Times.\n",
                "链接": "https://arxiv.org/abs/2309.14250"
            },
            {
                "文章ID": "111100",
                "标题": "MindLLM: Pre-training Lightweight Large Language Model from Scratch,\n  Evaluations and Domain Applications",
                "作者": " Yizhe Yang,  Huashan Sun,  Jiawei Li,  Runheng Liu,  Yinghao Li,  Yuhang Liu,  Heyan Huang,  Yang Gao",
                "发布日期": "2023-10-31",
                "摘要": "  Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious natural language tasks, marking significant strides towards general\nartificial intelligence. While general artificial intelligence is leveraged by\ndeveloping increasingly large-scale models, there could be another branch to\ndevelop lightweight custom models that better serve certain domains, taking\ninto account the high cost of training and deploying LLMs and the scarcity of\nresources. In this paper, we present MindLLM, a novel series of bilingual\nlightweight large language models, trained from scratch, alleviating such\nburdens by offering models with 1.3 billion and 3 billion parameters. A\nthorough account of experiences accrued during large model development is\ngiven, covering every step of the process, including data construction, model\narchitecture, evaluation, and applications. Such insights are hopefully\nvaluable for fellow academics and developers. MindLLM consistently matches or\nsurpasses the performance of other open-source larger models on some public\nbenchmarks. We also introduce an innovative instruction tuning framework\ntailored for smaller models to enhance their capabilities efficiently.\nMoreover, we explore the application of MindLLM in specific vertical domains\nsuch as law and finance, underscoring the agility and adaptability of our\nlightweight models.\n",
                "链接": "https://arxiv.org/abs/2310.15777"
            }
        ]
    },
    {
        "question": {
            "question": "对比解码相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "95351",
                "标题": "Auditory Attention Decoding with Task-Related Multi-View Contrastive\n  Learning",
                "作者": " Xiaoyu Chen,  Changde Du,  Qiongyi Zhou,  Huiguang He",
                "发布日期": "2023-08-09",
                "摘要": "  The human brain can easily focus on one speaker and suppress others in\nscenarios such as a cocktail party. Recently, researchers found that auditory\nattention can be decoded from the electroencephalogram (EEG) data. However,\nmost existing deep learning methods are difficult to use prior knowledge of\ndifferent views (that is attended speech and EEG are task-related views) and\nextract an unsatisfactory representation. Inspired by Broadbent's filter model,\nwe decode auditory attention in a multi-view paradigm and extract the most\nrelevant and important information utilizing the missing view. Specifically, we\npropose an auditory attention decoding (AAD) method based on multi-view VAE\nwith task-related multi-view contrastive (TMC) learning. Employing TMC learning\nin multi-view VAE can utilize the missing view to accumulate prior knowledge of\ndifferent views into the fusion of representation, and extract the approximate\ntask-related representation. We examine our method on two popular AAD datasets,\nand demonstrate the superiority of our method by comparing it to the\nstate-of-the-art method.\n",
                "链接": "https://arxiv.org/abs/2308.04244"
            },
            {
                "文章ID": "5664",
                "标题": "On Deciding Feature Membership in Explanations of SDD & Related\n  Classifiers",
                "作者": " Xuanxiang Huang,  Joao Marques-Silva",
                "发布日期": "2022-02-16",
                "摘要": "  When reasoning about explanations of Machine Learning (ML) classifiers, a\npertinent query is to decide whether some sensitive features can serve for\nexplaining a given prediction. Recent work showed that the feature membership\nproblem (FMP) is hard for $\\Sigma_2^P$ for a broad class of classifiers. In\ncontrast, this paper shows that for a number of families of classifiers, FMP is\nin NP. Concretely, the paper proves that any classifier for which an\nexplanation can be computed in polynomial time, then deciding feature\nmembership in an explanation can be decided with one NP oracle call. The paper\nthen proposes propositional encodings for classifiers represented with\nSentential Decision Diagrams (SDDs) and for other related propositional\nlanguages. The experimental results confirm the practical efficiency of the\nproposed approach.\n",
                "链接": "https://arxiv.org/abs/2202.07553"
            },
            {
                "文章ID": "61490",
                "标题": "Relatedly: Scaffolding Literature Reviews with Existing Related Work\n  Sections",
                "作者": " Srishti Palani,  Aakanksha Naik,  Doug Downey,  Amy X. Zhang,  Jonathan Bragg,  Joseph Chee Chang",
                "发布日期": "2023-02-15",
                "摘要": "  Scholars who want to research a scientific topic must take time to read,\nextract meaning, and identify connections across many papers. As scientific\nliterature grows, this becomes increasingly challenging. Meanwhile, authors\nsummarize prior research in papers' related work sections, though this is\nscoped to support a single paper. A formative study found that while reading\nmultiple related work paragraphs helps overview a topic, it is hard to navigate\noverlapping and diverging references and research foci. In this work, we design\na system, Relatedly, that scaffolds exploring and reading multiple related work\nparagraphs on a topic, with features including dynamic re-ranking and\nhighlighting to spotlight unexplored dissimilar information, auto-generated\ndescriptive paragraph headings, and low-lighting of redundant information. From\na within-subjects user study (n=15), we found that scholars generate more\ncoherent, insightful, and comprehensive topic outlines using Relatedly compared\nto a baseline paper list.\n",
                "链接": "https://arxiv.org/abs/2302.06754"
            },
            {
                "文章ID": "101013",
                "标题": "Encoding Multi-Domain Scientific Papers by Ensembling Multiple CLS\n  Tokens",
                "作者": " Ronald Seoh,  Haw-Shiuan Chang,  Andrew McCallum",
                "发布日期": "2023-09-11",
                "摘要": "  Many useful tasks on scientific documents, such as topic classification and\ncitation prediction, involve corpora that span multiple scientific domains.\nTypically, such tasks are accomplished by representing the text with a vector\nembedding obtained from a Transformer's single CLS token. In this paper, we\nargue that using multiple CLS tokens could make a Transformer better specialize\nto multiple scientific domains. We present Multi2SPE: it encourages each of\nmultiple CLS tokens to learn diverse ways of aggregating token embeddings, then\nsums them up together to create a single vector representation. We also propose\nour new multi-domain benchmark, Multi-SciDocs, to test scientific paper vector\nencoders under multi-domain settings. We show that Multi2SPE reduces error by\nup to 25 percent in multi-domain citation prediction, while requiring only a\nnegligible amount of computation in addition to one BERT forward pass.\n",
                "链接": "https://arxiv.org/abs/2309.04333"
            },
            {
                "文章ID": "91604",
                "标题": "Non-stationary Delayed Combinatorial Semi-Bandit with Causally Related\n  Rewards",
                "作者": " Saeed Ghoorchian,  Setareh Maghsudi",
                "发布日期": "2023-07-19",
                "摘要": "  Sequential decision-making under uncertainty is often associated with long\nfeedback delays. Such delays degrade the performance of the learning agent in\nidentifying a subset of arms with the optimal collective reward in the long\nrun. This problem becomes significantly challenging in a non-stationary\nenvironment with structural dependencies amongst the reward distributions\nassociated with the arms. Therefore, besides adapting to delays and\nenvironmental changes, learning the causal relations alleviates the adverse\neffects of feedback delay on the decision-making process. We formalize the\ndescribed setting as a non-stationary and delayed combinatorial semi-bandit\nproblem with causally related rewards. We model the causal relations by a\ndirected graph in a stationary structural equation model. The agent maximizes\nthe long-term average payoff, defined as a linear function of the base arms'\nrewards. We develop a policy that learns the structural dependencies from\ndelayed feedback and utilizes that to optimize the decision-making while\nadapting to drifts. We prove a regret bound for the performance of the proposed\nalgorithm. Besides, we evaluate our method via numerical analysis using\nsynthetic and real-world datasets to detect the regions that contribute the\nmost to the spread of Covid-19 in Italy.\n",
                "链接": "https://arxiv.org/abs/2307.09093"
            },
            {
                "文章ID": "68991",
                "标题": "Joint fMRI Decoding and Encoding with Latent Embedding Alignment",
                "作者": " Xuelin Qian,  Yikai Wang,  Yanwei Fu,  Xinwei Sun,  Xiangyang Xue,  Jianfeng Feng",
                "发布日期": "2023-06-06",
                "摘要": "  The connection between brain activity and corresponding visual stimuli is\ncrucial in comprehending the human brain. While deep generative models have\nexhibited advancement in recovering brain recordings by generating images\nconditioned on fMRI signals, accomplishing high-quality generation with\nconsistent semantics continues to pose challenges. Moreover, the prediction of\nbrain activity from visual stimuli remains a formidable undertaking. In this\npaper, we introduce a unified framework that addresses both fMRI decoding and\nencoding. Commencing with the establishment of two latent spaces capable of\nrepresenting and reconstructing fMRI signals and visual images, respectively,\nwe proceed to align the fMRI signals and visual images within the latent space,\nthereby enabling a bidirectional transformation between the two domains. Our\nLatent Embedding Alignment (LEA) model concurrently recovers visual stimuli\nfrom fMRI signals and predicts brain activity from images within a unified\nframework. The performance of LEA surpasses that of existing methods on\nmultiple benchmark fMRI decoding and encoding datasets. By integrating fMRI\ndecoding and encoding, LEA offers a comprehensive solution for modeling the\nintricate relationship between brain activity and visual stimuli.\n",
                "链接": "https://arxiv.org/abs/2303.14730"
            },
            {
                "文章ID": "77552",
                "标题": "A Hierarchical Encoding-Decoding Scheme for Abstractive Multi-document\n  Summarization",
                "作者": " Chenhui Shen,  Liying Cheng,  Xuan-Phi Nguyen,  Yang You,  Lidong Bing",
                "发布日期": "2023-11-02",
                "摘要": "  Pre-trained language models (PLMs) have achieved outstanding achievements in\nabstractive single-document summarization (SDS). However, such benefits may not\nfully extend to multi-document summarization (MDS), where the handling of\ncross-document information is more complex. Previous works either design new\nMDS architectures or apply PLMs bluntly with concatenated source documents as a\nreformulated SDS task. While the former does not utilize previous pre-training\nefforts and may not generalize well across different domains, the latter may\nnot sufficiently attend to the intricate cross-document relationships unique to\nMDS tasks. Instead, we enforce hierarchy on both the encoder and decoder to\nbetter utilize a PLM to facilitate multi-document interactions for the MDS\ntask. Across 10 MDS benchmarks from various domains, our method outperforms or\nis competitive with the previous best models, including those with additional\nMDS pre-training or with more parameters. It outperforms its corresponding PLM\nbackbone by up to 3 Rouge-L and is favored by humans.\n",
                "链接": "https://arxiv.org/abs/2305.08503"
            },
            {
                "文章ID": "75375",
                "标题": "The Benefits of Bad Advice: Autocontrastive Decoding across Model Layers",
                "作者": " Ariel Gera,  Roni Friedman,  Ofir Arviv,  Chulaka Gunasekara,  Benjamin Sznajder,  Noam Slonim,  Eyal Shnarch",
                "发布日期": "2023-05-03",
                "摘要": "  Applying language models to natural language processing tasks typically\nrelies on the representations in the final model layer, as intermediate hidden\nlayer representations are presumed to be less informative. In this work, we\nargue that due to the gradual improvement across model layers, additional\ninformation can be gleaned from the contrast between higher and lower layers\nduring inference. Specifically, in choosing between the probable next token\npredictions of a generative model, the predictions of lower layers can be used\nto highlight which candidates are best avoided. We propose a novel approach\nthat utilizes the contrast between layers to improve text generation outputs,\nand show that it mitigates degenerative behaviors of the model in open-ended\ngeneration, significantly improving the quality of generated texts.\nFurthermore, our results indicate that contrasting between model layers at\ninference time can yield substantial benefits to certain aspects of general\nlanguage model capabilities, more effectively extracting knowledge during\ninference from a given set of model parameters.\n",
                "链接": "https://arxiv.org/abs/2305.01628"
            },
            {
                "文章ID": "7583",
                "标题": "Paper Plain: Making Medical Research Papers Approachable to Healthcare\n  Consumers with Natural Language Processing",
                "作者": " Tal August,  Lucy Lu Wang,  Jonathan Bragg,  Marti A. Hearst,  Andrew Head,  Kyle Lo",
                "发布日期": "2022-03-02",
                "摘要": "  When seeking information not covered in patient-friendly documents, like\nmedical pamphlets, healthcare consumers may turn to the research literature.\nReading medical papers, however, can be a challenging experience. To improve\naccess to medical papers, we introduce a novel interactive interface-Paper\nPlain-with four features powered by natural language processing: definitions of\nunfamiliar terms, in-situ plain language section summaries, a collection of key\nquestions that guide readers to answering passages, and plain language\nsummaries of the answering passages. We evaluate Paper Plain, finding that\nparticipants who use Paper Plain have an easier time reading and understanding\nresearch papers without a loss in paper comprehension compared to those who use\na typical PDF reader. Altogether, the study results suggest that guiding\nreaders to relevant passages and providing plain language summaries, or\n\"gists,\" alongside the original paper content can make reading medical papers\neasier and give readers more confidence to approach these papers.\n",
                "链接": "https://arxiv.org/abs/2203.00130"
            }
        ]
    },
    {
        "question": {
            "question": "请找到缓和噪声标签影响的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "79671",
                "标题": "Mitigating Label Noise through Data Ambiguation",
                "作者": " Julian Lienen,  Eyke Hüllermeier",
                "发布日期": "2023-05-24",
                "摘要": "  Label noise poses an important challenge in machine learning, especially in\ndeep learning, in which large models with high expressive power dominate the\nfield. Models of that kind are prone to memorizing incorrect labels, thereby\nharming generalization performance. Many methods have been proposed to address\nthis problem, including robust loss functions and more complex label correction\napproaches. Robust loss functions are appealing due to their simplicity, but\ntypically lack flexibility, while label correction usually adds substantial\ncomplexity to the training setup. In this paper, we suggest to address the\nshortcomings of both methodologies by \"ambiguating\" the target information,\nadding additional, complementary candidate labels in case the learner is not\nsufficiently convinced of the observed training label. More precisely, we\nleverage the framework of so-called superset learning to construct set-valued\ntargets based on a confidence threshold, which deliver imprecise yet more\nreliable beliefs about the ground-truth, effectively helping the learner to\nsuppress the memorization effect. In an extensive empirical evaluation, our\nmethod demonstrates favorable learning behavior on synthetic and real-world\nnoise, confirming the effectiveness in detecting and correcting erroneous\ntraining labels.\n",
                "链接": "https://arxiv.org/abs/2305.13764"
            },
            {
                "文章ID": "48397",
                "标题": "Quantifying the Impact of Label Noise on Federated Learning",
                "作者": " Shuqi Ke,  Chao Huang,  Xin Liu",
                "发布日期": "2023-04-04",
                "摘要": "  Federated Learning (FL) is a distributed machine learning paradigm where\nclients collaboratively train a model using their local (human-generated)\ndatasets. While existing studies focus on FL algorithm development to tackle\ndata heterogeneity across clients, the important issue of data quality (e.g.,\nlabel noise) in FL is overlooked. This paper aims to fill this gap by providing\na quantitative study on the impact of label noise on FL. We derive an upper\nbound for the generalization error that is linear in the clients' label noise\nlevel. Then we conduct experiments on MNIST and CIFAR-10 datasets using various\nFL algorithms. Our empirical results show that the global model accuracy\nlinearly decreases as the noise level increases, which is consistent with our\ntheoretical analysis. We further find that label noise slows down the\nconvergence of FL training, and the global model tends to overfit when the\nnoise level is high.\n",
                "链接": "https://arxiv.org/abs/2211.07816"
            },
            {
                "文章ID": "52026",
                "标题": "CrossSplit: Mitigating Label Noise Memorization through Data Splitting",
                "作者": " Jihye Kim,  Aristide Baratin,  Yan Zhang,  Simon Lacoste-Julien",
                "发布日期": "2023-04-27",
                "摘要": "  We approach the problem of improving robustness of deep learning algorithms\nin the presence of label noise. Building upon existing label correction and\nco-teaching methods, we propose a novel training procedure to mitigate the\nmemorization of noisy labels, called CrossSplit, which uses a pair of neural\nnetworks trained on two disjoint parts of the labelled dataset. CrossSplit\ncombines two main ingredients: (i) Cross-split label correction. The idea is\nthat, since the model trained on one part of the data cannot memorize\nexample-label pairs from the other part, the training labels presented to each\nnetwork can be smoothly adjusted by using the predictions of its peer network;\n(ii) Cross-split semi-supervised training. A network trained on one part of the\ndata also uses the unlabeled inputs of the other part. Extensive experiments on\nCIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that\nour method can outperform the current state-of-the-art in a wide range of noise\nratios.\n",
                "链接": "https://arxiv.org/abs/2212.01674"
            },
            {
                "文章ID": "106205",
                "标题": "Quantifying and mitigating the impact of label errors on model disparity\n  metrics",
                "作者": " Julius Adebayo,  Melissa Hall,  Bowen Yu,  Bobbie Chern",
                "发布日期": "2023-10-05",
                "摘要": "  Errors in labels obtained via human annotation adversely affect a model's\nperformance. Existing approaches propose ways to mitigate the effect of label\nerror on a model's downstream accuracy, yet little is known about its impact on\na model's disparity metrics. Here we study the effect of label error on a\nmodel's disparity metrics. We empirically characterize how varying levels of\nlabel error, in both training and test data, affect these disparity metrics. We\nfind that group calibration and other metrics are sensitive to train-time and\ntest-time label error -- particularly for minority groups. This disparate\neffect persists even for models trained with noise-aware algorithms. To\nmitigate the impact of training-time label error, we present an approach to\nestimate the influence of a training input's label on a model's group disparity\nmetric. We empirically assess the proposed approach on a variety of datasets\nand find significant improvement, compared to alternative approaches, in\nidentifying training inputs that improve a model's disparity metric. We\ncomplement the approach with an automatic relabel-and-finetune scheme that\nproduces updated models with, provably, improved group calibration error.\n",
                "链接": "https://arxiv.org/abs/2310.02533"
            },
            {
                "文章ID": "105104",
                "标题": "Understanding and Mitigating the Label Noise in Pre-training on\n  Downstream Tasks",
                "作者": " Hao Chen,  Jindong Wang,  Ankit Shah,  Ran Tao,  Hongxin Wei,  Xing Xie,  Masashi Sugiyama,  Bhiksha Raj",
                "发布日期": "2023-10-02",
                "摘要": "  Pre-training on large-scale datasets and then fine-tuning on downstream tasks\nhave become a standard practice in deep learning. However, pre-training data\noften contain label noise that may adversely affect the generalization of the\nmodel. This paper aims to understand the nature of noise in pre-training\ndatasets and to mitigate its impact on downstream tasks. More specifically,\nthrough extensive experiments of supervised pre-training models on synthetic\nnoisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise\nin pre-training can benefit in-domain (ID) transfer performance, where the\ntraining and testing data share the same distribution, it always deteriorates\nout-of-domain (OOD) performance, where training and testing data distribution\nare different. We empirically verify that the reason behind is noise in\npre-training shapes the feature space differently. We then propose a\nlightweight black-box tuning method (NMTune) to affine the feature space to\nmitigate the malignant effect of noise and improve generalization on both ID\nand OOD tasks, considering one may not be able to fully fine-tune or even\naccess the pre-trained models. We conduct practical experiments on popular\nvision and language models that are pre-trained on noisy data for evaluation of\nour approach. Our analysis and results show the importance of this interesting\nand novel research direction, which we term Noisy Model Learning.\n",
                "链接": "https://arxiv.org/abs/2309.17002"
            },
            {
                "文章ID": "88300",
                "标题": "Systematic analysis of the impact of label noise correction on ML\n  Fairness",
                "作者": " I. Oliveira e Silva,  C. Soares,  I. Sousa,  R. Ghani",
                "发布日期": "2023-06-29",
                "摘要": "  Arbitrary, inconsistent, or faulty decision-making raises serious concerns,\nand preventing unfair models is an increasingly important challenge in Machine\nLearning. Data often reflect past discriminatory behavior, and models trained\non such data may reflect bias on sensitive attributes, such as gender, race, or\nage. One approach to developing fair models is to preprocess the training data\nto remove the underlying biases while preserving the relevant information, for\nexample, by correcting biased labels. While multiple label noise correction\nmethods are available, the information about their behavior in identifying\ndiscrimination is very limited. In this work, we develop an empirical\nmethodology to systematically evaluate the effectiveness of label noise\ncorrection techniques in ensuring the fairness of models trained on biased\ndatasets. Our methodology involves manipulating the amount of label noise and\ncan be used with fairness benchmarks but also with standard ML datasets. We\napply the methodology to analyze six label noise correction methods according\nto several fairness metrics on standard OpenML datasets. Our results suggest\nthat the Hybrid Label Noise Correction method achieves the best trade-off\nbetween predictive performance and fairness. Clustering-Based Correction can\nreduce discrimination the most, however, at the cost of lower predictive\nperformance.\n",
                "链接": "https://arxiv.org/abs/2306.15994"
            },
            {
                "文章ID": "15729",
                "标题": "Is BERT Robust to Label Noise? A Study on Learning with Noisy Labels in\n  Text Classification",
                "作者": " Dawei Zhu,  Michael A. Hedderich,  Fangzhou Zhai,  David Ifeoluwa Adelani,  Dietrich Klakow",
                "发布日期": "2022-04-21",
                "摘要": "  Incorrect labels in training data occur when human annotators make mistakes\nor when the data is generated via weak or distant supervision. It has been\nshown that complex noise-handling techniques - by modeling, cleaning or\nfiltering the noisy instances - are required to prevent models from fitting\nthis label noise. However, we show in this work that, for text classification\ntasks with modern NLP models like BERT, over a variety of noise types, existing\nnoisehandling methods do not always improve its performance, and may even\ndeteriorate it, suggesting the need for further investigation. We also back our\nobservations with a comprehensive analysis.\n",
                "链接": "https://arxiv.org/abs/2204.09371"
            },
            {
                "文章ID": "124269",
                "标题": "Universal Noise Annotation: Unveiling the Impact of Noisy annotation on\n  Object Detection",
                "作者": " Kwangrok Ryoo,  Yeonsik Jo,  Seungjun Lee,  Mira Kim,  Ahra Jo,  Seung Hwan Kim,  Seungryong Kim,  Soonyoung Lee",
                "发布日期": "2023-12-22",
                "摘要": "  For object detection task with noisy labels, it is important to consider not\nonly categorization noise, as in image classification, but also localization\nnoise, missing annotations, and bogus bounding boxes. However, previous studies\nhave only addressed certain types of noise (e.g., localization or\ncategorization). In this paper, we propose Universal-Noise Annotation (UNA), a\nmore practical setting that encompasses all types of noise that can occur in\nobject detection, and analyze how UNA affects the performance of the detector.\nWe analyzed the development direction of previous works of detection algorithms\nand examined the factors that impact the robustness of detection model learning\nmethod. We open-source the code for injecting UNA into the dataset and all the\ntraining log and weight are also shared.\n",
                "链接": "https://arxiv.org/abs/2312.13822"
            },
            {
                "文章ID": "82308",
                "标题": "Noisy-label Learning with Sample Selection based on Noise Rate Estimate",
                "作者": " Arpit Garg,  Cuong Nguyen,  Rafael Felix,  Thanh-Toan Do,  Gustavo Carneiro",
                "发布日期": "2023-06-01",
                "摘要": "  Noisy-labels are challenging for deep learning due to the high capacity of\nthe deep models that can overfit noisy-label training samples. Arguably the\nmost realistic and coincidentally challenging type of label noise is the\ninstance-dependent noise (IDN), where the labelling errors are caused by the\nambivalent information present in the images. The most successful label noise\nlearning techniques to address IDN problems usually contain a noisy-label\nsample selection stage to separate clean and noisy-label samples during\ntraining. Such sample selection depends on a criterion, such as loss or\ngradient, and on a curriculum to define the proportion of training samples to\nbe classified as clean at each training epoch.\n  Even though the estimated noise rate from the training set appears to be a\nnatural signal to be used in the definition of this curriculum, previous\napproaches generally rely on arbitrary thresholds or pre-defined selection\nfunctions to the best of our knowledge. This paper addresses this research gap\nby proposing a new noisy-label learning graphical model that can easily\naccommodate state-of-the-art (SOTA) noisy-label learning methods and provide\nthem with a reliable noise rate estimate to be used in a new sample selection\ncurriculum. We show empirically that our model integrated with many SOTA\nmethods can improve their results in many IDN benchmarks, including synthetic\nand real-world datasets.\n",
                "链接": "https://arxiv.org/abs/2305.19486"
            },
            {
                "文章ID": "61606",
                "标题": "When Mitigating Bias is Unfair: A Comprehensive Study on the Impact of\n  Bias Mitigation Algorithms",
                "作者": " Natasa Krco,  Thibault Laugel,  Jean-Michel Loubes,  Marcin Detyniecki",
                "发布日期": "2023-02-15",
                "摘要": "  Most works on the fairness of machine learning systems focus on the blind\noptimization of common fairness metrics, such as Demographic Parity and\nEqualized Odds. In this paper, we conduct a comparative study of several bias\nmitigation approaches to investigate their behaviors at a fine grain, the\nprediction level. Our objective is to characterize the differences between fair\nmodels obtained with different approaches. With comparable performances in\nfairness and accuracy, are the different bias mitigation approaches impacting a\nsimilar number of individuals? Do they mitigate bias in a similar way? Do they\naffect the same individuals when debiasing a model? Our findings show that bias\nmitigation approaches differ a lot in their strategies, both in the number of\nimpacted individuals and the populations targeted. More surprisingly, we show\nthese results even apply for several runs of the same mitigation approach.\nThese findings raise questions about the limitations of the current group\nfairness metrics, as well as the arbitrariness, hence unfairness, of the whole\ndebiasing process.\n",
                "链接": "https://arxiv.org/abs/2302.07185"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下文本检索任务上，是否有关于大模型在语义坍缩问题上的研究",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "81507",
                "标题": "Reward Collapse in Aligning Large Language Models",
                "作者": " Ziang Song,  Tianle Cai,  Jason D. Lee,  Weijie J. Su",
                "发布日期": "2023-05-30",
                "摘要": "  The extraordinary capabilities of large language models (LLMs) such as\nChatGPT and GPT-4 are in part unleashed by aligning them with reward models\nthat are trained on human preferences, which are often represented as rankings\nof responses to prompts. In this paper, we document the phenomenon of\n\\textit{reward collapse}, an empirical observation where the prevailing\nranking-based approach results in an \\textit{identical} reward distribution\n\\textit{regardless} of the prompts during the terminal phase of training. This\noutcome is undesirable as open-ended prompts like ``write a short story about\nyour best friend'' should yield a continuous range of rewards for their\ncompletions, while specific prompts like ``what is the capital of New Zealand''\nshould generate either high or low rewards. Our theoretical investigation\nreveals that reward collapse is primarily due to the insufficiency of the\nranking-based objective function to incorporate prompt-related information\nduring optimization. This insight allows us to derive closed-form expressions\nfor the reward distribution associated with a set of utility functions in an\nasymptotic regime. To overcome reward collapse, we introduce a prompt-aware\noptimization scheme that provably admits a prompt-dependent reward distribution\nwithin the interpolating regime. Our experimental results suggest that our\nproposed prompt-aware utility functions significantly alleviate reward collapse\nduring the training of reward models.\n",
                "链接": "https://arxiv.org/abs/2305.17608"
            },
            {
                "文章ID": "12053",
                "标题": "Image-text Retrieval: A Survey on Recent Research and Development",
                "作者": " Min Cao,  Shiping Li,  Juntao Li,  Liqiang Nie,  Min Zhang",
                "发布日期": "2022-11-21",
                "摘要": "  In the past few years, cross-modal image-text retrieval (ITR) has experienced\nincreased interest in the research community due to its excellent research\nvalue and broad real-world application. It is designed for the scenarios where\nthe queries are from one modality and the retrieval galleries from another\nmodality. This paper presents a comprehensive and up-to-date survey on the ITR\napproaches from four perspectives. By dissecting an ITR system into two\nprocesses: feature extraction and feature alignment, we summarize the recent\nadvance of the ITR approaches from these two perspectives. On top of this, the\nefficiency-focused study on the ITR system is introduced as the third\nperspective. To keep pace with the times, we also provide a pioneering overview\nof the cross-modal pre-training ITR approaches as the fourth perspective.\nFinally, we outline the common benchmark datasets and valuation metric for ITR,\nand conduct the accuracy comparison among the representative ITR approaches.\nSome critical yet less studied issues are discussed at the end of the paper.\n",
                "链接": "https://arxiv.org/abs/2203.14713"
            },
            {
                "文章ID": "66370",
                "标题": "Algorithmic Ghost in the Research Shell: Large Language Models and\n  Academic Knowledge Creation in Management Research",
                "作者": " Nigel Williams,  Stanislav Ivanov,  Dimitrios Buhalis",
                "发布日期": "2023-03-14",
                "摘要": "  The paper looks at the role of large language models in academic knowledge\ncreation based on a scoping review (2018 to January 2023) of how researchers\nhave previously used the language model GPT to assist in the performance of\nacademic knowledge creation tasks beyond data analysis. These tasks include\nwriting, editing, reviewing, dataset creation and curation, which have been\ndifficult to perform using earlier ML tools. Based on a synthesis of these\npapers, this study identifies pathways for a future academic research landscape\nthat incorporates wider usage of large language models based on the current\nmodes of adoption in published articles as a Co-Writer, Research Assistant and\nRespondent.\n",
                "链接": "https://arxiv.org/abs/2303.07304"
            },
            {
                "文章ID": "79114",
                "标题": "A PhD Student's Perspective on Research in NLP in the Era of Very Large\n  Language Models",
                "作者": " Oana Ignat,  Zhijing Jin,  Artem Abzaliev,  Laura Biester,  Santiago Castro,  Naihao Deng,  Xinyi Gao,  Aylin Gunal,  Jacky He,  Ashkan Kazemi,  Muhammad Khalifa,  Namho Koh,  Andrew Lee,  Siyang Liu,  Do June Min,  Shinka Mori,  Joan Nwatu,  Veronica Perez-Rosas,  Siqi Shen,  Zekun Wang,  Winston Wu,  Rada Mihalcea",
                "发布日期": "2023-05-23",
                "摘要": "  Recent progress in large language models has enabled the deployment of many\ngenerative NLP applications. At the same time, it has also led to a misleading\npublic discourse that ``it's all been solved.'' Not surprisingly, this has in\nturn made many NLP researchers -- especially those at the beginning of their\ncareer -- wonder about what NLP research area they should focus on. This\ndocument is a compilation of NLP research directions that are rich for\nexploration, reflecting the views of a diverse group of PhD students in an\nacademic research lab. While we identify many research areas, many others\nexist; we do not cover those areas that are currently addressed by LLMs but\nwhere LLMs lag behind in performance, or those focused on LLM development. We\nwelcome suggestions for other research directions to include:\nhttps://bit.ly/nlp-era-llm\n",
                "链接": "https://arxiv.org/abs/2305.12544"
            },
            {
                "文章ID": "108065",
                "标题": "Survey on Factuality in Large Language Models: Knowledge, Retrieval and\n  Domain-Specificity",
                "作者": " Cunxiang Wang,  Xiaoze Liu,  Yuanhao Yue,  Xiangru Tang,  Tianhang Zhang,  Cheng Jiayang,  Yunzhi Yao,  Wenyang Gao,  Xuming Hu,  Zehan Qi,  Yidong Wang,  Linyi Yang,  Jindong Wang,  Xing Xie,  Zheng Zhang,  Yue Zhang",
                "发布日期": "2023-12-19",
                "摘要": "  This survey addresses the crucial issue of factuality in Large Language\nModels (LLMs). As LLMs find applications across diverse domains, the\nreliability and accuracy of their outputs become vital. We define the\nFactuality Issue as the probability of LLMs to produce content inconsistent\nwith established facts. We first delve into the implications of these\ninaccuracies, highlighting the potential consequences and challenges posed by\nfactual errors in LLM outputs. Subsequently, we analyze the mechanisms through\nwhich LLMs store and process facts, seeking the primary causes of factual\nerrors. Our discussion then transitions to methodologies for evaluating LLM\nfactuality, emphasizing key metrics, benchmarks, and studies. We further\nexplore strategies for enhancing LLM factuality, including approaches tailored\nfor specific domains. We focus two primary LLM configurations standalone LLMs\nand Retrieval-Augmented LLMs that utilizes external data, we detail their\nunique challenges and potential enhancements. Our survey offers a structured\nguide for researchers aiming to fortify the factual reliability of LLMs.\n",
                "链接": "https://arxiv.org/abs/2310.07521"
            },
            {
                "文章ID": "100150",
                "标题": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
                "作者": " Jiawei Chen,  Hongyu Lin,  Xianpei Han,  Le Sun",
                "发布日期": "2023-12-21",
                "摘要": "  Retrieval-Augmented Generation (RAG) is a promising approach for mitigating\nthe hallucination of large language models (LLMs). However, existing research\nlacks rigorous evaluation of the impact of retrieval-augmented generation on\ndifferent large language models, which make it challenging to identify the\npotential bottlenecks in the capabilities of RAG for different LLMs. In this\npaper, we systematically investigate the impact of Retrieval-Augmented\nGeneration on large language models. We analyze the performance of different\nlarge language models in 4 fundamental abilities required for RAG, including\nnoise robustness, negative rejection, information integration, and\ncounterfactual robustness. To this end, we establish Retrieval-Augmented\nGeneration Benchmark (RGB), a new corpus for RAG evaluation in both English and\nChinese. RGB divides the instances within the benchmark into 4 separate\ntestbeds based on the aforementioned fundamental abilities required to resolve\nthe case. Then we evaluate 6 representative LLMs on RGB to diagnose the\nchallenges of current LLMs when applying RAG. Evaluation reveals that while\nLLMs exhibit a certain degree of noise robustness, they still struggle\nsignificantly in terms of negative rejection, information integration, and\ndealing with false information. The aforementioned assessment outcomes indicate\nthat there is still a considerable journey ahead to effectively apply RAG to\nLLMs.\n",
                "链接": "https://arxiv.org/abs/2309.01431"
            },
            {
                "文章ID": "34574",
                "标题": "Query-Response Interactions by Multi-tasks in Semantic Search for\n  Chatbot Candidate Retrieval",
                "作者": " Libin Shi,  Kai Zhang,  Wenge Rong",
                "发布日期": "2022-08-24",
                "摘要": "  Semantic search for candidate retrieval is an important yet neglected problem\nin retrieval-based Chatbots, which aims to select a bunch of candidate\nresponses efficiently from a large pool. The existing bottleneck is to ensure\nthe model architecture having two points: 1) rich interactions between a query\nand a response to produce query-relevant responses; 2) ability of separately\nprojecting the query and the response into latent spaces to apply efficiently\nin semantic search during online inference. To tackle this problem, we propose\na novel approach, called Multitask-based Semantic Search Neural Network (MSSNN)\nfor candidate retrieval, which accomplishes query-response interactions through\nmulti-tasks. The method employs a Seq2Seq modeling task to learn a good query\nencoder, and then performs a word prediction task to build response embeddings,\nfinally conducts a simple matching model to form the dot-product scorer.\nExperimental studies have demonstrated the potential of the proposed approach.\n",
                "链接": "https://arxiv.org/abs/2208.11018"
            },
            {
                "文章ID": "106796",
                "标题": "A Comprehensive Evaluation of Large Language Models on Benchmark\n  Biomedical Text Processing Tasks",
                "作者": " Israt Jahan,  Md Tahmid Rahman Laskar,  Chun Peng,  Jimmy Huang",
                "发布日期": "2023-10-11",
                "摘要": "  Recently, Large Language Models (LLM) have demonstrated impressive capability\nto solve a wide range of tasks. However, despite their success across various\ntasks, no prior work has investigated their capability in the biomedical domain\nyet. To this end, this paper aims to evaluate the performance of LLMs on\nbenchmark biomedical tasks. For this purpose, we conduct a comprehensive\nevaluation of 4 popular LLMs in 6 diverse biomedical tasks across 26 datasets.\nTo the best of our knowledge, this is the first work that conducts an extensive\nevaluation and comparison of various LLMs in the biomedical domain.\nInterestingly, we find based on our evaluation that in biomedical datasets that\nhave smaller training sets, zero-shot LLMs even outperform the current\nstate-of-the-art fine-tuned biomedical models. This suggests that pretraining\non large text corpora makes LLMs quite specialized even in the biomedical\ndomain. We also find that not a single LLM can outperform other LLMs in all\ntasks, with the performance of different LLMs may vary depending on the task.\nWhile their performance is still quite poor in comparison to the biomedical\nmodels that were fine-tuned on large training sets, our findings demonstrate\nthat LLMs have the potential to be a valuable tool for various biomedical tasks\nthat lack large annotated data.\n",
                "链接": "https://arxiv.org/abs/2310.04270"
            },
            {
                "文章ID": "77612",
                "标题": "Sensitivity and Robustness of Large Language Models to Prompt Template\n  in Japanese Text Classification Tasks",
                "作者": " Chengguang Gan,  Tatsunori Mori",
                "发布日期": "2023-06-09",
                "摘要": "  Prompt engineering relevance research has seen a notable surge in recent\nyears, primarily driven by advancements in pre-trained language models and\nlarge language models. However, a critical issue has been identified within\nthis domain: the inadequate of sensitivity and robustness of these models\ntowards Prompt Templates, particularly in lesser-studied languages such as\nJapanese. This paper explores this issue through a comprehensive evaluation of\nseveral representative Large Language Models (LLMs) and a widely-utilized\npre-trained model(PLM). These models are scrutinized using a benchmark dataset\nin Japanese, with the aim to assess and analyze the performance of the current\nmultilingual models in this context. Our experimental results reveal startling\ndiscrepancies. A simple modification in the sentence structure of the Prompt\nTemplate led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44.\nThis observation underscores the fact that even the highly performance GPT-4\nmodel encounters significant stability issues when dealing with diverse\nJapanese prompt templates, rendering the consistency of the model's output\nresults questionable. In light of these findings, we conclude by proposing\npotential research trajectories to further enhance the development and\nperformance of Large Language Models in their current stage.\n",
                "链接": "https://arxiv.org/abs/2305.08714"
            },
            {
                "文章ID": "89478",
                "标题": "Open-Source Large Language Models Outperform Crowd Workers and Approach\n  ChatGPT in Text-Annotation Tasks",
                "作者": " Meysam Alizadeh,  Maël Kubli,  Zeynab Samei,  Shirin Dehghani,  Juan Diego Bermeo,  Maria Korobeynikova,  Fabrizio Gilardi",
                "发布日期": "2023-07-06",
                "摘要": "  This study examines the performance of open-source Large Language Models\n(LLMs) in text annotation tasks and compares it with proprietary models like\nChatGPT and human-based services such as MTurk. While prior research\ndemonstrated the high performance of ChatGPT across numerous NLP tasks,\nopen-source LLMs like HugginChat and FLAN are gaining attention for their\ncost-effectiveness, transparency, reproducibility, and superior data\nprotection. We assess these models using both zero-shot and few-shot approaches\nand different temperature parameters across a range of text annotation tasks.\nOur findings show that while ChatGPT achieves the best performance in most\ntasks, open-source LLMs not only outperform MTurk but also demonstrate\ncompetitive potential against ChatGPT in specific tasks.\n",
                "链接": "https://arxiv.org/abs/2307.02179"
            }
        ]
    },
    {
        "question": {
            "question": "帮我查找对机器翻译数据集质量进行评估的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "83169",
                "标题": "Evaluating Machine Translation Quality with Conformal Predictive\n  Distributions",
                "作者": " Patrizio Giovannotti",
                "发布日期": "2023-06-05",
                "摘要": "  This paper presents a new approach for assessing uncertainty in machine\ntranslation by simultaneously evaluating translation quality and providing a\nreliable confidence score. Our approach utilizes conformal predictive\ndistributions to produce prediction intervals with guaranteed coverage, meaning\nthat for any given significance level $\\epsilon$, we can expect the true\nquality score of a translation to fall out of the interval at a rate of\n$1-\\epsilon$. In this paper, we demonstrate how our method outperforms a\nsimple, but effective baseline on six different language pairs in terms of\ncoverage and sharpness. Furthermore, we validate that our approach requires the\ndata exchangeability assumption to hold for optimal performance.\n",
                "链接": "https://arxiv.org/abs/2306.01549"
            },
            {
                "文章ID": "84997",
                "标题": "Conformalizing Machine Translation Evaluation",
                "作者": " Chrysoula Zerva,  André F. T. Martins",
                "发布日期": "2023-06-13",
                "摘要": "  Several uncertainty estimation methods have been recently proposed for\nmachine translation evaluation. While these methods can provide a useful\nindication of when not to trust model predictions, we show in this paper that\nthe majority of them tend to underestimate model uncertainty, and as a result\nthey often produce misleading confidence intervals that do not cover the ground\ntruth. We propose as an alternative the use of conformal prediction, a\ndistribution-free method to obtain confidence intervals with a theoretically\nestablished guarantee on coverage. First, we demonstrate that split conformal\nprediction can ``correct'' the confidence intervals of previous methods to\nyield a desired coverage level. Then, we highlight biases in estimated\nconfidence intervals, both in terms of the translation language pairs and the\nquality of translations. We apply conditional conformal prediction techniques\nto obtain calibration subsets for each data subgroup, leading to equalized\ncoverage.\n",
                "链接": "https://arxiv.org/abs/2306.06221"
            },
            {
                "文章ID": "87010",
                "标题": "Evaluation of Chinese-English Machine Translation of Emotion-Loaded\n  Microblog Texts: A Human Annotated Dataset for the Quality Assessment of\n  Emotion Translation",
                "作者": " Shenbin Qian,  Constantin Orasan,  Felix do Carmo,  Qiuliang Li,  Diptesh Kanojia",
                "发布日期": "2023-06-22",
                "摘要": "  In this paper, we focus on how current Machine Translation (MT) tools perform\non the translation of emotion-loaded texts by evaluating outputs from Google\nTranslate according to a framework proposed in this paper. We propose this\nevaluation framework based on the Multidimensional Quality Metrics (MQM) and\nperform a detailed error analysis of the MT outputs. From our analysis, we\nobserve that about 50% of the MT outputs fail to preserve the original emotion.\nAfter further analysis of the errors, we find that emotion carrying words and\nlinguistic phenomena such as polysemous words, negation, abbreviation etc., are\ncommon causes for these translation errors.\n",
                "链接": "https://arxiv.org/abs/2306.11900"
            },
            {
                "文章ID": "59415",
                "标题": "An Evaluation of Persian-English Machine Translation Datasets with\n  Transformers",
                "作者": " Amir Sartipi,  Meghdad Dehghan,  Afsaneh Fatemi",
                "发布日期": "2023-02-02",
                "摘要": "  Nowadays, many researchers are focusing their attention on the subject of\nmachine translation (MT). However, Persian machine translation has remained\nunexplored despite a vast amount of research being conducted in languages with\nhigh resources, such as English. Moreover, while a substantial amount of\nresearch has been undertaken in statistical machine translation for some\ndatasets in Persian, there is currently no standard baseline for\ntransformer-based text2text models on each corpus. This study collected and\nanalysed the most popular and valuable parallel corpora, which were used for\nPersian-English translation. Furthermore, we fine-tuned and evaluated two\nstate-of-the-art attention-based seq2seq models on each dataset separately (48\nresults). We hope this paper will assist researchers in comparing their Persian\nto English and vice versa machine translation results to a standard baseline.\n",
                "链接": "https://arxiv.org/abs/2302.00321"
            },
            {
                "文章ID": "39818",
                "标题": "QUAK: A Synthetic Quality Estimation Dataset for Korean-English Neural\n  Machine Translation",
                "作者": " Sugyeong Eo,  Chanjun Park,  Hyeonseok Moon,  Jaehyung Seo,  Gyeongmin Kim,  Jungseob Lee,  Heuiseok Lim",
                "发布日期": "2022-11-30",
                "摘要": "  With the recent advance in neural machine translation demonstrating its\nimportance, research on quality estimation (QE) has been steadily progressing.\nQE aims to automatically predict the quality of machine translation (MT) output\nwithout reference sentences. Despite its high utility in the real world, there\nremain several limitations concerning manual QE data creation: inevitably\nincurred non-trivial costs due to the need for translation experts, and issues\nwith data scaling and language expansion. To tackle these limitations, we\npresent QUAK, a Korean-English synthetic QE dataset generated in a fully\nautomatic manner. This consists of three sub-QUAK datasets QUAK-M, QUAK-P, and\nQUAK-H, produced through three strategies that are relatively free from\nlanguage constraints. Since each strategy requires no human effort, which\nfacilitates scalability, we scale our data up to 1.58M for QUAK-P, H and 6.58M\nfor QUAK-M. As an experiment, we quantitatively analyze word-level QE results\nin various ways while performing statistical analysis. Moreover, we show that\ndatasets scaled in an efficient way also contribute to performance improvements\nby observing meaningful performance gains in QUAK-M, P when adding data up to\n1.58M.\n",
                "链接": "https://arxiv.org/abs/2209.15285"
            },
            {
                "文章ID": "50351",
                "标题": "Competency-Aware Neural Machine Translation: Can Machine Translation\n  Know its Own Translation Quality?",
                "作者": " Pei Zhang,  Baosong Yang,  Haoran Wei,  Dayiheng Liu,  Kai Fan,  Luo Si,  Jun Xie",
                "发布日期": "2022-11-28",
                "摘要": "  Neural machine translation (NMT) is often criticized for failures that happen\nwithout awareness. The lack of competency awareness makes NMT untrustworthy.\nThis is in sharp contrast to human translators who give feedback or conduct\nfurther investigations whenever they are in doubt about predictions. To fill\nthis gap, we propose a novel competency-aware NMT by extending conventional NMT\nwith a self-estimator, offering abilities to translate a source sentence and\nestimate its competency. The self-estimator encodes the information of the\ndecoding procedure and then examines whether it can reconstruct the original\nsemantics of the source sentence. Experimental results on four translation\ntasks demonstrate that the proposed method not only carries out translation\ntasks intact but also delivers outstanding performance on quality estimation.\nWithout depending on any reference or annotated data typically required by\nstate-of-the-art metric and quality estimation methods, our model yields an\neven higher correlation with human quality judgments than a variety of\naforementioned methods, such as BLEURT, COMET, and BERTScore. Quantitative and\nqualitative analyses show better robustness of competency awareness in our\nmodel.\n",
                "链接": "https://arxiv.org/abs/2211.13865"
            },
            {
                "文章ID": "37434",
                "标题": "Rethinking Round-Trip Translation for Machine Translation Evaluation",
                "作者": " Terry Yue Zhuo,  Qiongkai Xu,  Xuanli He,  Trevor Cohn",
                "发布日期": "2023-05-16",
                "摘要": "  Automatic evaluation on low-resource language translation suffers from a\ndeficiency of parallel corpora. Round-trip translation could be served as a\nclever and straightforward technique to alleviate the requirement of the\nparallel evaluation corpus. However, there was an observation of obscure\ncorrelations between the evaluation scores by forward and round-trip\ntranslations in the era of statistical machine translation (SMT). In this\npaper, we report the surprising finding that round-trip translation can be used\nfor automatic evaluation without the references. Firstly, our revisit on the\nround-trip translation in SMT evaluation unveils that its long-standing\nmisunderstanding is essentially caused by copying mechanism. After removing\ncopying mechanism in SMT, round-trip translation scores can appropriately\nreflect the forward translation performance. Then, we demonstrate the\nrectification is overdue as round-trip translation could benefit multiple\nmachine translation evaluation tasks. To be more specific, round-trip\ntranslation could be used i) to predict corresponding forward translation\nscores; ii) to improve the performance of the recently advanced quality\nestimation model; and iii) to identify adversarial competitors in shared tasks\nvia cross-system verification.\n",
                "链接": "https://arxiv.org/abs/2209.07351"
            },
            {
                "文章ID": "6641",
                "标题": "An Overview on Machine Translation Evaluation",
                "作者": " Lifeng Han",
                "发布日期": "2022-02-23",
                "摘要": "  Since the 1950s, machine translation (MT) has become one of the important\ntasks of AI and development, and has experienced several different periods and\nstages of development, including rule-based methods, statistical methods, and\nrecently proposed neural network-based learning methods. Accompanying these\nstaged leaps is the evaluation research and development of MT, especially the\nimportant role of evaluation methods in statistical translation and neural\ntranslation research. The evaluation task of MT is not only to evaluate the\nquality of machine translation, but also to give timely feedback to machine\ntranslation researchers on the problems existing in machine translation itself,\nhow to improve and how to optimise. In some practical application fields, such\nas in the absence of reference translations, the quality estimation of machine\ntranslation plays an important role as an indicator to reveal the credibility\nof automatically translated target languages. This report mainly includes the\nfollowing contents: a brief history of machine translation evaluation (MTE),\nthe classification of research methods on MTE, and the the cutting-edge\nprogress, including human evaluation, automatic evaluation, and evaluation of\nevaluation methods (meta-evaluation). Manual evaluation and automatic\nevaluation include reference-translation based and reference-translation\nindependent participation; automatic evaluation methods include traditional\nn-gram string matching, models applying syntax and semantics, and deep learning\nmodels; evaluation of evaluation methods includes estimating the credibility of\nhuman evaluations, the reliability of the automatic evaluation, the reliability\nof the test set, etc. Advances in cutting-edge evaluation methods include\ntask-based evaluation, using pre-trained language models based on big data, and\nlightweight optimisation models using distillation techniques.\n",
                "链接": "https://arxiv.org/abs/2202.11027"
            },
            {
                "文章ID": "14818",
                "标题": "Disentangling Uncertainty in Machine Translation Evaluation",
                "作者": " Chrysoula Zerva,  Taisiya Glushkova,  Ricardo Rei,  André F. T. Martins",
                "发布日期": "2022-12-01",
                "摘要": "  Trainable evaluation metrics for machine translation (MT) exhibit strong\ncorrelation with human judgements, but they are often hard to interpret and\nmight produce unreliable scores under noisy or out-of-domain data. Recent work\nhas attempted to mitigate this with simple uncertainty quantification\ntechniques (Monte Carlo dropout and deep ensembles), however these techniques\n(as we show) are limited in several ways -- for example, they are unable to\ndistinguish between different kinds of uncertainty, and they are time and\nmemory consuming. In this paper, we propose more powerful and efficient\nuncertainty predictors for MT evaluation, and we assess their ability to target\ndifferent sources of aleatoric and epistemic uncertainty. To this end, we\ndevelop and compare training objectives for the COMET metric to enhance it with\nan uncertainty prediction output, including heteroscedastic regression,\ndivergence minimization, and direct uncertainty prediction. Our experiments\nshow improved results on uncertainty prediction for the WMT metrics task\ndatasets, with a substantial reduction in computational costs. Moreover, they\ndemonstrate the ability of these predictors to address specific uncertainty\ncauses in MT evaluation, such as low quality references and out-of-domain data.\n",
                "链接": "https://arxiv.org/abs/2204.06546"
            },
            {
                "文章ID": "54518",
                "标题": "Extrinsic Evaluation of Machine Translation Metrics",
                "作者": " Nikita Moghe,  Tom Sherborne,  Mark Steedman,  Alexandra Birch",
                "发布日期": "2023-06-21",
                "摘要": "  Automatic machine translation (MT) metrics are widely used to distinguish the\ntranslation qualities of machine translation systems across relatively large\ntest sets (system-level evaluation). However, it is unclear if automatic\nmetrics are reliable at distinguishing good translations from bad translations\nat the sentence level (segment-level evaluation). In this paper, we investigate\nhow useful MT metrics are at detecting the success of a machine translation\ncomponent when placed in a larger platform with a downstream task. We evaluate\nthe segment-level performance of the most widely used MT metrics (chrF, COMET,\nBERTScore, etc.) on three downstream cross-lingual tasks (dialogue state\ntracking, question answering, and semantic parsing). For each task, we only\nhave access to a monolingual task-specific model. We calculate the correlation\nbetween the metric's ability to predict a good/bad translation with the\nsuccess/failure on the final task for the Translate-Test setup. Our experiments\ndemonstrate that all metrics exhibit negligible correlation with the extrinsic\nevaluation of the downstream outcomes. We also find that the scores provided by\nneural metrics are not interpretable mostly because of undefined ranges. We\nsynthesise our analysis into recommendations for future MT metrics to produce\nlabels rather than scores for more informative interaction between machine\ntranslation and multilingual language understanding.\n",
                "链接": "https://arxiv.org/abs/2212.10297"
            }
        ]
    },
    {
        "question": {
            "question": "查找大语言模型相关的分析类型的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "105928",
                "标题": "Can large language models provide useful feedback on research papers? A\n  large-scale empirical analysis",
                "作者": " Weixin Liang,  Yuhui Zhang,  Hancheng Cao,  Binglu Wang,  Daisy Ding,  Xinyu Yang,  Kailas Vodrahalli,  Siyu He,  Daniel Smith,  Yian Yin,  Daniel McFarland,  James Zou",
                "发布日期": "2023-10-04",
                "摘要": "  Expert feedback lays the foundation of rigorous research. However, the rapid\ngrowth of scholarly production and intricate knowledge specialization challenge\nthe conventional scientific feedback mechanisms. High-quality peer reviews are\nincreasingly difficult to obtain. Researchers who are more junior or from\nunder-resourced settings have especially hard times getting timely feedback.\nWith the breakthrough of large language models (LLM) such as GPT-4, there is\ngrowing interest in using LLMs to generate scientific feedback on research\nmanuscripts. However, the utility of LLM-generated feedback has not been\nsystematically studied. To address this gap, we created an automated pipeline\nusing GPT-4 to provide comments on the full PDFs of scientific papers. We\nevaluated the quality of GPT-4's feedback through two large-scale studies. We\nfirst quantitatively compared GPT-4's generated feedback with human peer\nreviewer feedback in 15 Nature family journals (3,096 papers in total) and the\nICLR machine learning conference (1,709 papers). The overlap in the points\nraised by GPT-4 and by human reviewers (average overlap 30.85% for Nature\njournals, 39.23% for ICLR) is comparable to the overlap between two human\nreviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The\noverlap between GPT-4 and human reviewers is larger for the weaker papers. We\nthen conducted a prospective user study with 308 researchers from 110 US\ninstitutions in the field of AI and computational biology to understand how\nresearchers perceive feedback generated by our GPT-4 system on their own\npapers. Overall, more than half (57.4%) of the users found GPT-4 generated\nfeedback helpful/very helpful and 82.4% found it more beneficial than feedback\nfrom at least some human reviewers. While our findings show that LLM-generated\nfeedback can help researchers, we also identify several limitations.\n",
                "链接": "https://arxiv.org/abs/2310.01783"
            },
            {
                "文章ID": "97463",
                "标题": "Can Large Language Models Find And Fix Vulnerable Software?",
                "作者": " David Noever",
                "发布日期": "2023-08-22",
                "摘要": "  In this study, we evaluated the capability of Large Language Models (LLMs),\nparticularly OpenAI's GPT-4, in detecting software vulnerabilities, comparing\ntheir performance against traditional static code analyzers like Snyk and\nFortify. Our analysis covered numerous repositories, including those from NASA\nand the Department of Defense. GPT-4 identified approximately four times the\nvulnerabilities than its counterparts. Furthermore, it provided viable fixes\nfor each vulnerability, demonstrating a low rate of false positives. Our tests\nencompassed 129 code samples across eight programming languages, revealing the\nhighest vulnerabilities in PHP and JavaScript. GPT-4's code corrections led to\na 90% reduction in vulnerabilities, requiring only an 11% increase in code\nlines. A critical insight was LLMs' ability to self-audit, suggesting fixes for\ntheir identified vulnerabilities and underscoring their precision. Future\nresearch should explore system-level vulnerabilities and integrate multiple\nstatic code analyzers for a holistic perspective on LLMs' potential.\n",
                "链接": "https://arxiv.org/abs/2308.10345"
            },
            {
                "文章ID": "107503",
                "标题": "An evolutionary model of personality traits related to cooperative\n  behavior using a large language model",
                "作者": " Reiji Suzuki,  Takaya Arita",
                "发布日期": "2023-10-11",
                "摘要": "  This paper aims to shed light on the evolutionary dynamics of diverse and\nsocial populations by introducing the rich expressiveness of generative models\ninto the trait expression of social agent-based evolutionary models.\nSpecifically, we focus on the evolution of personality traits in the context of\na game-theoretic relationship as a situation in which inter-individual\ninterests exert strong selection pressures. We construct an agent model in\nwhich linguistic descriptions of personality traits related to cooperative\nbehavior are used as genes. The deterministic strategies extracted from Large\nLanguage Model (LLM) that make behavioral decisions based on these personality\ntraits are used as behavioral traits. The population is evolved according to\nselection based on average payoff and mutation of genes by asking LLM to\nslightly modify the parent gene toward cooperative or selfish. Through\npreliminary experiments and analyses, we clarify that such a model can indeed\nexhibit the evolution of cooperative behavior based on the diverse and\nhigher-order representation of personality traits. We also observed the\nrepeated intrusion of cooperative and selfish personality traits through\nchanges in the expression of personality traits, and found that the emerging\nwords in the evolved gene well reflected the behavioral tendency of its\npersonality in terms of their semantics.\n",
                "链接": "https://arxiv.org/abs/2310.05976"
            },
            {
                "文章ID": "125173",
                "标题": "A Large Language Model-based Computational Approach to Improve\n  Identity-Related Write-Ups",
                "作者": " Alex Doboli",
                "发布日期": "2023-12-29",
                "摘要": "  Creating written products is essential to modern life, including writings\nabout one's identity and personal experiences. However, writing is often a\ndifficult activity that requires extensive effort to frame the central ideas,\nthe pursued approach to communicate the central ideas, e.g., using analogies,\nmetaphors, or other possible means, the needed presentation structure, and the\nactual verbal expression. Large Language Models, a recently emerged approach in\nMachine Learning, can offer a significant help in reducing the effort and\nimproving the quality of written products. This paper proposes a new\ncomputational approach to explore prompts that given as inputs to a Large\nLanguage Models can generate cues to improve the considered written products.\nTwo case studies on improving write-ups, one based on an analogy and one on a\nmetaphor, are also presented in the paper.\n",
                "链接": "https://arxiv.org/abs/2312.16659"
            },
            {
                "文章ID": "70928",
                "标题": "Revolutionizing Single Cell Analysis: The Power of Large Language Models\n  for Cell Type Annotation",
                "作者": " Zehua Zeng,  Hongwu Du",
                "发布日期": "2023-04-07",
                "摘要": "  In recent years, single cell RNA sequencing has become a widely used\ntechnique to study cellular diversity and function. However, accurately\nannotating cell types from single cell data has been a challenging task, as it\nrequires extensive knowledge of cell biology and gene function. The emergence\nof large language models such as ChatGPT and New Bing in 2023 has\nrevolutionized this process by integrating the scientific literature and\nproviding accurate annotations of cell types. This breakthrough enables\nresearchers to conduct literature reviews more efficiently and accurately, and\ncan potentially uncover new insights into cell type annotation. By using\nChatGPT to annotate single cell data, we can relate rare cell type to their\nfunction and reveal specific differentiation trajectories of cell subtypes that\nwere previously overlooked. This can have important applications in\nunderstanding cancer progression, mammalian development, and stem cell\ndifferentiation, and can potentially lead to the discovery of key cells that\ninterrupt the differentiation pathway and solve key problems in the life\nsciences. Overall, the future of cell type annotation in single cell data looks\npromising and the Large Language model will be an important milestone in the\nhistory of single cell analysis.\n",
                "链接": "https://arxiv.org/abs/2304.02697"
            },
            {
                "文章ID": "92138",
                "标题": "Topics, Authors, and Networks in Large Language Model Research: Trends\n  from a Survey of 17K arXiv Papers",
                "作者": " Rajiv Movva,  Sidhika Balachandar,  Kenny Peng,  Gabriel Agostini,  Nikhil Garg,  Emma Pierson",
                "发布日期": "2023-10-24",
                "摘要": "  Large language model (LLM) research is dramatically impacting society, making\nit essential to understand the topics and values it prioritizes, the authors\nand institutions driving it, and its networks of collaboration. Due to the\nrecent growth of the field, many of these fundamental attributes lack\nsystematic description. We gather, annotate, and analyze a new dataset of\n16,979 LLM-related arXiv papers, focusing on changes in 2023 vs. 2018-2022. We\nshow that LLM research increasingly focuses on societal impacts: the Computers\nand Society sub-arXiv has seen 20x growth in its proportion of LLM-related\npapers in 2023. This change is driven in part by an influx of new authors: a\nmajority of 2023 papers are first-authored by researchers who have not\npreviously written an LLM-related paper, and these papers focus particularly on\napplications and societal considerations. While a handful of companies hold\noutsize influence, academia publishes a much larger fraction of papers than\nindustry overall, and this gap widens in 2023. LLM research is also being\nshaped by social dynamics: there are gender and academic/industry differences\nin the topics authors prioritize, and a stark U.S./China schism in the\ncollaboration network. Overall, our analysis documents how LLM research both\nshapes and is shaped by society, attesting to the necessity of sociotechnical\nlenses; we discuss implications for researchers and policymakers.\n",
                "链接": "https://arxiv.org/abs/2307.10700"
            },
            {
                "文章ID": "67955",
                "标题": "Clustering US Counties to Find Patterns Related to the COVID-19 Pandemic",
                "作者": " Cora Brown,  Sarah Milstein,  Tianyi Sun,  Cooper Zhao",
                "发布日期": "2023-03-22",
                "摘要": "  When COVID-19 first started spreading and quarantine was implemented, the\nSociety for Industrial and Applied Mathematics (SIAM) Student Chapter at the\nUniversity of Minnesota-Twin Cities began a collaboration with Ecolab to use\nour skills as data scientists and mathematicians to extract useful insights\nfrom relevant data relating to the pandemic. This collaboration consisted of\nmultiple groups working on different projects. In this write-up we focus on\nusing clustering techniques to help us find groups of similar counties in the\nUS and use that to help us understand the pandemic. Our team for this project\nconsisted of University of Minnesota students Cora Brown, Sarah Milstein,\nTianyi Sun, and Cooper Zhao, with help from Ecolab Data Scientist Jimmy\nBroomfield and University of Minnesota student Skye Ke. In the sections below\nwe describe all of the work done for this project. In Section 2, we list the\ndata we gathered, as well as the feature engineering we performed. In Section\n3, we describe the metrics we used for evaluating our models. In Section 4, we\nexplain the methods we used for interpreting the results of our various\nclustering approaches. In Section 5, we describe the different clustering\nmethods we implemented. In Section 6, we present the results of our clustering\ntechniques and provide relevant interpretation. Finally, in Section 7, we\nprovide some concluding remarks comparing the different clustering methods.\n",
                "链接": "https://arxiv.org/abs/2303.11936"
            },
            {
                "文章ID": "105266",
                "标题": "A Large Language Model Approach to Educational Survey Feedback Analysis",
                "作者": " Michael J. Parker,  Caitlin Anderson,  Claire Stone,  YeaRim Oh",
                "发布日期": "2023-10-02",
                "摘要": "  This paper assesses the potential for the large language models (LLMs) GPT-4\nand GPT-3.5 to aid in deriving insight from education feedback surveys.\nExploration of LLM use cases in education has focused on teaching and learning,\nwith less exploration of capabilities in education feedback analysis. Survey\nanalysis in education involves goals such as finding gaps in curricula or\nevaluating teachers, often requiring time-consuming manual processing of\ntextual responses. LLMs have the potential to provide a flexible means of\nachieving these goals without specialized machine learning models or\nfine-tuning. We demonstrate a versatile approach to such goals by treating them\nas sequences of natural language processing (NLP) tasks including\nclassification (multi-label, multi-class, and binary), extraction, thematic\nanalysis, and sentiment analysis, each performed by LLM. We apply these\nworkflows to a real-world dataset of 2500 end-of-course survey comments from\nbiomedical science courses, and evaluate a zero-shot approach (i.e., requiring\nno examples or labeled training data) across all tasks, reflecting education\nsettings, where labeled data is often scarce. By applying effective prompting\npractices, we achieve human-level performance on multiple tasks with GPT-4,\nenabling workflows necessary to achieve typical goals. We also show the\npotential of inspecting LLMs' chain-of-thought (CoT) reasoning for providing\ninsight that may foster confidence in practice. Moreover, this study features\ndevelopment of a versatile set of classification categories, suitable for\nvarious course types (online, hybrid, or in-person) and amenable to\ncustomization. Our results suggest that LLMs can be used to derive a range of\ninsights from survey text.\n",
                "链接": "https://arxiv.org/abs/2309.17447"
            },
            {
                "文章ID": "122505",
                "标题": "Evaluating Large Language Models for Health-related Queries with\n  Presuppositions",
                "作者": " Navreet Kaur,  Monojit Choudhury,  Danish Pruthi",
                "发布日期": "2023-12-15",
                "摘要": "  As corporations rush to integrate large language models (LLMs) to their\nsearch offerings, it is critical that they provide factually accurate\ninformation that is robust to any presuppositions that a user may express. In\nthis work, we introduce UPHILL, a dataset consisting of health-related queries\nwith varying degrees of presuppositions. Using UPHILL, we evaluate the factual\naccuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find\nthat while model responses rarely disagree with true health claims (posed as\nquestions), they often fail to challenge false claims: responses from\nInstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%.\nAs we increase the extent of presupposition in input queries, the responses\nfrom InstructGPT and ChatGPT agree with the claim considerably more often,\nregardless of its veracity. Responses from BingChat, which rely on retrieved\nwebpages, are not as susceptible. Given the moderate factual accuracy, and the\ninability of models to consistently correct false assumptions, our work calls\nfor a careful assessment of current LLMs for use in high-stakes scenarios.\n",
                "链接": "https://arxiv.org/abs/2312.08800"
            },
            {
                "文章ID": "6997",
                "标题": "Matching Papers and Reviewers at Large Conferences",
                "作者": " Kevin Leyton-Brown,   Mausam,  Yatin Nandwani,  Hedayat Zarkoob,  Chris Cameron,  Neil Newman,  Dinesh Raghu",
                "发布日期": "2022-08-08",
                "摘要": "  Peer-reviewed conferences, the main publication venues in CS, rely critically\non matching highly qualified reviewers for each paper. Because of the growing\nscale of these conferences, the tight timelines on which they operate, and a\nrecent surge in explicitly dishonest behavior, there is now no alternative to\nperforming this matching in an automated way. This paper studies a novel\nreviewer-paper matching approach that was recently deployed in the 35th AAAI\nConference on Artificial Intelligence (AAAI 2021), and has since been adopted\n(wholly or partially) by other conferences including ICML 2022, AAAI 2022, and\nIJCAI 2022. This approach has three main elements: (1) collecting and\nprocessing input data to identify problematic matches and generate\nreviewer-paper scores; (2) formulating and solving an optimization problem to\nfind good reviewer-paper matchings; and (3) a two-phase reviewing process that\nshifts reviewing resources away from papers likely to be rejected and towards\npapers closer to the decision boundary. This paper also describes an evaluation\nof these innovations based on an extensive post-hoc analysis on real data --\nincluding a comparison with the matching algorithm used in AAAI's previous\n(2020) iteration -- and supplements this with additional numerical\nexperimentation.\n",
                "链接": "https://arxiv.org/abs/2202.12273"
            }
        ]
    },
    {
        "question": {
            "question": "查询2022年以来指令遵循数据集构建相关的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "2562",
                "标题": "Aerospace Human System Integration Evolution over the Last 40 Years",
                "作者": " Guy Andre Boy",
                "发布日期": "2022-01-26",
                "摘要": "  This chapter focuses on the evolution of Human-Centered Design (HCD) in\naerospace systems over the last forty years. Human Factors and Ergonomics first\nshifted from the study of physical and medical issues to cognitive issues circa\nthe 1980s. The advent of computers brought with it the development of\nhuman-computer interaction (HCI), which then expanded into the field of digital\ninteraction design and User Experience (UX). We ended up with the concept of\ninteractive cockpits, not because pilots interacted with mechanical things, but\nbecause they interacted using pointing devices on computer displays. Since the\nearly 2000s, complexity and organizational issues gained prominence to the\npoint that complex systems design and management found itself center stage,\nwith the spotlight on the role of the human element and organizational setups.\nToday, Human Systems Integration (HSI) is no longer only a single-agent\nproblem, but a multi-agent research field. Systems are systems of systems,\nconsidered as representations of people and machines. They are made of\nstatically and dynamically articulated structures and functions. When they are\nat work, they are living organisms that generate emerging functions and\nstructures that need to be considered in evolution (i.e., in their constant\nredesign). This chapter will more specifically, focus on human factors such as\nhuman-centered systemic representations, life critical systems, organizational\nissues, complexity management, modeling and simulation, flexibility,\ntangibility and autonomy. The discussion will be based on several examples in\ncivil aviation and air combat, as well as aerospace.\n",
                "链接": "https://arxiv.org/abs/2201.10275"
            },
            {
                "文章ID": "5563",
                "标题": "Case law retrieval: problems, methods, challenges and evaluations in the\n  last 20 years",
                "作者": " Daniel Locke,  Guido Zuccon",
                "发布日期": "2022-02-16",
                "摘要": "  Case law retrieval is the retrieval of judicial decisions relevant to a legal\nquestion. Case law retrieval comprises a significant amount of a lawyer's time,\nand is important to ensure accurate advice and reduce workload. We survey\nmethods for case law retrieval from the past 20 years and outline the problems\nand challenges facing evaluation of case law retrieval systems going forward.\nLimited published work has focused on improving ranking in ad-hoc case law\nretrieval. But there has been significant work in other areas of case law\nretrieval, and legal information retrieval generally. This is likely due to\nlegal search providers being unwilling to give up the secrets of their success\nto competitors. Most evaluations of case law retrieval have been undertaken on\nsmall collections and focus on related tasks such as question-answer systems or\nrecommender systems. Work has not focused on Cranfield style evaluations and\nbaselines of methods for case law retrieval on publicly available test\ncollections are not present. This presents a major challenge going forward. But\nthere are reasons to question the extent of this problem, at least in a\ncommercial setting. Without test collections to baseline approaches it cannot\nbe known whether methods are promising. Works by commercial legal search\nproviders show the effectiveness of natural language systems as well as query\nexpansion for case law retrieval. Machine learning is being applied to more and\nmore legal search tasks, and undoubtedly this represents the future of case law\nretrieval.\n",
                "链接": "https://arxiv.org/abs/2202.07209"
            },
            {
                "文章ID": "82082",
                "标题": "Large Car-following Data Based on Lyft level-5 Open Dataset: Following\n  Autonomous Vehicles vs. Human-driven Vehicles",
                "作者": " Guopeng Li,  Yiru Jiao,  Victor L. Knoop,  Simeon C. Calvert,  J. W. C. van Lint",
                "发布日期": "2023-11-22",
                "摘要": "  Car-Following (CF), as a fundamental driving behaviour, has significant\ninfluences on the safety and efficiency of traffic flow. Investigating how\nhuman drivers react differently when following autonomous vs. human-driven\nvehicles (HV) is thus critical for mixed traffic flow. Research in this field\ncan be expedited with trajectory datasets collected by Autonomous Vehicles\n(AVs). However, trajectories collected by AVs are noisy and not readily\napplicable for studying CF behaviour. This paper extracts and enhances two\ncategories of CF data, HV-following-AV (H-A) and HV-following-HV (H-H), from\nthe open Lyft level-5 dataset. First, CF pairs are selected based on specific\nrules. Next, the quality of raw data is assessed by anomaly analysis. Then, the\nraw CF data is corrected and enhanced via motion planning, Kalman filtering,\nand wavelet denoising. As a result, 29k+ H-A and 42k+ H-H car-following\nsegments are obtained, with a total driving distance of 150k+ km. A diversity\nassessment shows that the processed data cover complete CF regimes for\ncalibrating CF models. This open and ready-to-use dataset provides the\nopportunity to investigate the CF behaviours of following AVs vs. HVs from\nreal-world data. It can further facilitate studies on exploring the impact of\nAVs on mixed urban traffic.\n",
                "链接": "https://arxiv.org/abs/2305.18921"
            },
            {
                "文章ID": "55146",
                "标题": "Hybrid Representation Learning for Cognitive Diagnosis in Late-Life\n  Depression Over 5 Years with Structural MRI",
                "作者": " Lintao Zhang,  Lihong Wang,  Minhui Yu,  Rong Wu,  David C. Steffens,  Guy G. Potter,  Mingxia Liu",
                "发布日期": "2022-12-27",
                "摘要": "  Late-life depression (LLD) is a highly prevalent mood disorder occurring in\nolder adults and is frequently accompanied by cognitive impairment (CI).\nStudies have shown that LLD may increase the risk of Alzheimer's disease (AD).\nHowever, the heterogeneity of presentation of geriatric depression suggests\nthat multiple biological mechanisms may underlie it. Current biological\nresearch on LLD progression incorporates machine learning that combines\nneuroimaging data with clinical observations. There are few studies on incident\ncognitive diagnostic outcomes in LLD based on structural MRI (sMRI). In this\npaper, we describe the development of a hybrid representation learning (HRL)\nframework for predicting cognitive diagnosis over 5 years based on T1-weighted\nsMRI data. Specifically, we first extract prediction-oriented MRI features via\na deep neural network, and then integrate them with handcrafted MRI features\nvia a Transformer encoder for cognitive diagnosis prediction. Two tasks are\ninvestigated in this work, including (1) identifying cognitively normal\nsubjects with LLD and never-depressed older healthy subjects, and (2)\nidentifying LLD subjects who developed CI (or even AD) and those who stayed\ncognitively normal over five years. To the best of our knowledge, this is among\nthe first attempts to study the complex heterogeneous progression of LLD based\non task-oriented and handcrafted MRI features. We validate the proposed HRL on\n294 subjects with T1-weighted MRIs from two clinically harmonized studies.\nExperimental results suggest that the HRL outperforms several classical machine\nlearning and state-of-the-art deep learning methods in LLD identification and\nprediction tasks.\n",
                "链接": "https://arxiv.org/abs/2212.12810"
            },
            {
                "文章ID": "123662",
                "标题": "The Validity of a Machine Learning-Based Video Game in the Objective\n  Screening of Attention Deficit Hyperactivity Disorder in Children Aged 5 to\n  12 Years",
                "作者": " Zeinab Zakani,  Hadi Moradi,  Sogand Ghasemzadeh,  Maryam Riazi,  Fatemeh Mortazavi",
                "发布日期": "2023-12-20",
                "摘要": "  Objective: Early identification of ADHD is necessary to provide the\nopportunity for timely treatment. However, screening the symptoms of ADHD on a\nlarge scale is not easy. This study aimed to validate a video game (FishFinder)\nfor the screening of ADHD using objective measurement of the core symptoms of\nthis disorder. Method: The FishFinder measures attention and impulsivity\nthrough in-game performance and evaluates the child's hyperactivity using\nsmartphone motion sensors. This game was tested on 26 children with ADHD and 26\nhealthy children aged 5 to 12 years. A Support Vector Machine was employed to\ndetect children with ADHD. results: This system showed 92.3% accuracy, 90%\nsensitivity, and 93.7% specificity using a combination of in-game and movement\nfeatures. Conclusions: The FishFinder demonstrated a strong ability to identify\nADHD in children. So, this game can be used as an affordable, accessible, and\nenjoyable method for the objective screening of ADHD.\n",
                "链接": "https://arxiv.org/abs/2312.11832"
            },
            {
                "文章ID": "95837",
                "标题": "Follow Anything: Open-set detection, tracking, and following in\n  real-time",
                "作者": " Alaa Maalouf,  Ninad Jadhav,  Krishna Murthy Jatavallabhula,  Makram Chahine,  Daniel M. Vogt,  Robert J. Wood,  Antonio Torralba,  Daniela Rus",
                "发布日期": "2023-08-11",
                "摘要": "  Tracking and following objects of interest is critical to several robotics\nuse cases, ranging from industrial automation to logistics and warehousing, to\nhealthcare and security. In this paper, we present a robotic system to detect,\ntrack, and follow any object in real-time. Our approach, dubbed ``follow\nanything'' (FAn), is an open-vocabulary and multimodal model -- it is not\nrestricted to concepts seen at training time and can be applied to novel\nclasses at inference time using text, images, or click queries. Leveraging rich\nvisual descriptors from large-scale pre-trained models (foundation models), FAn\ncan detect and segment objects by matching multimodal queries (text, images,\nclicks) against an input image sequence. These detected and segmented objects\nare tracked across image frames, all while accounting for occlusion and object\nre-emergence. We demonstrate FAn on a real-world robotic system (a micro aerial\nvehicle) and report its ability to seamlessly follow the objects of interest in\na real-time control loop. FAn can be deployed on a laptop with a lightweight\n(6-8 GB) graphics card, achieving a throughput of 6-20 frames per second. To\nenable rapid adoption, deployment, and extensibility, we open-source all our\ncode on our project webpage at https://github.com/alaamaalouf/FollowAnything .\nWe also encourage the reader the watch our 5-minutes explainer video in this\nhttps://www.youtube.com/watch?v=6Mgt3EPytrw .\n",
                "链接": "https://arxiv.org/abs/2308.05737"
            },
            {
                "文章ID": "82232",
                "标题": "Visual Exploratory Data Analysis of the Covid-19 Pandemic in Nigeria:\n  Two Years after the Outbreak",
                "作者": " Ugochukwu Orji,  Modesta Ezema,  Elochukwu Ukwandu,  Chikaodili Ugwuishiwu,  Ezugwu Obianuju,  Malachi Egbugha",
                "发布日期": "2023-06-04",
                "摘要": "  The outbreak of the coronavirus disease in Nigeria and all over the world in\n2019/2020 caused havoc on the world's economy and put a strain on global\nhealthcare facilities and personnel. It also threw up many opportunities to\nimprove processes using artificial intelligence techniques like big data\nanalytics and business intelligence. The need to speedily make decisions that\ncould have far-reaching effects is prompting the boom in data analytics which\nis achieved via exploratory data analysis (EDA) to see trends, patterns, and\nrelationships in the data. Today, big data analytics is revolutionizing\nprocesses and helping improve productivity and decision-making capabilities in\nall aspects of life. The large amount of heterogeneous and, in most cases,\nopaque data now available has made it possible for researchers and businesses\nof all sizes to effectively deploy data analytics to gain action-oriented\ninsights into various problems in real time. In this paper, we deployed\nMicrosoft Excel and Python to perform EDA of the covid-19 pandemic data in\nNigeria and presented our results via visualizations and a dashboard using\nTableau. The dataset is from the Nigeria Centre for Disease Control (NCDC)\nrecorded between February 28th, 2020, and July 19th, 2022. This paper aims to\nfollow the data and visually show the trends over the past 2 years and also\nshow the powerful capabilities of these data analytics tools and techniques.\nFurthermore, our findings contribute to the current literature on Covid-19\nresearch by showcasing how the virus has progressed in Nigeria over time and\nthe insights thus far.\n",
                "链接": "https://arxiv.org/abs/2305.19297"
            },
            {
                "文章ID": "24457",
                "标题": "The Open Kidney Ultrasound Data Set",
                "作者": " Rohit Singla,  Cailin Ringstrom,  Grace Hu,  Victoria Lessoway,  Janice Reid,  Christopher Nguan,  Robert Rohling",
                "发布日期": "2022-12-06",
                "摘要": "  Ultrasound, because of its low cost, non-ionizing, and non-invasive\ncharacteristics, has established itself as a cornerstone radiological\nexamination. Research on ultrasound applications has also expanded, especially\nwith image analysis with machine learning. However, ultrasound data are\nfrequently restricted to closed data sets, with only a few openly available.\nDespite being a frequently examined organ, the kidney lacks a publicly\navailable ultrasonography data set. The proposed Open Kidney Ultrasound Data\nSet is the first publicly available set of kidney brightness mode (B-mode)\nultrasound data that includes annotations for multi-class semantic\nsegmentation. It is based on data retrospectively collected in a 5-year period\nfrom over 500 patients with a mean age of 53.2 +/- 14.7 years, body mass index\nof 27.0 +/- 5.4 kg/m2, and most common primary diseases being diabetes\nmellitus, immunoglobulin A (IgA) nephropathy, and hypertension. There are\nlabels for the view and fine-grained manual annotations from two expert\nsonographers. Notably, this data includes native and transplanted kidneys.\nInitial bench-marking measurements are performed, demonstrating a\nstate-of-the-art algorithm achieving a Dice Sorenson Coefficient of 0.85 for\nthe kidney capsule. This data set is a high-quality data set, including two\nsets of expert annotations, with a larger breadth of images than previously\navailable. In increasing access to kidney ultrasound data, future researchers\nmay be able to create novel image analysis techniques for tissue\ncharacterization, disease detection, and prognostication.\n",
                "链接": "https://arxiv.org/abs/2206.06657"
            },
            {
                "文章ID": "41679",
                "标题": "How to construct the symmetric cycle of length 5 using Haj\\'os\n  construction with an adapted Rank Genetic Algorithm",
                "作者": " Juan Carlos García-Altamirano,  Mika Olsen,  Jorge Cervantes-Ojeda",
                "发布日期": "2023-06-22",
                "摘要": "  In 2020 Bang-Jensen et. al. generalized the Haj\\'os join of two graphs to the\nclass of digraphs and generalized several results for vertex colorings in\ndigraphs. Although, as a consequence of these results, a digraph can be\nobtained by Haj\\'os constructions (directed Haj\\'os join and identifying\nnon-adjacent vertices), determining the Haj\\'os constructions to obtain the\ndigraph is a complex problem. In particular, Bang-Jensen et al. posed the\nproblem of determining the Haj\\'os operations to construct the symmetric\n5-cycle from the complete symmetric digraph of order 3 using only Haj\\'os\nconstructions. We successfully adapted a rank-based genetic algorithm to solve\nthis problem by the introduction of innovative recombination and mutation\noperators from graph theory. The Haj\\'os Join became the recombination operator\nand the identification of independent vertices became the mutation operator. In\nthis way, we were able to obtain a sequence of only 16 Haj\\'os operations to\nconstruct the symmetric cycle of order 5.\n",
                "链接": "https://arxiv.org/abs/2210.05080"
            },
            {
                "文章ID": "86386",
                "标题": "TOBY: A Tool for Exploring Data in Academic Survey Papers",
                "作者": " Tathagata Chakraborti,  Jungkoo Kang,  Christian Muise,  Sarath Sreedharan,  Michael Walker,  Daniel Szafir,  Tom Williams",
                "发布日期": "2023-06-21",
                "摘要": "  This paper describes TOBY, a visualization tool that helps a user explore the\ncontents of an academic survey paper. The visualization consists of four\ncomponents: a hierarchical view of taxonomic data in the survey, a document\nsimilarity view in the space of taxonomic classes, a network view of citations,\nand a new paper recommendation tool. In this paper, we will discuss these\nfeatures in the context of three separate deployments of the tool.\n",
                "链接": "https://arxiv.org/abs/2306.10051"
            }
        ]
    },
    {
        "question": {
            "question": "查找关于gpt4自动生成prompt的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "79870",
                "标题": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
                "作者": " Tiedong Liu,  Bryan Kian Hsiang Low",
                "发布日期": "2023-05-24",
                "摘要": "  We introduce Goat, a fine-tuned LLaMA model that significantly outperforms\nGPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated\ndataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic\nsub-task. In particular, the zero-shot Goat-7B matches or even surpasses the\naccuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve\nnear-perfect accuracy on large-number addition and subtraction through\nsupervised fine-tuning only, which is almost impossible with previous\npretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute\nGoat's exceptional performance to LLaMA's consistent tokenization of numbers.\nTo tackle more challenging tasks like large-number multiplication and division,\nwe propose an approach that classifies tasks based on their learnability, and\nsubsequently decomposes unlearnable tasks, such as multi-digit multiplication\nand division, into a series of learnable tasks by leveraging basic arithmetic\nprinciples. We thoroughly examine the performance of our model, offering a\ncomprehensive evaluation of the effectiveness of our proposed decomposition\nsteps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM\nGPU, facilitating reproducibility for other researchers. We release our model,\ndataset, and the Python script for dataset generation.\n",
                "链接": "https://arxiv.org/abs/2305.14201"
            },
            {
                "文章ID": "122336",
                "标题": "Prompt Engineering-assisted Malware Dynamic Analysis Using GPT-4",
                "作者": " Pei Yan,  Shunquan Tan,  Miaohui Wang,  Jiwu Huang",
                "发布日期": "2023-12-14",
                "摘要": "  Dynamic analysis methods effectively identify shelled, wrapped, or obfuscated\nmalware, thereby preventing them from invading computers. As a significant\nrepresentation of dynamic malware behavior, the API (Application Programming\nInterface) sequence, comprised of consecutive API calls, has progressively\nbecome the dominant feature of dynamic analysis methods. Though there have been\nnumerous deep learning models for malware detection based on API sequences, the\nquality of API call representations produced by those models is limited. These\nmodels cannot generate representations for unknown API calls, which weakens\nboth the detection performance and the generalization. Further, the concept\ndrift phenomenon of API calls is prominent. To tackle these issues, we\nintroduce a prompt engineering-assisted malware dynamic analysis using GPT-4.\nIn this method, GPT-4 is employed to create explanatory text for each API call\nwithin the API sequence. Afterward, the pre-trained language model BERT is used\nto obtain the representation of the text, from which we derive the\nrepresentation of the API sequence. Theoretically, this proposed method is\ncapable of generating representations for all API calls, excluding the\nnecessity for dataset training during the generation process. Utilizing the\nrepresentation, a CNN-based detection model is designed to extract the feature.\nWe adopt five benchmark datasets to validate the performance of the proposed\nmodel. The experimental results reveal that the proposed detection algorithm\nperforms better than the state-of-the-art method (TextCNN). Specifically, in\ncross-database experiments and few-shot learning experiments, the proposed\nmodel achieves excellent detection performance and almost a 100% recall rate\nfor malware, verifying its superior generalization performance. The code is\navailable at: github.com/yan-scnu/Prompted_Dynamic_Detection.\n",
                "链接": "https://arxiv.org/abs/2312.08317"
            },
            {
                "文章ID": "102940",
                "标题": "An Evaluation of GPT-4 on the ETHICS Dataset",
                "作者": " Sergey Rodionov,  Zarathustra Amadeus Goertzel,  Ben Goertzel",
                "发布日期": "2023-09-20",
                "摘要": "  This report summarizes a short study of the performance of GPT-4 on the\nETHICS dataset. The ETHICS dataset consists of five sub-datasets covering\ndifferent fields of ethics: Justice, Deontology, Virtue Ethics, Utilitarianism,\nand Commonsense Ethics. The moral judgments were curated so as to have a high\ndegree of agreement with the aim of representing shared human values rather\nthan moral dilemmas. GPT-4's performance is much better than that of previous\nmodels and suggests that learning to work with common human values is not the\nhard problem for AI ethics.\n",
                "链接": "https://arxiv.org/abs/2309.10492"
            },
            {
                "文章ID": "112767",
                "标题": "Does GPT-4 Pass the Turing Test?",
                "作者": " Cameron Jones,  Benjamin Bergen",
                "发布日期": "2023-11-01",
                "摘要": "  We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4\nprompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and\nGPT-3.5 (14%), but falling short of chance and the baseline set by human\nparticipants (63%). Participants' decisions were based mainly on linguistic\nstyle (35%) and socio-emotional traits (27%), supporting the idea that\nintelligence is not sufficient to pass the Turing Test. Participants'\ndemographics, including education and familiarity with LLMs, did not predict\ndetection rate, suggesting that even those who understand systems deeply and\ninteract with them frequently may be susceptible to deception. Despite known\nlimitations as a test of intelligence, we argue that the Turing Test continues\nto be relevant as an assessment of naturalistic communication and deception. AI\nmodels with the ability to masquerade as humans could have widespread societal\nconsequences, and we analyse the effectiveness of different strategies and\ncriteria for judging humanlikeness.\n",
                "链接": "https://arxiv.org/abs/2310.20216"
            },
            {
                "文章ID": "92427",
                "标题": "Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts",
                "作者": " Mayug Maniparambil,  Chris Vorster,  Derek Molloy,  Noel Murphy,  Kevin McGuinness,  Noel E. O'Connor",
                "发布日期": "2023-08-09",
                "摘要": "  Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have\nrevolutionized visual representation learning by providing good performance on\ndownstream datasets. VLMs are 0-shot adapted to a downstream dataset by\ndesigning prompts that are relevant to the dataset. Such prompt engineering\nmakes use of domain expertise and a validation dataset. Meanwhile, recent\ndevelopments in generative pretrained models like GPT-4 mean they can be used\nas advanced internet search tools. They can also be manipulated to provide\nvisual information in any structure. In this work, we show that GPT-4 can be\nused to generate text that is visually descriptive and how this can be used to\nadapt CLIP to downstream tasks. We show considerable improvements in 0-shot\ntransfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD\n(~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt.\nWe also design a simple few-shot adapter that learns to choose the best\npossible sentences to construct generalizable classifiers that outperform the\nrecently proposed CoCoOP by ~2% on average and by over 4% on 4 specialized\nfine-grained datasets. The code, prompts, and auxiliary text dataset is\navailable at https://github.com/mayug/VDT-Adapter.\n",
                "链接": "https://arxiv.org/abs/2307.11661"
            },
            {
                "文章ID": "67765",
                "标题": "Mind meets machine: Unravelling GPT-4's cognitive psychology",
                "作者": " Sifatkaur Dhingra,  Manmeet Singh,  Vaisakh SB,  Neetiraj Malviya,  Sukhpal Singh Gill",
                "发布日期": "2023-04-13",
                "摘要": "  Cognitive psychology delves on understanding perception, attention, memory,\nlanguage, problem-solving, decision-making, and reasoning. Large language\nmodels (LLMs) are emerging as potent tools increasingly capable of performing\nhuman-level tasks. The recent development in the form of GPT-4 and its\ndemonstrated success in tasks complex to humans exam and complex problems has\nled to an increased confidence in the LLMs to become perfect instruments of\nintelligence. Although GPT-4 report has shown performance on some cognitive\npsychology tasks, a comprehensive assessment of GPT-4, via the existing\nwell-established datasets is required. In this study, we focus on the\nevaluation of GPT-4's performance on a set of cognitive psychology datasets\nsuch as CommonsenseQA, SuperGLUE, MATH and HANS. In doing so, we understand how\nGPT-4 processes and integrates cognitive psychology with contextual\ninformation, providing insight into the underlying cognitive processes that\nenable its ability to generate the responses. We show that GPT-4 exhibits a\nhigh level of accuracy in cognitive psychology tasks relative to the prior\nstate-of-the-art models. Our results strengthen the already available\nassessments and confidence on GPT-4's cognitive psychology abilities. It has\nsignificant potential to revolutionize the field of AI, by enabling machines to\nbridge the gap between human and machine reasoning.\n",
                "链接": "https://arxiv.org/abs/2303.11436"
            },
            {
                "文章ID": "114775",
                "标题": "Removing RLHF Protections in GPT-4 via Fine-Tuning",
                "作者": " Qiusi Zhan,  Richard Fang,  Rohan Bindu,  Akul Gupta,  Tatsunori Hashimoto,  Daniel Kang",
                "发布日期": "2023-11-14",
                "摘要": "  As large language models (LLMs) have increased in their capabilities, so does\ntheir potential for dual use. To reduce harmful outputs, produces and vendors\nof LLMs have used reinforcement learning with human feedback (RLHF). In tandem,\nLLM vendors have been increasingly enabling fine-tuning of their most powerful\nmodels. However, concurrent work has shown that fine-tuning can remove RLHF\nprotections. We may expect that the most powerful models currently available\n(GPT-4) are less susceptible to fine-tuning attacks.\n  In this work, we show the contrary: fine-tuning allows attackers to remove\nRLHF protections with as few as 340 examples and a 95% success rate. These\ntraining examples can be automatically generated with weaker models. We further\nshow that removing RLHF protections does not decrease usefulness on\nnon-censored outputs, providing evidence that our fine-tuning strategy does not\ndecrease usefulness despite using weaker models to generate training data. Our\nresults show the need for further research on protections on LLMs.\n",
                "链接": "https://arxiv.org/abs/2311.05553"
            },
            {
                "文章ID": "66852",
                "标题": "GPT-4 Technical Report",
                "作者": "Rai   OpenAI, Rai   :, Rai  Josh Achiam, Rai  Steven Adler, Rai  Sandhini Agarwal, Rai  Lama Ahmad, Rai  Ilge Akkaya, Rai  Florencia Leoni Aleman, Rai  Diogo Almeida, Rai  Janko Altenschmidt, Rai  Sam Altman, Rai  Shyamal Anadkat, Rai  Red Avila, Rai  Igor Babuschkin, Rai  Suchir Balaji, Rai  Valerie Balcom, Rai  Paul Baltescu, Rai  Haiming Bao, Rai  Mo Bavarian, Rai  Jeff Belgum, Rai  Irwan Bello, Rai  Jake Berdine, Rai  Gabriel Bernadett-Shapiro, Rai  Christopher Berner, Rai  Lenny Bogdonoff, Rai  Oleg Boiko, Rai  Madelaine Boyd, Rai  Anna-Luisa Brakman, Rai  Greg Brockman, Rai  Tim Brooks, Rai  Miles Brundage, Rai  Kevin Button, Rai  Trevor Cai, Rai  Rosie Campbell, Rai  Andrew Cann, Rai  Brittany Carey, Rai  Chelsea Carlson, Rai  Rory Carmichael, Rai  Brooke Chan, Rai  Che Chang, Rai  Fotis Chantzis, Rai  Derek Chen, Rai  Sully Chen, Rai  Ruby Chen, Rai  Jason Chen, Rai  Mark Chen, Rai  Ben Chess, Rai  Chester Cho, Rai  Casey Chu, Rai  Hyung Won Chung, Rai  Dave Cummings, Rai  Jeremiah Currier, Rai  Yunxing Dai, Rai  Cory Decareaux, Rai  Thomas Degry, Rai  Noah Deutsch, Rai  Damien Deville, Rai  Arka Dhar, Rai  David Dohan, Rai  Steve Dowling, Rai  Sheila Dunning, Rai  Adrien Ecoffet, Rai  Atty Eleti, Rai  Tyna Eloundou, Rai  David Farhi, Rai  Liam Fedus, Rai  Niko Felix, Rai  Simón Posada Fishman, Rai  Juston Forte, Rai  Isabella Fulford, Rai  Leo Gao, Rai  Elie Georges, Rai  Christian Gibson, Rai  Vik Goel, Rai  Tarun Gogineni, Rai  Gabriel Goh, Rai  Rapha Gontijo-Lopes, Rai  Jonathan Gordon, Rai  Morgan Grafstein, Rai  Scott Gray, Rai  Ryan Greene, Rai  Joshua Gross, Rai  Shixiang Shane Gu, Rai  Yufei Guo, Rai  Chris Hallacy, Rai  Jesse Han, Rai  Jeff Harris, Rai  Yuchen He, Rai  Mike Heaton, Rai  Johannes Heidecke, Rai  Chris Hesse, Rai  Alan Hickey, Rai  Wade Hickey, Rai  Peter Hoeschele, Rai  Brandon Houghton, Rai  Kenny Hsu, Rai  Shengli Hu, Rai  Xin Hu, Rai  Joost Huizinga, Rai  Shantanu Jain, Rai  Shawn Jain, Rai  Joanne Jang, Rai  Angela Jiang, Rai  Roger Jiang, Rai  Haozhun Jin, Rai  Denny Jin, Rai  Shino Jomoto, Rai  Billie Jonn, Rai  Heewoo Jun, Rai  Tomer Kaftan, Rai  Łukasz Kaiser, Rai  Ali Kamali, Rai  Ingmar Kanitscheider, Rai  Nitish Shirish Keskar, Rai  Tabarak Khan, Rai  Logan Kilpatrick, Rai  Jong Wook Kim, Rai  Christina Kim, Rai  Yongjik Kim, Rai  Hendrik Kirchner, Rai  Jamie Kiros, Rai  Matt Knight, Rai  Daniel Kokotajlo, Rai  Łukasz Kondraciuk, Rai  Andrew Kondrich, Rai  Aris Konstantinidis, Rai  Kyle Kosic, Rai  Gretchen Krueger, Rai  Vishal Kuo, Rai  Michael Lampe, Rai  Ikai Lan, Rai  Teddy Lee, Rai  Jan Leike, Rai  Jade Leung, Rai  Daniel Levy, Rai  Chak Ming Li, Rai  Rachel Lim, Rai  Molly Lin, Rai  Stephanie Lin, Rai  Mateusz Litwin, Rai  Theresa Lopez, Rai  Ryan Lowe, Rai  Patricia Lue, Rai  Anna Makanju, Rai  Kim Malfacini, Rai  Sam Manning, Rai  Todor Markov, Rai  Yaniv Markovski, Rai  Bianca Martin, Rai  Katie Mayer, Rai  Andrew Mayne, Rai  Bob McGrew, Rai  Scott Mayer McKinney, Rai  Christine McLeavey, Rai  Paul McMillan, Rai  Jake McNeil, Rai  David Medina, Rai  Aalok Mehta, Rai  Jacob Menick, Rai  Luke Metz, Rai  Andrey Mishchenko, Rai  Pamela Mishkin, Rai  Vinnie Monaco, Rai  Evan Morikawa, Rai  Daniel Mossing, Rai  Tong Mu, Rai  Mira Murati, Rai  Oleg Murk, Rai  David Mély, Rai  Ashvin Nair, Rai  Reiichiro Nakano, Rai  Rajeev Nayak, Rai  Arvind Neelakantan, Rai  Richard Ngo, Rai  Hyeonwoo Noh, Rai  Long Ouyang, Rai  Cullen O'Keefe, Rai  Jakub Pachocki, Rai  Alex Paino, Rai  Joe Palermo, Rai  Ashley Pantuliano, Rai  Giambattista Parascandolo, Rai  Joel Parish, Rai  Emy Parparita, Rai  Alex Passos, Rai  Mikhail Pavlov, Rai  Andrew Peng, Rai  Adam Perelman, Rai  Filipe de Avila Belbute Peres, Rai  Michael Petrov, Rai  Henrique Ponde de Oliveira Pinto, Rai   Michael,   Pokorny,  Michelle Pokrass,  Vitchyr Pong,  Tolly Powell,  Alethea Power,  Boris Power,  Elizabeth Proehl,  Raul Puri,  Alec Radford,  Jack Rae,  Aditya Ramesh,  Cameron Raymond,  Francis Real,  Kendra Rimbach,  Carl Ross,  Bob Rotsted,  Henri Roussez,  Nick Ryder,  Mario Saltarelli,  Ted Sanders,  Shibani Santurkar,  Girish Sastry,  Heather Schmidt,  David Schnurr,  John Schulman,  Daniel Selsam,  Kyla Sheppard,  Toki Sherbakov,  Jessica Shieh,  Sarah Shoker,  Pranav Shyam,  Szymon Sidor,  Eric Sigler,  Maddie Simens,  Jordan Sitkin,  Katarina Slama,  Ian Sohl,  Benjamin Sokolowsky,  Yang Song,  Natalie Staudacher,  Felipe Petroski Such,  Natalie Summers,  Ilya Sutskever,  Jie Tang,  Nikolas Tezak,  Madeleine Thompson,  Phil Tillet,  Amin Tootoonchian,  Elizabeth Tseng,  Preston Tuggle,  Nick Turley,  Jerry Tworek,  Juan Felipe Cerón Uribe,  Andrea Vallone,  Arun Vijayvergiya,  Chelsea Voss,  Carroll Wainwright,  Justin Jay Wang,  Alvin Wang,  Ben Wang,  Jonathan Ward,  Jason Wei,  CJ Weinmann,  Akila Welihinda,  Peter Welinder,  Jiayi Weng,  Lilian Weng,  Matt Wiethoff,  Dave Willner,  Clemens Winter,  Samuel Wolrich,  Hannah Wong,  Lauren Workman,  Sherwin Wu,  Jeff Wu,  Michael Wu,  Kai Xiao,  Tao Xu,  Sarah Yoo,  Kevin Yu,  Qiming Yuan,  Wojciech Zaremba,  Rowan Zellers,  Chong Zhang,  Marvin Zhang,  Shengjia Zhao,  Tianhao Zheng,  Juntang Zhuang,  William Zhuk,  Barret Zoph",
                "发布日期": "2023-12-20",
                "摘要": "  We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.\n",
                "链接": "https://arxiv.org/abs/2303.08774"
            },
            {
                "文章ID": "95177",
                "标题": "GPT-4 Can't Reason",
                "作者": " Konstantine Arkoudas",
                "发布日期": "2023-08-11",
                "摘要": "  GPT-4 was released in March 2023 to wide acclaim, marking a very substantial\nimprovement across the board over GPT-3.5 (OpenAI's previously best model,\nwhich had powered the initial release of ChatGPT). However, despite the\ngenuinely impressive improvement, there are good reasons to be highly skeptical\nof GPT-4's ability to reason. This position paper discusses the nature of\nreasoning; criticizes the current formulation of reasoning problems in the NLP\ncommunity, as well as the way in which LLM reasoning performance is currently\nevaluated; introduces a small collection of 21 diverse reasoning problems; and\nperforms a detailed qualitative evaluation of GPT-4's performance on those\nproblems. Based on this analysis, the paper concludes that, despite its\noccasional flashes of analytical brilliance, GPT-4 at present is utterly\nincapable of reasoning.\n",
                "链接": "https://arxiv.org/abs/2308.03762"
            },
            {
                "文章ID": "68454",
                "标题": "Capabilities of GPT-4 on Medical Challenge Problems",
                "作者": " Harsha Nori,  Nicholas King,  Scott Mayer McKinney,  Dean Carignan,  Eric Horvitz",
                "发布日期": "2023-04-13",
                "摘要": "  Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation across various domains, including\nmedicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art\nLLM, on medical competency examinations and benchmark datasets. GPT-4 is a\ngeneral-purpose model that is not specialized for medical problems through\ntraining or engineered to solve clinical tasks. Our analysis covers two sets of\nofficial practice materials for the USMLE, a three-step examination program\nused to assess clinical competency and grant licensure in the United States. We\nalso evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond\nmeasuring model performance, experiments were conducted to investigate the\ninfluence of test questions containing both text and images on model\nperformance, probe for memorization of content during training, and study\nprobability calibration, which is of critical importance in high-stakes\napplications like medicine. Our results show that GPT-4, without any\nspecialized prompt crafting, exceeds the passing score on USMLE by over 20\npoints and outperforms earlier general-purpose models (GPT-3.5) as well as\nmodels specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned\nversion of Flan-PaLM 540B). In addition, GPT-4 is significantly better\ncalibrated than GPT-3.5, demonstrating a much-improved ability to predict the\nlikelihood that its answers are correct. We also explore the behavior of the\nmodel qualitatively through a case study that shows the ability of GPT-4 to\nexplain medical reasoning, personalize explanations to students, and\ninteractively craft new counterfactual scenarios around a medical case.\nImplications of the findings are discussed for potential uses of GPT-4 in\nmedical education, assessment, and clinical practice, with appropriate\nattention to challenges of accuracy and safety.\n",
                "链接": "https://arxiv.org/abs/2303.13375"
            }
        ]
    },
    {
        "question": {
            "question": "查找多模态agent, 具身智能的相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "2627",
                "标题": "A Method to Predict Semantic Relations on Artificial Intelligence Papers",
                "作者": " Francisco Andrades,  Ricardo Ñanculef",
                "发布日期": "2022-01-26",
                "摘要": "  Predicting the emergence of links in large evolving networks is a difficult\ntask with many practical applications. Recently, the Science4cast competition\nhas illustrated this challenge presenting a network of 64.000 AI concepts and\nasking the participants to predict which topics are going to be researched\ntogether in the future. In this paper, we present a solution to this problem\nbased on a new family of deep learning approaches, namely Graph Neural\nNetworks. The results of the challenge show that our solution is competitive\neven if we had to impose severe restrictions to obtain a computationally\nefficient and parsimonious model: ignoring the intrinsic dynamics of the graph\nand using only a small subset of the nodes surrounding a target link.\nPreliminary experiments presented in this paper suggest the model is learning\ntwo related, but different patterns: the absorption of a node by a sub-graph\nand union of more dense sub-graphs. The model seems to excel at recognizing the\nfirst type of pattern.\n",
                "链接": "https://arxiv.org/abs/2201.10518"
            },
            {
                "文章ID": "65134",
                "标题": "Can an Embodied Agent Find Your \"Cat-shaped Mug\"? LLM-Guided Exploration\n  for Zero-Shot Object Navigation",
                "作者": " Vishnu Sashank Dorbala, Jr. James F. Mullen,  Dinesh Manocha",
                "发布日期": "2023-11-07",
                "摘要": "  We present LGX (Language-guided Exploration), a novel algorithm for\nLanguage-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied\nagent navigates to a uniquely described target object in a previously unseen\nenvironment. Our approach makes use of Large Language Models (LLMs) for this\ntask by leveraging the LLM's commonsense reasoning capabilities for making\nsequential navigational decisions. Simultaneously, we perform generalized\ntarget object detection using a pre-trained Vision-Language grounding model. We\nachieve state-of-the-art zero-shot object navigation results on RoboTHOR with a\nsuccess rate (SR) improvement of over 27% over the current baseline of the\nOWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for\nrobot navigation and present an analysis of various prompting strategies\naffecting the model output. Finally, we showcase the benefits of our approach\nvia \\textit{real-world} experiments that indicate the superior performance of\nLGX in detecting and navigating to visually unique objects.\n",
                "链接": "https://arxiv.org/abs/2303.03480"
            },
            {
                "文章ID": "93106",
                "标题": "Heterogeneous Embodied Multi-Agent Collaboration",
                "作者": " Xinzhu Liu,  Di Guo,  Huaping Liu",
                "发布日期": "2023-07-28",
                "摘要": "  Multi-agent embodied tasks have recently been studied in complex indoor\nvisual environments. Collaboration among multiple agents can improve work\nefficiency and has significant practical value. However, most of the existing\nresearch focuses on homogeneous multi-agent tasks. Compared with homogeneous\nagents, heterogeneous agents can leverage their different capabilities to\nallocate corresponding sub-tasks and cooperate to complete complex tasks.\nHeterogeneous multi-agent tasks are common in real-world scenarios, and the\ncollaboration strategy among heterogeneous agents is a challenging and\nimportant problem to be solved. To study collaboration among heterogeneous\nagents, we propose the heterogeneous multi-agent tidying-up task, in which\nmultiple heterogeneous agents with different capabilities collaborate with each\nother to detect misplaced objects and place them in reasonable locations. This\nis a demanding task since it requires agents to make the best use of their\ndifferent capabilities to conduct reasonable task planning and complete the\nwhole task. To solve this task, we build a heterogeneous multi-agent tidying-up\nbenchmark dataset in a large number of houses with multiple rooms based on\nProcTHOR-10K. We propose the hierarchical decision model based on misplaced\nobject detection, reasonable receptacle prediction, as well as the\nhandshake-based group communication mechanism. Extensive experiments are\nconducted to demonstrate the effectiveness of the proposed model. The project's\nwebsite and videos of experiments can be found at https://hetercol.github.io/.\n",
                "链接": "https://arxiv.org/abs/2307.13957"
            },
            {
                "文章ID": "118026",
                "标题": "Agent as Cerebrum, Controller as Cerebellum: Implementing an Embodied\n  LMM-based Agent on Drones",
                "作者": " Haoran Zhao,  Fengxing Pan,  Huqiuyue Ping,  Yaoming Zhou",
                "发布日期": "2023-11-28",
                "摘要": "  In this study, we present a novel paradigm for industrial robotic embodied\nagents, encapsulating an 'agent as cerebrum, controller as cerebellum'\narchitecture. Our approach harnesses the power of Large Multimodal Models\n(LMMs) within an agent framework known as AeroAgent, tailored for drone\ntechnology in industrial settings. To facilitate seamless integration with\nrobotic systems, we introduce ROSchain, a bespoke linkage framework connecting\nLMM-based agents to the Robot Operating System (ROS). We report findings from\nextensive empirical research, including simulated experiments on the Airgen and\nreal-world case study, particularly in individual search and rescue operations.\nThe results demonstrate AeroAgent's superior performance in comparison to\nexisting Deep Reinforcement Learning (DRL)-based agents, highlighting the\nadvantages of the embodied LMM in complex, real-world scenarios.\n",
                "链接": "https://arxiv.org/abs/2311.15033"
            },
            {
                "文章ID": "93062",
                "标题": "MAEA: Multimodal Attribution for Embodied AI",
                "作者": " Vidhi Jain,  Jayant Sravan Tamarapalli,  Sahiti Yerramilli,  Yonatan Bisk",
                "发布日期": "2023-07-27",
                "摘要": "  Understanding multimodal perception for embodied AI is an open question\nbecause such inputs may contain highly complementary as well as redundant\ninformation for the task. A relevant direction for multimodal policies is\nunderstanding the global trends of each modality at the fusion layer. To this\nend, we disentangle the attributions for visual, language, and previous action\ninputs across different policies trained on the ALFRED dataset. Attribution\nanalysis can be utilized to rank and group the failure scenarios, investigate\nmodeling and dataset biases, and critically analyze multimodal EAI policies for\nrobustness and user trust before deployment. We present MAEA, a framework to\ncompute global attributions per modality of any differentiable policy. In\naddition, we show how attributions enable lower-level behavior analysis in EAI\npolicies for language and visual attributions.\n",
                "链接": "https://arxiv.org/abs/2307.13850"
            },
            {
                "文章ID": "51062",
                "标题": "Instance-Specific Image Goal Navigation: Training Embodied Agents to\n  Find Object Instances",
                "作者": " Jacob Krantz,  Stefan Lee,  Jitendra Malik,  Dhruv Batra,  Devendra Singh Chaplot",
                "发布日期": "2022-11-30",
                "摘要": "  We consider the problem of embodied visual navigation given an image-goal\n(ImageNav) where an agent is initialized in an unfamiliar environment and\ntasked with navigating to a location 'described' by an image. Unlike related\nnavigation tasks, ImageNav does not have a standardized task definition which\nmakes comparison across methods difficult. Further, existing formulations have\ntwo problematic properties; (1) image-goals are sampled from random locations\nwhich can lead to ambiguity (e.g., looking at walls), and (2) image-goals match\nthe camera specification and embodiment of the agent; this rigidity is limiting\nwhen considering user-driven downstream applications. We present the\nInstance-specific ImageNav task (InstanceImageNav) to address these\nlimitations. Specifically, the goal image is 'focused' on some particular\nobject instance in the scene and is taken with camera parameters independent of\nthe agent. We instantiate InstanceImageNav in the Habitat Simulator using\nscenes from the Habitat-Matterport3D dataset (HM3D) and release a standardized\nbenchmark to measure community progress.\n",
                "链接": "https://arxiv.org/abs/2211.15876"
            },
            {
                "文章ID": "67955",
                "标题": "Clustering US Counties to Find Patterns Related to the COVID-19 Pandemic",
                "作者": " Cora Brown,  Sarah Milstein,  Tianyi Sun,  Cooper Zhao",
                "发布日期": "2023-03-22",
                "摘要": "  When COVID-19 first started spreading and quarantine was implemented, the\nSociety for Industrial and Applied Mathematics (SIAM) Student Chapter at the\nUniversity of Minnesota-Twin Cities began a collaboration with Ecolab to use\nour skills as data scientists and mathematicians to extract useful insights\nfrom relevant data relating to the pandemic. This collaboration consisted of\nmultiple groups working on different projects. In this write-up we focus on\nusing clustering techniques to help us find groups of similar counties in the\nUS and use that to help us understand the pandemic. Our team for this project\nconsisted of University of Minnesota students Cora Brown, Sarah Milstein,\nTianyi Sun, and Cooper Zhao, with help from Ecolab Data Scientist Jimmy\nBroomfield and University of Minnesota student Skye Ke. In the sections below\nwe describe all of the work done for this project. In Section 2, we list the\ndata we gathered, as well as the feature engineering we performed. In Section\n3, we describe the metrics we used for evaluating our models. In Section 4, we\nexplain the methods we used for interpreting the results of our various\nclustering approaches. In Section 5, we describe the different clustering\nmethods we implemented. In Section 6, we present the results of our clustering\ntechniques and provide relevant interpretation. Finally, in Section 7, we\nprovide some concluding remarks comparing the different clustering methods.\n",
                "链接": "https://arxiv.org/abs/2303.11936"
            },
            {
                "文章ID": "117303",
                "标题": "An Embodied Generalist Agent in 3D World",
                "作者": " Jiangyong Huang,  Silong Yong,  Xiaojian Ma,  Xiongkun Linghu,  Puhao Li,  Yan Wang,  Qing Li,  Song-Chun Zhu,  Baoxiong Jia,  Siyuan Huang",
                "发布日期": "2023-11-23",
                "摘要": "  Leveraging massive knowledge and learning schemes from large language models\n(LLMs), recent machine learning models show notable successes in building\ngeneralist agents that exhibit the capability of general-purpose task solving\nin diverse domains, including natural language processing, computer vision, and\nrobotics. However, a significant challenge remains as these models exhibit\nlimited ability in understanding and interacting with the 3D world. We argue\nthis limitation significantly hinders the current models from performing\nreal-world tasks and further achieving general intelligence. To this end, we\nintroduce an embodied multi-modal and multi-task generalist agent that excels\nin perceiving, grounding, reasoning, planning, and acting in the 3D world. Our\nproposed agent, referred to as LEO, is trained with shared LLM-based model\narchitectures, objectives, and weights in two stages: (i) 3D vision-language\nalignment and (ii) 3D vision-language-action instruction tuning. To facilitate\nthe training, we meticulously curate and generate an extensive dataset\ncomprising object-level and scene-level multi-modal tasks with exceeding scale\nand complexity, necessitating a deep understanding of and interaction with the\n3D world. Through rigorous experiments, we demonstrate LEO's remarkable\nproficiency across a wide spectrum of tasks, including 3D captioning, question\nanswering, embodied reasoning, embodied navigation, and robotic manipulation.\nOur ablation results further provide valuable insights for the development of\nfuture embodied generalist agents.\n",
                "链接": "https://arxiv.org/abs/2311.12871"
            },
            {
                "文章ID": "19828",
                "标题": "On the Limits of Evaluating Embodied Agent Model Generalization Using\n  Validation Sets",
                "作者": " Hyounghun Kim,  Aishwarya Padmakumar,  Di Jin,  Mohit Bansal,  Dilek Hakkani-Tur",
                "发布日期": "2022-05-20",
                "摘要": "  Natural language guided embodied task completion is a challenging problem\nsince it requires understanding natural language instructions, aligning them\nwith egocentric visual observations, and choosing appropriate actions to\nexecute in the environment to produce desired changes. We experiment with\naugmenting a transformer model for this task with modules that effectively\nutilize a wider field of view and learn to choose whether the next step\nrequires a navigation or manipulation action. We observed that the proposed\nmodules resulted in improved, and in fact state-of-the-art performance on an\nunseen validation set of a popular benchmark dataset, ALFRED. However, our best\nmodel selected using the unseen validation set underperforms on the unseen test\nsplit of ALFRED, indicating that performance on the unseen validation set may\nnot in itself be a sufficient indicator of whether model improvements\ngeneralize to unseen test sets. We highlight this result as we believe it may\nbe a wider phenomenon in machine learning tasks but primarily noticeable only\nin benchmarks that limit evaluations on test splits, and highlights the need to\nmodify benchmark design to better account for variance in model performance.\n",
                "链接": "https://arxiv.org/abs/2205.09249"
            },
            {
                "文章ID": "44810",
                "标题": "Embodied, Situated, and Grounded Intelligence: Implications for AI",
                "作者": " Tyler Millhouse,  Melanie Moses,  Melanie Mitchell",
                "发布日期": "2022-10-26",
                "摘要": "  In April of 2022, the Santa Fe Institute hosted a workshop on embodied,\nsituated, and grounded intelligence as part of the Institute's Foundations of\nIntelligence project. The workshop brought together computer scientists,\npsychologists, philosophers, social scientists, and others to discuss the\nscience of embodiment and related issues in human intelligence, and its\nimplications for building robust, human-level AI. In this report, we summarize\neach of the talks and the subsequent discussions. We also draw out a number of\nkey themes and identify important frontiers for future research.\n",
                "链接": "https://arxiv.org/abs/2210.13589"
            }
        ]
    },
    {
        "question": {
            "question": "新的大模型结构相关探索的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "92138",
                "标题": "Topics, Authors, and Networks in Large Language Model Research: Trends\n  from a Survey of 17K arXiv Papers",
                "作者": " Rajiv Movva,  Sidhika Balachandar,  Kenny Peng,  Gabriel Agostini,  Nikhil Garg,  Emma Pierson",
                "发布日期": "2023-10-24",
                "摘要": "  Large language model (LLM) research is dramatically impacting society, making\nit essential to understand the topics and values it prioritizes, the authors\nand institutions driving it, and its networks of collaboration. Due to the\nrecent growth of the field, many of these fundamental attributes lack\nsystematic description. We gather, annotate, and analyze a new dataset of\n16,979 LLM-related arXiv papers, focusing on changes in 2023 vs. 2018-2022. We\nshow that LLM research increasingly focuses on societal impacts: the Computers\nand Society sub-arXiv has seen 20x growth in its proportion of LLM-related\npapers in 2023. This change is driven in part by an influx of new authors: a\nmajority of 2023 papers are first-authored by researchers who have not\npreviously written an LLM-related paper, and these papers focus particularly on\napplications and societal considerations. While a handful of companies hold\noutsize influence, academia publishes a much larger fraction of papers than\nindustry overall, and this gap widens in 2023. LLM research is also being\nshaped by social dynamics: there are gender and academic/industry differences\nin the topics authors prioritize, and a stark U.S./China schism in the\ncollaboration network. Overall, our analysis documents how LLM research both\nshapes and is shaped by society, attesting to the necessity of sociotechnical\nlenses; we discuss implications for researchers and policymakers.\n",
                "链接": "https://arxiv.org/abs/2307.10700"
            },
            {
                "文章ID": "105928",
                "标题": "Can large language models provide useful feedback on research papers? A\n  large-scale empirical analysis",
                "作者": " Weixin Liang,  Yuhui Zhang,  Hancheng Cao,  Binglu Wang,  Daisy Ding,  Xinyu Yang,  Kailas Vodrahalli,  Siyu He,  Daniel Smith,  Yian Yin,  Daniel McFarland,  James Zou",
                "发布日期": "2023-10-04",
                "摘要": "  Expert feedback lays the foundation of rigorous research. However, the rapid\ngrowth of scholarly production and intricate knowledge specialization challenge\nthe conventional scientific feedback mechanisms. High-quality peer reviews are\nincreasingly difficult to obtain. Researchers who are more junior or from\nunder-resourced settings have especially hard times getting timely feedback.\nWith the breakthrough of large language models (LLM) such as GPT-4, there is\ngrowing interest in using LLMs to generate scientific feedback on research\nmanuscripts. However, the utility of LLM-generated feedback has not been\nsystematically studied. To address this gap, we created an automated pipeline\nusing GPT-4 to provide comments on the full PDFs of scientific papers. We\nevaluated the quality of GPT-4's feedback through two large-scale studies. We\nfirst quantitatively compared GPT-4's generated feedback with human peer\nreviewer feedback in 15 Nature family journals (3,096 papers in total) and the\nICLR machine learning conference (1,709 papers). The overlap in the points\nraised by GPT-4 and by human reviewers (average overlap 30.85% for Nature\njournals, 39.23% for ICLR) is comparable to the overlap between two human\nreviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The\noverlap between GPT-4 and human reviewers is larger for the weaker papers. We\nthen conducted a prospective user study with 308 researchers from 110 US\ninstitutions in the field of AI and computational biology to understand how\nresearchers perceive feedback generated by our GPT-4 system on their own\npapers. Overall, more than half (57.4%) of the users found GPT-4 generated\nfeedback helpful/very helpful and 82.4% found it more beneficial than feedback\nfrom at least some human reviewers. While our findings show that LLM-generated\nfeedback can help researchers, we also identify several limitations.\n",
                "链接": "https://arxiv.org/abs/2310.01783"
            },
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "68450",
                "标题": "ChatGPT and a New Academic Reality: Artificial Intelligence-Written\n  Research Papers and the Ethics of the Large Language Models in Scholarly\n  Publishing",
                "作者": " Brady Lund,  Ting Wang,  Nishith Reddy Mannuru,  Bing Nie,  Somipam Shimray,  Ziang Wang",
                "发布日期": "2023-04-03",
                "摘要": "  This paper discusses OpenAIs ChatGPT, a generative pre-trained transformer,\nwhich uses natural language processing to fulfill text-based user requests\n(i.e., a chatbot). The history and principles behind ChatGPT and similar models\nare discussed. This technology is then discussed in relation to its potential\nimpact on academia and scholarly research and publishing. ChatGPT is seen as a\npotential model for the automated preparation of essays and other types of\nscholarly manuscripts. Potential ethical issues that could arise with the\nemergence of large language models like GPT-3, the underlying technology behind\nChatGPT, and its usage by academics and researchers, are discussed and situated\nwithin the context of broader advancements in artificial intelligence, machine\nlearning, and natural language processing for research and scholarly\npublishing.\n",
                "链接": "https://arxiv.org/abs/2303.13367"
            },
            {
                "文章ID": "57742",
                "标题": "Neural Architecture Search: Insights from 1000 Papers",
                "作者": " Colin White,  Mahmoud Safari,  Rhea Sukthanker,  Binxin Ru,  Thomas Elsken,  Arber Zela,  Debadeepta Dey,  Frank Hutter",
                "发布日期": "2023-01-26",
                "摘要": "  In the past decade, advances in deep learning have resulted in breakthroughs\nin a variety of areas, including computer vision, natural language\nunderstanding, speech recognition, and reinforcement learning. Specialized,\nhigh-performing neural architectures are crucial to the success of deep\nlearning in these areas. Neural architecture search (NAS), the process of\nautomating the design of neural architectures for a given task, is an\ninevitable next step in automating machine learning and has already outpaced\nthe best human-designed architectures on many tasks. In the past few years,\nresearch in NAS has been progressing rapidly, with over 1000 papers released\nsince 2020 (Deng and Lindauer, 2021). In this survey, we provide an organized\nand comprehensive guide to neural architecture search. We give a taxonomy of\nsearch spaces, algorithms, and speedup techniques, and we discuss resources\nsuch as benchmarks, best practices, other surveys, and open-source libraries.\n",
                "链接": "https://arxiv.org/abs/2301.08727"
            },
            {
                "文章ID": "122987",
                "标题": "Generative AI in Writing Research Papers: A New Type of Algorithmic Bias\n  and Uncertainty in Scholarly Work",
                "作者": " Rishab Jain,  Aditya Jain",
                "发布日期": "2023-12-19",
                "摘要": "  The use of artificial intelligence (AI) in research across all disciplines is\nbecoming ubiquitous. However, this ubiquity is largely driven by hyperspecific\nAI models developed during scientific studies for accomplishing a well-defined,\ndata-dense task. These AI models introduce apparent, human-recognizable biases\nbecause they are trained with finite, specific data sets and parameters.\nHowever, the efficacy of using large language models (LLMs) -- and LLM-powered\ngenerative AI tools, such as ChatGPT -- to assist the research process is\ncurrently indeterminate. These generative AI tools, trained on general and\nimperceptibly large datasets along with human feedback, present challenges in\nidentifying and addressing biases. Furthermore, these models are susceptible to\ngoal misgeneralization, hallucinations, and adversarial attacks such as red\nteaming prompts -- which can be unintentionally performed by human researchers,\nresulting in harmful outputs. These outputs are reinforced in research -- where\nan increasing number of individuals have begun to use generative AI to compose\nmanuscripts. Efforts into AI interpretability lag behind development, and the\nimplicit variations that occur when prompting and providing context to a\nchatbot introduce uncertainty and irreproducibility. We thereby find that\nincorporating generative AI in the process of writing research manuscripts\nintroduces a new type of context-induced algorithmic bias and has unintended\nside effects that are largely detrimental to academia, knowledge production,\nand communicating research.\n",
                "链接": "https://arxiv.org/abs/2312.10057"
            },
            {
                "文章ID": "70693",
                "标题": "Summary of ChatGPT-Related Research and Perspective Towards the Future\n  of Large Language Models",
                "作者": " Yiheng Liu,  Tianle Han,  Siyuan Ma,  Jiayue Zhang,  Yuanyuan Yang,  Jiaming Tian,  Hao He,  Antong Li,  Mengshen He,  Zhengliang Liu,  Zihao Wu,  Lin Zhao,  Dajiang Zhu,  Xiang Li,  Ning Qiang,  Dingang Shen,  Tianming Liu,  Bao Ge",
                "发布日期": "2023-08-25",
                "摘要": "  This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and\nGPT-4) research, state-of-the-art large language models (LLM) from the GPT\nseries, and their prospective applications across diverse domains. Indeed, key\ninnovations such as large-scale pre-training that captures knowledge across the\nentire world wide web, instruction fine-tuning and Reinforcement Learning from\nHuman Feedback (RLHF) have played significant roles in enhancing LLMs'\nadaptability and performance. We performed an in-depth analysis of 194 relevant\npapers on arXiv, encompassing trend analysis, word cloud representation, and\ndistribution analysis across various application domains. The findings reveal a\nsignificant and increasing interest in ChatGPT-related research, predominantly\ncentered on direct natural language processing applications, while also\ndemonstrating considerable potential in areas ranging from education and\nhistory to mathematics, medicine, and physics. This study endeavors to furnish\ninsights into ChatGPT's capabilities, potential implications, ethical concerns,\nand offer direction for future advancements in this field.\n",
                "链接": "https://arxiv.org/abs/2304.01852"
            },
            {
                "文章ID": "11999",
                "标题": "Specialized Document Embeddings for Aspect-based Similarity of Research\n  Papers",
                "作者": " Malte Ostendorff,  Till Blume,  Terry Ruas,  Bela Gipp,  Georg Rehm",
                "发布日期": "2022-03-29",
                "摘要": "  Document embeddings and similarity measures underpin content-based\nrecommender systems, whereby a document is commonly represented as a single\ngeneric embedding. However, similarity computed on single vector\nrepresentations provides only one perspective on document similarity that\nignores which aspects make two documents alike. To address this limitation,\naspect-based similarity measures have been developed using document\nsegmentation or pairwise multi-class document classification. While\nsegmentation harms the document coherence, the pairwise classification approach\nscales poorly to large scale corpora. In this paper, we treat aspect-based\nsimilarity as a classical vector similarity problem in aspect-specific\nembedding spaces. We represent a document not as a single generic embedding but\nas multiple specialized embeddings. Our approach avoids document segmentation\nand scales linearly w.r.t.the corpus size. In an empirical study, we use the\nPapers with Code corpus containing 157,606 research papers and consider the\ntask, method, and dataset of the respective research papers as their aspects.\nWe compare and analyze three generic document embeddings, six specialized\ndocument embeddings and a pairwise classification baseline in the context of\nresearch paper recommendations. As generic document embeddings, we consider\nFastText, SciBERT, and SPECTER. To compute the specialized document embeddings,\nwe compare three alternative methods inspired by retrofitting, fine-tuning, and\nSiamese networks. In our experiments, Siamese SciBERT achieved the highest\nscores. Additional analyses indicate an implicit bias of the generic document\nembeddings towards the dataset aspect and against the method aspect of each\nresearch paper. Our approach of aspect-based document embeddings mitigates\npotential risks arising from implicit biases by making them explicit.\n",
                "链接": "https://arxiv.org/abs/2203.14541"
            },
            {
                "文章ID": "15455",
                "标题": "Research on Domain Information Mining and Theme Evolution of Scientific\n  Papers",
                "作者": " Changwei Zheng,  Zhe Xue,  Meiyu Liang,  Feifei Kou,  Zeli Guan",
                "发布日期": "2022-04-20",
                "摘要": "  In recent years, with the increase of social investment in scientific\nresearch, the number of research results in various fields has increased\nsignificantly. Cross-disciplinary research results have gradually become an\nemerging frontier research direction. There is a certain dependence between a\nlarge number of research results. It is difficult to effectively analyze\ntoday's scientific research results when looking at a single research field in\nisolation. How to effectively use the huge number of scientific papers to help\nresearchers becomes a challenge. This paper introduces the research status at\nhome and abroad in terms of domain information mining and topic evolution law\nof scientific and technological papers from three aspects: the semantic feature\nrepresentation learning of scientific and technological papers, the field\ninformation mining of scientific and technological papers, and the mining and\nprediction of research topic evolution rules of scientific and technological\npapers.\n",
                "链接": "https://arxiv.org/abs/2204.08476"
            },
            {
                "文章ID": "6997",
                "标题": "Matching Papers and Reviewers at Large Conferences",
                "作者": " Kevin Leyton-Brown,   Mausam,  Yatin Nandwani,  Hedayat Zarkoob,  Chris Cameron,  Neil Newman,  Dinesh Raghu",
                "发布日期": "2022-08-08",
                "摘要": "  Peer-reviewed conferences, the main publication venues in CS, rely critically\non matching highly qualified reviewers for each paper. Because of the growing\nscale of these conferences, the tight timelines on which they operate, and a\nrecent surge in explicitly dishonest behavior, there is now no alternative to\nperforming this matching in an automated way. This paper studies a novel\nreviewer-paper matching approach that was recently deployed in the 35th AAAI\nConference on Artificial Intelligence (AAAI 2021), and has since been adopted\n(wholly or partially) by other conferences including ICML 2022, AAAI 2022, and\nIJCAI 2022. This approach has three main elements: (1) collecting and\nprocessing input data to identify problematic matches and generate\nreviewer-paper scores; (2) formulating and solving an optimization problem to\nfind good reviewer-paper matchings; and (3) a two-phase reviewing process that\nshifts reviewing resources away from papers likely to be rejected and towards\npapers closer to the decision boundary. This paper also describes an evaluation\nof these innovations based on an extensive post-hoc analysis on real data --\nincluding a comparison with the matching algorithm used in AAAI's previous\n(2020) iteration -- and supplements this with additional numerical\nexperimentation.\n",
                "链接": "https://arxiv.org/abs/2202.12273"
            }
        ]
    },
    {
        "question": {
            "question": "多模态细粒度图片编辑和生成相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "124621",
                "标题": "FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing",
                "作者": " Mingyuan Zhang,  Huirong Li,  Zhongang Cai,  Jiawei Ren,  Lei Yang,  Ziwei Liu",
                "发布日期": "2023-12-27",
                "摘要": "  Text-driven motion generation has achieved substantial progress with the\nemergence of diffusion models. However, existing methods still struggle to\ngenerate complex motion sequences that correspond to fine-grained descriptions,\ndepicting detailed and accurate spatio-temporal actions. This lack of fine\ncontrollability limits the usage of motion generation to a larger audience. To\ntackle these challenges, we present FineMoGen, a diffusion-based motion\ngeneration and editing framework that can synthesize fine-grained motions, with\nspatial-temporal composition to the user instructions. Specifically, FineMoGen\nbuilds upon diffusion model with a novel transformer architecture dubbed\nSpatio-Temporal Mixture Attention (SAMI). SAMI optimizes the generation of the\nglobal attention template from two perspectives: 1) explicitly modeling the\nconstraints of spatio-temporal composition; and 2) utilizing sparsely-activated\nmixture-of-experts to adaptively extract fine-grained features. To facilitate a\nlarge-scale study on this new fine-grained motion generation task, we\ncontribute the HuMMan-MoGen dataset, which consists of 2,968 videos and 102,336\nfine-grained spatio-temporal descriptions. Extensive experiments validate that\nFineMoGen exhibits superior motion generation quality over state-of-the-art\nmethods. Notably, FineMoGen further enables zero-shot motion editing\ncapabilities with the aid of modern large language models (LLM), which\nfaithfully manipulates motion sequences with fine-grained instructions. Project\nPage: https://mingyuan-zhang.github.io/projects/FineMoGen.html\n",
                "链接": "https://arxiv.org/abs/2312.15004"
            },
            {
                "文章ID": "13260",
                "标题": "Flexible Portrait Image Editing with Fine-Grained Control",
                "作者": " Linlin Liu,  Qian Fu,  Fei Hou,  Ying He",
                "发布日期": "2022-04-05",
                "摘要": "  We develop a new method for portrait image editing, which supports\nfine-grained editing of geometries, colors, lights and shadows using a single\nneural network model. We adopt a novel asymmetric conditional GAN architecture:\nthe generators take the transformed conditional inputs, such as edge maps,\ncolor palette, sliders and masks, that can be directly edited by the user; the\ndiscriminators take the conditional inputs in the way that can guide\ncontrollable image generation more effectively. Taking color editing as an\nexample, we feed color palettes (which can be edited easily) into the\ngenerator, and color maps (which contain positional information of colors) into\nthe discriminator. We also design a region-weighted discriminator so that\nhigher weights are assigned to more important regions, like eyes and skin.\nUsing a color palette, the user can directly specify the desired colors of\nhair, skin, eyes, lip and background. Color sliders allow the user to blend\ncolors in an intuitive manner. The user can also edit lights and shadows by\nmodifying the corresponding masks. We demonstrate the effectiveness of our\nmethod by evaluating it on the CelebAMask-HQ dataset with a wide range of\ntasks, including geometry/color/shadow/light editing, hand-drawn sketch to\nimage translation, and color transfer. We also present ablation studies to\njustify our design.\n",
                "链接": "https://arxiv.org/abs/2204.01318"
            },
            {
                "文章ID": "52126",
                "标题": "Fine-grained Image Editing by Pixel-wise Guidance Using Diffusion Models",
                "作者": " Naoki Matsunaga,  Masato Ishii,  Akio Hayakawa,  Kenji Suzuki,  Takuya Narihira",
                "发布日期": "2023-06-01",
                "摘要": "  Our goal is to develop fine-grained real-image editing methods suitable for\nreal-world applications. In this paper, we first summarize four requirements\nfor these methods and propose a novel diffusion-based image editing framework\nwith pixel-wise guidance that satisfies these requirements. Specifically, we\ntrain pixel-classifiers with a few annotated data and then infer the\nsegmentation map of a target image. Users then manipulate the map to instruct\nhow the image will be edited. We utilize a pre-trained diffusion model to\ngenerate edited images aligned with the user's intention with pixel-wise\nguidance. The effective combination of proposed guidance and other techniques\nenables highly controllable editing with preserving the outside of the edited\narea, which results in meeting our requirements. The experimental results\ndemonstrate that our proposal outperforms the GAN-based method for editing\nquality and speed.\n",
                "链接": "https://arxiv.org/abs/2212.02024"
            },
            {
                "文章ID": "123024",
                "标题": "Focus on Your Instruction: Fine-grained and Multi-instruction Image\n  Editing by Attention Modulation",
                "作者": " Qin Guo,  Tianwei Lin",
                "发布日期": "2023-12-19",
                "摘要": "  Recently, diffusion-based methods, like InstructPix2Pix (IP2P), have achieved\neffective instruction-based image editing, requiring only natural language\ninstructions from the user. However, these methods often inadvertently alter\nunintended areas and struggle with multi-instruction editing, resulting in\ncompromised outcomes. To address these issues, we introduce the Focus on Your\nInstruction (FoI), a method designed to ensure precise and harmonious editing\nacross multiple instructions without extra training or test-time optimization.\nIn the FoI, we primarily emphasize two aspects: (1) precisely extracting\nregions of interest for each instruction and (2) guiding the denoising process\nto concentrate within these regions of interest. For the first objective, we\nidentify the implicit grounding capability of IP2P from the cross-attention\nbetween instruction and image, then develop an effective mask extraction\nmethod. For the second objective, we introduce a cross attention modulation\nmodule for rough isolation of target editing regions and unrelated regions.\nAdditionally, we introduce a mask-guided disentangle sampling strategy to\nfurther ensure clear region isolation. Experimental results demonstrate that\nFoI surpasses existing methods in both quantitative and qualitative\nevaluations, especially excelling in multi-instruction editing task.\n",
                "链接": "https://arxiv.org/abs/2312.10113"
            },
            {
                "文章ID": "124901",
                "标题": "SERF: Fine-Grained Interactive 3D Segmentation and Editing with Radiance\n  Fields",
                "作者": " Kaichen Zhou,  Lanqing Hong,  Enze Xie,  Yongxin Yang,  Zhenguo Li,  Wei Zhang",
                "发布日期": "2023-12-27",
                "摘要": "  Although significant progress has been made in the field of 2D-based\ninteractive editing, fine-grained 3D-based interactive editing remains\nrelatively unexplored. This limitation can be attributed to two main\nchallenges: the lack of an efficient 3D representation robust to different\nmodifications and the absence of an effective 3D interactive segmentation\nmethod. In this paper, we introduce a novel fine-grained interactive 3D\nsegmentation and editing algorithm with radiance fields, which we refer to as\nSERF. Our method entails creating a neural mesh representation by integrating\nmulti-view algorithms with pre-trained 2D models. Building upon this\nrepresentation, we introduce a novel surface rendering technique that preserves\nlocal information and is robust to deformation. Moreover, this representation\nforms the basis for achieving accurate and interactive 3D segmentation without\nrequiring 3D supervision. Harnessing this representation facilitates a range of\ninteractive 3D editing operations, encompassing tasks such as interactive\ngeometry editing and texture painting. Extensive experiments and visualization\nexamples of editing on both real and synthetic data demonstrate the superiority\nof our method on representation quality and editing ability.\n",
                "链接": "https://arxiv.org/abs/2312.15856"
            },
            {
                "文章ID": "96626",
                "标题": "DragNUWA: Fine-grained Control in Video Generation by Integrating Text,\n  Image, and Trajectory",
                "作者": " Shengming Yin,  Chenfei Wu,  Jian Liang,  Jie Shi,  Houqiang Li,  Gong Ming,  Nan Duan",
                "发布日期": "2023-08-17",
                "摘要": "  Controllable video generation has gained significant attention in recent\nyears. However, two main limitations persist: Firstly, most existing works\nfocus on either text, image, or trajectory-based control, leading to an\ninability to achieve fine-grained control in videos. Secondly, trajectory\ncontrol research is still in its early stages, with most experiments being\nconducted on simple datasets like Human3.6M. This constraint limits the models'\ncapability to process open-domain images and effectively handle complex curved\ntrajectories. In this paper, we propose DragNUWA, an open-domain\ndiffusion-based video generation model. To tackle the issue of insufficient\ncontrol granularity in existing works, we simultaneously introduce text, image,\nand trajectory information to provide fine-grained control over video content\nfrom semantic, spatial, and temporal perspectives. To resolve the problem of\nlimited open-domain trajectory control in current research, We propose\ntrajectory modeling with three aspects: a Trajectory Sampler (TS) to enable\nopen-domain control of arbitrary trajectories, a Multiscale Fusion (MF) to\ncontrol trajectories in different granularities, and an Adaptive Training (AT)\nstrategy to generate consistent videos following trajectories. Our experiments\nvalidate the effectiveness of DragNUWA, demonstrating its superior performance\nin fine-grained control in video generation. The homepage link is\n\\url{https://www.microsoft.com/en-us/research/project/dragnuwa/}\n",
                "链接": "https://arxiv.org/abs/2308.08089"
            },
            {
                "文章ID": "117252",
                "标题": "A Fine-Grained Image Description Generation Method Based on Joint\n  Objectives",
                "作者": " Yifan Zhang,  Chunzhen Lin,  Donglin Cao,  Dazhen Lin",
                "发布日期": "2023-11-23",
                "摘要": "  The goal of fine-grained image description generation techniques is to learn\ndetailed information from images and simulate human-like descriptions that\nprovide coherent and comprehensive textual details about the image content.\nCurrently, most of these methods face two main challenges: description\nrepetition and omission. Moreover, the existing evaluation metrics cannot\nclearly reflect the performance of models on these two issues. To address these\nchallenges, we propose an innovative Fine-grained Image Description Generation\nmodel based on Joint Objectives. Furthermore, we introduce new object-based\nevaluation metrics to more intuitively assess the model's performance in\nhandling description repetition and omission. This novel approach combines\nvisual features at both the image level and object level to maximize their\nadvantages and incorporates an object penalty mechanism to reduce description\nrepetition. Experimental results demonstrate that our proposed method\nsignificantly improves the CIDEr evaluation metric, indicating its excellent\nperformance in addressing description repetition and omission issues.\n",
                "链接": "https://arxiv.org/abs/2311.12799"
            },
            {
                "文章ID": "122275",
                "标题": "Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic\n  Image-Report Generation",
                "作者": " Wenting Chen,  Linlin Shen,  Xiang Li,  Yixuan Yuan",
                "发布日期": "2023-12-29",
                "摘要": "  To address these issues, we propose a novel Adaptive patch-word Matching\n(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in\nmedical reports and apply it to CXR-report generation to provide explainability\nfor the generation process. AdaMatch exploits the fine-grained relation between\nadaptive patches and words to provide explanations of specific image regions\nwith corresponding words. To capture the abnormal regions of varying sizes and\npositions, we introduce the Adaptive Patch extraction (AdaPatch) module to\nacquire the adaptive patches for these regions adaptively. In order to provide\nexplicit explainability for CXR-report generation task, we propose an\nAdaMatch-based bidirectional large language model for Cyclic CXR-report\ngeneration (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords\nfor CXR images and `keypatches' for medical reports as hints to guide\nCXR-report generation. Extensive experiments on two publicly available CXR\ndatasets prove the effectiveness of our method and its superior performance to\nexisting methods.\n",
                "链接": "https://arxiv.org/abs/2312.08078"
            },
            {
                "文章ID": "69760",
                "标题": "Hierarchical Fine-Grained Image Forgery Detection and Localization",
                "作者": " Xiao Guo,  Xiaohong Liu,  Zhiyuan Ren,  Steven Grosz,  Iacopo Masi,  Xiaoming Liu",
                "发布日期": "2023-03-31",
                "摘要": "  Differences in forgery attributes of images generated in CNN-synthesized and\nimage-editing domains are large, and such differences make a unified image\nforgery detection and localization (IFDL) challenging. To this end, we present\na hierarchical fine-grained formulation for IFDL representation learning.\nSpecifically, we first represent forgery attributes of a manipulated image with\nmultiple labels at different levels. Then we perform fine-grained\nclassification at these levels using the hierarchical dependency between them.\nAs a result, the algorithm is encouraged to learn both comprehensive features\nand inherent hierarchical nature of different forgery attributes, thereby\nimproving the IFDL representation. Our proposed IFDL framework contains three\ncomponents: multi-branch feature extractor, localization and classification\nmodules. Each branch of the feature extractor learns to classify forgery\nattributes at one level, while localization and classification modules segment\nthe pixel-level forgery region and detect image-level forgery, respectively.\nLastly, we construct a hierarchical fine-grained dataset to facilitate our\nstudy. We demonstrate the effectiveness of our method on $7$ different\nbenchmarks, for both tasks of IFDL and forgery attribute classification. Our\nsource code and dataset can be found:\n\\href{https://github.com/CHELSEA234/HiFi_IFDL}{github.com/CHELSEA234/HiFi-IFDL}.\n",
                "链接": "https://arxiv.org/abs/2303.17111"
            },
            {
                "文章ID": "40110",
                "标题": "Fine-grained Contrastive Learning for Definition Generation",
                "作者": " Hengyuan Zhang,  Dawei Li,  Shiping Yang,  Yanran Li",
                "发布日期": "2022-10-04",
                "摘要": "  Recently, pre-trained transformer-based models have achieved great success in\nthe task of definition generation (DG). However, previous encoder-decoder\nmodels lack effective representation learning to contain full semantic\ncomponents of the given word, which leads to generating under-specific\ndefinitions. To address this problem, we propose a novel contrastive learning\nmethod, encouraging the model to capture more detailed semantic representations\nfrom the definition sequence encoding. According to both automatic and manual\nevaluation, the experimental results on three mainstream benchmarks demonstrate\nthat the proposed method could generate more specific and high-quality\ndefinitions compared with several state-of-the-art models.\n",
                "链接": "https://arxiv.org/abs/2210.00543"
            }
        ]
    },
    {
        "question": {
            "question": "帮我查找风格化机器翻译相关的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "27958",
                "标题": "DCT-Net: Domain-Calibrated Translation for Portrait Stylization",
                "作者": " Yifang Men,  Yuan Yao,  Miaomiao Cui,  Zhouhui Lian,  Xuansong Xie",
                "发布日期": "2022-07-07",
                "摘要": "  This paper introduces DCT-Net, a novel image translation architecture for\nfew-shot portrait stylization. Given limited style exemplars ($\\sim$100), the\nnew architecture can produce high-quality style transfer results with advanced\nability to synthesize high-fidelity contents and strong generality to handle\ncomplicated scenes (e.g., occlusions and accessories). Moreover, it enables\nfull-body image translation via one elegant evaluation network trained by\npartial observations (i.e., stylized heads). Few-shot learning based style\ntransfer is challenging since the learned model can easily become overfitted in\nthe target domain, due to the biased distribution formed by only a few training\nexamples. This paper aims to handle the challenge by adopting the key idea of\n\"calibration first, translation later\" and exploring the augmented global\nstructure with locally-focused translation. Specifically, the proposed DCT-Net\nconsists of three modules: a content adapter borrowing the powerful prior from\nsource photos to calibrate the content distribution of target samples; a\ngeometry expansion module using affine transformations to release spatially\nsemantic constraints; and a texture translation module leveraging samples\nproduced by the calibrated distribution to learn a fine-grained conversion.\nExperimental results demonstrate the proposed method's superiority over the\nstate of the art in head stylization and its effectiveness on full image\ntranslation with adaptive deformations.\n",
                "链接": "https://arxiv.org/abs/2207.02426"
            },
            {
                "文章ID": "79369",
                "标题": "Decomposed Prompting for Machine Translation Between Related Languages\n  using Large Language Models",
                "作者": " Ratish Puduppully,  Anoop Kunchukuttan,  Raj Dabre,  Ai Ti Aw,  Nancy F. Chen",
                "发布日期": "2023-10-24",
                "摘要": "  This study investigates machine translation between related languages i.e.,\nlanguages within the same family that share linguistic characteristics such as\nword order and lexical similarity. Machine translation through few-shot\nprompting leverages a small set of translation pair examples to generate\ntranslations for test sentences. This procedure requires the model to learn how\nto generate translations while simultaneously ensuring that token ordering is\nmaintained to produce a fluent and accurate translation. We propose that for\nrelated languages, the task of machine translation can be simplified by\nleveraging the monotonic alignment characteristic of such languages. We\nintroduce DecoMT, a novel approach of few-shot prompting that decomposes the\ntranslation process into a sequence of word chunk translations. Through\nautomatic and human evaluation conducted on multiple related language pairs\nacross various language families, we demonstrate that our proposed approach of\ndecomposed prompting surpasses multiple established few-shot baseline\napproaches. For example, DecoMT outperforms the strong few-shot prompting BLOOM\nmodel with an average improvement of 8 chrF++ scores across the examined\nlanguages.\n",
                "链接": "https://arxiv.org/abs/2305.13085"
            },
            {
                "文章ID": "67955",
                "标题": "Clustering US Counties to Find Patterns Related to the COVID-19 Pandemic",
                "作者": " Cora Brown,  Sarah Milstein,  Tianyi Sun,  Cooper Zhao",
                "发布日期": "2023-03-22",
                "摘要": "  When COVID-19 first started spreading and quarantine was implemented, the\nSociety for Industrial and Applied Mathematics (SIAM) Student Chapter at the\nUniversity of Minnesota-Twin Cities began a collaboration with Ecolab to use\nour skills as data scientists and mathematicians to extract useful insights\nfrom relevant data relating to the pandemic. This collaboration consisted of\nmultiple groups working on different projects. In this write-up we focus on\nusing clustering techniques to help us find groups of similar counties in the\nUS and use that to help us understand the pandemic. Our team for this project\nconsisted of University of Minnesota students Cora Brown, Sarah Milstein,\nTianyi Sun, and Cooper Zhao, with help from Ecolab Data Scientist Jimmy\nBroomfield and University of Minnesota student Skye Ke. In the sections below\nwe describe all of the work done for this project. In Section 2, we list the\ndata we gathered, as well as the feature engineering we performed. In Section\n3, we describe the metrics we used for evaluating our models. In Section 4, we\nexplain the methods we used for interpreting the results of our various\nclustering approaches. In Section 5, we describe the different clustering\nmethods we implemented. In Section 6, we present the results of our clustering\ntechniques and provide relevant interpretation. Finally, in Section 7, we\nprovide some concluding remarks comparing the different clustering methods.\n",
                "链接": "https://arxiv.org/abs/2303.11936"
            },
            {
                "文章ID": "82162",
                "标题": "Context-Preserving Two-Stage Video Domain Translation for Portrait\n  Stylization",
                "作者": " Doyeon Kim,  Eunji Ko,  Hyunsu Kim,  Yunji Kim,  Junho Kim,  Dongchan Min,  Junmo Kim,  Sung Ju Hwang",
                "发布日期": "2023-05-31",
                "摘要": "  Portrait stylization, which translates a real human face image into an\nartistically stylized image, has attracted considerable interest and many prior\nworks have shown impressive quality in recent years. However, despite their\nremarkable performances in the image-level translation tasks, prior methods\nshow unsatisfactory results when they are applied to the video domain. To\naddress the issue, we propose a novel two-stage video translation framework\nwith an objective function which enforces a model to generate a temporally\ncoherent stylized video while preserving context in the source video.\nFurthermore, our model runs in real-time with the latency of 0.011 seconds per\nframe and requires only 5.6M parameters, and thus is widely applicable to\npractical real-world applications.\n",
                "链接": "https://arxiv.org/abs/2305.19135"
            },
            {
                "文章ID": "6641",
                "标题": "An Overview on Machine Translation Evaluation",
                "作者": " Lifeng Han",
                "发布日期": "2022-02-23",
                "摘要": "  Since the 1950s, machine translation (MT) has become one of the important\ntasks of AI and development, and has experienced several different periods and\nstages of development, including rule-based methods, statistical methods, and\nrecently proposed neural network-based learning methods. Accompanying these\nstaged leaps is the evaluation research and development of MT, especially the\nimportant role of evaluation methods in statistical translation and neural\ntranslation research. The evaluation task of MT is not only to evaluate the\nquality of machine translation, but also to give timely feedback to machine\ntranslation researchers on the problems existing in machine translation itself,\nhow to improve and how to optimise. In some practical application fields, such\nas in the absence of reference translations, the quality estimation of machine\ntranslation plays an important role as an indicator to reveal the credibility\nof automatically translated target languages. This report mainly includes the\nfollowing contents: a brief history of machine translation evaluation (MTE),\nthe classification of research methods on MTE, and the the cutting-edge\nprogress, including human evaluation, automatic evaluation, and evaluation of\nevaluation methods (meta-evaluation). Manual evaluation and automatic\nevaluation include reference-translation based and reference-translation\nindependent participation; automatic evaluation methods include traditional\nn-gram string matching, models applying syntax and semantics, and deep learning\nmodels; evaluation of evaluation methods includes estimating the credibility of\nhuman evaluations, the reliability of the automatic evaluation, the reliability\nof the test set, etc. Advances in cutting-edge evaluation methods include\ntask-based evaluation, using pre-trained language models based on big data, and\nlightweight optimisation models using distillation techniques.\n",
                "链接": "https://arxiv.org/abs/2202.11027"
            },
            {
                "文章ID": "10270",
                "标题": "On Vision Features in Multimodal Machine Translation",
                "作者": " Bei Li,  Chuanhao Lv,  Zefan Zhou,  Tao Zhou,  Tong Xiao,  Anxiang Ma,  JingBo Zhu",
                "发布日期": "2022-03-18",
                "摘要": "  Previous work on multimodal machine translation (MMT) has focused on the way\nof incorporating vision features into translation but little attention is on\nthe quality of vision models. In this work, we investigate the impact of vision\nmodels on MMT. Given the fact that Transformer is becoming popular in computer\nvision, we experiment with various strong models (such as Vision Transformer)\nand enhanced features (such as object-detection and image captioning). We\ndevelop a selective attention model to study the patch-level contribution of an\nimage in MMT. On detailed probing tasks, we find that stronger vision models\nare helpful for learning translation from the visual modality. Our results also\nsuggest the need of carefully examining MMT models, especially when current\nbenchmarks are small-scale and biased. Our code could be found at\n\\url{https://github.com/libeineu/fairseq_mmt}.\n",
                "链接": "https://arxiv.org/abs/2203.09173"
            },
            {
                "文章ID": "22655",
                "标题": "Real-Time Portrait Stylization on the Edge",
                "作者": " Yanyu Li,  Xuan Shen,  Geng Yuan,  Jiexiong Guan,  Wei Niu,  Hao Tang,  Bin Ren,  Yanzhi Wang",
                "发布日期": "2022-06-06",
                "摘要": "  In this work we demonstrate real-time portrait stylization, specifically,\ntranslating self-portrait into cartoon or anime style on mobile devices. We\npropose a latency-driven differentiable architecture search method, maintaining\nrealistic generative quality. With our framework, we obtain $10\\times$\ncomputation reduction on the generative model and achieve real-time video\nstylization on off-the-shelf smartphone using mobile GPUs.\n",
                "链接": "https://arxiv.org/abs/2206.01244"
            },
            {
                "文章ID": "66434",
                "标题": "HCI Papers Cite HCI Papers, Increasingly So",
                "作者": " Xiang 'Anthony' Chen",
                "发布日期": "2023-03-15",
                "摘要": "  We propose X-index -- the proportion of papers' citations coming from outside\ntheir research field -- and use this metric to analyze citations of CHI, UIST,\nand CSCW papers between 2010 and 2022. We found an overall decreasing X-index\nby several measures, indicating that HCI papers have been more and more likely\nto be cited by HCI papers rather than by non-HCI papers.\n",
                "链接": "https://arxiv.org/abs/2303.07539"
            },
            {
                "文章ID": "50967",
                "标题": "Realtime Fewshot Portrait Stylization Based On Geometric Alignment",
                "作者": " Xinrui Wang,  Zhuoru Li,  Xiao Zhou,  Yusuke Iwasawa,  Yutaka Matsuo",
                "发布日期": "2022-11-29",
                "摘要": "  This paper presents a portrait stylization method designed for real-time\nmobile applications with limited style examples available. Previous learning\nbased stylization methods suffer from the geometric and semantic gaps between\nportrait domain and style domain, which obstacles the style information to be\ncorrectly transferred to the portrait images, leading to poor stylization\nquality. Based on the geometric prior of human facial attributions, we propose\nto utilize geometric alignment to tackle this issue. Firstly, we apply\nThin-Plate-Spline (TPS) on feature maps in the generator network and also\ndirectly to style images in pixel space, generating aligned portrait-style\nimage pairs with identical landmarks, which closes the geometric gaps between\ntwo domains. Secondly, adversarial learning maps the textures and colors of\nportrait images to the style domain. Finally, geometric aware cycle consistency\npreserves the content and identity information unchanged, and deformation\ninvariant constraint suppresses artifacts and distortions. Qualitative and\nquantitative comparison validate our method outperforms existing methods, and\nexperiments proof our method could be trained with limited style examples (100\nor less) in real-time (more than 40 FPS) on mobile devices. Ablation study\ndemonstrates the effectiveness of each component in the framework.\n",
                "链接": "https://arxiv.org/abs/2211.15549"
            },
            {
                "文章ID": "74223",
                "标题": "What Causes Exceptions in Machine Learning Applications? Mining Machine\n  Learning-Related Stack Traces on Stack Overflow",
                "作者": " Amin Ghadesi,  Maxime Lamothe,  Heng Li",
                "发布日期": "2023-04-26",
                "摘要": "  Machine learning (ML), including deep learning, has recently gained\ntremendous popularity in a wide range of applications. However, like\ntraditional software, ML applications are not immune to the bugs that result\nfrom programming errors. Explicit programming errors usually manifest through\nerror messages and stack traces. These stack traces describe the chain of\nfunction calls that lead to an anomalous situation, or exception. Indeed, these\nexceptions may cross the entire software stack (including applications and\nlibraries). Thus, studying the patterns in stack traces can help practitioners\nand researchers understand the causes of exceptions in ML applications and the\nchallenges faced by ML developers. To that end, we mine Stack Overflow (SO) and\nstudy 11,449 stack traces related to seven popular Python ML libraries. First,\nwe observe that ML questions that contain stack traces gain more popularity\nthan questions without stack traces; however, they are less likely to get\naccepted answers. Second, we observe that recurrent patterns exists in ML stack\ntraces, even across different ML libraries, with a small portion of patterns\ncovering many stack traces. Third, we derive five high-level categories and 25\nlow-level types from the stack trace patterns: most patterns are related to\npython basic syntax, model training, parallelization, data transformation, and\nsubprocess invocation. Furthermore, the patterns related to subprocess\ninvocation, external module execution, and remote API call are among the least\nlikely to get accepted answers on SO. Our findings provide insights for\nresearchers, ML library providers, and ML application developers to improve the\nquality of ML libraries and their applications.\n",
                "链接": "https://arxiv.org/abs/2304.12857"
            }
        ]
    },
    {
        "question": {
            "question": "查找中文ner常用的数据集论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "6079",
                "标题": "TURNER: The Uncertainty-based Retrieval Framework for Chinese NER",
                "作者": " Zhichao Geng,  Hang Yan,  Zhangyue Yin,  Chenxin An,  Xipeng Qiu",
                "发布日期": "2022-02-21",
                "摘要": "  Chinese NER is a difficult undertaking due to the ambiguity of Chinese\ncharacters and the absence of word boundaries. Previous work on Chinese NER\nfocus on lexicon-based methods to introduce boundary information and reduce\nout-of-vocabulary (OOV) cases during prediction. However, it is expensive to\nobtain and dynamically maintain high-quality lexicons in specific domains,\nwhich motivates us to utilize more general knowledge resources, e.g., search\nengines. In this paper, we propose TURNER: The Uncertainty-based Retrieval\nframework for Chinese NER. The idea behind TURNER is to imitate human behavior:\nwe frequently retrieve auxiliary knowledge as assistance when encountering an\nunknown or uncertain entity. To improve the efficiency and effectiveness of\nretrieval, we first propose two types of uncertainty sampling methods for\nselecting the most ambiguous entity-level uncertain components of the input\ntext. Then, the Knowledge Fusion Model re-predict the uncertain samples by\ncombining retrieved knowledge. Experiments on four benchmark datasets\ndemonstrate TURNER's effectiveness. TURNER outperforms existing lexicon-based\napproaches and achieves the new SOTA.\n",
                "链接": "https://arxiv.org/abs/2202.09022"
            },
            {
                "文章ID": "5948",
                "标题": "AISHELL-NER: Named Entity Recognition from Chinese Speech",
                "作者": " Boli Chen,  Guangwei Xu,  Xiaobin Wang,  Pengjun Xie,  Meishan Zhang,  Fei Huang",
                "发布日期": "2022-02-18",
                "摘要": "  Named Entity Recognition (NER) from speech is among Spoken Language\nUnderstanding (SLU) tasks, aiming to extract semantic information from the\nspeech signal. NER from speech is usually made through a two-step pipeline that\nconsists of (1) processing the audio using an Automatic Speech Recognition\n(ASR) system and (2) applying an NER tagger to the ASR outputs. Recent works\nhave shown the capability of the End-to-End (E2E) approach for NER from English\nand French speech, which is essentially entity-aware ASR. However, due to the\nmany homophones and polyphones that exist in Chinese, NER from Chinese speech\nis effectively a more challenging task. In this paper, we introduce a new\ndataset AISEHLL-NER for NER from Chinese speech. Extensive experiments are\nconducted to explore the performance of several state-of-the-art methods. The\nresults demonstrate that the performance could be improved by combining\nentity-aware ASR and pretrained NER tagger, which can be easily applied to the\nmodern SLU pipeline. The dataset is publicly available at\ngithub.com/Alibaba-NLP/AISHELL-NER.\n",
                "链接": "https://arxiv.org/abs/2202.08533"
            },
            {
                "文章ID": "50704",
                "标题": "PUnifiedNER: A Prompting-based Unified NER System for Diverse Datasets",
                "作者": " Jinghui Lu,  Rui Zhao,  Brian Mac Namee,  Fei Tan",
                "发布日期": "2023-02-23",
                "摘要": "  Much of named entity recognition (NER) research focuses on developing\ndataset-specific models based on data from the domain of interest, and a\nlimited set of related entity types. This is frustrating as each new dataset\nrequires a new model to be trained and stored. In this work, we present a\n``versatile'' model -- the Prompting-based Unified NER system (PUnifiedNER) --\nthat works with data from different domains and can recognise up to 37 entity\ntypes simultaneously, and theoretically it could be as many as possible. By\nusing prompt learning, PUnifiedNER is a novel approach that is able to jointly\ntrain across multiple corpora, implementing intelligent on-demand entity\nrecognition. Experimental results show that PUnifiedNER leads to significant\nprediction benefits compared to dataset-specific models with impressively\nreduced model deployment costs. Furthermore, the performance of PUnifiedNER can\nachieve competitive or even better performance than state-of-the-art\ndomain-specific methods for some datasets. We also perform comprehensive pilot\nand ablation studies to support in-depth analysis of each component in\nPUnifiedNER.\n",
                "链接": "https://arxiv.org/abs/2211.14838"
            },
            {
                "文章ID": "44485",
                "标题": "Improving Chinese Named Entity Recognition by Search Engine Augmentation",
                "作者": " Qinghua Mao,  Jiatong Li,  Kui Meng",
                "发布日期": "2022-10-25",
                "摘要": "  Compared with English, Chinese suffers from more grammatical ambiguities,\nlike fuzzy word boundaries and polysemous words. In this case, contextual\ninformation is not sufficient to support Chinese named entity recognition\n(NER), especially for rare and emerging named entities. Semantic augmentation\nusing external knowledge is a potential way to alleviate this problem, while\nhow to obtain and leverage external knowledge for the NER task remains a\nchallenge. In this paper, we propose a neural-based approach to perform\nsemantic augmentation using external knowledge from search engine for Chinese\nNER. In particular, a multi-channel semantic fusion model is adopted to\ngenerate the augmented input representations, which aggregates external related\ntexts retrieved from the search engine. Experiments have shown the superiority\nof our model across 4 NER datasets, including formal and social media language\ncontexts, which further prove the effectiveness of our approach.\n",
                "链接": "https://arxiv.org/abs/2210.12662"
            },
            {
                "文章ID": "28049",
                "标题": "Rethinking the Value of Gazetteer in Chinese Named Entity Recognition",
                "作者": " Qianglong Chen,  Xiangji Zeng,  Jiangang Zhu,  Yin Zhang,  Bojia Lin,  Yang Yang,  Daxin Jiang",
                "发布日期": "2022-07-19",
                "摘要": "  Gazetteer is widely used in Chinese named entity recognition (NER) to enhance\nspan boundary detection and type classification. However, to further understand\nthe generalizability and effectiveness of gazetteers, the NLP community still\nlacks a systematic analysis of the gazetteer-enhanced NER model. In this paper,\nwe first re-examine the effectiveness several common practices of the\ngazetteer-enhanced NER models and carry out a series of detailed analysis to\nevaluate the relationship between the model performance and the gazetteer\ncharacteristics, which can guide us to build a more suitable gazetteer. The\nfindings of this paper are as follows: (1) the gazetteer improves most of the\nsituations that the traditional NER model datasets are difficult to learn. (2)\nthe performance of model greatly benefits from the high-quality pre-trained\nlexeme embeddings. (3) a good gazetteer should cover more entities that can be\nmatched in both the training set and testing set.\n",
                "链接": "https://arxiv.org/abs/2207.02802"
            },
            {
                "文章ID": "32734",
                "标题": "An Embarrassingly Easy but Strong Baseline for Nested Named Entity\n  Recognition",
                "作者": " Hang Yan,  Yu Sun,  Xiaonan Li,  Xipeng Qiu",
                "发布日期": "2022-09-16",
                "摘要": "  Named entity recognition (NER) is the task to detect and classify the entity\nspans in the text. When entity spans overlap between each other, this problem\nis named as nested NER. Span-based methods have been widely used to tackle the\nnested NER. Most of these methods will get a score $n \\times n$ matrix, where\n$n$ means the length of sentence, and each entry corresponds to a span.\nHowever, previous work ignores spatial relations in the score matrix. In this\npaper, we propose using Convolutional Neural Network (CNN) to model these\nspatial relations in the score matrix. Despite being simple, experiments in\nthree commonly used nested NER datasets show that our model surpasses several\nrecently proposed methods with the same pre-trained encoders. Further analysis\nshows that using CNN can help the model find more nested entities. Besides, we\nfound that different papers used different sentence tokenizations for the three\nnested NER datasets, which will influence the comparison. Thus, we release a\npre-processing script to facilitate future comparison.\n",
                "链接": "https://arxiv.org/abs/2208.04534"
            },
            {
                "文章ID": "125250",
                "标题": "Unified Lattice Graph Fusion for Chinese Named Entity Recognition",
                "作者": " Dixiang Zhang,  Junyu Lu,  Pingjian Zhang",
                "发布日期": "2023-12-29",
                "摘要": "  Integrating lexicon into character-level sequence has been proven effective\nto leverage word boundary and semantic information in Chinese named entity\nrecognition (NER). However, prior approaches usually utilize feature weighting\nand position coupling to integrate word information, but ignore the semantic\nand contextual correspondence between the fine-grained semantic units in the\ncharacter-word space. To solve this issue, we propose a Unified Lattice Graph\nFusion (ULGF) approach for Chinese NER. ULGF can explicitly capture various\nsemantic and boundary relations across different semantic units with the\nadjacency matrix by converting the lattice structure into a unified graph. We\nstack multiple graph-based intra-source self-attention and inter-source\ncross-gating fusion layers that iteratively carry out semantic interactions to\nlearn node representations. To alleviate the over-reliance on word information,\nwe further propose to leverage lexicon entity classification as an auxiliary\ntask. Experiments on four Chinese NER benchmark datasets demonstrate the\nsuperiority of our ULGF approach.\n",
                "链接": "https://arxiv.org/abs/2312.16917"
            },
            {
                "文章ID": "44907",
                "标题": "Instance Segmentation for Chinese Character Stroke Extraction, Datasets\n  and Benchmarks",
                "作者": " Lizhao Liu,  Kunyang Lin,  Shangxin Huang,  Zhongli Li,  Chao Li,  Yunbo Cao,  Qingyu Zhou",
                "发布日期": "2022-10-26",
                "摘要": "  Stroke is the basic element of Chinese character and stroke extraction has\nbeen an important and long-standing endeavor. Existing stroke extraction\nmethods are often handcrafted and highly depend on domain expertise due to the\nlimited training data. Moreover, there are no standardized benchmarks to\nprovide a fair comparison between different stroke extraction methods, which,\nwe believe, is a major impediment to the development of Chinese character\nstroke understanding and related tasks. In this work, we present the first\npublic available Chinese Character Stroke Extraction (CCSE) benchmark, with two\nnew large-scale datasets: Kaiti CCSE (CCSE-Kai) and Handwritten CCSE (CCSE-HW).\nWith the large-scale datasets, we hope to leverage the representation power of\ndeep models such as CNNs to solve the stroke extraction task, which, however,\nremains an open question. To this end, we turn the stroke extraction problem\ninto a stroke instance segmentation problem. Using the proposed datasets to\ntrain a stroke instance segmentation model, we surpass previous methods by a\nlarge margin. Moreover, the models trained with the proposed datasets benefit\nthe downstream font generation and handwritten aesthetic assessment tasks. We\nhope these benchmark results can facilitate further research. The source code\nand datasets are publicly available at: https://github.com/lizhaoliu-Lec/CCSE.\n",
                "链接": "https://arxiv.org/abs/2210.13826"
            },
            {
                "文章ID": "14564",
                "标题": "Delving Deep into Regularity: A Simple but Effective Method for Chinese\n  Named Entity Recognition",
                "作者": " Yingjie Gu,  Xiaoye Qu,  Zhefeng Wang,  Yi Zheng,  Baoxing Huai,  Nicholas Jing Yuan",
                "发布日期": "2022-04-19",
                "摘要": "  Recent years have witnessed the improving performance of Chinese Named Entity\nRecognition (NER) from proposing new frameworks or incorporating word lexicons.\nHowever, the inner composition of entity mentions in character-level Chinese\nNER has been rarely studied. Actually, most mentions of regular types have\nstrong name regularity. For example, entities end with indicator words such as\n\"company\" or \"bank\" usually belong to organization. In this paper, we propose a\nsimple but effective method for investigating the regularity of entity spans in\nChinese NER, dubbed as Regularity-Inspired reCOgnition Network (RICON).\nSpecifically, the proposed model consists of two branches: a regularity-aware\nmodule and a regularityagnostic module. The regularity-aware module captures\nthe internal regularity of each span for better entity type prediction, while\nthe regularity-agnostic module is employed to locate the boundary of entities\nand relieve the excessive attention to span regularity. An orthogonality space\nis further constructed to encourage two modules to extract different aspects of\nregularity features. To verify the effectiveness of our method, we conduct\nextensive experiments on three benchmark datasets and a practical medical\ndataset. The experimental results show that our RICON significantly outperforms\nprevious state-of-the-art methods, including various lexicon-based methods.\n",
                "链接": "https://arxiv.org/abs/2204.05544"
            },
            {
                "文章ID": "15120",
                "标题": "Qtrade AI at SemEval-2022 Task 11: An Unified Framework for Multilingual\n  NER Task",
                "作者": " Weichao Gan,  Yuanping Lin,  Guangbo Yu,  Guimin Chen,  Qian Ye",
                "发布日期": "2022-04-18",
                "摘要": "  This paper describes our system, which placed third in the Multilingual Track\n(subtask 11), fourth in the Code-Mixed Track (subtask 12), and seventh in the\nChinese Track (subtask 9) in the SemEval 2022 Task 11: MultiCoNER Multilingual\nComplex Named Entity Recognition. Our system's key contributions are as\nfollows: 1) For multilingual NER tasks, we offer an unified framework with\nwhich one can easily execute single-language or multilingual NER tasks, 2) for\nlow-resource code-mixed NER task, one can easily enhance his or her dataset\nthrough implementing several simple data augmentation methods and 3) for\nChinese tasks, we propose a model that can capture Chinese lexical semantic,\nlexical border, and lexical graph structural information. Finally, our system\nachieves macro-f1 scores of 77.66, 84.35, and 74.00 on subtasks 11, 12, and 9,\nrespectively, during the testing phase.\n",
                "链接": "https://arxiv.org/abs/2204.07459"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下最近用反事实做数据增强的文章",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "81687",
                "标题": "On Counterfactual Data Augmentation Under Confounding",
                "作者": " Abbavaram Gowtham Reddy,  Saketh Bachu,  Saloni Dash,  Charchit Sharma,  Amit Sharma,  Vineeth N Balasubramanian",
                "发布日期": "2023-11-22",
                "摘要": "  Counterfactual data augmentation has recently emerged as a method to mitigate\nconfounding biases in the training data. These biases, such as spurious\ncorrelations, arise due to various observed and unobserved confounding\nvariables in the data generation process. In this paper, we formally analyze\nhow confounding biases impact downstream classifiers and present a causal\nviewpoint to the solutions based on counterfactual data augmentation. We\nexplore how removing confounding biases serves as a means to learn invariant\nfeatures, ultimately aiding in generalization beyond the observed data\ndistribution. Additionally, we present a straightforward yet powerful algorithm\nfor generating counterfactual images, which effectively mitigates the influence\nof confounding effects on downstream classifiers. Through experiments on MNIST\nvariants and the CelebA datasets, we demonstrate how our simple augmentation\nmethod helps existing state-of-the-art methods achieve good results.\n",
                "链接": "https://arxiv.org/abs/2305.18183"
            },
            {
                "文章ID": "34843",
                "标题": "Data Augmentation for Graph Data: Recent Advancements",
                "作者": " Maria Marrium,  Arif Mahmood",
                "发布日期": "2022-08-26",
                "摘要": "  Graph Neural Network (GNNs) based methods have recently become a popular tool\nto deal with graph data because of their ability to incorporate structural\ninformation. The only hurdle in the performance of GNNs is the lack of labeled\ndata. Data Augmentation techniques for images and text data can not be used for\ngraph data because of the complex and non-euclidean structure of graph data.\nThis gap has forced researchers to shift their focus towards the development of\ndata augmentation techniques for graph data. Most of the proposed Graph Data\nAugmentation (GDA) techniques are task-specific. In this paper, we survey the\nexisting GDA techniques based on different graph tasks. This survey not only\nprovides a reference to the research community of GDA but also provides the\nnecessary information to the researchers of other domains.\n",
                "链接": "https://arxiv.org/abs/2208.11973"
            },
            {
                "文章ID": "36857",
                "标题": "Bias Challenges in Counterfactual Data Augmentation",
                "作者": " S Chandra Mouli,  Yangze Zhou,  Bruno Ribeiro",
                "发布日期": "2022-09-15",
                "摘要": "  Deep learning models tend not to be out-of-distribution robust primarily due\nto their reliance on spurious features to solve the task. Counterfactual data\naugmentations provide a general way of (approximately) achieving\nrepresentations that are counterfactual-invariant to spurious features, a\nrequirement for out-of-distribution (OOD) robustness. In this work, we show\nthat counterfactual data augmentations may not achieve the desired\ncounterfactual-invariance if the augmentation is performed by a\ncontext-guessing machine, an abstract machine that guesses the most-likely\ncontext of a given input. We theoretically analyze the invariance imposed by\nsuch counterfactual data augmentations and describe an exemplar NLP task where\ncounterfactual data augmentation by a context-guessing machine does not lead to\nrobust OOD classifiers.\n",
                "链接": "https://arxiv.org/abs/2209.05104"
            },
            {
                "文章ID": "43960",
                "标题": "MoCoDA: Model-based Counterfactual Data Augmentation",
                "作者": " Silviu Pitis,  Elliot Creager,  Ajay Mandlekar,  Animesh Garg",
                "发布日期": "2022-10-21",
                "摘要": "  The number of states in a dynamic process is exponential in the number of\nobjects, making reinforcement learning (RL) difficult in complex, multi-object\ndomains. For agents to scale to the real world, they will need to react to and\nreason about unseen combinations of objects. We argue that the ability to\nrecognize and use local factorization in transition dynamics is a key element\nin unlocking the power of multi-object reasoning. To this end, we show that (1)\nknown local structure in the environment transitions is sufficient for an\nexponential reduction in the sample complexity of training a dynamics model,\nand (2) a locally factored dynamics model provably generalizes\nout-of-distribution to unseen states and actions. Knowing the local structure\nalso allows us to predict which unseen states and actions this dynamics model\nwill generalize to. We propose to leverage these observations in a novel\nModel-based Counterfactual Data Augmentation (MoCoDA) framework. MoCoDA applies\na learned locally factored dynamics model to an augmented distribution of\nstates and actions to generate counterfactual transitions for RL. MoCoDA works\nwith a broader set of local structures than prior work and allows for direct\ncontrol over the augmented training distribution. We show that MoCoDA enables\nRL agents to learn policies that generalize to unseen states and actions. We\nuse MoCoDA to train an offline RL agent to solve an out-of-distribution\nrobotics manipulation task on which standard offline RL algorithms fail.\n",
                "链接": "https://arxiv.org/abs/2210.11287"
            },
            {
                "文章ID": "114161",
                "标题": "Counterfactual Data Augmentation with Contrastive Learning",
                "作者": " Ahmed Aloui,  Juncheng Dong,  Cat P. Le,  Vahid Tarokh",
                "发布日期": "2023-11-08",
                "摘要": "  Statistical disparity between distinct treatment groups is one of the most\nsignificant challenges for estimating Conditional Average Treatment Effects\n(CATE). To address this, we introduce a model-agnostic data augmentation method\nthat imputes the counterfactual outcomes for a selected subset of individuals.\nSpecifically, we utilize contrastive learning to learn a representation space\nand a similarity measure such that in the learned representation space close\nindividuals identified by the learned similarity measure have similar potential\noutcomes. This property ensures reliable imputation of counterfactual outcomes\nfor the individuals with close neighbors from the alternative treatment group.\nBy augmenting the original dataset with these reliable imputations, we can\neffectively reduce the discrepancy between different treatment groups, while\ninducing minimal imputation error. The augmented dataset is subsequently\nemployed to train CATE estimation models. Theoretical analysis and experimental\nstudies on synthetic and semi-synthetic benchmarks demonstrate that our method\nachieves significant improvements in both performance and robustness to\noverfitting across state-of-the-art models.\n",
                "链接": "https://arxiv.org/abs/2311.03630"
            },
            {
                "文章ID": "74413",
                "标题": "Implicit Counterfactual Data Augmentation for Deep Neural Networks",
                "作者": " Xiaoling Zhou,  Ou Wu",
                "发布日期": "2023-04-27",
                "摘要": "  Machine-learning models are prone to capturing the spurious correlations\nbetween non-causal attributes and classes, with counterfactual data\naugmentation being a promising direction for breaking these spurious\nassociations. However, explicitly generating counterfactual data is\nchallenging, with the training efficiency declining. Therefore, this study\nproposes an implicit counterfactual data augmentation (ICDA) method to remove\nspurious correlations and make stable predictions. Specifically, first, a novel\nsample-wise augmentation strategy is developed that generates semantically and\ncounterfactually meaningful deep features with distinct augmentation strength\nfor each sample. Second, we derive an easy-to-compute surrogate loss on the\naugmented feature set when the number of augmented samples becomes infinite.\nThird, two concrete schemes are proposed, including direct quantification and\nmeta-learning, to derive the key parameters for the robust loss. In addition,\nICDA is explained from a regularization aspect, with extensive experiments\nindicating that our method consistently improves the generalization performance\nof popular depth networks on multiple typical learning scenarios that require\nout-of-distribution generalization.\n",
                "链接": "https://arxiv.org/abs/2304.13431"
            },
            {
                "文章ID": "21004",
                "标题": "Counterfactual Data Augmentation improves Factuality of Abstractive\n  Summarization",
                "作者": " Dheeraj Rajagopal,  Siamak Shakeri,  Cicero Nogueira dos Santos,  Eduard Hovy,  Chung-Ching Chang",
                "发布日期": "2022-05-26",
                "摘要": "  Abstractive summarization systems based on pretrained language models often\ngenerate coherent but factually inconsistent sentences. In this paper, we\npresent a counterfactual data augmentation approach where we augment data with\nperturbed summaries that increase the training data diversity. Specifically, we\npresent three augmentation approaches based on replacing (i) entities from\nother and the same category and (ii) nouns with their corresponding WordNet\nhypernyms. We show that augmenting the training data with our approach improves\nthe factual correctness of summaries without significantly affecting the ROUGE\nscore. We show that in two commonly used summarization datasets (CNN/Dailymail\nand XSum), we improve the factual correctness by about 2.5 points on average\n",
                "链接": "https://arxiv.org/abs/2205.12416"
            },
            {
                "文章ID": "1305",
                "标题": "Investigation of Data Augmentation Techniques for Disordered Speech\n  Recognition",
                "作者": " Mengzhe Geng,  Xurong Xie,  Shansong Liu,  Jianwei Yu,  Shoukang Hu,  Xunying Liu,  Helen Meng",
                "发布日期": "2022-01-20",
                "摘要": "  Disordered speech recognition is a highly challenging task. The underlying\nneuro-motor conditions of people with speech disorders, often compounded with\nco-occurring physical disabilities, lead to the difficulty in collecting large\nquantities of speech required for system development. This paper investigates a\nset of data augmentation techniques for disordered speech recognition,\nincluding vocal tract length perturbation (VTLP), tempo perturbation and speed\nperturbation. Both normal and disordered speech were exploited in the\naugmentation process. Variability among impaired speakers in both the original\nand augmented data was modeled using learning hidden unit contributions (LHUC)\nbased speaker adaptive training. The final speaker adapted system constructed\nusing the UASpeech corpus and the best augmentation approach based on speed\nperturbation produced up to 2.92% absolute (9.3% relative) word error rate\n(WER) reduction over the baseline system without data augmentation, and gave an\noverall WER of 26.37% on the test set containing 16 dysarthric speakers.\n",
                "链接": "https://arxiv.org/abs/2201.05562"
            },
            {
                "文章ID": "81064",
                "标题": "GDA: Generative Data Augmentation Techniques for Relation Extraction\n  Tasks",
                "作者": " Xuming Hu,  Aiwei Liu,  Zeqi Tan,  Xin Zhang,  Chenwei Zhang,  Irwin King,  Philip S. Yu",
                "发布日期": "2023-06-16",
                "摘要": "  Relation extraction (RE) tasks show promising performance in extracting\nrelations from two entities mentioned in sentences, given sufficient\nannotations available during training. Such annotations would be\nlabor-intensive to obtain in practice. Existing work adopts data augmentation\ntechniques to generate pseudo-annotated sentences beyond limited annotations.\nThese techniques neither preserve the semantic consistency of the original\nsentences when rule-based augmentations are adopted, nor preserve the syntax\nstructure of sentences when expressing relations using seq2seq models,\nresulting in less diverse augmentations. In this work, we propose a dedicated\naugmentation technique for relational texts, named GDA, which uses two\ncomplementary modules to preserve both semantic consistency and syntax\nstructures. We adopt a generative formulation and design a multi-tasking\nsolution to achieve synergies. Furthermore, GDA adopts entity hints as the\nprior knowledge of the generative model to augment diverse sentences.\nExperimental results in three datasets under a low-resource setting showed that\nGDA could bring {\\em 2.0\\%} F1 improvements compared with no augmentation\ntechnique. Source code and data are available.\n",
                "链接": "https://arxiv.org/abs/2305.16663"
            },
            {
                "文章ID": "114320",
                "标题": "Modelling Sentiment Analysis: LLMs and data augmentation techniques",
                "作者": " Guillem Senabre Prades",
                "发布日期": "2023-11-08",
                "摘要": "  This paper provides different approaches for a binary sentiment\nclassification on a small training dataset. LLMs that provided state-of-the-art\nresults in sentiment analysis and similar domains are being used, such as BERT,\nRoBERTa and XLNet.\n",
                "链接": "https://arxiv.org/abs/2311.04139"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下利用蒙特卡罗树搜索解决序列决策问题的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "53055",
                "标题": "Lookahead Pathology in Monte-Carlo Tree Search",
                "作者": " Khoi P. N. Nguyen,  Raghuram Ramanujan",
                "发布日期": "2022-12-13",
                "摘要": "  Monte-Carlo Tree Search (MCTS) is an adversarial search paradigm that first\nfound prominence with its success in the domain of computer Go. Early\ntheoretical work established the game-theoretic soundness and convergence\nbounds for Upper Confidence bounds applied to Trees (UCT), the most popular\ninstantiation of MCTS; however, there remain notable gaps in our understanding\nof how UCT behaves in practice. In this work, we address one such gap by\nconsidering the question of whether UCT can exhibit lookahead pathology -- a\nparadoxical phenomenon first observed in Minimax search where greater search\neffort leads to worse decision-making. We introduce a novel family of synthetic\ngames that offer rich modeling possibilities while remaining amenable to\nmathematical analysis. Our theoretical and experimental results suggest that\nUCT is indeed susceptible to pathological behavior in a range of games drawn\nfrom this family.\n",
                "链接": "https://arxiv.org/abs/2212.05208"
            },
            {
                "文章ID": "54835",
                "标题": "Feature Acquisition using Monte Carlo Tree Search",
                "作者": " Sungsoo Lim,  Diego Klabjan,  Mark Shapiro",
                "发布日期": "2022-12-23",
                "摘要": "  Feature acquisition algorithms address the problem of acquiring informative\nfeatures while balancing the costs of acquisition to improve the learning\nperformances of ML models. Previous approaches have focused on calculating the\nexpected utility values of features to determine the acquisition sequences.\nOther approaches formulated the problem as a Markov Decision Process (MDP) and\napplied reinforcement learning based algorithms. In comparison to previous\napproaches, we focus on 1) formulating the feature acquisition problem as a MDP\nand applying Monte Carlo Tree Search, 2) calculating the intermediary rewards\nfor each acquisition step based on model improvements and acquisition costs and\n3) simultaneously optimizing model improvement and acquisition costs with\nmulti-objective Monte Carlo Tree Search. With Proximal Policy Optimization and\nDeep Q-Network algorithms as benchmark, we show the effectiveness of our\nproposed approach with experimental study.\n",
                "链接": "https://arxiv.org/abs/2212.11360"
            },
            {
                "文章ID": "67092",
                "标题": "Proof Number Based Monte-Carlo Tree Search",
                "作者": " Jakub Kowalski,  Elliot Doe,  Mark H. M. Winands,  Daniel Górski,  Dennis J. N. J. Soemers",
                "发布日期": "2023-12-22",
                "摘要": "  This paper proposes a new game-search algorithm, PN-MCTS, which combines\nMonte-Carlo Tree Search (MCTS) and Proof-Number Search (PNS). These two\nalgorithms have been successfully applied for decision making in a range of\ndomains. We define three areas where the additional knowledge provided by the\nproof and disproof numbers gathered in MCTS trees might be used: final move\nselection, solving subtrees, and the UCB1 selection mechanism. We test all\npossible combinations on different time settings, playing against vanilla UCT\non several games: Lines of Action ($7$$\\times$$7$ and $8$$\\times$$8$ board\nsizes), MiniShogi, Knightthrough, and Awari. Furthermore, we extend this new\nalgorithm to properly address games with draws, like Awari, by adding an\nadditional layer of PNS on top of the MCTS tree. The experiments show that\nPN-MCTS confidently outperforms MCTS in all tested game domains, achieving win\nrates up to 96.2\\% for Lines of Action.\n",
                "链接": "https://arxiv.org/abs/2303.09449"
            },
            {
                "文章ID": "102301",
                "标题": "Maneuver Decision-Making Through Proximal Policy Optimization And Monte\n  Carlo Tree Search",
                "作者": " Zhang Hong-Peng",
                "发布日期": "2023-09-19",
                "摘要": "  Maneuver decision-making can be regarded as a Markov decision process and can\nbe address by reinforcement learning. However, original reinforcement learning\nalgorithms can hardly solve the maneuvering decision-making problem. One reason\nis that agents use random actions in the early stages of training, which makes\nit difficult to get rewards and learn how to make effective decisions. To\naddress this issue, a method based on proximal policy optimization and Monte\nCarlo tree search is proposed. The method uses proximal policy optimization to\ntrain the agent, and regards the results of air combat as targets to train the\nvalue network. Then, based on the value network and the visit count of each\nnode, Monte Carlo tree search is used to find the actions with more expected\nreturns than random actions, which can improve the training performance. The\nablation studies and simulation experiments indicate that agents trained by the\nproposed method can make different decisions according to different states,\nwhich demonstrates that the method can solve the maneuvering decision problem\nthat the original reinforcement learning algorithm cannot solve.\n",
                "链接": "https://arxiv.org/abs/2309.08611"
            },
            {
                "文章ID": "23592",
                "标题": "Combining Monte-Carlo Tree Search with Proof-Number Search",
                "作者": " Elliot Doe,  Mark H. M. Winands,  Dennis J. N. J. Soemers,  Cameron Browne",
                "发布日期": "2022-06-09",
                "摘要": "  Proof-Number Search (PNS) and Monte-Carlo Tree Search (MCTS) have been\nsuccessfully applied for decision making in a range of games. This paper\nproposes a new approach called PN-MCTS that combines these two tree-search\nmethods by incorporating the concept of proof and disproof numbers into the UCT\nformula of MCTS. Experimental results demonstrate that PN-MCTS outperforms\nbasic MCTS in several games including Lines of Action, MiniShogi,\nKnightthrough, and Awari, achieving win rates up to 94.0%.\n",
                "链接": "https://arxiv.org/abs/2206.03965"
            },
            {
                "文章ID": "7216",
                "标题": "Decision Making in Non-Stationary Environments with Policy-Augmented\n  Monte Carlo Tree Search",
                "作者": " Geoffrey Pettet,  Ayan Mukhopadhyay,  Abhishek Dubey",
                "发布日期": "2022-03-01",
                "摘要": "  Decision-making under uncertainty (DMU) is present in many important\nproblems. An open challenge is DMU in non-stationary environments, where the\ndynamics of the environment can change over time. Reinforcement Learning (RL),\na popular approach for DMU problems, learns a policy by interacting with a\nmodel of the environment offline. Unfortunately, if the environment changes the\npolicy can become stale and take sub-optimal actions, and relearning the policy\nfor the updated environment takes time and computational effort. An alternative\nis online planning approaches such as Monte Carlo Tree Search (MCTS), which\nperform their computation at decision time. Given the current environment, MCTS\nplans using high-fidelity models to determine promising action trajectories.\nThese models can be updated as soon as environmental changes are detected to\nimmediately incorporate them into decision making. However, MCTS's convergence\ncan be slow for domains with large state-action spaces. In this paper, we\npresent a novel hybrid decision-making approach that combines the strengths of\nRL and planning while mitigating their weaknesses. Our approach, called Policy\nAugmented MCTS (PA-MCTS), integrates a policy's actin-value estimates into\nMCTS, using the estimates to seed the action trajectories favored by the\nsearch. We hypothesize that PA-MCTS will converge more quickly than standard\nMCTS while making better decisions than the policy can make on its own when\nfaced with nonstationary environments. We test our hypothesis by comparing\nPA-MCTS with pure MCTS and an RL agent applied to the classical CartPole\nenvironment. We find that PC-MCTS can achieve higher cumulative rewards than\nthe policy in isolation under several environmental shifts while converging in\nsignificantly fewer iterations than pure MCTS.\n",
                "链接": "https://arxiv.org/abs/2202.13003"
            },
            {
                "文章ID": "22592",
                "标题": "Policy Gradient Algorithms with Monte-Carlo Tree Search for Non-Markov\n  Decision Processes",
                "作者": " Tetsuro Morimura,  Kazuhiro Ota,  Kenshi Abe,  Peinan Zhang",
                "发布日期": "2022-06-03",
                "摘要": "  Policy gradient (PG) is a reinforcement learning (RL) approach that optimizes\na parameterized policy model for an expected return using gradient ascent.\nGiven a well-parameterized policy model, such as a neural network model, with\nappropriate initial parameters, the PG algorithms work well even when\nenvironment does not have the Markov property. Otherwise, they can be trapped\non a plateau or suffer from peakiness effects. As another successful RL\napproach, algorithms based on Monte-Carlo Tree Search (MCTS), which include\nAlphaZero, have obtained groundbreaking results especially on the board game\nplaying domain. They are also suitable to be applied to non-Markov decision\nprocesses. However, since the standard MCTS does not have the ability to learn\nstate representation, the size of the tree-search space can be too large to\nsearch. In this work, we examine a mixture policy of PG and MCTS to complement\neach other's difficulties and take advantage of them. We derive conditions for\nasymptotic convergence with results of a two-timescale stochastic approximation\nand propose an algorithm that satisfies these conditions. The effectivity of\nthe proposed methods is verified through numerical experiments on non-Markov\ndecision processes.\n",
                "链接": "https://arxiv.org/abs/2206.01011"
            },
            {
                "文章ID": "107708",
                "标题": "Accelerating Monte Carlo Tree Search with Probability Tree State\n  Abstraction",
                "作者": " Yangqing Fu,  Ming Sun,  Buqing Nie,  Yue Gao",
                "发布日期": "2023-10-11",
                "摘要": "  Monte Carlo Tree Search (MCTS) algorithms such as AlphaGo and MuZero have\nachieved superhuman performance in many challenging tasks. However, the\ncomputational complexity of MCTS-based algorithms is influenced by the size of\nthe search space. To address this issue, we propose a novel probability tree\nstate abstraction (PTSA) algorithm to improve the search efficiency of MCTS. A\ngeneral tree state abstraction with path transitivity is defined. In addition,\nthe probability tree state abstraction is proposed for fewer mistakes during\nthe aggregation step. Furthermore, the theoretical guarantees of the\ntransitivity and aggregation error bound are justified. To evaluate the\neffectiveness of the PTSA algorithm, we integrate it with state-of-the-art\nMCTS-based algorithms, such as Sampled MuZero and Gumbel MuZero. Experimental\nresults on different tasks demonstrate that our method can accelerate the\ntraining process of state-of-the-art algorithms with 10%-45% search space\nreduction.\n",
                "链接": "https://arxiv.org/abs/2310.06513"
            },
            {
                "文章ID": "108378",
                "标题": "LightZero: A Unified Benchmark for Monte Carlo Tree Search in General\n  Sequential Decision Scenarios",
                "作者": " Yazhe Niu,  Yuan Pu,  Zhenjie Yang,  Xueyan Li,  Tong Zhou,  Jiyuan Ren,  Shuai Hu,  Hongsheng Li,  Yu Liu",
                "发布日期": "2023-10-13",
                "摘要": "  Building agents based on tree-search planning capabilities with learned\nmodels has achieved remarkable success in classic decision-making problems,\nsuch as Go and Atari. However, it has been deemed challenging or even\ninfeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse\nreal-world applications, especially when these environments involve complex\naction spaces and significant simulation costs, or inherent stochasticity. In\nthis work, we introduce LightZero, the first unified benchmark for deploying\nMCTS/MuZero in general sequential decision scenarios. Specificially, we\nsummarize the most critical challenges in designing a general MCTS-style\ndecision-making solver, then decompose the tightly-coupled algorithm and system\ndesign of tree-search RL methods into distinct sub-modules. By incorporating\nmore appropriate exploration and optimization strategies, we can significantly\nenhance these sub-modules and construct powerful LightZero agents to tackle\ntasks across a wide range of domains, such as board games, Atari, MuJoCo,\nMiniGrid and GoBigger. Detailed benchmark results reveal the significant\npotential of such methods in building scalable and efficient decision\nintelligence. The code is available as part of OpenDILab at\nhttps://github.com/opendilab/LightZero.\n",
                "链接": "https://arxiv.org/abs/2310.08348"
            },
            {
                "文章ID": "3914",
                "标题": "On Monte Carlo Tree Search for Weighted Vertex Coloring",
                "作者": " Cyril Grelier,  Olivier Goudet,  Jin-Kao Hao",
                "发布日期": "2022-04-08",
                "摘要": "  This work presents the first study of using the popular Monte Carlo Tree\nSearch (MCTS) method combined with dedicated heuristics for solving the\nWeighted Vertex Coloring Problem. Starting with the basic MCTS algorithm, we\ngradually introduce a number of algorithmic variants where MCTS is extended by\nvarious simulation strategies including greedy and local search heuristics. We\nconduct experiments on well-known benchmark instances to assess the value of\neach studied combination. We also provide empirical evidence to shed light on\nthe advantages and limits of each strategy.\n",
                "链接": "https://arxiv.org/abs/2202.01665"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下利用gpt4做评测指标优缺点的文章",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "123333",
                "标题": "The Pros and Cons of Adversarial Robustness",
                "作者": " Yacine Izza,  Joao Marques-Silva",
                "发布日期": "2023-12-19",
                "摘要": "  Robustness is widely regarded as a fundamental problem in the analysis of\nmachine learning (ML) models. Most often robustness equates with deciding the\nnon-existence of adversarial examples, where adversarial examples denote\nsituations where small changes on some inputs cause a change in the prediction.\nThe perceived importance of ML model robustness explains the continued progress\nobserved for most of the last decade. Whereas robustness is often assessed\nlocally, i.e. given some target point in feature space, robustness can also be\ndefined globally, i.e. where any point in feature space can be considered. The\nimportance of ML model robustness is illustrated for example by the existence\nof competitions evaluating the progress of robustness tools, namely in the case\nof neural networks (NNs) but also by efforts towards robustness certification.\nMore recently, robustness tools have also been used for computing rigorous\nexplanations of ML models. In contrast with the observed successes of\nrobustness, this paper uncovers some limitations with existing definitions of\nrobustness, both global and local, but also with efforts towards robustness\ncertification. The paper also investigates uses of adversarial examples besides\nthose related with robustness.\n",
                "链接": "https://arxiv.org/abs/2312.10911"
            },
            {
                "文章ID": "33038",
                "标题": "On the Pros and Cons of Momentum Encoder in Self-Supervised Visual\n  Representation Learning",
                "作者": " Trung Pham,  Chaoning Zhang,  Axi Niu,  Kang Zhang,  Chang D. Yoo",
                "发布日期": "2022-08-12",
                "摘要": "  Exponential Moving Average (EMA or momentum) is widely used in modern\nself-supervised learning (SSL) approaches, such as MoCo, for enhancing\nperformance. We demonstrate that such momentum can also be plugged into\nmomentum-free SSL frameworks, such as SimCLR, for a performance boost. Despite\nits wide use as a fundamental component in modern SSL frameworks, the benefit\ncaused by momentum is not well understood. We find that its success can be at\nleast partly attributed to the stability effect. In the first attempt, we\nanalyze how EMA affects each part of the encoder and reveal that the portion\nnear the encoder's input plays an insignificant role while the latter parts\nhave much more influence. By monitoring the gradient of the overall loss with\nrespect to the output of each block in the encoder, we observe that the final\nlayers tend to fluctuate much more than other layers during backpropagation,\ni.e. less stability. Interestingly, we show that using EMA to the final part of\nthe SSL encoder, i.e. projector, instead of the whole deep network encoder can\ngive comparable or preferable performance. Our proposed projector-only momentum\nhelps maintain the benefit of EMA but avoids the double forward computation.\n",
                "链接": "https://arxiv.org/abs/2208.05744"
            },
            {
                "文章ID": "38917",
                "标题": "News Summarization and Evaluation in the Era of GPT-3",
                "作者": " Tanya Goyal,  Junyi Jessy Li,  Greg Durrett",
                "发布日期": "2023-05-25",
                "摘要": "  The recent success of prompting large language models like GPT-3 has led to a\nparadigm shift in NLP research. In this paper, we study its impact on text\nsummarization, focusing on the classic benchmark domain of news summarization.\nFirst, we investigate how GPT-3 compares against fine-tuned models trained on\nlarge summarization datasets. We show that not only do humans overwhelmingly\nprefer GPT-3 summaries, prompted using only a task description, but these also\ndo not suffer from common dataset-specific issues such as poor factuality.\nNext, we study what this means for evaluation, particularly the role of gold\nstandard test sets. Our experiments show that both reference-based and\nreference-free automatic metrics cannot reliably evaluate GPT-3 summaries.\nFinally, we evaluate models on a setting beyond generic summarization,\nspecifically keyword-based summarization, and show how dominant fine-tuning\napproaches compare to prompting.\n  To support further research, we release: (a) a corpus of 10K generated\nsummaries from fine-tuned and prompt-based models across 4 standard\nsummarization benchmarks, (b) 1K human preference judgments comparing different\nsystems for generic- and keyword-based summarization.\n",
                "链接": "https://arxiv.org/abs/2209.12356"
            },
            {
                "文章ID": "115113",
                "标题": "The Pros and Cons of Using Machine Learning and Interpretable Machine\n  Learning Methods in psychiatry detection applications, specifically\n  depression disorder: A Brief Review",
                "作者": " Hossein Simchi,  Samira Tajik",
                "发布日期": "2023-11-14",
                "摘要": "  The COVID-19 pandemic has forced many people to limit their social\nactivities, which has resulted in a rise in mental illnesses, particularly\ndepression. To diagnose these illnesses with accuracy and speed, and prevent\nsevere outcomes such as suicide, the use of machine learning has become\nincreasingly important. Additionally, to provide precise and understandable\ndiagnoses for better treatment, AI scientists and researchers must develop\ninterpretable AI-based solutions. This article provides an overview of relevant\narticles in the field of machine learning and interpretable AI, which helps to\nunderstand the advantages and disadvantages of using AI in psychiatry disorder\ndetection applications.\n",
                "链接": "https://arxiv.org/abs/2311.06633"
            },
            {
                "文章ID": "83021",
                "标题": "Systematic Evaluation of GPT-3 for Zero-Shot Personality Estimation",
                "作者": " Adithya V Ganesan,  Yash Kumar Lal,  August Håkan Nilsson,  H. Andrew Schwartz",
                "发布日期": "2023-06-05",
                "摘要": "  Very large language models (LLMs) perform extremely well on a spectrum of NLP\ntasks in a zero-shot setting. However, little is known about their performance\non human-level NLP problems which rely on understanding psychological concepts,\nsuch as assessing personality traits. In this work, we investigate the\nzero-shot ability of GPT-3 to estimate the Big 5 personality traits from users'\nsocial media posts. Through a set of systematic experiments, we find that\nzero-shot GPT-3 performance is somewhat close to an existing pre-trained SotA\nfor broad classification upon injecting knowledge about the trait in the\nprompts. However, when prompted to provide fine-grained classification, its\nperformance drops to close to a simple most frequent class (MFC) baseline. We\nfurther analyze where GPT-3 performs better, as well as worse, than a\npretrained lexical model, illustrating systematic errors that suggest ways to\nimprove LLMs on human-level NLP tasks.\n",
                "链接": "https://arxiv.org/abs/2306.01183"
            },
            {
                "文章ID": "102774",
                "标题": "Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction",
                "作者": " Shaika Chowdhury,  Sivaraman Rajaganapathy,  Lichao Sun,  James Cerhan,  Nansu Zong",
                "发布日期": "2023-09-20",
                "摘要": "  In this study, we investigated the potential of GPT-3 for the anti-cancer\ndrug sensitivity prediction task using structured pharmacogenomics data across\nfive tissue types and evaluated its performance with zero-shot prompting and\nfine-tuning paradigms. The drug's smile representation and cell line's genomic\nmutation features were predictive of the drug response. The results from this\nstudy have the potential to pave the way for designing more efficient treatment\nprotocols in precision oncology.\n",
                "链接": "https://arxiv.org/abs/2309.10016"
            },
            {
                "文章ID": "5926",
                "标题": "On the Evaluation Metrics for Paraphrase Generation",
                "作者": " Lingfeng Shen,  Lemao Liu,  Haiyun Jiang,  Shuming Shi",
                "发布日期": "2022-10-11",
                "摘要": "  In this paper we revisit automatic metrics for paraphrase evaluation and\nobtain two findings that disobey conventional wisdom: (1) Reference-free\nmetrics achieve better performance than their reference-based counterparts. (2)\nMost commonly used metrics do not align well with human annotation. Underlying\nreasons behind the above findings are explored through additional experiments\nand in-depth analyses. Based on the experiments and analyses, we propose\nParaScore, a new evaluation metric for paraphrase generation. It possesses the\nmerits of reference-based and reference-free metrics and explicitly models\nlexical divergence. Experimental results demonstrate that ParaScore\nsignificantly outperforms existing metrics.\n",
                "链接": "https://arxiv.org/abs/2202.08479"
            },
            {
                "文章ID": "51762",
                "标题": "a survey on GPT-3",
                "作者": " Mingyu Zong,  Bhaskar Krishnamachari",
                "发布日期": "2022-12-05",
                "摘要": "  This paper provides an introductory survey to GPT-3. We cover some of the\nhistorical development behind this technology, some of the key features of\nGPT-3, and discuss the machine learning model and the datasets used. We survey\nboth academic and commercial efforts applying GPT-3 in diverse domains such as\ndeveloping conversational AI chatbots, software development, creative work,\ndomain knowledge, and business productivity. We discuss some of the challenges\nthat GPT-3 faces such as the problems of training complexity, bias, and\nhallucination/incorrect answers. We also discuss the future research\nopportunities in this area.\n",
                "链接": "https://arxiv.org/abs/2212.00857"
            },
            {
                "文章ID": "27574",
                "标题": "On the Effect of Ranking Axioms on IR Evaluation Metrics",
                "作者": " Fernando Giner",
                "发布日期": "2022-07-05",
                "摘要": "  The study of IR evaluation metrics through axiomatic analysis enables a\nbetter understanding of their numerical properties. Some works have modelled\nthe effectiveness of retrieval metrics with axioms that capture desirable\nproperties on the set of rankings of documents. This paper formally explores\nthe effect of these ranking axioms on the numerical values of some IR\nevaluation metrics. It focuses on the set of ranked lists of documents with\nmultigrade relevance. The possible orderings in this set are derived from three\ncommonly accepted ranking axioms on retrieval metrics; then, they are\nclassified by their latticial properties. When relevant documents are\nprioritised, a subset of document rankings are identified: the join-irreducible\nelements, which have some resemblance to the concept of basis in vector space.\nIt is possible to compute the precision, recall, RBP or DCG values of any\nranking from their values in the join-irreducible elements. However this is not\nthe case when the swapping of documents is considered.\n",
                "链接": "https://arxiv.org/abs/2207.01201"
            },
            {
                "文章ID": "102940",
                "标题": "An Evaluation of GPT-4 on the ETHICS Dataset",
                "作者": " Sergey Rodionov,  Zarathustra Amadeus Goertzel,  Ben Goertzel",
                "发布日期": "2023-09-20",
                "摘要": "  This report summarizes a short study of the performance of GPT-4 on the\nETHICS dataset. The ETHICS dataset consists of five sub-datasets covering\ndifferent fields of ethics: Justice, Deontology, Virtue Ethics, Utilitarianism,\nand Commonsense Ethics. The moral judgments were curated so as to have a high\ndegree of agreement with the aim of representing shared human values rather\nthan moral dilemmas. GPT-4's performance is much better than that of previous\nmodels and suggests that learning to work with common human values is not the\nhard problem for AI ethics.\n",
                "链接": "https://arxiv.org/abs/2309.10492"
            }
        ]
    },
    {
        "question": {
            "question": "查找使用GPT4v完成多模态智能体的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "102940",
                "标题": "An Evaluation of GPT-4 on the ETHICS Dataset",
                "作者": " Sergey Rodionov,  Zarathustra Amadeus Goertzel,  Ben Goertzel",
                "发布日期": "2023-09-20",
                "摘要": "  This report summarizes a short study of the performance of GPT-4 on the\nETHICS dataset. The ETHICS dataset consists of five sub-datasets covering\ndifferent fields of ethics: Justice, Deontology, Virtue Ethics, Utilitarianism,\nand Commonsense Ethics. The moral judgments were curated so as to have a high\ndegree of agreement with the aim of representing shared human values rather\nthan moral dilemmas. GPT-4's performance is much better than that of previous\nmodels and suggests that learning to work with common human values is not the\nhard problem for AI ethics.\n",
                "链接": "https://arxiv.org/abs/2309.10492"
            },
            {
                "文章ID": "112047",
                "标题": "Using GPT-4 to Augment Unbalanced Data for Automatic Scoring",
                "作者": " Luyang Fang,  Gyeong-Geon Lee,  Xiaoming Zhai",
                "发布日期": "2023-11-21",
                "摘要": "  Machine learning-based automatic scoring can be challenging if students'\nresponses are unbalanced across scoring categories, as it introduces\nuncertainty in the machine training process. To meet this challenge, we\nintroduce a novel text data augmentation framework using GPT-4, a generative\nlarge language model, specifically tailored for unbalanced datasets in\nautomatic scoring. Our experimental dataset comprised student-written responses\nto two science items. We crafted prompts for GPT-4 to generate responses\nresembling student-written answers, particularly for the minority scoring\nclasses, to augment the data. We then finetuned DistillBERT for automatic\nscoring based on the augmented and original datasets. Model performance was\nassessed using accuracy, precision, recall, and F1 score. We incorporate varied\namounts of augmented data to examine scoring performance, and our findings\nrevealed remarkedly improved model performance. The average maximum increase\nobserved across two items is: 3.5% for accuracy, 30.6% for precision, 21.1% for\nrecall, and 24.2% for F1 score. Notably, using just 5% of the augmented data\nled to substantial improvements: 2.6%, 29.2%, 15.1%, and 19.6%. Interestingly,\nthe extent of improvement varied depending on specific datasets. Moreover, we\nfound that a varying amount of augmented data (5%-40%) was needed to obtain a\nstable improvement. We also compare models trained with GPT-4 augmented data\nand those trained with additional student-written responses. The findings\nindicate that former ones match or even exceed the performance of the latter.\nSpecifically, there is an average difference of 1.7%, 1.9%, 11.0%, and 7.8% for\nfour metrics separately. This research underscores the potential and\neffectiveness of data augmentation techniques utilizing GPT-4 in addressing\nunbalanced datasets within automated assessment.\n",
                "链接": "https://arxiv.org/abs/2310.18365"
            },
            {
                "文章ID": "70587",
                "标题": "GPT-4 to GPT-3.5: 'Hold My Scalpel' -- A Look at the Competency of\n  OpenAI's GPT on the Plastic Surgery In-Service Training Exam",
                "作者": " Jonathan D. Freedman,  Ian A. Nappier",
                "发布日期": "2023-04-05",
                "摘要": "  The Plastic Surgery In-Service Training Exam (PSITE) is an important\nindicator of resident proficiency and serves as a useful benchmark for\nevaluating OpenAI's GPT. Unlike many of the simulated tests or practice\nquestions shown in the GPT-4 Technical Paper, the multiple-choice questions\nevaluated here are authentic PSITE questions. These questions offer realistic\nclinical vignettes that a plastic surgeon commonly encounters in practice and\nscores highly correlate with passing the written boards required to become a\nBoard Certified Plastic Surgeon. Our evaluation shows dramatic improvement of\nGPT-4 (without vision) over GPT-3.5 with both the 2022 and 2021 exams\nrespectively increasing the score from 8th to 88th percentile and 3rd to 99th\npercentile. The final results of the 2023 PSITE are set to be released on April\n11, 2023, and this is an exciting moment to continue our research with a fresh\nexam. Our evaluation pipeline is ready for the moment that the exam is released\nso long as we have access via OpenAI to the GPT-4 API. With multimodal input,\nwe may achieve superhuman performance on the 2023.\n",
                "链接": "https://arxiv.org/abs/2304.01503"
            },
            {
                "文章ID": "112767",
                "标题": "Does GPT-4 Pass the Turing Test?",
                "作者": " Cameron Jones,  Benjamin Bergen",
                "发布日期": "2023-11-01",
                "摘要": "  We evaluated GPT-4 in a public online Turing Test. The best-performing GPT-4\nprompt passed in 41% of games, outperforming baselines set by ELIZA (27%) and\nGPT-3.5 (14%), but falling short of chance and the baseline set by human\nparticipants (63%). Participants' decisions were based mainly on linguistic\nstyle (35%) and socio-emotional traits (27%), supporting the idea that\nintelligence is not sufficient to pass the Turing Test. Participants'\ndemographics, including education and familiarity with LLMs, did not predict\ndetection rate, suggesting that even those who understand systems deeply and\ninteract with them frequently may be susceptible to deception. Despite known\nlimitations as a test of intelligence, we argue that the Turing Test continues\nto be relevant as an assessment of naturalistic communication and deception. AI\nmodels with the ability to masquerade as humans could have widespread societal\nconsequences, and we analyse the effectiveness of different strategies and\ncriteria for judging humanlikeness.\n",
                "链接": "https://arxiv.org/abs/2310.20216"
            },
            {
                "文章ID": "115334",
                "标题": "The Impact of Large Language Models on Scientific Discovery: a\n  Preliminary Study using GPT-4",
                "作者": " Microsoft Research AI4Science,  Microsoft Azure Quantum",
                "发布日期": "2023-12-11",
                "摘要": "  In recent years, groundbreaking advancements in natural language processing\nhave culminated in the emergence of powerful large language models (LLMs),\nwhich have showcased remarkable capabilities across a vast array of domains,\nincluding the understanding, generation, and translation of natural language,\nand even tasks that extend beyond language processing. In this report, we delve\ninto the performance of LLMs within the context of scientific discovery,\nfocusing on GPT-4, the state-of-the-art language model. Our investigation spans\na diverse range of scientific areas encompassing drug discovery, biology,\ncomputational chemistry (density functional theory (DFT) and molecular dynamics\n(MD)), materials design, and partial differential equations (PDE). Evaluating\nGPT-4 on scientific tasks is crucial for uncovering its potential across\nvarious research domains, validating its domain-specific expertise,\naccelerating scientific progress, optimizing resource allocation, guiding\nfuture model development, and fostering interdisciplinary research. Our\nexploration methodology primarily consists of expert-driven case assessments,\nwhich offer qualitative insights into the model's comprehension of intricate\nscientific concepts and relationships, and occasionally benchmark testing,\nwhich quantitatively evaluates the model's capacity to solve well-defined\ndomain-specific problems. Our preliminary exploration indicates that GPT-4\nexhibits promising potential for a variety of scientific applications,\ndemonstrating its aptitude for handling complex problem-solving and knowledge\nintegration tasks. Broadly speaking, we evaluate GPT-4's knowledge base,\nscientific understanding, scientific numerical calculation abilities, and\nvarious scientific prediction capabilities.\n",
                "链接": "https://arxiv.org/abs/2311.07361"
            },
            {
                "文章ID": "66852",
                "标题": "GPT-4 Technical Report",
                "作者": "Rai   OpenAI, Rai   :, Rai  Josh Achiam, Rai  Steven Adler, Rai  Sandhini Agarwal, Rai  Lama Ahmad, Rai  Ilge Akkaya, Rai  Florencia Leoni Aleman, Rai  Diogo Almeida, Rai  Janko Altenschmidt, Rai  Sam Altman, Rai  Shyamal Anadkat, Rai  Red Avila, Rai  Igor Babuschkin, Rai  Suchir Balaji, Rai  Valerie Balcom, Rai  Paul Baltescu, Rai  Haiming Bao, Rai  Mo Bavarian, Rai  Jeff Belgum, Rai  Irwan Bello, Rai  Jake Berdine, Rai  Gabriel Bernadett-Shapiro, Rai  Christopher Berner, Rai  Lenny Bogdonoff, Rai  Oleg Boiko, Rai  Madelaine Boyd, Rai  Anna-Luisa Brakman, Rai  Greg Brockman, Rai  Tim Brooks, Rai  Miles Brundage, Rai  Kevin Button, Rai  Trevor Cai, Rai  Rosie Campbell, Rai  Andrew Cann, Rai  Brittany Carey, Rai  Chelsea Carlson, Rai  Rory Carmichael, Rai  Brooke Chan, Rai  Che Chang, Rai  Fotis Chantzis, Rai  Derek Chen, Rai  Sully Chen, Rai  Ruby Chen, Rai  Jason Chen, Rai  Mark Chen, Rai  Ben Chess, Rai  Chester Cho, Rai  Casey Chu, Rai  Hyung Won Chung, Rai  Dave Cummings, Rai  Jeremiah Currier, Rai  Yunxing Dai, Rai  Cory Decareaux, Rai  Thomas Degry, Rai  Noah Deutsch, Rai  Damien Deville, Rai  Arka Dhar, Rai  David Dohan, Rai  Steve Dowling, Rai  Sheila Dunning, Rai  Adrien Ecoffet, Rai  Atty Eleti, Rai  Tyna Eloundou, Rai  David Farhi, Rai  Liam Fedus, Rai  Niko Felix, Rai  Simón Posada Fishman, Rai  Juston Forte, Rai  Isabella Fulford, Rai  Leo Gao, Rai  Elie Georges, Rai  Christian Gibson, Rai  Vik Goel, Rai  Tarun Gogineni, Rai  Gabriel Goh, Rai  Rapha Gontijo-Lopes, Rai  Jonathan Gordon, Rai  Morgan Grafstein, Rai  Scott Gray, Rai  Ryan Greene, Rai  Joshua Gross, Rai  Shixiang Shane Gu, Rai  Yufei Guo, Rai  Chris Hallacy, Rai  Jesse Han, Rai  Jeff Harris, Rai  Yuchen He, Rai  Mike Heaton, Rai  Johannes Heidecke, Rai  Chris Hesse, Rai  Alan Hickey, Rai  Wade Hickey, Rai  Peter Hoeschele, Rai  Brandon Houghton, Rai  Kenny Hsu, Rai  Shengli Hu, Rai  Xin Hu, Rai  Joost Huizinga, Rai  Shantanu Jain, Rai  Shawn Jain, Rai  Joanne Jang, Rai  Angela Jiang, Rai  Roger Jiang, Rai  Haozhun Jin, Rai  Denny Jin, Rai  Shino Jomoto, Rai  Billie Jonn, Rai  Heewoo Jun, Rai  Tomer Kaftan, Rai  Łukasz Kaiser, Rai  Ali Kamali, Rai  Ingmar Kanitscheider, Rai  Nitish Shirish Keskar, Rai  Tabarak Khan, Rai  Logan Kilpatrick, Rai  Jong Wook Kim, Rai  Christina Kim, Rai  Yongjik Kim, Rai  Hendrik Kirchner, Rai  Jamie Kiros, Rai  Matt Knight, Rai  Daniel Kokotajlo, Rai  Łukasz Kondraciuk, Rai  Andrew Kondrich, Rai  Aris Konstantinidis, Rai  Kyle Kosic, Rai  Gretchen Krueger, Rai  Vishal Kuo, Rai  Michael Lampe, Rai  Ikai Lan, Rai  Teddy Lee, Rai  Jan Leike, Rai  Jade Leung, Rai  Daniel Levy, Rai  Chak Ming Li, Rai  Rachel Lim, Rai  Molly Lin, Rai  Stephanie Lin, Rai  Mateusz Litwin, Rai  Theresa Lopez, Rai  Ryan Lowe, Rai  Patricia Lue, Rai  Anna Makanju, Rai  Kim Malfacini, Rai  Sam Manning, Rai  Todor Markov, Rai  Yaniv Markovski, Rai  Bianca Martin, Rai  Katie Mayer, Rai  Andrew Mayne, Rai  Bob McGrew, Rai  Scott Mayer McKinney, Rai  Christine McLeavey, Rai  Paul McMillan, Rai  Jake McNeil, Rai  David Medina, Rai  Aalok Mehta, Rai  Jacob Menick, Rai  Luke Metz, Rai  Andrey Mishchenko, Rai  Pamela Mishkin, Rai  Vinnie Monaco, Rai  Evan Morikawa, Rai  Daniel Mossing, Rai  Tong Mu, Rai  Mira Murati, Rai  Oleg Murk, Rai  David Mély, Rai  Ashvin Nair, Rai  Reiichiro Nakano, Rai  Rajeev Nayak, Rai  Arvind Neelakantan, Rai  Richard Ngo, Rai  Hyeonwoo Noh, Rai  Long Ouyang, Rai  Cullen O'Keefe, Rai  Jakub Pachocki, Rai  Alex Paino, Rai  Joe Palermo, Rai  Ashley Pantuliano, Rai  Giambattista Parascandolo, Rai  Joel Parish, Rai  Emy Parparita, Rai  Alex Passos, Rai  Mikhail Pavlov, Rai  Andrew Peng, Rai  Adam Perelman, Rai  Filipe de Avila Belbute Peres, Rai  Michael Petrov, Rai  Henrique Ponde de Oliveira Pinto, Rai   Michael,   Pokorny,  Michelle Pokrass,  Vitchyr Pong,  Tolly Powell,  Alethea Power,  Boris Power,  Elizabeth Proehl,  Raul Puri,  Alec Radford,  Jack Rae,  Aditya Ramesh,  Cameron Raymond,  Francis Real,  Kendra Rimbach,  Carl Ross,  Bob Rotsted,  Henri Roussez,  Nick Ryder,  Mario Saltarelli,  Ted Sanders,  Shibani Santurkar,  Girish Sastry,  Heather Schmidt,  David Schnurr,  John Schulman,  Daniel Selsam,  Kyla Sheppard,  Toki Sherbakov,  Jessica Shieh,  Sarah Shoker,  Pranav Shyam,  Szymon Sidor,  Eric Sigler,  Maddie Simens,  Jordan Sitkin,  Katarina Slama,  Ian Sohl,  Benjamin Sokolowsky,  Yang Song,  Natalie Staudacher,  Felipe Petroski Such,  Natalie Summers,  Ilya Sutskever,  Jie Tang,  Nikolas Tezak,  Madeleine Thompson,  Phil Tillet,  Amin Tootoonchian,  Elizabeth Tseng,  Preston Tuggle,  Nick Turley,  Jerry Tworek,  Juan Felipe Cerón Uribe,  Andrea Vallone,  Arun Vijayvergiya,  Chelsea Voss,  Carroll Wainwright,  Justin Jay Wang,  Alvin Wang,  Ben Wang,  Jonathan Ward,  Jason Wei,  CJ Weinmann,  Akila Welihinda,  Peter Welinder,  Jiayi Weng,  Lilian Weng,  Matt Wiethoff,  Dave Willner,  Clemens Winter,  Samuel Wolrich,  Hannah Wong,  Lauren Workman,  Sherwin Wu,  Jeff Wu,  Michael Wu,  Kai Xiao,  Tao Xu,  Sarah Yoo,  Kevin Yu,  Qiming Yuan,  Wojciech Zaremba,  Rowan Zellers,  Chong Zhang,  Marvin Zhang,  Shengjia Zhao,  Tianhao Zheng,  Juntang Zhuang,  William Zhuk,  Barret Zoph",
                "发布日期": "2023-12-20",
                "摘要": "  We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.\n",
                "链接": "https://arxiv.org/abs/2303.08774"
            },
            {
                "文章ID": "95177",
                "标题": "GPT-4 Can't Reason",
                "作者": " Konstantine Arkoudas",
                "发布日期": "2023-08-11",
                "摘要": "  GPT-4 was released in March 2023 to wide acclaim, marking a very substantial\nimprovement across the board over GPT-3.5 (OpenAI's previously best model,\nwhich had powered the initial release of ChatGPT). However, despite the\ngenuinely impressive improvement, there are good reasons to be highly skeptical\nof GPT-4's ability to reason. This position paper discusses the nature of\nreasoning; criticizes the current formulation of reasoning problems in the NLP\ncommunity, as well as the way in which LLM reasoning performance is currently\nevaluated; introduces a small collection of 21 diverse reasoning problems; and\nperforms a detailed qualitative evaluation of GPT-4's performance on those\nproblems. Based on this analysis, the paper concludes that, despite its\noccasional flashes of analytical brilliance, GPT-4 at present is utterly\nincapable of reasoning.\n",
                "链接": "https://arxiv.org/abs/2308.03762"
            },
            {
                "文章ID": "68454",
                "标题": "Capabilities of GPT-4 on Medical Challenge Problems",
                "作者": " Harsha Nori,  Nicholas King,  Scott Mayer McKinney,  Dean Carignan,  Eric Horvitz",
                "发布日期": "2023-04-13",
                "摘要": "  Large language models (LLMs) have demonstrated remarkable capabilities in\nnatural language understanding and generation across various domains, including\nmedicine. We present a comprehensive evaluation of GPT-4, a state-of-the-art\nLLM, on medical competency examinations and benchmark datasets. GPT-4 is a\ngeneral-purpose model that is not specialized for medical problems through\ntraining or engineered to solve clinical tasks. Our analysis covers two sets of\nofficial practice materials for the USMLE, a three-step examination program\nused to assess clinical competency and grant licensure in the United States. We\nalso evaluate performance on the MultiMedQA suite of benchmark datasets. Beyond\nmeasuring model performance, experiments were conducted to investigate the\ninfluence of test questions containing both text and images on model\nperformance, probe for memorization of content during training, and study\nprobability calibration, which is of critical importance in high-stakes\napplications like medicine. Our results show that GPT-4, without any\nspecialized prompt crafting, exceeds the passing score on USMLE by over 20\npoints and outperforms earlier general-purpose models (GPT-3.5) as well as\nmodels specifically fine-tuned on medical knowledge (Med-PaLM, a prompt-tuned\nversion of Flan-PaLM 540B). In addition, GPT-4 is significantly better\ncalibrated than GPT-3.5, demonstrating a much-improved ability to predict the\nlikelihood that its answers are correct. We also explore the behavior of the\nmodel qualitatively through a case study that shows the ability of GPT-4 to\nexplain medical reasoning, personalize explanations to students, and\ninteractively craft new counterfactual scenarios around a medical case.\nImplications of the findings are discussed for potential uses of GPT-4 in\nmedical education, assessment, and clinical practice, with appropriate\nattention to challenges of accuracy and safety.\n",
                "链接": "https://arxiv.org/abs/2303.13375"
            },
            {
                "文章ID": "81941",
                "标题": "Controllable Text-to-Image Generation with GPT-4",
                "作者": " Tianjun Zhang,  Yi Zhang,  Vibhav Vineet,  Neel Joshi,  Xin Wang",
                "发布日期": "2023-05-31",
                "摘要": "  Current text-to-image generation models often struggle to follow textual\ninstructions, especially the ones requiring spatial reasoning. On the other\nhand, Large Language Models (LLMs), such as GPT-4, have shown remarkable\nprecision in generating code snippets for sketching out text inputs\ngraphically, e.g., via TikZ. In this work, we introduce Control-GPT to guide\nthe diffusion-based text-to-image pipelines with programmatic sketches\ngenerated by GPT-4, enhancing their abilities for instruction following.\nControl-GPT works by querying GPT-4 to write TikZ code, and the generated\nsketches are used as references alongside the text instructions for diffusion\nmodels (e.g., ControlNet) to generate photo-realistic images. One major\nchallenge to training our pipeline is the lack of a dataset containing aligned\ntext, images, and sketches. We address the issue by converting instance masks\nin existing datasets into polygons to mimic the sketches used at test time. As\na result, Control-GPT greatly boosts the controllability of image generation.\nIt establishes a new state-of-art on the spatial arrangement and object\npositioning generation and enhances users' control of object positions, sizes,\netc., nearly doubling the accuracy of prior models. Our work, as a first\nattempt, shows the potential for employing LLMs to enhance the performance in\ncomputer vision tasks.\n",
                "链接": "https://arxiv.org/abs/2305.18583"
            },
            {
                "文章ID": "69729",
                "标题": "Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission\n  Exams",
                "作者": " Desnes Nunes,  Ricardo Primi,  Ramon Pires,  Roberto Lotufo,  Rodrigo Nogueira",
                "发布日期": "2023-03-31",
                "摘要": "  The present study aims to explore the capabilities of Language Models (LMs)\nin tackling high-stakes multiple-choice tests, represented here by the Exame\nNacional do Ensino M\\'edio (ENEM), a multidisciplinary entrance examination\nwidely adopted by Brazilian universities. This exam poses challenging tasks for\nLMs, since its questions may span into multiple fields of knowledge, requiring\nunderstanding of information from diverse domains. For instance, a question may\nrequire comprehension of both statistics and biology to be solved. This work\nanalyzed responses generated by GPT-3.5 and GPT-4 models for questions\npresented in the 2009-2017 exams, as well as for questions of the 2022 exam,\nwhich were made public after the training of the models was completed.\nFurthermore, different prompt strategies were tested, including the use of\nChain-of-Thought (CoT) prompts to generate explanations for answers. On the\n2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy\nof 87%, largely surpassing GPT-3.5 by 11 points. The code and data used on\nexperiments are available at https://github.com/piresramon/gpt-4-enem.\n",
                "链接": "https://arxiv.org/abs/2303.17003"
            }
        ]
    },
    {
        "question": {
            "question": "使用LLM进行蛋白质结构/功能/性质预测的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "109510",
                "标题": "Protein 3D Graph Structure Learning for Robust Structure-based Protein\n  Property Prediction",
                "作者": " Yufei Huang,  Siyuan Li,  Jin Su,  Lirong Wu,  Odin Zhang,  Haitao Lin,  Jingqi Qi,  Zihan Liu,  Zhangyang Gao,  Yuyang Liu,  Jiangbin Zheng,  Stan. ZQ. Li",
                "发布日期": "2023-10-20",
                "摘要": "  Protein structure-based property prediction has emerged as a promising\napproach for various biological tasks, such as protein function prediction and\nsub-cellular location estimation. The existing methods highly rely on\nexperimental protein structure data and fail in scenarios where these data are\nunavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were\nutilized as alternatives. However, we observed that current practices, which\nsimply employ accurately predicted structures during inference, suffer from\nnotable degradation in prediction accuracy. While similar phenomena have been\nextensively studied in general fields (e.g., Computer Vision) as model\nrobustness, their impact on protein property prediction remains unexplored. In\nthis paper, we first investigate the reason behind the performance decrease\nwhen utilizing predicted structures, attributing it to the structure embedding\nbias from the perspective of structure representation learning. To study this\nproblem, we identify a Protein 3D Graph Structure Learning Problem for Robust\nProtein Property Prediction (PGSL-RP3), collect benchmark datasets, and present\na protein Structure embedding Alignment Optimization framework (SAO) to\nmitigate the problem of structure embedding bias between the predicted and\nexperimental protein structures. Extensive experiments have shown that our\nframework is model-agnostic and effective in improving the property prediction\nof both predicted structures and experimental structures. The benchmark\ndatasets and codes will be released to benefit the community.\n",
                "链接": "https://arxiv.org/abs/2310.11466"
            },
            {
                "文章ID": "34638",
                "标题": "Secondary Protein Structure Prediction Using Neural Networks",
                "作者": " Sidharth Malhotra,  Robin Walters",
                "发布日期": "2022-08-25",
                "摘要": "  In this paper we experiment with using neural network structures to predict a\nprotein's secondary structure ({\\alpha} helix positions) from only its primary\nstructure (amino acid sequence). We implement a fully connected neural network\n(FCNN) and preform three experiments using that FCNN. Firstly, we do a\ncross-species comparison of models trained and tested on mouse and human\ndatasets. Secondly, we test the impact of varying the length of protein\nsequence we input into the model. Thirdly, we compare custom error functions\ndesigned to focus on the center of the input window. At the end of paper we\npropose a alternative, recurrent neural network model which can be applied to\nthe problem.\n",
                "链接": "https://arxiv.org/abs/2208.11248"
            },
            {
                "文章ID": "26245",
                "标题": "PSP: Million-level Protein Sequence Dataset for Protein Structure\n  Prediction",
                "作者": " Sirui Liu,  Jun Zhang,  Haotian Chu,  Min Wang,  Boxin Xue,  Ningxi Ni,  Jialiang Yu,  Yuhao Xie,  Zhenyu Chen,  Mengyun Chen,  Yuan Liu,  Piya Patra,  Fan Xu,  Jie Chen,  Zidong Wang,  Lijiang Yang,  Fan Yu,  Lei Chen,  Yi Qin Gao",
                "发布日期": "2022-06-27",
                "摘要": "  Proteins are essential component of human life and their structures are\nimportant for function and mechanism analysis. Recent work has shown the\npotential of AI-driven methods for protein structure prediction. However, the\ndevelopment of new models is restricted by the lack of dataset and benchmark\ntraining procedure. To the best of our knowledge, the existing open source\ndatasets are far less to satisfy the needs of modern protein sequence-structure\nrelated research. To solve this problem, we present the first million-level\nprotein structure prediction dataset with high coverage and diversity, named as\nPSP. This dataset consists of 570k true structure sequences (10TB) and 745k\ncomplementary distillation sequences (15TB). We provide in addition the\nbenchmark training procedure for SOTA protein structure prediction model on\nthis dataset. We validate the utility of this dataset for training by\nparticipating CAMEO contest in which our model won the first place. We hope our\nPSP dataset together with the training benchmark can enable a broader community\nof AI/biology researchers for AI-driven protein related research.\n",
                "链接": "https://arxiv.org/abs/2206.12240"
            },
            {
                "文章ID": "31301",
                "标题": "HelixFold-Single: MSA-free Protein Structure Prediction by Using Protein\n  Language Model as an Alternative",
                "作者": " Xiaomin Fang,  Fan Wang,  Lihang Liu,  Jingzhou He,  Dayong Lin,  Yingfei Xiang,  Xiaonan Zhang,  Hua Wu,  Hui Li,  Le Song",
                "发布日期": "2023-10-19",
                "摘要": "  AI-based protein structure prediction pipelines, such as AlphaFold2, have\nachieved near-experimental accuracy. These advanced pipelines mainly rely on\nMultiple Sequence Alignments (MSAs) as inputs to learn the co-evolution\ninformation from the homologous sequences. Nonetheless, searching MSAs from\nprotein databases is time-consuming, usually taking dozens of minutes.\nConsequently, we attempt to explore the limits of fast protein structure\nprediction by using only primary sequences of proteins. HelixFold-Single is\nproposed to combine a large-scale protein language model with the superior\ngeometric learning capability of AlphaFold2. Our proposed method,\nHelixFold-Single, first pre-trains a large-scale protein language model (PLM)\nwith thousands of millions of primary sequences utilizing the self-supervised\nlearning paradigm, which will be used as an alternative to MSAs for learning\nthe co-evolution information. Then, by combining the pre-trained PLM and the\nessential components of AlphaFold2, we obtain an end-to-end differentiable\nmodel to predict the 3D coordinates of atoms from only the primary sequence.\nHelixFold-Single is validated in datasets CASP14 and CAMEO, achieving\ncompetitive accuracy with the MSA-based methods on the targets with large\nhomologous families. Furthermore, HelixFold-Single consumes much less time than\nthe mainstream pipelines for protein structure prediction, demonstrating its\npotential in tasks requiring many predictions. The code of HelixFold-Single is\navailable at\nhttps://github.com/PaddlePaddle/PaddleHelix/tree/dev/apps/protein_folding/helixfold-single,\nand we also provide stable web services on\nhttps://paddlehelix.baidu.com/app/drug/protein-single/forecast.\n",
                "链接": "https://arxiv.org/abs/2207.13921"
            },
            {
                "文章ID": "8982",
                "标题": "Structured Multi-task Learning for Molecular Property Prediction",
                "作者": " Shengchao Liu,  Meng Qu,  Zuobai Zhang,  Huiyu Cai,  Jian Tang",
                "发布日期": "2022-10-07",
                "摘要": "  Multi-task learning for molecular property prediction is becoming\nincreasingly important in drug discovery. However, in contrast to other\ndomains, the performance of multi-task learning in drug discovery is still not\nsatisfying as the number of labeled data for each task is too limited, which\ncalls for additional data to complement the data scarcity. In this paper, we\nstudy multi-task learning for molecular property prediction in a novel setting,\nwhere a relation graph between tasks is available. We first construct a dataset\n(ChEMBL-STRING) including around 400 tasks as well as a task relation graph.\nThen to better utilize such relation graph, we propose a method called SGNN-EBM\nto systematically investigate the structured task modeling from two\nperspectives. (1) In the \\emph{latent} space, we model the task representations\nby applying a state graph neural network (SGNN) on the relation graph. (2) In\nthe \\emph{output} space, we employ structured prediction with the energy-based\nmodel (EBM), which can be efficiently trained through noise-contrastive\nestimation (NCE) approach. Empirical results justify the effectiveness of\nSGNN-EBM. Code is available on https://github.com/chao1224/SGNN-EBM.\n",
                "链接": "https://arxiv.org/abs/2203.04695"
            },
            {
                "文章ID": "48975",
                "标题": "A Review of Deep Learning Techniques for Protein Function Prediction",
                "作者": " Divyanshu Aggarwal,  Yasha Hasija",
                "发布日期": "2022-11-18",
                "摘要": "  Deep Learning and big data have shown tremendous success in bioinformatics\nand computational biology in recent years; artificial intelligence methods have\nalso significantly contributed in the task of protein function classification.\nThis review paper analyzes the recent developments in approaches for the task\nof predicting protein function using deep learning. We explain the importance\nof determining the protein function and why automating the following task is\ncrucial. Then, after reviewing the widely used deep learning techniques for\nthis task, we continue our review and highlight the emergence of the modern\nState of The Art (SOTA) deep learning models which have achieved groundbreaking\nresults in the field of computer vision, natural language processing and\nmulti-modal learning in the last few years. We hope that this review will\nprovide a broad view of the current role and advances of deep learning in\nbiological sciences, especially in predicting protein function tasks and\nencourage new researchers to contribute to this area.\n",
                "链接": "https://arxiv.org/abs/2211.09705"
            },
            {
                "文章ID": "125227",
                "标题": "Molecular Property Prediction Based on Graph Structure Learning",
                "作者": " Bangyi Zhao,  Weixia Xu,  Jihong Guan,  Shuigeng Zhou",
                "发布日期": "2023-12-29",
                "摘要": "  Molecular property prediction (MPP) is a fundamental but challenging task in\nthe computer-aided drug discovery process. More and more recent works employ\ndifferent graph-based models for MPP, which have made considerable progress in\nimproving prediction performance. However, current models often ignore\nrelationships between molecules, which could be also helpful for MPP. For this\nsake, in this paper we propose a graph structure learning (GSL) based MPP\napproach, called GSL-MPP. Specifically, we first apply graph neural network\n(GNN) over molecular graphs to extract molecular representations. Then, with\nmolecular fingerprints, we construct a molecular similarity graph (MSG).\nFollowing that, we conduct graph structure learning on the MSG (i.e.,\nmolecule-level graph structure learning) to get the final molecular embeddings,\nwhich are the results of fusing both GNN encoded molecular representations and\nthe relationships among molecules, i.e., combining both intra-molecule and\ninter-molecule information. Finally, we use these molecular embeddings to\nperform MPP. Extensive experiments on seven various benchmark datasets show\nthat our method could achieve state-of-the-art performance in most cases,\nespecially on classification tasks. Further visualization studies also\ndemonstrate the good molecular representations of our method.\n",
                "链接": "https://arxiv.org/abs/2312.16855"
            },
            {
                "文章ID": "84482",
                "标题": "Multi-level Protein Representation Learning for Blind Mutational Effect\n  Prediction",
                "作者": " Yang Tan,  Bingxin Zhou,  Yuanhong Jiang,  Yu Guang Wang,  Liang Hong",
                "发布日期": "2023-06-09",
                "摘要": "  Directed evolution plays an indispensable role in protein engineering that\nrevises existing protein sequences to attain new or enhanced functions.\nAccurately predicting the effects of protein variants necessitates an in-depth\nunderstanding of protein structure and function. Although large self-supervised\nlanguage models have demonstrated remarkable performance in zero-shot inference\nusing only protein sequences, these models inherently do not interpret the\nspatial characteristics of protein structures, which are crucial for\ncomprehending protein folding stability and internal molecular interactions.\nThis paper introduces a novel pre-training framework that cascades sequential\nand geometric analyzers for protein primary and tertiary structures. It guides\nmutational directions toward desired traits by simulating natural selection on\nwild-type proteins and evaluates the effects of variants based on their fitness\nto perform the function. We assess the proposed approach using a public\ndatabase and two new databases for a variety of variant effect prediction\ntasks, which encompass a diverse set of proteins and assays from different\ntaxa. The prediction results achieve state-of-the-art performance over other\nzero-shot learning methods for both single-site mutations and deep mutations.\n",
                "链接": "https://arxiv.org/abs/2306.04899"
            },
            {
                "文章ID": "70802",
                "标题": "EigenFold: Generative Protein Structure Prediction with Diffusion Models",
                "作者": " Bowen Jing,  Ezra Erives,  Peter Pao-Huang,  Gabriele Corso,  Bonnie Berger,  Tommi Jaakkola",
                "发布日期": "2023-04-06",
                "摘要": "  Protein structure prediction has reached revolutionary levels of accuracy on\nsingle structures, yet distributional modeling paradigms are needed to capture\nthe conformational ensembles and flexibility that underlie biological function.\nTowards this goal, we develop EigenFold, a diffusion generative modeling\nframework for sampling a distribution of structures from a given protein\nsequence. We define a diffusion process that models the structure as a system\nof harmonic oscillators and which naturally induces a cascading-resolution\ngenerative process along the eigenmodes of the system. On recent CAMEO targets,\nEigenFold achieves a median TMScore of 0.84, while providing a more\ncomprehensive picture of model uncertainty via the ensemble of sampled\nstructures relative to existing methods. We then assess EigenFold's ability to\nmodel and predict conformational heterogeneity for fold-switching proteins and\nligand-induced conformational change. Code is available at\nhttps://github.com/bjing2016/EigenFold.\n",
                "链接": "https://arxiv.org/abs/2304.02198"
            },
            {
                "文章ID": "32889",
                "标题": "Semi-Supervised Junction Tree Variational Autoencoder for Molecular\n  Property Prediction",
                "作者": " Atia Hamidizadeh,  Tony Shen,  Martin Ester",
                "发布日期": "2023-01-18",
                "摘要": "  Molecular Representation Learning is essential to solving many drug discovery\nand computational chemistry problems. It is a challenging problem due to the\ncomplex structure of molecules and the vast chemical space. Graph\nrepresentations of molecules are more expressive than traditional\nrepresentations, such as molecular fingerprints. Therefore, they can improve\nthe performance of machine learning models. We propose SeMole, a method that\naugments the Junction Tree Variational Autoencoders, a state-of-the-art\ngenerative model for molecular graphs, with semi-supervised learning. SeMole\naims to improve the accuracy of molecular property prediction when having\nlimited labeled data by exploiting unlabeled data. We enforce that the model\ngenerates molecular graphs conditioned on target properties by incorporating\nthe property into the latent representation. We propose an additional\npre-training phase to improve the training process for our semi-supervised\ngenerative model. We perform an experimental evaluation on the ZINC dataset\nusing three different molecular properties and demonstrate the benefits of\nsemi-supervision.\n",
                "链接": "https://arxiv.org/abs/2208.05119"
            }
        ]
    },
    {
        "question": {
            "question": "帮我查找一下2018年以后使用强化学习做机器翻译任务的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "102364",
                "标题": "Projected Task-Specific Layers for Multi-Task Reinforcement Learning",
                "作者": " Josselin Somerville Roberts,  Julia Di",
                "发布日期": "2023-09-19",
                "摘要": "  Multi-task reinforcement learning could enable robots to scale across a wide\nvariety of manipulation tasks in homes and workplaces. However, generalizing\nfrom one task to another and mitigating negative task interference still\nremains a challenge. Addressing this challenge by successfully sharing\ninformation across tasks will depend on how well the structure underlying the\ntasks is captured. In this work, we introduce our new architecture, Projected\nTask-Specific Layers (PTSL), that leverages a common policy with dense\ntask-specific corrections through task-specific layers to better express shared\nand variable task information. We then show that our model outperforms the\nstate of the art on the MT10 and MT50 benchmarks of Meta-World consisting of 10\nand 50 goal-conditioned tasks for a Sawyer arm.\n",
                "链接": "https://arxiv.org/abs/2309.08776"
            },
            {
                "文章ID": "75710",
                "标题": "Learning Language-Specific Layers for Multilingual Machine Translation",
                "作者": " Telmo Pessoa Pires,  Robin M. Schmidt,  Yi-Hsiu Liao,  Stephan Peitz",
                "发布日期": "2023-05-05",
                "摘要": "  Multilingual Machine Translation promises to improve translation quality\nbetween non-English languages. This is advantageous for several reasons, namely\nlower latency (no need to translate twice), and reduced error cascades (e.g.,\navoiding losing gender and formality information when translating through\nEnglish). On the downside, adding more languages reduces model capacity per\nlanguage, which is usually countered by increasing the overall model size,\nmaking training harder and inference slower. In this work, we introduce\nLanguage-Specific Transformer Layers (LSLs), which allow us to increase model\ncapacity, while keeping the amount of computation and the number of parameters\nused in the forward pass constant. The key idea is to have some layers of the\nencoder be source or target language-specific, while keeping the remaining\nlayers shared. We study the best way to place these layers using a neural\narchitecture search inspired approach, and achieve an improvement of 1.3 chrF\n(1.5 spBLEU) points over not using LSLs on a separate decoder architecture, and\n1.9 chrF (2.2 spBLEU) on a shared decoder one.\n",
                "链接": "https://arxiv.org/abs/2305.02665"
            },
            {
                "文章ID": "29736",
                "标题": "MAD for Robust Reinforcement Learning in Machine Translation",
                "作者": " Domenic Donato,  Lei Yu,  Wang Ling,  Chris Dyer",
                "发布日期": "2022-07-19",
                "摘要": "  We introduce a new distributed policy gradient algorithm and show that it\noutperforms existing reward-aware training procedures such as REINFORCE,\nminimum risk training (MRT) and proximal policy optimization (PPO) in terms of\ntraining stability and generalization performance when optimizing machine\ntranslation models. Our algorithm, which we call MAD (on account of using the\nmean absolute deviation in the importance weighting calculation), has\ndistributed data generators sampling multiple candidates per source sentence on\nworker nodes, while a central learner updates the policy. MAD depends crucially\non two variance reduction strategies: (1) a conditional reward normalization\nmethod that ensures each source sentence has both positive and negative reward\ntranslation examples and (2) a new robust importance weighting scheme that acts\nas a conditional entropy regularizer. Experiments on a variety of translation\ntasks show that policies learned using the MAD algorithm perform very well when\nusing both greedy decoding and beam search, and that the learned policies are\nsensitive to the specific reward used during training.\n",
                "链接": "https://arxiv.org/abs/2207.08583"
            },
            {
                "文章ID": "99348",
                "标题": "Task-Based MoE for Multitask Multilingual Machine Translation",
                "作者": " Hai Pham,  Young Jin Kim,  Subhabrata Mukherjee,  David P. Woodruff,  Barnabas Poczos,  Hany Hassan Awadalla",
                "发布日期": "2023-10-26",
                "摘要": "  Mixture-of-experts (MoE) architecture has been proven a powerful method for\ndiverse tasks in training deep models in many applications. However, current\nMoE implementations are task agnostic, treating all tokens from different tasks\nin the same manner. In this work, we instead design a novel method that\nincorporates task information into MoE models at different granular levels with\nshared dynamic task-based adapters. Our experiments and analysis show the\nadvantages of our approaches over the dense and canonical MoE models on\nmulti-task multilingual machine translations. With task-specific adapters, our\nmodels can additionally generalize to new tasks efficiently.\n",
                "链接": "https://arxiv.org/abs/2308.15772"
            },
            {
                "文章ID": "40957",
                "标题": "Reinforcement Learning with Large Action Spaces for Neural Machine\n  Translation",
                "作者": " Asaf Yehudai,  Leshem Choshen,  Lior Fox,  Omri Abend",
                "发布日期": "2022-10-07",
                "摘要": "  Applying Reinforcement learning (RL) following maximum likelihood estimation\n(MLE) pre-training is a versatile method for enhancing neural machine\ntranslation (NMT) performance. However, recent work has argued that the gains\nproduced by RL for NMT are mostly due to promoting tokens that have already\nreceived a fairly high probability in pre-training. We hypothesize that the\nlarge action space is a main obstacle to RL's effectiveness in MT, and conduct\ntwo sets of experiments that lend support to our hypothesis. First, we find\nthat reducing the size of the vocabulary improves RL's effectiveness. Second,\nwe find that effectively reducing the dimension of the action space without\nchanging the vocabulary also yields notable improvement as evaluated by BLEU,\nsemantic similarity, and human evaluation. Indeed, by initializing the\nnetwork's final fully connected layer (that maps the network's internal\ndimension to the vocabulary dimension), with a layer that generalizes over\nsimilar actions, we obtain a substantial improvement in RL performance: 1.5\nBLEU points on average.\n",
                "链接": "https://arxiv.org/abs/2210.03053"
            },
            {
                "文章ID": "50762",
                "标题": "Summer: WeChat Neural Machine Translation Systems for the WMT22\n  Biomedical Translation Task",
                "作者": " Ernan Li,  Fandong Meng,  Jie Zhou",
                "发布日期": "2022-11-29",
                "摘要": "  This paper introduces WeChat's participation in WMT 2022 shared biomedical\ntranslation task on Chinese to English. Our systems are based on the\nTransformer, and use several different Transformer structures to improve the\nquality of translation. In our experiments, we employ data filtering, data\ngeneration, several variants of Transformer, fine-tuning and model ensemble.\nOur Chinese$\\to$English system, named Summer, achieves the highest BLEU score\namong all submissions.\n",
                "链接": "https://arxiv.org/abs/2211.15022"
            },
            {
                "文章ID": "44053",
                "标题": "The VolcTrans System for WMT22 Multilingual Machine Translation Task",
                "作者": " Xian Qian,  Kai Hu,  Jiaqiang Wang,  Yifeng Liu,  Xingyuan Pan,  Jun Cao,  Mingxuan Wang",
                "发布日期": "2022-10-24",
                "摘要": "  This report describes our VolcTrans system for the WMT22 shared task on\nlarge-scale multilingual machine translation. We participated in the\nunconstrained track which allows the use of external resources. Our system is a\ntransformerbased multilingual model trained on data from multiple sources\nincluding the public training set from the data track, NLLB data provided by\nMeta AI, self-collected parallel corpora, and pseudo bitext from\nback-translation. A series of heuristic rules clean both bilingual and\nmonolingual texts. On the official test set, our system achieves 17.3 BLEU,\n21.9 spBLEU, and 41.9 chrF2++ on average over all language pairs. The average\ninference speed is 11.5 sentences per second using a single Nvidia Tesla V100\nGPU. Our code and trained models are available at\nhttps://github.com/xian8/wmt22\n",
                "链接": "https://arxiv.org/abs/2210.11599"
            },
            {
                "文章ID": "48289",
                "标题": "Findings of the Covid-19 MLIA Machine Translation Task",
                "作者": " Francisco Casacuberta,  Alexandru Ceausu,  Khalid Choukri,  Miltos Deligiannis,  Miguel Domingo,  Mercedes García-Martínez,  Manuel Herranz,  Guillaume Jacquet,  Vassilis Papavassiliou,  Stelios Piperidis,  Prokopis Prokopidis,  Dimitris Roussis,  Marwa Hadj Salah",
                "发布日期": "2022-11-15",
                "摘要": "  This work presents the results of the machine translation (MT) task from the\nCovid-19 MLIA @ Eval initiative, a community effort to improve the generation\nof MT systems focused on the current Covid-19 crisis. Nine teams took part in\nthis event, which was divided in two rounds and involved seven different\nlanguage pairs. Two different scenarios were considered: one in which only the\nprovided data was allowed, and a second one in which the use of external\nresources was allowed. Overall, best approaches were based on multilingual\nmodels and transfer learning, with an emphasis on the importance of applying a\ncleaning process to the training data.\n",
                "链接": "https://arxiv.org/abs/2211.07465"
            },
            {
                "文章ID": "19633",
                "标题": "Consistent Human Evaluation of Machine Translation across Language Pairs",
                "作者": " Daniel Licht,  Cynthia Gao,  Janice Lam,  Francisco Guzman,  Mona Diab,  Philipp Koehn",
                "发布日期": "2022-05-18",
                "摘要": "  Obtaining meaningful quality scores for machine translation systems through\nhuman evaluation remains a challenge given the high variability between human\nevaluators, partly due to subjective expectations for translation quality for\ndifferent language pairs. We propose a new metric called XSTS that is more\nfocused on semantic equivalence and a cross-lingual calibration method that\nenables more consistent assessment. We demonstrate the effectiveness of these\nnovel contributions in large scale evaluation studies across up to 14 language\npairs, with translation both into and out of English.\n",
                "链接": "https://arxiv.org/abs/2205.08533"
            }
        ]
    },
    {
        "question": {
            "question": "请找出使用Transformer模型并在大规模数据集上进行预训练的论文。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "95268",
                "标题": "Continual Pre-Training of Large Language Models: How to (re)warm your\n  model?",
                "作者": " Kshitij Gupta,  Benjamin Thérien,  Adam Ibrahim,  Mats L. Richter,  Quentin Anthony,  Eugene Belilovsky,  Irina Rish,  Timothée Lesort",
                "发布日期": "2023-09-08",
                "摘要": "  Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to restart the process over again once new data becomes available. A much\ncheaper and more efficient solution would be to enable the continual\npre-training of these models, i.e. updating pre-trained models with new data\ninstead of re-training them from scratch. However, the distribution shift\ninduced by novel data typically results in degraded performance on past data.\nTaking a step towards efficient continual pre-training, in this work, we\nexamine the effect of different warm-up strategies. Our hypothesis is that the\nlearning rate must be re-increased to improve compute efficiency when training\non a new dataset. We study the warmup phase of models pre-trained on the Pile\n(upstream data, 300B tokens) as we continue to pre-train on SlimPajama\n(downstream data, 297B tokens), following a linear warmup and cosine decay\nschedule. We conduct all experiments on the Pythia 410M language model\narchitecture and evaluate performance through validation perplexity. We\nexperiment with different pre-training checkpoints, various maximum learning\nrates, and various warmup lengths. Our results show that while rewarming models\nfirst increases the loss on upstream and downstream data, in the longer run it\nimproves the downstream performance, outperforming models trained from\nscratch$\\unicode{x2013}$even for a large downstream dataset.\n",
                "链接": "https://arxiv.org/abs/2308.04014"
            },
            {
                "文章ID": "91534",
                "标题": "Federated Large Language Model: A Position Paper",
                "作者": " Chaochao Chen,  Xiaohua Feng,  Jun Zhou,  Jianwei Yin,  Xiaolin Zheng",
                "发布日期": "2023-07-19",
                "摘要": "  Large scale language models (LLM) have received significant attention and\nfound diverse applications across various domains, but their development\nencounters challenges in real-world scenarios. These challenges arise due to\nthe scarcity of public domain data availability and the need to maintain\nprivacy with respect to private domain data. To address these issues, federated\nlearning (FL) has emerged as a promising technology that enables collaborative\ntraining of shared models while preserving decentralized data. We propose the\nconcept of federated LLM, which comprises three key components, i.e., federated\nLLM pre-training, federated LLM fine-tuning, and federated LLM prompt\nengineering. For each component, we discuss its advantage over traditional LLM\ntraining methods and propose specific engineering strategies for\nimplementation. Furthermore, we explore the novel challenges introduced by the\nintegration of FL and LLM. We analyze existing solutions and identify potential\nobstacles faced by these solutions within the context of federated LLM.\n",
                "链接": "https://arxiv.org/abs/2307.08925"
            },
            {
                "文章ID": "82786",
                "标题": "AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud\n  Dataset",
                "作者": " Jiakang Yuan,  Bo Zhang,  Xiangchao Yan,  Tao Chen,  Botian Shi,  Yikang Li,  Yu Qiao",
                "发布日期": "2023-10-27",
                "摘要": "  It is a long-term vision for Autonomous Driving (AD) community that the\nperception models can learn from a large-scale point cloud dataset, to obtain\nunified representations that can achieve promising results on different tasks\nor benchmarks. Previous works mainly focus on the self-supervised pre-training\npipeline, meaning that they perform the pre-training and fine-tuning on the\nsame benchmark, which is difficult to attain the performance scalability and\ncross-dataset application for the pre-training checkpoint. In this paper, for\nthe first time, we are committed to building a large-scale pre-training\npoint-cloud dataset with diverse data distribution, and meanwhile learning\ngeneralizable representations from such a diverse pre-training dataset. We\nformulate the point-cloud pre-training task as a semi-supervised problem, which\nleverages the few-shot labeled and massive unlabeled point-cloud data to\ngenerate the unified backbone representations that can be directly applied to\nmany baseline models and benchmarks, decoupling the AD-related pre-training\nprocess and downstream fine-tuning task. During the period of backbone\npre-training, by enhancing the scene- and instance-level distribution diversity\nand exploiting the backbone's ability to learn from unknown instances, we\nachieve significant performance gains on a series of downstream perception\nbenchmarks including Waymo, nuScenes, and KITTI, under different baseline\nmodels like PV-RCNN++, SECOND, CenterPoint.\n",
                "链接": "https://arxiv.org/abs/2306.00612"
            },
            {
                "文章ID": "95128",
                "标题": "Exploring Visual Pre-training for Robot Manipulation: Datasets, Models\n  and Methods",
                "作者": " Ya Jing,  Xuelin Zhu,  Xingbin Liu,  Qie Sima,  Taozheng Yang,  Yunhai Feng,  Tao Kong",
                "发布日期": "2023-08-08",
                "摘要": "  Visual pre-training with large-scale real-world data has made great progress\nin recent years, showing great potential in robot learning with pixel\nobservations. However, the recipes of visual pre-training for robot\nmanipulation tasks are yet to be built. In this paper, we thoroughly\ninvestigate the effects of visual pre-training strategies on robot manipulation\ntasks from three fundamental perspectives: pre-training datasets, model\narchitectures and training methods. Several significant experimental findings\nare provided that are beneficial for robot learning. Further, we propose a\nvisual pre-training scheme for robot manipulation termed Vi-PRoM, which\ncombines self-supervised learning and supervised learning. Concretely, the\nformer employs contrastive learning to acquire underlying patterns from\nlarge-scale unlabeled data, while the latter aims learning visual semantics and\ntemporal dynamics. Extensive experiments on robot manipulations in various\nsimulation environments and the real robot demonstrate the superiority of the\nproposed scheme. Videos and more details can be found on\n\\url{https://explore-pretrain-robot.github.io}.\n",
                "链接": "https://arxiv.org/abs/2308.03620"
            },
            {
                "文章ID": "75161",
                "标题": "Automated Paper Screening for Clinical Reviews Using Large Language\n  Models",
                "作者": " Eddie Guo,  Mehul Gupta,  Jiawen Deng,  Ye-Jean Park,  Mike Paget,  Christopher Naugler",
                "发布日期": "2023-10-09",
                "摘要": "  Objective: To assess the performance of the OpenAI GPT API in accurately and\nefficiently identifying relevant titles and abstracts from real-world clinical\nreview datasets and compare its performance against ground truth labelling by\ntwo independent human reviewers.\n  Methods: We introduce a novel workflow using the OpenAI GPT API for screening\ntitles and abstracts in clinical reviews. A Python script was created to make\ncalls to the GPT API with the screening criteria in natural language and a\ncorpus of title and abstract datasets that have been filtered by a minimum of\ntwo human reviewers. We compared the performance of our model against\nhuman-reviewed papers across six review papers, screening over 24,000 titles\nand abstracts.\n  Results: Our results show an accuracy of 0.91, a sensitivity of excluded\npapers of 0.91, and a sensitivity of included papers of 0.76. On a randomly\nselected subset of papers, the GPT API demonstrated the ability to provide\nreasoning for its decisions and corrected its initial decision upon being asked\nto explain its reasoning for a subset of incorrect classifications.\n  Conclusion: The GPT API has the potential to streamline the clinical review\nprocess, save valuable time and effort for researchers, and contribute to the\noverall quality of clinical reviews. By prioritizing the workflow and acting as\nan aid rather than a replacement for researchers and reviewers, the GPT API can\nenhance efficiency and lead to more accurate and reliable conclusions in\nmedical research.\n",
                "链接": "https://arxiv.org/abs/2305.00844"
            },
            {
                "文章ID": "115735",
                "标题": "Efficient Continual Pre-training for Building Domain Specific Large\n  Language Models",
                "作者": " Yong Xie,  Karan Aggarwal,  Aitzaz Ahmad",
                "发布日期": "2023-11-16",
                "摘要": "  Large language models (LLMs) have demonstrated remarkable open-domain\ncapabilities. Traditionally, LLMs tailored for a domain are trained from\nscratch to excel at handling domain-specific tasks. In this work, we explore an\nalternative strategy of continual pre-training as a means to develop\ndomain-specific LLMs. We introduce FinPythia-6.9B, developed through\ndomain-adaptive continual pre-training on the financial domain. Continual\npre-trained FinPythia showcases consistent improvements on financial tasks over\nthe original foundational model. We further explore simple but effective data\nselection strategies for continual pre-training. Our data selection strategies\noutperforms vanilla continual pre-training's performance with just 10% of\ncorpus size and cost, without any degradation on open-domain standard tasks.\nOur work proposes an alternative solution to building domain-specific LLMs from\nscratch in a cost-effective manner.\n",
                "链接": "https://arxiv.org/abs/2311.08545"
            },
            {
                "文章ID": "125245",
                "标题": "Spike No More: Stabilizing the Pre-training of Large Language Models",
                "作者": " Sho Takase,  Shun Kiyono,  Sosuke Kobayashi,  Jun Suzuki",
                "发布日期": "2023-12-29",
                "摘要": "  The loss spike often occurs during pre-training of a large language model.\nThe spikes degrade the performance of a large language model, and sometimes\nruin the pre-training. Since the pre-training needs a vast computational\nbudget, we should avoid such spikes. To investigate a cause of loss spikes, we\nfocus on gradients of internal layers in this study. Through theoretical\nanalyses, we introduce two causes of the exploding gradients, and provide\nrequirements to prevent the explosion. In addition, we introduce the\ncombination of the initialization method and a simple modification to\nembeddings as a method to satisfy the requirements. We conduct various\nexperiments to verify our theoretical analyses empirically. Experimental\nresults indicate that the combination is effective in preventing spikes during\npre-training.\n",
                "链接": "https://arxiv.org/abs/2312.16903"
            },
            {
                "文章ID": "114514",
                "标题": "Training CLIP models on Data from Scientific Papers",
                "作者": " Calvin Metzger",
                "发布日期": "2023-11-09",
                "摘要": "  Contrastive Language-Image Pretraining (CLIP) models are able to capture the\nsemantic relationship of images and texts and have enabled a wide range of\napplications, from image retrieval to classification. These models are trained\nwith datasets extracted from web crawls, which are of large quantity but\nlimited quality. This paper explores whether limited amounts higher quality\ndata in a specific domain improve the general performance of CLIP models. To\nthis purpose, we extract text-image data from scientific papers hosted in the\narXiv and PubMed Central repositories. Experiments on small-scale CLIP models\n(ViT B/32) show that model performance increases on average, but only\nmoderately. This result indicates that using the data sources considered in the\npaper to train large-scale CLIP models is a worthwile research direction.\n",
                "链接": "https://arxiv.org/abs/2311.04711"
            },
            {
                "文章ID": "106387",
                "标题": "AstroCLIP: Cross-Modal Pre-Training for Astronomical Foundation Models",
                "作者": " Francois Lanusse,  Liam Parker,  Siavash Golkar,  Miles Cranmer,  Alberto Bietti,  Michael Eickenberg,  Geraud Krawezik,  Michael McCabe,  Ruben Ohana,  Mariel Pettee,  Bruno Regaldo-Saint Blancard,  Tiberiu Tesileanu,  Kyunghyun Cho,  Shirley Ho",
                "发布日期": "2023-10-05",
                "摘要": "  We present AstroCLIP, a strategy to facilitate the construction of\nastronomical foundation models that bridge the gap between diverse\nobservational modalities. We demonstrate that a cross-modal contrastive\nlearning approach between images and optical spectra of galaxies yields highly\ninformative embeddings of both modalities. In particular, we apply our method\non multi-band images and optical spectra from the Dark Energy Spectroscopic\nInstrument (DESI), and show that: (1) these embeddings are well-aligned between\nmodalities and can be used for accurate cross-modal searches, and (2) these\nembeddings encode valuable physical information about the galaxies -- in\nparticular redshift and stellar mass -- that can be used to achieve competitive\nzero- and few- shot predictions without further finetuning. Additionally, in\nthe process of developing our approach, we also construct a novel,\ntransformer-based model and pretraining approach for processing galaxy spectra.\n",
                "链接": "https://arxiv.org/abs/2310.03024"
            },
            {
                "文章ID": "107865",
                "标题": "Zero-Shot Open-Vocabulary Tracking with Large Pre-Trained Models",
                "作者": " Wen-Hsuan Chu,  Adam W. Harley,  Pavel Tokmakov,  Achal Dave,  Leonidas Guibas,  Katerina Fragkiadaki",
                "发布日期": "2023-10-12",
                "摘要": "  Object tracking is central to robot perception and scene understanding.\nTracking-by-detection has long been a dominant paradigm for object tracking of\nspecific object categories. Recently, large-scale pre-trained models have shown\npromising advances in detecting and segmenting objects and parts in 2D static\nimages in the wild. This begs the question: can we re-purpose these large-scale\npre-trained static image models for open-vocabulary video tracking? In this\npaper, we re-purpose an open-vocabulary detector, segmenter, and dense optical\nflow estimator, into a model that tracks and segments objects of any category\nin 2D videos. Our method predicts object and part tracks with associated\nlanguage descriptions in monocular videos, rebuilding the pipeline of Tractor\nwith modern large pre-trained models for static image detection and\nsegmentation: we detect open-vocabulary object instances and propagate their\nboxes from frame to frame using a flow-based motion model, refine the\npropagated boxes with the box regression module of the visual detector, and\nprompt an open-world segmenter with the refined box to segment the objects. We\ndecide the termination of an object track based on the objectness score of the\npropagated boxes, as well as forward-backward optical flow consistency. We\nre-identify objects across occlusions using deep feature matching. We show that\nour model achieves strong performance on multiple established video object\nsegmentation and tracking benchmarks, and can produce reasonable tracks in\nmanipulation data. In particular, our model outperforms previous\nstate-of-the-art in UVO and BURST, benchmarks for open-world object tracking\nand segmentation, despite never being explicitly trained for tracking. We hope\nthat our approach can serve as a simple and extensible framework for future\nresearch.\n",
                "链接": "https://arxiv.org/abs/2310.06992"
            }
        ]
    },
    {
        "question": {
            "question": "找一下使用强化学习做代码生成的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "64359",
                "标题": "Reinforcement Learning Guided Multi-Objective Exam Paper Generation",
                "作者": " Yuhu Shang,  Xuexiong Luo,  Lihong Wang,  Hao Peng,  Xiankun Zhang,  Yimeng Ren,  Kun Liang",
                "发布日期": "2023-03-03",
                "摘要": "  To reduce the repetitive and complex work of instructors, exam paper\ngeneration (EPG) technique has become a salient topic in the intelligent\neducation field, which targets at generating high-quality exam paper\nautomatically according to instructor-specified assessment criteria. The\ncurrent advances utilize the ability of heuristic algorithms to optimize\nseveral well-known objective constraints, such as difficulty degree, number of\nquestions, etc., for producing optimal solutions. However, in real scenarios,\nconsidering other equally relevant objectives (e.g., distribution of exam\nscores, skill coverage) is extremely important. Besides, how to develop an\nautomatic multi-objective solution that finds an optimal subset of questions\nfrom a huge search space of large-sized question datasets and thus composes a\nhigh-quality exam paper is urgent but non-trivial. To this end, we skillfully\ndesign a reinforcement learning guided Multi-Objective Exam Paper Generation\nframework, termed MOEPG, to simultaneously optimize three exam domain-specific\nobjectives including difficulty degree, distribution of exam scores, and skill\ncoverage. Specifically, to accurately measure the skill proficiency of the\nexaminee group, we first employ deep knowledge tracing to model the interaction\ninformation between examinees and response logs. We then design the flexible\nExam Q-Network, a function approximator, which automatically selects the\nappropriate question to update the exam paper composition process. Later, MOEPG\ndivides the decision space into multiple subspaces to better guide the updated\ndirection of the exam paper. Through extensive experiments on two real-world\ndatasets, we demonstrate that MOEPG is feasible in addressing the multiple\ndilemmas of exam paper generation scenario.\n",
                "链接": "https://arxiv.org/abs/2303.01042"
            },
            {
                "文章ID": "30296",
                "标题": "CodeT: Code Generation with Generated Tests",
                "作者": " Bei Chen,  Fengji Zhang,  Anh Nguyen,  Daoguang Zan,  Zeqi Lin,  Jian-Guang Lou,  Weizhu Chen",
                "发布日期": "2022-11-24",
                "摘要": "  The task of generating code solutions for a given programming problem can\nbenefit from the use of pre-trained language models such as Codex, which can\nproduce multiple diverse samples. However, a major challenge for this task is\nto select the most appropriate solution from the multiple samples generated by\nthe pre-trained language models. A natural way to evaluate the quality and\ncorrectness of a code solution is to run it against a set of test cases, but\nthe manual creation of such test cases is often costly and time-consuming. In\nthis paper, we propose a novel method, CodeT, that leverages the same\npre-trained language models to automatically generate test cases for the code\nsamples, thus reducing the human effort and increasing the coverage of the test\nscenarios. CodeT then executes the code samples using the generated test cases,\nand performs a dual execution agreement, which considers both the consistency\nof the outputs against the generated test cases and the agreement of the\noutputs with other code samples. We conduct comprehensive experiments on four\nbenchmarks, HumanEval, MBPP, APPS and CodeContests, using five different\npre-trained language models with varying sizes and capabilities. Our results\nshow that CodeT can significantly improve the performance of code solution\nselection over previous methods, achieving remarkable and consistent gains\nacross different models and benchmarks. For instance, CodeT improves the pass@1\nmetric on HumanEval to 65.8%, which represents an absolute improvement of 18.8%\nover the code-davinci-002 model, and an absolute improvement of more than 20%\nover the previous state-of-the-art results.\n",
                "链接": "https://arxiv.org/abs/2207.10397"
            },
            {
                "文章ID": "51256",
                "标题": "Coder Reviewer Reranking for Code Generation",
                "作者": " Tianyi Zhang,  Tao Yu,  Tatsunori B. Hashimoto,  Mike Lewis,  Wen-tau Yih,  Daniel Fried,  Sida I. Wang",
                "发布日期": "2022-11-30",
                "摘要": "  Sampling diverse programs from a code language model and reranking with model\nlikelihood is a popular method for code generation but it is prone to\npreferring degenerate solutions. Inspired by collaborative programming, we\npropose Coder-Reviewer reranking. We augment Coder language models from past\nwork, which generate programs given language instructions, with Reviewer\nmodels, which evaluate the likelihood of the instruction given the generated\nprograms. We perform an extensive study across six datasets with eight models\nfrom three model families. Experimental results show that Coder-Reviewer\nreranking leads to consistent and significant improvement (up to 17% absolute\naccuracy gain) over reranking with the Coder model only. When combined with\nexecutability filtering, Coder-Reviewer reranking can often outperform the\nminimum Bayes risk method. Coder-Reviewer reranking is easy to implement by\nprompting, can generalize to different programming languages, and works well\nwith off-the-shelf hyperparameters.\n",
                "链接": "https://arxiv.org/abs/2211.16490"
            },
            {
                "文章ID": "59289",
                "标题": "Execution-based Code Generation using Deep Reinforcement Learning",
                "作者": " Parshin Shojaee,  Aneesh Jain,  Sindhu Tipirneni,  Chandan K. Reddy",
                "发布日期": "2023-07-21",
                "摘要": "  The utilization of programming language (PL) models, pre-trained on\nlarge-scale code corpora, as a means of automating software engineering\nprocesses has demonstrated considerable potential in streamlining various code\ngeneration tasks such as code completion, code translation, and program\nsynthesis. However, current approaches mainly rely on supervised fine-tuning\nobjectives borrowed from text generation, neglecting unique sequence-level\ncharacteristics of code, including but not limited to compilability as well as\nsyntactic and functional correctness. To address this limitation, we propose\nPPOCoder, a new framework for code generation that synergistically combines\npre-trained PL models with Proximal Policy Optimization (PPO) which is a widely\nused deep reinforcement learning technique. By utilizing non-differentiable\nfeedback from code execution and structure alignment, PPOCoder seamlessly\nintegrates external code-specific knowledge into the model optimization\nprocess. It's important to note that PPOCoder is a task-agnostic and\nmodel-agnostic framework that can be used across different code generation\ntasks and PLs. Extensive experiments on three code generation tasks demonstrate\nthe effectiveness of our proposed approach compared to SOTA methods, achieving\nsignificant improvements in compilation success rates and functional\ncorrectness across different PLs.\n",
                "链接": "https://arxiv.org/abs/2301.13816"
            },
            {
                "文章ID": "27761",
                "标题": "CodeRL: Mastering Code Generation through Pretrained Models and Deep\n  Reinforcement Learning",
                "作者": " Hung Le,  Yue Wang,  Akhilesh Deepak Gotmare,  Silvio Savarese,  Steven C. H. Hoi",
                "发布日期": "2022-11-04",
                "摘要": "  Program synthesis or code generation aims to generate a program that\nsatisfies a problem specification. Recent approaches using large-scale\npretrained language models (LMs) have shown promising results, yet they have\nsome critical limitations. In particular, they often follow a standard\nsupervised fine-tuning procedure to train a code generation model only from the\npairs of natural-language problem descriptions and ground-truth programs. Such\nparadigm largely ignores some important but potentially useful signals in the\nproblem specification such as unit tests, which thus often results in poor\nperformance when solving complex unseen coding tasks. To address the\nlimitations, we propose \"CodeRL\", a new framework for program synthesis tasks\nthrough pretrained LMs and deep reinforcement learning (RL). Specifically,\nduring training, we treat the code-generating LM as an actor network, and\nintroduce a critic network that is trained to predict the functional\ncorrectness of generated programs and provide dense feedback signals to the\nactor. During inference, we introduce a new generation procedure with a\ncritical sampling strategy that allows a model to automatically regenerate\nprograms based on feedback from example unit tests and critic scores. For the\nmodel backbones, we extended the encoder-decoder architecture of CodeT5 with\nenhanced learning objectives, larger model sizes, and better pretraining data.\nOur method not only achieves new SOTA results on the challenging APPS\nbenchmark, but also shows strong zero-shot transfer capability with new SOTA\nresults on the simpler MBPP benchmark.\n",
                "链接": "https://arxiv.org/abs/2207.01780"
            },
            {
                "文章ID": "110290",
                "标题": "Automatic Unit Test Data Generation and Actor-Critic Reinforcement\n  Learning for Code Synthesis",
                "作者": " Philip John Gorinski,  Matthieu Zimmer,  Gerasimos Lampouras,  Derrick Goh Xin Deik,  Ignacio Iacobacci",
                "发布日期": "2023-10-23",
                "摘要": "  The advent of large pre-trained language models in the domain of Code\nSynthesis has shown remarkable performance on various benchmarks, treating the\nproblem of Code Generation in a fashion similar to Natural Language Generation,\ntrained with a Language Modelling (LM) objective. In addition, the property of\nprogramming language code being precisely evaluable with respect to its\nsemantics -- through the use of Unit Tests to check its functional correctness\n-- lends itself to using Reinforcement Learning (RL) as a further training\nparadigm. Previous work has shown that RL can be applied as such to improve\nmodels' coding capabilities; however, such RL-based methods rely on a reward\nsignal based on defined Unit Tests, which are much harder to obtain compared to\nthe huge crawled code datasets used in LM objectives. In this work, we present\na novel approach to automatically obtain data consisting of function signatures\nand associated Unit Tests, suitable for RL training of Code Synthesis models.\nWe also introduce a straightforward, simple yet effective Actor-Critic RL\ntraining scheme and show that it, in conjunction with automatically generated\ntraining data, leads to improvement of a pre-trained code language model's\nperformance by up to 9.9% improvement over the original underlying code\nsynthesis LM, and up to 4.3% over RL-based models trained with standard PPO or\nCodeRL.\n",
                "链接": "https://arxiv.org/abs/2310.13669"
            },
            {
                "文章ID": "72734",
                "标题": "Stochastic Code Generation",
                "作者": " Swapnil Sharma,  Nikita Anand, V Kranthi Kiran G.",
                "发布日期": "2023-04-18",
                "摘要": "  Large language models pre-trained for code generation can generate\nhigh-quality short code but often struggle with generating coherent long code\nand understanding higher-level or system-level specifications. This issue is\nalso observed in language modeling for long text generation, and one proposed\nsolution is the use of a latent stochastic process. This approach involves\ngenerating a document plan and then producing text that is consistent with it.\n  In this study, we investigate whether this technique can be applied to code\ngeneration to improve coherence. We base our proposed encoder and decoder on\nthe pre-trained GPT-2 based CodeParrot model and utilize the APPS dataset for\ntraining. We evaluate our results using the HumanEval benchmark and observe\nthat the modified Time Control model performs similarly to CodeParrot on this\nevaluation.\n",
                "链接": "https://arxiv.org/abs/2304.08243"
            },
            {
                "文章ID": "89545",
                "标题": "Exploring Continual Learning for Code Generation Models",
                "作者": " Prateek Yadav,  Qing Sun,  Hantian Ding,  Xiaopeng Li,  Dejiao Zhang,  Ming Tan,  Xiaofei Ma,  Parminder Bhatia,  Ramesh Nallapati,  Murali Krishna Ramanathan,  Mohit Bansal,  Bing Xiang",
                "发布日期": "2023-07-06",
                "摘要": "  Large-scale code generation models such as Codex and CodeT5 have achieved\nimpressive performance. However, libraries are upgraded or deprecated very\nfrequently and re-training large-scale language models is computationally\nexpensive. Therefore, Continual Learning (CL) is an important aspect that\nremains underexplored in the code domain. In this paper, we introduce a\nbenchmark called CodeTask-CL that covers a wide range of tasks, including code\ngeneration, translation, summarization, and refinement, with different input\nand output programming languages. Next, on our CodeTask-CL benchmark, we\ncompare popular CL techniques from NLP and Vision domains. We find that\neffective methods like Prompt Pooling (PP) suffer from catastrophic forgetting\ndue to the unstable training of the prompt selection mechanism caused by stark\ndistribution shifts in coding tasks. We address this issue with our proposed\nmethod, Prompt Pooling with Teacher Forcing (PP-TF), that stabilizes training\nby enforcing constraints on the prompt selection mechanism and leads to a\n21.54% improvement over Prompt Pooling. Along with the benchmark, we establish\na training pipeline that can be used for CL on code models, which we believe\ncan motivate further development of CL methods for code models. Our code is\navailable at https://github.com/amazon-science/codetaskcl-pptf\n",
                "链接": "https://arxiv.org/abs/2307.02435"
            },
            {
                "文章ID": "7583",
                "标题": "Paper Plain: Making Medical Research Papers Approachable to Healthcare\n  Consumers with Natural Language Processing",
                "作者": " Tal August,  Lucy Lu Wang,  Jonathan Bragg,  Marti A. Hearst,  Andrew Head,  Kyle Lo",
                "发布日期": "2022-03-02",
                "摘要": "  When seeking information not covered in patient-friendly documents, like\nmedical pamphlets, healthcare consumers may turn to the research literature.\nReading medical papers, however, can be a challenging experience. To improve\naccess to medical papers, we introduce a novel interactive interface-Paper\nPlain-with four features powered by natural language processing: definitions of\nunfamiliar terms, in-situ plain language section summaries, a collection of key\nquestions that guide readers to answering passages, and plain language\nsummaries of the answering passages. We evaluate Paper Plain, finding that\nparticipants who use Paper Plain have an easier time reading and understanding\nresearch papers without a loss in paper comprehension compared to those who use\na typical PDF reader. Altogether, the study results suggest that guiding\nreaders to relevant passages and providing plain language summaries, or\n\"gists,\" alongside the original paper content can make reading medical papers\neasier and give readers more confidence to approach these papers.\n",
                "链接": "https://arxiv.org/abs/2203.00130"
            },
            {
                "文章ID": "119060",
                "标题": "Self-Infilling Code Generation",
                "作者": " Lin Zheng,  Jianbo Yuan,  Zhi Zhang,  Hongxia Yang,  Lingpeng Kong",
                "发布日期": "2023-12-01",
                "摘要": "  This work introduces a general code generation framework that incorporates\ninfilling operations into auto-regressive decoding. Our approach capitalizes on\nthe observation that recent code language models with infilling capabilities\ncan perform \\emph{self-infilling}: whereas infilling operations aim to fill in\nthe middle based on a predefined prefix and suffix, self-infilling sequentially\ngenerates both such surrounding context and the infilled content. We utilize\nthis feature to develop an infilling-augmented decoding process that\nfacilitates non-monotonic generation. This approach allows for postponing the\ngeneration of uncertain code snippets until a definitive suffix is established,\nleading to improved control over the generation sequence. In addition, it\nfacilitates a looping mechanism, which can iteratively update and synchronize\neach piece of generation in a cyclic manner. Extensive experiments are\nconducted to demonstrate that our proposed decoding process is effective in\nenhancing regularity and quality across several code generation benchmarks.\n",
                "链接": "https://arxiv.org/abs/2311.17972"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下关于大模型使用工具的安全性的论文。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "106425",
                "标题": "MetaTool Benchmark for Large Language Models: Deciding Whether to Use\n  Tools and Which to Use",
                "作者": " Yue Huang,  Jiawen Shi,  Yuan Li,  Chenrui Fan,  Siyuan Wu,  Qihui Zhang,  Yixin Liu,  Pan Zhou,  Yao Wan,  Neil Zhenqiang Gong,  Lichao Sun",
                "发布日期": "2023-10-25",
                "摘要": "  Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving nine popular\nLLMs and find that the majority of them still struggle to effectively select\ntools, highlighting the existing gaps between LLMs and genuine intelligent\nagents. However, through the error analysis, we found there is still\nsignificant room for improvement. Finally, we conclude with insights for tool\ndevelopers that follow ChatGPT to provide detailed descriptions that can\nenhance the tool selection performance of LLMs.\n",
                "链接": "https://arxiv.org/abs/2310.03128"
            },
            {
                "文章ID": "122196",
                "标题": "Causality Analysis for Evaluating the Security of Large Language Models",
                "作者": " Wei Zhao,  Zhe Li,  Jun Sun",
                "发布日期": "2023-12-14",
                "摘要": "  Large Language Models (LLMs) such as GPT and Llama2 are increasingly adopted\nin many safety-critical applications. Their security is thus essential. Even\nwith considerable efforts spent on reinforcement learning from human feedback\n(RLHF), recent studies have shown that LLMs are still subject to attacks such\nas adversarial perturbation and Trojan attacks. Further research is thus needed\nto evaluate their security and/or understand the lack of it. In this work, we\npropose a framework for conducting light-weight causality-analysis of LLMs at\nthe token, layer, and neuron level. We applied our framework to open-source\nLLMs such as Llama2 and Vicuna and had multiple interesting discoveries. Based\non a layer-level causality analysis, we show that RLHF has the effect of\noverfitting a model to harmful prompts. It implies that such security can be\neasily overcome by `unusual' harmful prompts. As evidence, we propose an\nadversarial perturbation method that achieves 100\\% attack success rate on the\nred-teaming tasks of the Trojan Detection Competition 2023. Furthermore, we\nshow the existence of one mysterious neuron in both Llama2 and Vicuna that has\nan unreasonably high causal effect on the output. While we are uncertain on why\nsuch a neuron exists, we show that it is possible to conduct a ``Trojan''\nattack targeting that particular neuron to completely cripple the LLM, i.e., we\ncan generate transferable suffixes to prompts that frequently make the LLM\nproduce meaningless responses.\n",
                "链接": "https://arxiv.org/abs/2312.07876"
            },
            {
                "文章ID": "70693",
                "标题": "Summary of ChatGPT-Related Research and Perspective Towards the Future\n  of Large Language Models",
                "作者": " Yiheng Liu,  Tianle Han,  Siyuan Ma,  Jiayue Zhang,  Yuanyuan Yang,  Jiaming Tian,  Hao He,  Antong Li,  Mengshen He,  Zhengliang Liu,  Zihao Wu,  Lin Zhao,  Dajiang Zhu,  Xiang Li,  Ning Qiang,  Dingang Shen,  Tianming Liu,  Bao Ge",
                "发布日期": "2023-08-25",
                "摘要": "  This paper presents a comprehensive survey of ChatGPT-related (GPT-3.5 and\nGPT-4) research, state-of-the-art large language models (LLM) from the GPT\nseries, and their prospective applications across diverse domains. Indeed, key\ninnovations such as large-scale pre-training that captures knowledge across the\nentire world wide web, instruction fine-tuning and Reinforcement Learning from\nHuman Feedback (RLHF) have played significant roles in enhancing LLMs'\nadaptability and performance. We performed an in-depth analysis of 194 relevant\npapers on arXiv, encompassing trend analysis, word cloud representation, and\ndistribution analysis across various application domains. The findings reveal a\nsignificant and increasing interest in ChatGPT-related research, predominantly\ncentered on direct natural language processing applications, while also\ndemonstrating considerable potential in areas ranging from education and\nhistory to mathematics, medicine, and physics. This study endeavors to furnish\ninsights into ChatGPT's capabilities, potential implications, ethical concerns,\nand offer direction for future advancements in this field.\n",
                "链接": "https://arxiv.org/abs/2304.01852"
            },
            {
                "文章ID": "60869",
                "标题": "Toolformer: Language Models Can Teach Themselves to Use Tools",
                "作者": " Timo Schick,  Jane Dwivedi-Yu,  Roberto Dessì,  Roberta Raileanu,  Maria Lomeli,  Luke Zettlemoyer,  Nicola Cancedda,  Thomas Scialom",
                "发布日期": "2023-02-10",
                "摘要": "  Language models (LMs) exhibit remarkable abilities to solve new tasks from\njust a few examples or textual instructions, especially at scale. They also,\nparadoxically, struggle with basic functionality, such as arithmetic or factual\nlookup, where much simpler and smaller models excel. In this paper, we show\nthat LMs can teach themselves to use external tools via simple APIs and achieve\nthe best of both worlds. We introduce Toolformer, a model trained to decide\nwhich APIs to call, when to call them, what arguments to pass, and how to best\nincorporate the results into future token prediction. This is done in a\nself-supervised way, requiring nothing more than a handful of demonstrations\nfor each API. We incorporate a range of tools, including a calculator, a Q\\&A\nsystem, two different search engines, a translation system, and a calendar.\nToolformer achieves substantially improved zero-shot performance across a\nvariety of downstream tasks, often competitive with much larger models, without\nsacrificing its core language modeling abilities.\n",
                "链接": "https://arxiv.org/abs/2302.04761"
            },
            {
                "文章ID": "50884",
                "标题": "On the Security Vulnerabilities of Text-to-SQL Models",
                "作者": " Xutan Peng,  Yipeng Zhang,  Jingfeng Yang,  Mark Stevenson",
                "发布日期": "2023-10-13",
                "摘要": "  Although it has been demonstrated that Natural Language Processing (NLP)\nalgorithms are vulnerable to deliberate attacks, the question of whether such\nweaknesses can lead to software security threats is under-explored. To bridge\nthis gap, we conducted vulnerability tests on Text-to-SQL systems that are\ncommonly used to create natural language interfaces to databases. We showed\nthat the Text-to-SQL modules within six commercial applications can be\nmanipulated to produce malicious code, potentially leading to data breaches and\nDenial of Service attacks. This is the first demonstration that NLP models can\nbe exploited as attack vectors in the wild. In addition, experiments using four\nopen-source language models verified that straightforward backdoor attacks on\nText-to-SQL systems achieve a 100% success rate without affecting their\nperformance. The aim of this work is to draw the community's attention to\npotential software security issues associated with NLP algorithms and encourage\nexploration of methods to mitigate against them.\n",
                "链接": "https://arxiv.org/abs/2211.15363"
            },
            {
                "文章ID": "115021",
                "标题": "Smart Agent-Based Modeling: On the Use of Large Language Models in\n  Computer Simulations",
                "作者": " Zengqing Wu,  Run Peng,  Xu Han,  Shuyuan Zheng,  Yixin Zhang,  Chuan Xiao",
                "发布日期": "2023-12-19",
                "摘要": "  Computer simulations offer a robust toolset for exploring complex systems\nacross various disciplines. A particularly impactful approach within this realm\nis Agent-Based Modeling (ABM), which harnesses the interactions of individual\nagents to emulate intricate system dynamics. ABM's strength lies in its\nbottom-up methodology, illuminating emergent phenomena by modeling the\nbehaviors of individual components of a system. Yet, ABM has its own set of\nchallenges, notably its struggle with modeling natural language instructions\nand common sense in mathematical equations or rules. This paper seeks to\ntranscend these boundaries by integrating Large Language Models (LLMs) like GPT\ninto ABM. This amalgamation gives birth to a novel framework, Smart Agent-Based\nModeling (SABM). Building upon the concept of smart agents -- entities\ncharacterized by their intelligence, adaptability, and computation ability --\nwe explore in the direction of utilizing LLM-powered agents to simulate\nreal-world scenarios with increased nuance and realism. In this comprehensive\nexploration, we elucidate the state of the art of ABM, introduce SABM's\npotential and methodology, and present three case studies (source codes\navailable at https://github.com/Roihn/SABM), demonstrating the SABM methodology\nand validating its effectiveness in modeling real-world systems. Furthermore,\nwe cast a vision towards several aspects of the future of SABM, anticipating a\nbroader horizon for its applications. Through this endeavor, we aspire to\nredefine the boundaries of computer simulations, enabling a more profound\nunderstanding of complex systems.\n",
                "链接": "https://arxiv.org/abs/2311.06330"
            },
            {
                "文章ID": "97463",
                "标题": "Can Large Language Models Find And Fix Vulnerable Software?",
                "作者": " David Noever",
                "发布日期": "2023-08-22",
                "摘要": "  In this study, we evaluated the capability of Large Language Models (LLMs),\nparticularly OpenAI's GPT-4, in detecting software vulnerabilities, comparing\ntheir performance against traditional static code analyzers like Snyk and\nFortify. Our analysis covered numerous repositories, including those from NASA\nand the Department of Defense. GPT-4 identified approximately four times the\nvulnerabilities than its counterparts. Furthermore, it provided viable fixes\nfor each vulnerability, demonstrating a low rate of false positives. Our tests\nencompassed 129 code samples across eight programming languages, revealing the\nhighest vulnerabilities in PHP and JavaScript. GPT-4's code corrections led to\na 90% reduction in vulnerabilities, requiring only an 11% increase in code\nlines. A critical insight was LLMs' ability to self-audit, suggesting fixes for\ntheir identified vulnerabilities and underscoring their precision. Future\nresearch should explore system-level vulnerabilities and integrate multiple\nstatic code analyzers for a holistic perspective on LLMs' potential.\n",
                "链接": "https://arxiv.org/abs/2308.10345"
            },
            {
                "文章ID": "94111",
                "标题": "Structural Embeddings of Tools for Large Language Models",
                "作者": " Eren Unlu",
                "发布日期": "2023-08-02",
                "摘要": "  It is evident that the current state of Large Language Models (LLMs)\nnecessitates the incorporation of external tools. The lack of straightforward\nalgebraic and logical reasoning is well documented and prompted researchers to\ndevelop frameworks which allow LLMs to operate via external tools. The\nontological nature of tool utilization for a specific task can be well\nformulated with a Directed Acyclic Graph (DAG). The central aim of the paper is\nto highlight the importance of graph based approaches to LLM-tool interaction\nin near future. We propose an exemplary framework to guide the orchestration of\nexponentially increasing numbers of external tools with LLMs,where objectives\nand functionalities of tools are graph encoded hierarchically. Assuming that\ntextual segments of a Chain-of-Thought (CoT) can be imagined as a tool as\ndefined here, the graph based framework can pave new avenues in that particular\ndirection as well.\n",
                "链接": "https://arxiv.org/abs/2308.00447"
            },
            {
                "文章ID": "82016",
                "标题": "GPT4Tools: Teaching Large Language Model to Use Tools via\n  Self-instruction",
                "作者": " Rui Yang,  Lin Song,  Yanwei Li,  Sijie Zhao,  Yixiao Ge,  Xiu Li,  Ying Shan",
                "发布日期": "2023-05-31",
                "摘要": "  This paper aims to efficiently enable Large Language Models (LLMs) to use\nmultimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, have\nshown great potential for tool usage through sophisticated prompt engineering.\nNevertheless, these models typically rely on prohibitive computational costs\nand publicly inaccessible data. To address these challenges, we propose the\nGPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and\nOPT, to use tools. It generates an instruction-following dataset by prompting\nan advanced teacher with various multi-modal contexts. By using the Low-Rank\nAdaptation (LoRA) optimization, our approach facilitates the open-source LLMs\nto solve a range of visual problems, including visual comprehension and image\ngeneration. Moreover, we provide a benchmark to evaluate the ability of LLMs to\nuse tools, which is performed in both zero-shot and fine-tuning ways. Extensive\nexperiments demonstrate the effectiveness of our method on various language\nmodels, which not only significantly improves the accuracy of invoking seen\ntools, but also enables the zero-shot capacity for unseen tools. The code and\ndemo are available at https://github.com/StevenGrove/GPT4Tools.\n",
                "链接": "https://arxiv.org/abs/2305.18752"
            },
            {
                "文章ID": "92680",
                "标题": "The Effectiveness of Large Language Models (ChatGPT and CodeBERT) for\n  Security-Oriented Code Analysis",
                "作者": " Zhilong Wang,  Lan Zhang,  Chen Cao,  Peng Liu",
                "发布日期": "2023-08-30",
                "摘要": "  Large Language Models (LLMs), such as GPT and BERT, have demonstrated\nremarkable capabilities in addressing neural language process tasks. Recently,\nthe release of ChatGPT has garnered significant attention due to its ability to\nanalyze, comprehend, and synthesize information from user inputs. Therefore,\nthese LLMs were adopted by researchers in many different domains. In the realm\nof code analysis, researchers have applied LLMs to tasks like code review and\ncode generation. However, we observed that the strengths and limitations of\nadopting these LLMs to the code analysis have not been investigated. In this\npaper, we delve into LLMs' capabilities in security-oriented program analysis,\nconsidering perspectives from both attackers and security analysts. We focus on\ntwo representative LLMs, ChatGPT and CodeBert, and evaluate their performance\nin solving typical analytic tasks with varying levels of difficulty. Given the\ndifferent natures of ChatGPT and CodeBERT, we conduct a qualitative analysis of\nthe model's output for ChatGPT and a quantitative analysis for CodeBERT,\nrespectively. For ChatGPT, we present a case study involving several\nsecurity-oriented program analysis tasks while deliberately introducing\nchallenges to assess its responses. On the other hand, for CodeBERT, we\nsystematically analyze and classify the features in code, quantitatively\nevaluating the impact of these features on the model's performance. Our study\ndemonstrates the LLM's efficiency in learning high-level semantics from code,\npositioning ChatGPT as a potential asset in security-oriented contexts.\nHowever, it is essential to acknowledge certain limitations, such as the heavy\nreliance on well-defined variable and function names, making them unable to\nlearn from anonymized code. We hope that our findings and analysis will offer\nvaluable insights for future researchers in this domain.\n",
                "链接": "https://arxiv.org/abs/2307.12488"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下对使用GPT生成数据集的训练步骤进行改进的论文。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "94735",
                "标题": "Learning to Generate Training Datasets for Robust Semantic Segmentation",
                "作者": " Marwane Hariat,  Olivier Laurent,  Rémi Kazmierczak,  Shihao Zhang,  Andrei Bursuc,  Angela Yao,  Gianni Franchi",
                "发布日期": "2023-08-21",
                "摘要": "  Semantic segmentation techniques have shown significant progress in recent\nyears, but their robustness to real-world perturbations and data samples not\nseen during training remains a challenge, particularly in safety-critical\napplications. In this paper, we propose a novel approach to improve the\nrobustness of semantic segmentation techniques by leveraging the synergy\nbetween label-to-image generators and image-to-label segmentation models.\nSpecifically, we design and train Robusta, a novel robust conditional\ngenerative adversarial network to generate realistic and plausible perturbed or\noutlier images that can be used to train reliable segmentation models. We\nconduct in-depth studies of the proposed generative model, assess the\nperformance and robustness of the downstream segmentation network, and\ndemonstrate that our approach can significantly enhance the robustness of\nsemantic segmentation techniques in the face of real-world perturbations,\ndistribution shifts, and out-of-distribution samples. Our results suggest that\nthis approach could be valuable in safety-critical applications, where the\nreliability of semantic segmentation techniques is of utmost importance and\ncomes with a limited computational budget in inference. We will release our\ncode shortly.\n",
                "链接": "https://arxiv.org/abs/2308.02535"
            },
            {
                "文章ID": "124649",
                "标题": "Two Steps Forward and One Step Back: The Right to Opt-out of Sale under\n  CPRA",
                "作者": " Jan Charatan,  Eleanor Birrell",
                "发布日期": "2023-12-27",
                "摘要": "  The California Privacy Rights Act (CPRA) was a ballot initiative that revised\nthe California Consumer Privacy Act (CCPA). Although often framed as expanding\nand enhancing privacy rights, a close analysis of textual revisions -- both\nchanges from the earlier law and changes from earlier drafts of the CPRA\nguidelines -- suggest that the reality might be more nuanced. In this work, we\nidentify three textual revisions that have potential to negatively impact the\nright to opt-out of sale under CPRA and evaluate the effect of these textual\nrevisions using (1) a large-scale longitudinal measurement study of 25,000\nwebsites over twelve months and (2) an experimental user study with 775\nparticipants recruited through Prolific. We find that all revisions negatively\nimpacted the usability, scope, and visibility of the right to opt-out of sale.\nOur results provide the first comprehensive evaluation of the impact of CPRA on\nInternet privacy. They also emphasize the importance of continued evaluation of\nlegal requirements as guidelines and case law evolve after a law goes into\neffect.\n",
                "链接": "https://arxiv.org/abs/2312.15094"
            },
            {
                "文章ID": "19921",
                "标题": "Deterministic training of generative autoencoders using invertible\n  layers",
                "作者": " Gianluigi Silvestri,  Daan Roos,  Luca Ambrogioni",
                "发布日期": "2023-03-06",
                "摘要": "  In this work, we provide a deterministic alternative to the stochastic\nvariational training of generative autoencoders. We refer to these new\ngenerative autoencoders as AutoEncoders within Flows (AEF), since the encoder\nand decoder are defined as affine layers of an overall invertible architecture.\nThis results in a deterministic encoding of the data, as opposed to the\nstochastic encoding of VAEs. The paper introduces two related families of AEFs.\nThe first family relies on a partition of the ambient space and is trained by\nexact maximum-likelihood. The second family exploits a deterministic expansion\nof the ambient space and is trained by maximizing the log-probability in this\nextended space. This latter case leaves complete freedom in the choice of\nencoder, decoder and prior architectures, making it a drop-in replacement for\nthe training of existing VAEs and VAE-style models. We show that these AEFs can\nhave strikingly higher performance than architecturally identical VAEs in terms\nof log-likelihood and sample quality, especially for low dimensional latent\nspaces. Importantly, we show that AEF samples are substantially sharper than\nVAE samples.\n",
                "链接": "https://arxiv.org/abs/2205.09546"
            },
            {
                "文章ID": "79177",
                "标题": "G3Detector: General GPT-Generated Text Detector",
                "作者": " Haolan Zhan,  Xuanli He,  Qiongkai Xu,  Yuxiang Wu,  Pontus Stenetorp",
                "发布日期": "2023-08-07",
                "摘要": "  The burgeoning progress in the field of Large Language Models (LLMs) heralds\nsignificant benefits due to their unparalleled capacities. However, it is\ncritical to acknowledge the potential misuse of these models, which could give\nrise to a spectrum of social and ethical dilemmas. Despite numerous preceding\nefforts centered around distinguishing synthetic text, most existing detection\nsystems fail to identify data synthesized by the latest LLMs, such as ChatGPT\nand GPT-4. In response to this challenge, we introduce an unpretentious yet\npotent detection approach proficient in identifying synthetic text across a\nwide array of fields. Moreover, our detector demonstrates outstanding\nperformance uniformly across various model architectures and decoding\nstrategies. It also possesses the capability to identify text generated\nutilizing a potent detection-evasion technique. Our comprehensive research\nunderlines our commitment to boosting the robustness and efficiency of\nmachine-generated text detection mechanisms, particularly in the context of\nswiftly progressing and increasingly adaptive AI technologies.\n",
                "链接": "https://arxiv.org/abs/2305.12680"
            },
            {
                "文章ID": "87393",
                "标题": "Prompt to GPT-3: Step-by-Step Thinking Instructions for Humor Generation",
                "作者": " Yuetian Chen,  Bowen Shi,  Mei Si",
                "发布日期": "2023-06-26",
                "摘要": "  Artificial intelligence has made significant progress in natural language\nprocessing, with models like GPT-3 demonstrating impressive capabilities.\nHowever, these models still have limitations when it comes to complex tasks\nthat require an understanding of the user, such as mastering human comedy\nwriting strategies. This paper explores humor generation using GPT-3 by\nmodeling human comedy writing theory and leveraging step-by-step thinking\ninstructions. In addition, we explore the role of cognitive distance in\ncreating humor.\n",
                "链接": "https://arxiv.org/abs/2306.13195"
            },
            {
                "文章ID": "81391",
                "标题": "DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of\n  GPT-Generated Text",
                "作者": " Xianjun Yang,  Wei Cheng,  Yue Wu,  Linda Petzold,  William Yang Wang,  Haifeng Chen",
                "发布日期": "2023-10-05",
                "摘要": "  Large language models (LLMs) have notably enhanced the fluency and diversity\nof machine-generated text. However, this progress also presents a significant\nchallenge in detecting the origin of a given text, and current research on\ndetection methods lags behind the rapid evolution of LLMs. Conventional\ntraining-based methods have limitations in flexibility, particularly when\nadapting to new domains, and they often lack explanatory power. To address this\ngap, we propose a novel training-free detection strategy called Divergent\nN-Gram Analysis (DNA-GPT). Given a text, we first truncate it in the middle and\nthen use only the preceding portion as input to the LLMs to regenerate the new\nremaining parts. By analyzing the differences between the original and new\nremaining parts through N-gram analysis in black-box or probability divergence\nin white-box, we unveil significant discrepancies between the distribution of\nmachine-generated text and the distribution of human-written text. We conducted\nextensive experiments on the most advanced LLMs from OpenAI, including\ntext-davinci-003, GPT-3.5-turbo, and GPT-4, as well as open-source models such\nas GPT-NeoX-20B and LLaMa-13B. Results show that our zero-shot approach\nexhibits state-of-the-art performance in distinguishing between human and\nGPT-generated text on four English and one German dataset, outperforming\nOpenAI's own classifier, which is trained on millions of text. Additionally,\nour methods provide reasonable explanations and evidence to support our claim,\nwhich is a unique feature of explainable detection. Our method is also robust\nunder the revised text attack and can additionally solve model sourcing. Codes\nare available at https://github.com/Xianjun-Yang/DNA-GPT.\n",
                "链接": "https://arxiv.org/abs/2305.17359"
            },
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "114071",
                "标题": "Leveraging Generative AI: Improving Software Metadata Classification\n  with Generated Code-Comment Pairs",
                "作者": " Samah Syed,  Angel Deborah S",
                "发布日期": "2023-11-08",
                "摘要": "  In software development, code comments play a crucial role in enhancing code\ncomprehension and collaboration. This research paper addresses the challenge of\nobjectively classifying code comments as \"Useful\" or \"Not Useful.\" We propose a\nnovel solution that harnesses contextualized embeddings, particularly BERT, to\nautomate this classification process. We address this task by incorporating\ngenerated code and comment pairs. The initial dataset comprised 9048 pairs of\ncode and comments written in C, labeled as either Useful or Not Useful. To\naugment this dataset, we sourced an additional 739 lines of code-comment pairs\nand generated labels using a Large Language Model Architecture, specifically\nBERT. The primary objective was to build classification models that can\neffectively differentiate between useful and not useful code comments. Various\nmachine learning algorithms were employed, including Logistic Regression,\nDecision Tree, K-Nearest Neighbors (KNN), Support Vector Machine (SVM),\nGradient Boosting, Random Forest, and a Neural Network. Each algorithm was\nevaluated using precision, recall, and F1-score metrics, both with the original\nseed dataset and the augmented dataset. This study showcases the potential of\ngenerative AI for enhancing binary code comment quality classification models,\nproviding valuable insights for software developers and researchers in the\nfield of natural language processing and software engineering.\n",
                "链接": "https://arxiv.org/abs/2311.03365"
            },
            {
                "文章ID": "14202",
                "标题": "FoundationLayerNorm: Scaling BERT and GPT to 1,000 Layers",
                "作者": " Dezhou Shen",
                "发布日期": "2022-04-12",
                "摘要": "  The mainstream BERT/GPT model contains only 10 to 20 layers, and there is\nlittle literature to discuss the training of deep BERT/GPT. This paper proposes\na simple yet effective method to stabilize BERT and GPT training. We\nsuccessfully scale up BERT and GPT to 1,000 layers, which is an order of\nmagnitude deeper than previous BERT and GPT. The proposed method\nFoundationLayerNormalization enables efficient training of deep neural networks\nand is validated at the 1000-layer scale.\n",
                "链接": "https://arxiv.org/abs/2204.04477"
            },
            {
                "文章ID": "26262",
                "标题": "InfoAT: Improving Adversarial Training Using the Information Bottleneck\n  Principle",
                "作者": " Mengting Xu,  Tao Zhang,  Zhongnian Li,  Daoqiang Zhang",
                "发布日期": "2022-06-27",
                "摘要": "  Adversarial training (AT) has shown excellent high performance in defending\nagainst adversarial examples. Recent studies demonstrate that examples are not\nequally important to the final robustness of models during AT, that is, the\nso-called hard examples that can be attacked easily exhibit more influence than\nrobust examples on the final robustness. Therefore, guaranteeing the robustness\nof hard examples is crucial for improving the final robustness of the model.\nHowever, defining effective heuristics to search for hard examples is still\ndifficult. In this article, inspired by the information bottleneck (IB)\nprinciple, we uncover that an example with high mutual information of the input\nand its associated latent representation is more likely to be attacked. Based\non this observation, we propose a novel and effective adversarial training\nmethod (InfoAT). InfoAT is encouraged to find examples with high mutual\ninformation and exploit them efficiently to improve the final robustness of\nmodels. Experimental results show that InfoAT achieves the best robustness\namong different datasets and models in comparison with several state-of-the-art\nmethods.\n",
                "链接": "https://arxiv.org/abs/2206.12292"
            }
        ]
    },
    {
        "question": {
            "question": "请找到利用clip做开放词汇检测的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "104220",
                "标题": "CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic\n  Segmentation For-Free",
                "作者": " Monika Wysoczańska,  Michaël Ramamonjisoa,  Tomasz Trzciński,  Oriane Siméoni",
                "发布日期": "2023-11-29",
                "摘要": "  The emergence of CLIP has opened the way for open-world image perception. The\nzero-shot classification capabilities of the model are impressive but are\nharder to use for dense tasks such as image segmentation. Several methods have\nproposed different modifications and learning schemes to produce dense output.\nInstead, we propose in this work an open-vocabulary semantic segmentation\nmethod, dubbed CLIP-DIY, which does not require any additional training or\nannotations, but instead leverages existing unsupervised object localization\napproaches. In particular, CLIP-DIY is a multi-scale approach that directly\nexploits CLIP classification abilities on patches of different sizes and\naggregates the decision in a single map. We further guide the segmentation\nusing foreground/background scores obtained using unsupervised object\nlocalization methods. With our method, we obtain state-of-the-art zero-shot\nsemantic segmentation results on PASCAL VOC and perform on par with the best\nmethods on COCO. The code is available at\nhttp://github.com/wysoczanska/clip-diy\n",
                "链接": "https://arxiv.org/abs/2309.14289"
            },
            {
                "文章ID": "41351",
                "标题": "Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP",
                "作者": " Feng Liang,  Bichen Wu,  Xiaoliang Dai,  Kunpeng Li,  Yinan Zhao,  Hang Zhang,  Peizhao Zhang,  Peter Vajda,  Diana Marculescu",
                "发布日期": "2023-04-04",
                "摘要": "  Open-vocabulary semantic segmentation aims to segment an image into semantic\nregions according to text descriptions, which may not have been seen during\ntraining. Recent two-stage methods first generate class-agnostic mask proposals\nand then leverage pre-trained vision-language models, e.g., CLIP, to classify\nmasked regions. We identify the performance bottleneck of this paradigm to be\nthe pre-trained CLIP model, since it does not perform well on masked images. To\naddress this, we propose to finetune CLIP on a collection of masked image\nregions and their corresponding text descriptions. We collect training data by\nmining an existing image-caption dataset (e.g., COCO Captions), using CLIP to\nmatch masked image regions to nouns in the image captions. Compared with the\nmore precise and manually annotated segmentation labels with fixed classes\n(e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain\nCLIP's generalization ability. Along with finetuning the entire model, we\nutilize the \"blank\" areas in masked images using a method we dub mask prompt\ntuning. Experiments demonstrate mask prompt tuning brings significant\nimprovement without modifying any weights of CLIP, and it can further improve a\nfully finetuned model. In particular, when trained on COCO and evaluated on\nADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the\nprevious state-of-the-art. For the first time, open-vocabulary generalist\nmodels match the performance of supervised specialist models in 2017 without\ndataset-specific adaptations.\n",
                "链接": "https://arxiv.org/abs/2210.04150"
            },
            {
                "文章ID": "50058",
                "标题": "Open-vocabulary Attribute Detection",
                "作者": " María A. Bravo,  Sudhanshu Mittal,  Simon Ging,  Thomas Brox",
                "发布日期": "2023-03-10",
                "摘要": "  Vision-language modeling has enabled open-vocabulary tasks where predictions\ncan be queried using any text prompt in a zero-shot manner. Existing\nopen-vocabulary tasks focus on object classes, whereas research on object\nattributes is limited due to the lack of a reliable attribute-focused\nevaluation benchmark. This paper introduces the Open-Vocabulary Attribute\nDetection (OVAD) task and the corresponding OVAD benchmark. The objective of\nthe novel task and benchmark is to probe object-level attribute information\nlearned by vision-language models. To this end, we created a clean and densely\nannotated test set covering 117 attribute classes on the 80 object classes of\nMS COCO. It includes positive and negative annotations, which enables\nopen-vocabulary evaluation. Overall, the benchmark consists of 1.4 million\nannotations. For reference, we provide a first baseline method for\nopen-vocabulary attribute detection. Moreover, we demonstrate the benchmark's\nvalue by studying the attribute detection performance of several foundation\nmodels. Project page https://ovad-benchmark.github.io\n",
                "链接": "https://arxiv.org/abs/2211.12914"
            },
            {
                "文章ID": "68353",
                "标题": "CORA: Adapting CLIP for Open-Vocabulary Detection with Region Prompting\n  and Anchor Pre-Matching",
                "作者": " Xiaoshi Wu,  Feng Zhu,  Rui Zhao,  Hongsheng Li",
                "发布日期": "2023-03-24",
                "摘要": "  Open-vocabulary detection (OVD) is an object detection task aiming at\ndetecting objects from novel categories beyond the base categories on which the\ndetector is trained. Recent OVD methods rely on large-scale visual-language\npre-trained models, such as CLIP, for recognizing novel objects. We identify\nthe two core obstacles that need to be tackled when incorporating these models\ninto detector training: (1) the distribution mismatch that happens when\napplying a VL-model trained on whole images to region recognition tasks; (2)\nthe difficulty of localizing objects of unseen classes. To overcome these\nobstacles, we propose CORA, a DETR-style framework that adapts CLIP for\nOpen-vocabulary detection by Region prompting and Anchor pre-matching. Region\nprompting mitigates the whole-to-region distribution gap by prompting the\nregion features of the CLIP-based region classifier. Anchor pre-matching helps\nlearning generalizable object localization by a class-aware matching mechanism.\nWe evaluate CORA on the COCO OVD benchmark, where we achieve 41.7 AP50 on novel\nclasses, which outperforms the previous SOTA by 2.4 AP50 even without resorting\nto extra training data. When extra training data is available, we train\nCORA$^+$ on both ground-truth base-category annotations and additional pseudo\nbounding box labels computed by CORA. CORA$^+$ achieves 43.1 AP50 on the COCO\nOVD benchmark and 28.1 box APr on the LVIS OVD benchmark.\n",
                "链接": "https://arxiv.org/abs/2303.13076"
            },
            {
                "文章ID": "12649",
                "标题": "PromptDet: Towards Open-vocabulary Detection using Uncurated Images",
                "作者": " Chengjian Feng,  Yujie Zhong,  Zequn Jie,  Xiangxiang Chu,  Haibing Ren,  Xiaolin Wei,  Weidi Xie,  Lin Ma",
                "发布日期": "2022-07-19",
                "摘要": "  The goal of this work is to establish a scalable pipeline for expanding an\nobject detector towards novel/unseen categories, using zero manual annotations.\nTo achieve that, we make the following four contributions: (i) in pursuit of\ngeneralisation, we propose a two-stage open-vocabulary object detector, where\nthe class-agnostic object proposals are classified with a text encoder from\npre-trained visual-language model; (ii) To pair the visual latent space (of RPN\nbox proposals) with that of the pre-trained text encoder, we propose the idea\nof regional prompt learning to align the textual embedding space with regional\nvisual object features; (iii) To scale up the learning procedure towards\ndetecting a wider spectrum of objects, we exploit the available online resource\nvia a novel self-training framework, which allows to train the proposed\ndetector on a large corpus of noisy uncurated web images. Lastly, (iv) to\nevaluate our proposed detector, termed as PromptDet, we conduct extensive\nexperiments on the challenging LVIS and MS-COCO dataset. PromptDet shows\nsuperior performance over existing approaches with fewer additional training\nimages and zero manual annotations whatsoever. Project page with code:\nhttps://fcjian.github.io/promptdet.\n",
                "链接": "https://arxiv.org/abs/2203.16513"
            },
            {
                "文章ID": "68341",
                "标题": "Open-Vocabulary Object Detection using Pseudo Caption Labels",
                "作者": " Han-Cheol Cho,  Won Young Jhoo,  Wooyoung Kang,  Byungseok Roh",
                "发布日期": "2023-03-24",
                "摘要": "  Recent open-vocabulary detection methods aim to detect novel objects by\ndistilling knowledge from vision-language models (VLMs) trained on a vast\namount of image-text pairs. To improve the effectiveness of these methods,\nresearchers have utilized datasets with a large vocabulary that contains a\nlarge number of object classes, under the assumption that such data will enable\nmodels to extract comprehensive knowledge on the relationships between various\nobjects and better generalize to unseen object classes. In this study, we argue\nthat more fine-grained labels are necessary to extract richer knowledge about\nnovel objects, including object attributes and relationships, in addition to\ntheir names. To address this challenge, we propose a simple and effective\nmethod named Pseudo Caption Labeling (PCL), which utilizes an image captioning\nmodel to generate captions that describe object instances from diverse\nperspectives. The resulting pseudo caption labels offer dense samples for\nknowledge distillation. On the LVIS benchmark, our best model trained on the\nde-duplicated VisualGenome dataset achieves an AP of 34.5 and an APr of 30.6,\ncomparable to the state-of-the-art performance. PCL's simplicity and\nflexibility are other notable features, as it is a straightforward\npre-processing technique that can be used with any image captioning model\nwithout imposing any restrictions on model architecture or training process.\n",
                "链接": "https://arxiv.org/abs/2303.13040"
            },
            {
                "文章ID": "116861",
                "标题": "CastDet: Toward Open Vocabulary Aerial Object Detection with\n  CLIP-Activated Student-Teacher Learning",
                "作者": " Yan Li,  Weiwei Guo,  Dunyun He,  Jiaqi Zhou,  Yuze Gao,  Wenxian Yu",
                "发布日期": "2023-11-21",
                "摘要": "  Object detection in aerial images is a pivotal task for various earth\nobservation applications, whereas current algorithms learn to detect only a\npre-defined set of object categories demanding sufficient bounding-box\nannotated training samples and fail to detect novel object categories. In this\npaper, we consider open-vocabulary object detection (OVD) in aerial images that\nenables the characterization of new objects beyond training categories on the\nearth surface without annotating training images for these new categories. The\nperformance of OVD depends on the quality of class-agnostic region proposals\nand pseudo-labels that can generalize well to novel object categories. To\nsimultaneously generate high-quality proposals and pseudo-labels, we propose\nCastDet, a CLIP-activated student-teacher open-vocabulary object Detection\nframework. Our end-to-end framework within the student-teacher mechanism\nemploys the CLIP model as an extra omniscient teacher of rich knowledge into\nthe student-teacher self-learning process. By doing so, our approach boosts\nnovel object proposals and classification. Furthermore, we design a dynamic\nlabel queue technique to maintain high-quality pseudo labels during batch\ntraining and mitigate label imbalance. We conduct extensive experiments on\nmultiple existing aerial object detection datasets, which are set up for the\nOVD task. Experimental results demonstrate our CastDet achieving superior\nopen-vocabulary detection performance, e.g., reaching 40.0 HM (Harmonic Mean),\nwhich outperforms previous methods Detic/ViLD by 26.9/21.1 on the VisDroneZSD\ndataset.\n",
                "链接": "https://arxiv.org/abs/2311.11646"
            },
            {
                "文章ID": "71939",
                "标题": "CLIP Surgery for Better Explainability with Enhancement in\n  Open-Vocabulary Tasks",
                "作者": " Yi Li,  Hualiang Wang,  Yiqun Duan,  Xiaomeng Li",
                "发布日期": "2023-04-13",
                "摘要": "  Contrastive Language-Image Pre-training (CLIP) is a powerful multimodal large\nvision model that has demonstrated significant benefits for downstream tasks,\nincluding many zero-shot learning and text-guided vision tasks. However, we\nnotice some severe problems regarding the model's explainability, which\nundermines its credibility and impedes related tasks. Specifically, we find\nCLIP prefers the background regions than the foregrounds according to the\npredicted similarity map, which contradicts human understanding. Besides, there\nare obvious noisy activations on the visualization results at irrelevant\npositions. To address these two issues, we conduct in-depth analyses and reveal\nthe reasons with new findings and evidences. Based on these insights, we\npropose the CLIP Surgery, a method that enables surgery-like modifications for\nthe inference architecture and features, for better explainability and\nenhancement in multiple open-vocabulary tasks. The proposed method has\nsignificantly improved the explainability of CLIP for both convolutional\nnetworks and vision transformers, surpassing existing methods by large margins.\nBesides, our approach also demonstrates remarkable improvements in\nopen-vocabulary segmentation and multi-label recognition tasks. For examples,\nthe mAP improvement on NUS-Wide multi-label recognition is 4.41% without any\nadditional training, and our CLIP Surgery surpasses the state-of-the-art method\nby 8.74% at mIoU on Cityscapes open-vocabulary semantic segmentation.\nFurthermore, our method benefits other tasks including multimodal visualization\nand interactive segmentation like Segment Anything Model (SAM). The code is\navailable at https://github.com/xmed-lab/CLIP_Surgery\n",
                "链接": "https://arxiv.org/abs/2304.05653"
            },
            {
                "文章ID": "86253",
                "标题": "Scaling Open-Vocabulary Object Detection",
                "作者": " Matthias Minderer,  Alexey Gritsenko,  Neil Houlsby",
                "发布日期": "2023-07-21",
                "摘要": "  Open-vocabulary object detection has benefited greatly from pretrained\nvision-language models, but is still limited by the amount of available\ndetection training data. While detection training data can be expanded by using\nWeb image-text pairs as weak supervision, this has not been done at scales\ncomparable to image-level pretraining. Here, we scale up detection data with\nself-training, which uses an existing detector to generate pseudo-box\nannotations on image-text pairs. Major challenges in scaling self-training are\nthe choice of label space, pseudo-annotation filtering, and training\nefficiency. We present the OWLv2 model and OWL-ST self-training recipe, which\naddress these challenges. OWLv2 surpasses the performance of previous\nstate-of-the-art open-vocabulary detectors already at comparable training\nscales (~10M examples). However, with OWL-ST, we can scale to over 1B examples,\nyielding further large improvement: With an L/14 architecture, OWL-ST improves\nAP on LVIS rare classes, for which the model has seen no human box annotations,\nfrom 31.2% to 44.6% (43% relative improvement). OWL-ST unlocks Web-scale\ntraining for open-world localization, similar to what has been seen for image\nclassification and language modelling.\n",
                "链接": "https://arxiv.org/abs/2306.09683"
            },
            {
                "文章ID": "115237",
                "标题": "Open-Vocabulary Video Anomaly Detection",
                "作者": " Peng Wu,  Xuerong Zhou,  Guansong Pang,  Yujia Sun,  Jing Liu,  Peng Wang,  Yanning Zhang",
                "发布日期": "2023-11-16",
                "摘要": "  Video anomaly detection (VAD) with weak supervision has achieved remarkable\nperformance in utilizing video-level labels to discriminate whether a video\nframe is normal or abnormal. However, current approaches are inherently limited\nto a closed-set setting and may struggle in open-world applications where there\ncan be anomaly categories in the test data unseen during training. A few recent\nstudies attempt to tackle a more realistic setting, open-set VAD, which aims to\ndetect unseen anomalies given seen anomalies and normal videos. However, such a\nsetting focuses on predicting frame anomaly scores, having no ability to\nrecognize the specific categories of anomalies, despite the fact that this\nability is essential for building more informed video surveillance systems.\nThis paper takes a step further and explores open-vocabulary video anomaly\ndetection (OVVAD), in which we aim to leverage pre-trained large models to\ndetect and categorize seen and unseen anomalies. To this end, we propose a\nmodel that decouples OVVAD into two mutually complementary tasks --\nclass-agnostic detection and class-specific classification -- and jointly\noptimizes both tasks. Particularly, we devise a semantic knowledge injection\nmodule to introduce semantic knowledge from large language models for the\ndetection task, and design a novel anomaly synthesis module to generate pseudo\nunseen anomaly videos with the help of large vision generation models for the\nclassification task. These semantic knowledge and synthesis anomalies\nsubstantially extend our model's capability in detecting and categorizing a\nvariety of seen and unseen anomalies. Extensive experiments on three\nwidely-used benchmarks demonstrate our model achieves state-of-the-art\nperformance on OVVAD task.\n",
                "链接": "https://arxiv.org/abs/2311.07042"
            }
        ]
    },
    {
        "question": {
            "question": "请找到使用自蒸馏加强目标检测性能的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "14912",
                "标题": "Spatial Likelihood Voting with Self-Knowledge Distillation for Weakly\n  Supervised Object Detection",
                "作者": " Ze Chen,  Zhihang Fu,  Jianqiang Huang,  Mingyuan Tao,  Rongxin Jiang,  Xiang Tian,  Yaowu Chen,  Xian-sheng Hua",
                "发布日期": "2022-04-15",
                "摘要": "  Weakly supervised object detection (WSOD), which is an effective way to train\nan object detection model using only image-level annotations, has attracted\nconsiderable attention from researchers. However, most of the existing methods,\nwhich are based on multiple instance learning (MIL), tend to localize instances\nto the discriminative parts of salient objects instead of the entire content of\nall objects. In this paper, we propose a WSOD framework called the Spatial\nLikelihood Voting with Self-knowledge Distillation Network (SLV-SD Net). In\nthis framework, we introduce a spatial likelihood voting (SLV) module to\nconverge region proposal localization without bounding box annotations.\nSpecifically, in every iteration during training, all the region proposals in a\ngiven image act as voters voting for the likelihood of each category in the\nspatial dimensions. After dilating the alignment on the area with large\nlikelihood values, the voting results are regularized as bounding boxes, which\nare then used for the final classification and localization. Based on SLV, we\nfurther propose a self-knowledge distillation (SD) module to refine the feature\nrepresentations of the given image. The likelihood maps generated by the SLV\nmodule are used to supervise the feature learning of the backbone network,\nencouraging the network to attend to wider and more diverse areas of the image.\nExtensive experiments on the PASCAL VOC 2007/2012 and MS-COCO datasets\ndemonstrate the excellent performance of SLV-SD Net. In addition, SLV-SD Net\nproduces new state-of-the-art results on these benchmarks.\n",
                "链接": "https://arxiv.org/abs/2204.06899"
            },
            {
                "文章ID": "65588",
                "标题": "Smooth and Stepwise Self-Distillation for Object Detection",
                "作者": " Jieren Deng,  Xin Zhou,  Hao Tian,  Zhihong Pan,  Derek Aguiar",
                "发布日期": "2023-03-10",
                "摘要": "  Distilling the structured information captured in feature maps has\ncontributed to improved results for object detection tasks, but requires\ncareful selection of baseline architectures and substantial pre-training.\nSelf-distillation addresses these limitations and has recently achieved\nstate-of-the-art performance for object detection despite making several\nsimplifying architectural assumptions. Building on this work, we propose Smooth\nand Stepwise Self-Distillation (SSSD) for object detection. Our SSSD\narchitecture forms an implicit teacher from object labels and a feature pyramid\nnetwork backbone to distill label-annotated feature maps using Jensen-Shannon\ndistance, which is smoother than distillation losses used in prior work. We\nadditionally add a distillation coefficient that is adaptively configured based\non the learning rate. We extensively benchmark SSSD against a baseline and two\nstate-of-the-art object detector architectures on the COCO dataset by varying\nthe coefficients and backbone and detector networks. We demonstrate that SSSD\nachieves higher average precision in most experimental settings, is robust to a\nwide range of coefficients, and benefits from our stepwise distillation\nprocedure.\n",
                "链接": "https://arxiv.org/abs/2303.05015"
            },
            {
                "文章ID": "11255",
                "标题": "Scale-Equivalent Distillation for Semi-Supervised Object Detection",
                "作者": " Qiushan Guo,  Yao Mu,  Jianyu Chen,  Tianqi Wang,  Yizhou Yu,  Ping Luo",
                "发布日期": "2022-03-29",
                "摘要": "  Recent Semi-Supervised Object Detection (SS-OD) methods are mainly based on\nself-training, i.e., generating hard pseudo-labels by a teacher model on\nunlabeled data as supervisory signals. Although they achieved certain success,\nthe limited labeled data in semi-supervised learning scales up the challenges\nof object detection. We analyze the challenges these methods meet with the\nempirical experiment results. We find that the massive False Negative samples\nand inferior localization precision lack consideration. Besides, the large\nvariance of object sizes and class imbalance (i.e., the extreme ratio between\nbackground and object) hinder the performance of prior arts. Further, we\novercome these challenges by introducing a novel approach, Scale-Equivalent\nDistillation (SED), which is a simple yet effective end-to-end knowledge\ndistillation framework robust to large object size variance and class\nimbalance. SED has several appealing benefits compared to the previous works.\n(1) SED imposes a consistency regularization to handle the large scale variance\nproblem. (2) SED alleviates the noise problem from the False Negative samples\nand inferior localization precision. (3) A re-weighting strategy can implicitly\nscreen the potential foreground regions of the unlabeled data to reduce the\neffect of class imbalance. Extensive experiments show that SED consistently\noutperforms the recent state-of-the-art methods on different datasets with\nsignificant margins. For example, it surpasses the supervised counterpart by\nmore than 10 mAP when using 5% and 10% labeled data on MS-COCO.\n",
                "链接": "https://arxiv.org/abs/2203.12244"
            },
            {
                "文章ID": "1542",
                "标题": "Self-Supervised Anomaly Detection by Self-Distillation and Negative\n  Sampling",
                "作者": " Nima Rafiee,  Rahil Gholamipoorfard,  Nikolas Adaloglou,  Simon Jaxy,  Julius Ramakers,  Markus Kollmann",
                "发布日期": "2022-01-19",
                "摘要": "  Detecting whether examples belong to a given in-distribution or are\nOut-Of-Distribution (OOD) requires identifying features specific to the\nin-distribution. In the absence of labels, these features can be learned by\nself-supervised techniques under the generic assumption that the most abstract\nfeatures are those which are statistically most over-represented in comparison\nto other distributions from the same domain. In this work, we show that\nself-distillation of the in-distribution training set together with contrasting\nagainst negative examples derived from shifting transformation of auxiliary\ndata strongly improves OOD detection. We find that this improvement depends on\nhow the negative samples are generated. In particular, we observe that by\nleveraging negative samples, which keep the statistics of low-level features\nwhile changing the high-level semantics, higher average detection performance\nis obtained. Furthermore, good negative sampling strategies can be identified\nfrom the sensitivity of the OOD detection score. The efficiency of our approach\nis demonstrated across a diverse range of OOD detection problems, setting new\nbenchmarks for unsupervised OOD detection in the visual domain.\n",
                "链接": "https://arxiv.org/abs/2201.06378"
            },
            {
                "文章ID": "28453",
                "标题": "A Study on Self-Supervised Object Detection Pretraining",
                "作者": " Trung Dang,  Simon Kornblith,  Huy Thong Nguyen,  Peter Chin,  Maryam Khademi",
                "发布日期": "2022-08-12",
                "摘要": "  In this work, we study different approaches to self-supervised pretraining of\nobject detection models. We first design a general framework to learn a\nspatially consistent dense representation from an image, by randomly sampling\nand projecting boxes to each augmented view and maximizing the similarity\nbetween corresponding box features. We study existing design choices in the\nliterature, such as box generation, feature extraction strategies, and using\nmultiple views inspired by its success on instance-level image representation\nlearning techniques. Our results suggest that the method is robust to different\nchoices of hyperparameters, and using multiple views is not as effective as\nshown for instance-level image representation learning. We also design two\nauxiliary tasks to predict boxes in one view from their features in the other\nview, by (1) predicting boxes from the sampled set by using a contrastive loss,\nand (2) predicting box coordinates using a transformer, which potentially\nbenefits downstream object detection tasks. We found that these tasks do not\nlead to better object detection performance when finetuning the pretrained\nmodel on labeled data.\n",
                "链接": "https://arxiv.org/abs/2207.04186"
            },
            {
                "文章ID": "92560",
                "标题": "Spatial Self-Distillation for Object Detection with Inaccurate Bounding\n  Boxes",
                "作者": " Di Wu,  Pengfei Chen,  Xuehui Yu,  Guorong Li,  Zhenjun Han,  Jianbin Jiao",
                "发布日期": "2023-08-16",
                "摘要": "  Object detection via inaccurate bounding boxes supervision has boosted a\nbroad interest due to the expensive high-quality annotation data or the\noccasional inevitability of low annotation quality (\\eg tiny objects). The\nprevious works usually utilize multiple instance learning (MIL), which highly\ndepends on category information, to select and refine a low-quality box. Those\nmethods suffer from object drift, group prediction and part domination problems\nwithout exploring spatial information. In this paper, we heuristically propose\na \\textbf{Spatial Self-Distillation based Object Detector (SSD-Det)} to mine\nspatial information to refine the inaccurate box in a self-distillation\nfashion. SSD-Det utilizes a Spatial Position Self-Distillation \\textbf{(SPSD)}\nmodule to exploit spatial information and an interactive structure to combine\nspatial information and category information, thus constructing a high-quality\nproposal bag. To further improve the selection procedure, a Spatial Identity\nSelf-Distillation \\textbf{(SISD)} module is introduced in SSD-Det to obtain\nspatial confidence to help select the best proposals. Experiments on MS-COCO\nand VOC datasets with noisy box annotation verify our method's effectiveness\nand achieve state-of-the-art performance. The code is available at\nhttps://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det.\n",
                "链接": "https://arxiv.org/abs/2307.12101"
            },
            {
                "文章ID": "38108",
                "标题": "Self-supervised 3D Object Detection from Monocular Pseudo-LiDAR",
                "作者": " Curie Kim,  Ue-Hwan Kim,  Jong-Hwan Kim",
                "发布日期": "2022-09-21",
                "摘要": "  There have been attempts to detect 3D objects by fusion of stereo camera\nimages and LiDAR sensor data or using LiDAR for pre-training and only monocular\nimages for testing, but there have been less attempts to use only monocular\nimage sequences due to low accuracy. In addition, when depth prediction using\nonly monocular images, only scale-inconsistent depth can be predicted, which is\nthe reason why researchers are reluctant to use monocular images alone.\nTherefore, we propose a method for predicting absolute depth and detecting 3D\nobjects using only monocular image sequences by enabling end-to-end learning of\ndetection networks and depth prediction networks. As a result, the proposed\nmethod surpasses other existing methods in performance on the KITTI 3D dataset.\nEven when monocular image and 3D LiDAR are used together during training in an\nattempt to improve performance, ours exhibit is the best performance compared\nto other methods using the same input. In addition, end-to-end learning not\nonly improves depth prediction performance, but also enables absolute depth\nprediction, because our network utilizes the fact that the size of a 3D object\nsuch as a car is determined by the approximate size.\n",
                "链接": "https://arxiv.org/abs/2209.09486"
            },
            {
                "文章ID": "96261",
                "标题": "PatchContrast: Self-Supervised Pre-training for 3D Object Detection",
                "作者": " Oren Shrout,  Ori Nitzan,  Yizhak Ben-Shabat,  Ayellet Tal",
                "发布日期": "2023-08-15",
                "摘要": "  Accurately detecting objects in the environment is a key challenge for\nautonomous vehicles. However, obtaining annotated data for detection is\nexpensive and time-consuming. We introduce PatchContrast, a novel\nself-supervised point cloud pre-training framework for 3D object detection. We\npropose to utilize two levels of abstraction to learn discriminative\nrepresentation from unlabeled data: proposal-level and patch-level. The\nproposal-level aims at localizing objects in relation to their surroundings,\nwhereas the patch-level adds information about the internal connections between\nthe object's components, hence distinguishing between different objects based\non their individual components. We demonstrate how these levels can be\nintegrated into self-supervised pre-training for various backbones to enhance\nthe downstream 3D detection task. We show that our method outperforms existing\nstate-of-the-art models on three commonly-used 3D detection datasets.\n",
                "链接": "https://arxiv.org/abs/2308.06985"
            },
            {
                "文章ID": "14674",
                "标题": "Localization Distillation for Object Detection",
                "作者": " Zhaohui Zheng,  Rongguang Ye,  Qibin Hou,  Dongwei Ren,  Ping Wang,  Wangmeng Zuo,  Ming-Ming Cheng",
                "发布日期": "2022-12-09",
                "摘要": "  Previous knowledge distillation (KD) methods for object detection mostly\nfocus on feature imitation instead of mimicking the prediction logits due to\nits inefficiency in distilling the localization information. In this paper, we\ninvestigate whether logit mimicking always lags behind feature imitation.\nTowards this goal, we first present a novel localization distillation (LD)\nmethod which can efficiently transfer the localization knowledge from the\nteacher to the student. Second, we introduce the concept of valuable\nlocalization region that can aid to selectively distill the classification and\nlocalization knowledge for a certain region. Combining these two new\ncomponents, for the first time, we show that logit mimicking can outperform\nfeature imitation and the absence of localization distillation is a critical\nreason for why logit mimicking underperforms for years. The thorough studies\nexhibit the great potential of logit mimicking that can significantly alleviate\nthe localization ambiguity, learn robust feature representation, and ease the\ntraining difficulty in the early stage. We also provide the theoretical\nconnection between the proposed LD and the classification KD, that they share\nthe equivalent optimization effect. Our distillation scheme is simple as well\nas effective and can be easily applied to both dense horizontal object\ndetectors and rotated object detectors. Extensive experiments on the MS COCO,\nPASCAL VOC, and DOTA benchmarks demonstrate that our method can achieve\nconsiderable AP improvement without any sacrifice on the inference speed. Our\nsource code and pretrained models are publicly available at\nhttps://github.com/HikariTJU/LD.\n",
                "链接": "https://arxiv.org/abs/2204.05957"
            },
            {
                "文章ID": "26404",
                "标题": "Self-Supervised 3D Monocular Object Detection by Recycling Bounding\n  Boxes",
                "作者": " Sugirtha T,  Sridevi M,  Khailash Santhakumar,  Hao Liu,  B Ravi Kiran,  Thomas Gauthier,  Senthil Yogamani",
                "发布日期": "2022-06-28",
                "摘要": "  Modern object detection architectures are moving towards employing\nself-supervised learning (SSL) to improve performance detection with related\npretext tasks. Pretext tasks for monocular 3D object detection have not yet\nbeen explored yet in literature. The paper studies the application of\nestablished self-supervised bounding box recycling by labeling random windows\nas the pretext task. The classifier head of the 3D detector is trained to\nclassify random windows containing different proportions of the ground truth\nobjects, thus handling the foreground-background imbalance. We evaluate the\npretext task using the RTM3D detection model as baseline, with and without the\napplication of data augmentation. We demonstrate improvements of between 2-3 %\nin mAP 3D and 0.9-1.5 % BEV scores using SSL over the baseline scores. We\npropose the inverse class frequency re-weighted (ICFW) mAP score that\nhighlights improvements in detection for low frequency classes in a class\nimbalanced dataset with long tails. We demonstrate improvements in ICFW both\nmAP 3D and BEV scores to take into account the class imbalance in the KITTI\nvalidation dataset. We see 4-5 % increase in ICFW metric with the pretext task.\n",
                "链接": "https://arxiv.org/abs/2206.12738"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下使用2020年以后CONLL 2004数据集进行NER评测的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "54351",
                "标题": "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?",
                "作者": " Shuheng Liu,  Alan Ritter",
                "发布日期": "2023-07-13",
                "摘要": "  The CoNLL-2003 English named entity recognition (NER) dataset has been widely\nused to train and evaluate NER models for almost 20 years. However, it is\nunclear how well models that are trained on this 20-year-old data and developed\nover a period of decades using the same test set will perform when applied on\nmodern data. In this paper, we evaluate the generalization of over 20 different\nmodels trained on CoNLL-2003, and show that NER models have very different\ngeneralization. Surprisingly, we find no evidence of performance degradation in\npre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using\ndecades-old data. We investigate why some models generalize well to new data\nwhile others do not, and attempt to disentangle the effects of temporal drift\nand overfitting due to test reuse. Our analysis suggests that most\ndeterioration is due to temporal mismatch between the pre-training corpora and\nthe downstream test sets. We found that four factors are important for good\ngeneralization: model architecture, number of parameters, time period of the\npre-training corpus, in addition to the amount of fine-tuning data. We suggest\ncurrent evaluation methods have, in some sense, underestimated progress on NER\nover the past 20 years, as NER models have not only improved on the original\nCoNLL-2003 test set, but improved even more on modern data. Our datasets can be\nfound at https://github.com/ShuhengL/acl2023_conllpp.\n",
                "链接": "https://arxiv.org/abs/2212.09747"
            },
            {
                "文章ID": "50504",
                "标题": "Finetuning BERT on Partially Annotated NER Corpora",
                "作者": " Viktor Scherbakov,  Vladimir Mayorov",
                "发布日期": "2022-11-29",
                "摘要": "  Most Named Entity Recognition (NER) models operate under the assumption that\ntraining datasets are fully labelled. While it is valid for established\ndatasets like CoNLL 2003 and OntoNotes, sometimes it is not feasible to obtain\nthe complete dataset annotation. These situations may occur, for instance,\nafter selective annotation of entities for cost reduction. This work presents\nan approach to finetuning BERT on such partially labelled datasets using\nself-supervision and label preprocessing. Our approach outperforms the previous\nLSTM-based label preprocessing baseline, significantly improving the\nperformance on poorly labelled datasets. We demonstrate that following our\napproach while finetuning RoBERTa on CoNLL 2003 dataset with only 10% of total\nentities labelled is enough to reach the performance of the baseline trained on\nthe same dataset with 50% of the entities labelled.\n",
                "链接": "https://arxiv.org/abs/2211.14360"
            },
            {
                "文章ID": "12159",
                "标题": "Federated Named Entity Recognition",
                "作者": " Joel Mathew,  Dimitris Stripelis,  José Luis Ambite",
                "发布日期": "2022-03-30",
                "摘要": "  We present an analysis of the performance of Federated Learning in a\nparadigmatic natural-language processing task: Named-Entity Recognition (NER).\nFor our evaluation, we use the language-independent CoNLL-2003 dataset as our\nbenchmark dataset and a Bi-LSTM-CRF model as our benchmark NER model. We show\nthat federated training reaches almost the same performance as the centralized\nmodel, though with some performance degradation as the learning environments\nbecome more heterogeneous. We also show the convergence rate of federated\nmodels for NER. Finally, we discuss existing challenges of Federated Learning\nfor NLP applications that can foster future research directions.\n",
                "链接": "https://arxiv.org/abs/2203.15101"
            },
            {
                "文章ID": "10782",
                "标题": "Leveraging Expert Guided Adversarial Augmentation For Improving\n  Generalization in Named Entity Recognition",
                "作者": " Aaron Reich,  Jiaao Chen,  Aastha Agrawal,  Yanzhe Zhang,  Diyi Yang",
                "发布日期": "2022-03-22",
                "摘要": "  Named Entity Recognition (NER) systems often demonstrate great performance on\nin-distribution data, but perform poorly on examples drawn from a shifted\ndistribution. One way to evaluate the generalization ability of NER models is\nto use adversarial examples, on which the specific variations associated with\nnamed entities are rarely considered. To this end, we propose leveraging\nexpert-guided heuristics to change the entity tokens and their surrounding\ncontexts thereby altering their entity types as adversarial attacks. Using\nexpert-guided heuristics, we augmented the CoNLL 2003 test set and manually\nannotated it to construct a high-quality challenging set. We found that\nstate-of-the-art NER systems trained on CoNLL 2003 training data drop\nperformance dramatically on our challenging set. By training on adversarial\naugmented training examples and using mixup for regularization, we were able to\nsignificantly improve the performance on the challenging set as well as improve\nout-of-domain generalization which we evaluated by using OntoNotes data. We\nhave publicly released our dataset and code at\nhttps://github.com/GT-SALT/Guided-Adversarial-Augmentation.\n",
                "链接": "https://arxiv.org/abs/2203.10693"
            },
            {
                "文章ID": "101950",
                "标题": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic\n  Classification in 200+ Languages and Dialects",
                "作者": " David Ifeoluwa Adelani,  Hannah Liu,  Xiaoyu Shen,  Nikita Vassilyev,  Jesujoba O. Alabi,  Yanke Mao,  Haonan Gao,  Annie En-Shiun Lee",
                "发布日期": "2023-09-15",
                "摘要": "  Despite the progress we have recorded in the last few years in multilingual\nnatural language processing, evaluation is typically limited to a small set of\nlanguages with available datasets which excludes a large number of low-resource\nlanguages. In this paper, we created SIB-200 -- a large-scale open-sourced\nbenchmark dataset for topic classification in 200 languages and dialects to\naddress the lack of evaluation dataset for Natural Language Understanding\n(NLU). For many of the languages covered in SIB-200, this is the first publicly\navailable evaluation dataset for NLU. The dataset is based on Flores-200\nmachine translation corpus. We annotated the English portion of the dataset and\nextended the sentence-level annotation to the remaining 203 languages covered\nin the corpus. Despite the simplicity of this task, our evaluation in\nfull-supervised setting, cross-lingual transfer setting and prompting of large\nlanguage model setting show that there is still a large gap between the\nperformance of high-resource and low-resource languages when multilingual\nevaluation is scaled to numerous world languages. We found that languages\nunseen during the pre-training of multilingual language models,\nunder-represented language families (like Nilotic and Altantic-Congo), and\nlanguages from the regions of Africa, Americas, Oceania and South East Asia,\noften have the lowest performance on our topic classification dataset. We hope\nour dataset will encourage a more inclusive evaluation of multilingual language\nmodels on a more diverse set of languages. https://github.com/dadelani/sib-200\n",
                "链接": "https://arxiv.org/abs/2309.07445"
            },
            {
                "文章ID": "46217",
                "标题": "Recognizing Nested Entities from Flat Supervision: A New NER Subtask,\n  Feasibility and Challenges",
                "作者": " Enwei Zhu,  Yiyang Liu,  Ming Jin,  Jinpeng Li",
                "发布日期": "2022-11-02",
                "摘要": "  Many recent named entity recognition (NER) studies criticize flat NER for its\nnon-overlapping assumption, and switch to investigating nested NER. However,\nexisting nested NER models heavily rely on training data annotated with nested\nentities, while labeling such data is costly. This study proposes a new\nsubtask, nested-from-flat NER, which corresponds to a realistic application\nscenario: given data annotated with flat entities only, one may still desire\nthe trained model capable of recognizing nested entities. To address this task,\nwe train span-based models and deliberately ignore the spans nested inside\nlabeled entities, since these spans are possibly unlabeled entities. With\nnested entities removed from the training data, our model achieves 54.8%, 54.2%\nand 41.1% F1 scores on the subset of spans within entities on ACE 2004, ACE\n2005 and GENIA, respectively. This suggests the effectiveness of our approach\nand the feasibility of the task. In addition, the model's performance on flat\nentities is entirely unaffected. We further manually annotate the nested\nentities in the test set of CoNLL 2003, creating a nested-from-flat NER\nbenchmark. Analysis results show that the main challenges stem from the data\nand annotation inconsistencies between the flat and nested entities.\n",
                "链接": "https://arxiv.org/abs/2211.00301"
            },
            {
                "文章ID": "76988",
                "标题": "Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker\n  Selection and Data Augmentation",
                "作者": " Yujie Wang,  Chao Huang,  Liner Yang,  Zhixuan Fang,  Yaping Huang,  Yang Liu,  Erhong Yang",
                "发布日期": "2023-05-12",
                "摘要": "  This paper introduces a novel worker selection algorithm, enhancing\nannotation quality and reducing costs in challenging span-based sequence\nlabeling tasks in Natural Language Processing (NLP). Unlike previous studies\ntargeting simpler tasks, this study contends with the complexities of label\ninterdependencies in sequence labeling tasks. The proposed algorithm utilizes a\nCombinatorial Multi-Armed Bandit (CMAB) approach for worker selection. The\nchallenge of dealing with imbalanced and small-scale datasets, which hinders\noffline simulation of worker selection, is tackled using an innovative data\naugmentation method termed shifting, expanding, and shrinking (SES). The SES\nmethod is designed specifically for sequence labeling tasks. Rigorous testing\non CoNLL 2003 NER and Chinese OEI datasets showcased the algorithm's\nefficiency, with an increase in F1 score up to 100.04% of the expert-only\nbaseline, alongside cost savings up to 65.97%. The paper also encompasses a\ndataset-independent test emulating annotation evaluation through a Bernoulli\ndistribution, which still led to an impressive 97.56% F1 score of the expert\nbaseline and 59.88% cost savings. This research addresses and overcomes\nnumerous obstacles in worker selection for complex NLP tasks.\n",
                "链接": "https://arxiv.org/abs/2305.06683"
            },
            {
                "文章ID": "73488",
                "标题": "IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named\n  Entity Recognition using Knowledge Bases",
                "作者": " Iker García-Ferrero,  Jon Ander Campos,  Oscar Sainz,  Ander Salaberria,  Dan Roth",
                "发布日期": "2023-05-01",
                "摘要": "  Named Entity Recognition (NER) is a core natural language processing task in\nwhich pre-trained language models have shown remarkable performance. However,\nstandard benchmarks like CoNLL 2003 do not address many of the challenges that\ndeployed NER systems face, such as having to classify emerging or complex\nentities in a fine-grained way. In this paper we present a novel NER cascade\napproach comprising three steps: first, identifying candidate entities in the\ninput sentence; second, linking the each candidate to an existing knowledge\nbase; third, predicting the fine-grained category for each entity candidate. We\nempirically demonstrate the significance of external knowledge bases in\naccurately classifying fine-grained and emerging entities. Our system exhibits\nrobust performance in the MultiCoNER2 shared task, even in the low-resource\nlanguage setting where we leverage knowledge bases of high-resource languages.\n",
                "链接": "https://arxiv.org/abs/2304.10637"
            },
            {
                "文章ID": "117569",
                "标题": "Efficient Transformer Knowledge Distillation: A Performance Review",
                "作者": " Nathan Brown,  Ashton Williamson,  Tahj Anderson,  Logan Lawrence",
                "发布日期": "2023-11-27",
                "摘要": "  As pretrained transformer language models continue to achieve\nstate-of-the-art performance, the Natural Language Processing community has\npushed for advances in model compression and efficient attention mechanisms to\naddress high computational requirements and limited input sequence length.\nDespite these separate efforts, no investigation has been done into the\nintersection of these two fields. In this work, we provide an evaluation of\nmodel compression via knowledge distillation on efficient attention\ntransformers. We provide cost-performance trade-offs for the compression of\nstate-of-the-art efficient attention architectures and the gains made in\nperformance in comparison to their full attention counterparts. Furthermore, we\nintroduce a new long-context Named Entity Recognition dataset, GONERD, to train\nand test the performance of NER models on long sequences. We find that\ndistilled efficient attention transformers can preserve a significant amount of\noriginal model performance, preserving up to 98.6% across short-context tasks\n(GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context\nQuestion-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on\nlong-context Named Entity Recognition (GONERD), while decreasing inference\ntimes by up to 57.8%. We find that, for most models on most tasks, performing\nknowledge distillation is an effective method to yield high-performing\nefficient attention models with low costs.\n",
                "链接": "https://arxiv.org/abs/2311.13657"
            },
            {
                "文章ID": "5419",
                "标题": "ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers",
                "作者": " Federico Ruggeri,  Mohsen Mesgar,  Iryna Gurevych",
                "发布日期": "2022-10-14",
                "摘要": "  The applications of conversational agents for scientific disciplines (as\nexpert domains) are understudied due to the lack of dialogue data to train such\nagents. While most data collection frameworks, such as Amazon Mechanical Turk,\nfoster data collection for generic domains by connecting crowd workers and task\ndesigners, these frameworks are not much optimized for data collection in\nexpert domains. Scientists are rarely present in these frameworks due to their\nlimited time budget. Therefore, we introduce a novel framework to collect\ndialogues between scientists as domain experts on scientific papers. Our\nframework lets scientists present their scientific papers as groundings for\ndialogues and participate in dialogue they like its paper title. We use our\nframework to collect a novel argumentative dialogue dataset, ArgSciChat. It\nconsists of 498 messages collected from 41 dialogues on 20 scientific papers.\nAlongside extensive analysis on ArgSciChat, we evaluate a recent conversational\nagent on our dataset. Experimental results show that this agent poorly performs\non ArgSciChat, motivating further research on argumentative scientific agents.\nWe release our framework and the dataset.\n",
                "链接": "https://arxiv.org/abs/2202.06690"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下使用CrossWoz或MultiWoz数据集进行DST评测的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "15284",
                "标题": "On the Origin of Hallucinations in Conversational Models: Is it the\n  Datasets or the Models?",
                "作者": " Nouha Dziri,  Sivan Milton,  Mo Yu,  Osmar Zaiane,  Siva Reddy",
                "发布日期": "2022-04-19",
                "摘要": "  Knowledge-grounded conversational models are known to suffer from producing\nfactually invalid statements, a phenomenon commonly called hallucination. In\nthis work, we investigate the underlying causes of this phenomenon: is\nhallucination due to the training data, or to the models? We conduct a\ncomprehensive human study on both existing knowledge-grounded conversational\nbenchmarks and several state-of-the-art models. Our study reveals that the\nstandard benchmarks consist of >60% hallucinated responses, leading to models\nthat not only hallucinate but even amplify hallucinations. Our findings raise\nimportant questions on the quality of existing datasets and models trained\nusing them. We make our annotations publicly available for future research.\n",
                "链接": "https://arxiv.org/abs/2204.07931"
            },
            {
                "文章ID": "95138",
                "标题": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using\n  EmotionBench",
                "作者": " Jen-tse Huang,  Man Ho Lam,  Eric John Li,  Shujie Ren,  Wenxuan Wang,  Wenxiang Jiao,  Zhaopeng Tu,  Michael R. Lyu",
                "发布日期": "2023-11-17",
                "摘要": "  Recently, the community has witnessed the advancement of Large Language\nModels (LLMs), which have shown remarkable performance on various downstream\ntasks. Led by powerful models like ChatGPT and Claude, LLMs are revolutionizing\nhow users engage with software, assuming more than mere tools but intelligent\nassistants. Consequently, evaluating LLMs' anthropomorphic capabilities becomes\nincreasingly important in contemporary discourse. Utilizing the emotion\nappraisal theory from psychology, we propose to evaluate the empathy ability of\nLLMs, i.e., how their feelings change when presented with specific situations.\nAfter a careful and comprehensive survey, we collect a dataset containing over\n400 situations that have proven effective in eliciting the eight emotions\ncentral to our study. Categorizing the situations into 36 factors, we conduct a\nhuman evaluation involving more than 1,200 subjects worldwide. With the human\nevaluation results as references, our evaluation includes five LLMs, covering\nboth commercial and open-source models, including variations in model sizes,\nfeaturing the latest iterations, such as GPT-4 and LLaMA 2. A conclusion can be\ndrawn from the results that, despite several misalignments, LLMs can generally\nrespond appropriately to certain situations. Nevertheless, they fall short in\nalignment with the emotional behaviors of human beings and cannot establish\nconnections between similar situations. Our collected dataset of situations,\nthe human evaluation results, and the code of our testing framework, dubbed\nEmotionBench, is made publicly in https://github.com/CUHK-ARISE/EmotionBench.\nWe aspire to contribute to the advancement of LLMs regarding better alignment\nwith the emotional behaviors of human beings, thereby enhancing their utility\nand applicability as intelligent assistants.\n",
                "链接": "https://arxiv.org/abs/2308.03656"
            },
            {
                "文章ID": "68580",
                "标题": "ChatGPT or Grammarly? Evaluating ChatGPT on Grammatical Error Correction\n  Benchmark",
                "作者": " Haoran Wu,  Wenxuan Wang,  Yuxuan Wan,  Wenxiang Jiao,  Michael Lyu",
                "发布日期": "2023-03-27",
                "摘要": "  ChatGPT is a cutting-edge artificial intelligence language model developed by\nOpenAI, which has attracted a lot of attention due to its surprisingly strong\nability in answering follow-up questions. In this report, we aim to evaluate\nChatGPT on the Grammatical Error Correction(GEC) task, and compare it with\ncommercial GEC product (e.g., Grammarly) and state-of-the-art models (e.g.,\nGECToR). By testing on the CoNLL2014 benchmark dataset, we find that ChatGPT\nperforms not as well as those baselines in terms of the automatic evaluation\nmetrics (e.g., $F_{0.5}$ score), particularly on long sentences. We inspect the\noutputs and find that ChatGPT goes beyond one-by-one corrections. Specifically,\nit prefers to change the surface expression of certain phrases or sentence\nstructure while maintaining grammatical correctness. Human evaluation\nquantitatively confirms this and suggests that ChatGPT produces less\nunder-correction or mis-correction issues but more over-corrections. These\nresults demonstrate that ChatGPT is severely under-estimated by the automatic\nevaluation metrics and could be a promising tool for GEC.\n",
                "链接": "https://arxiv.org/abs/2303.13648"
            },
            {
                "文章ID": "60862",
                "标题": "Find a witness or shatter: the landscape of computable PAC learning",
                "作者": " Valentino Delle Rose,  Alexander Kozachinskiy,  Cristobal Rojas,  Tomasz Steifer",
                "发布日期": "2023-02-24",
                "摘要": "  This paper contributes to the study of CPAC learnability -- a computable\nversion of PAC learning -- by solving three open questions from recent papers.\nFirstly, we prove that every improperly CPAC learnable class is contained in a\nclass which is properly CPAC learnable with polynomial sample complexity. This\nconfirms a conjecture by Agarwal et al (COLT 2021). Secondly, we show that\nthere exists a decidable class of hypothesis which is properly CPAC learnable,\nbut only with uncomputably fast growing sample complexity. This solves a\nquestion from Sterkenburg (COLT 2022). Finally, we construct a decidable class\nof finite Littlestone dimension which is not improperly CPAC learnable,\nstrengthening a recent result of Sterkenburg (2022) and answering a question\nposed by Hasrati and Ben-David (ALT 2023). Together with previous work, our\nresults provide a complete landscape for the learnability problem in the CPAC\nsetting.\n",
                "链接": "https://arxiv.org/abs/2302.04731"
            },
            {
                "文章ID": "114218",
                "标题": "Gender Inflected or Bias Inflicted: On Using Grammatical Gender Cues for\n  Bias Evaluation in Machine Translation",
                "作者": " Pushpdeep Singh",
                "发布日期": "2023-11-08",
                "摘要": "  Neural Machine Translation (NMT) models are state-of-the-art for machine\ntranslation. However, these models are known to have various social biases,\nespecially gender bias. Most of the work on evaluating gender bias in NMT has\nfocused primarily on English as the source language. For source languages\ndifferent from English, most of the studies use gender-neutral sentences to\nevaluate gender bias. However, practically, many sentences that we encounter do\nhave gender information. Therefore, it makes more sense to evaluate for bias\nusing such sentences. This allows us to determine if NMT models can identify\nthe correct gender based on the grammatical gender cues in the source sentence\nrather than relying on biased correlations with, say, occupation terms. To\ndemonstrate our point, in this work, we use Hindi as the source language and\nconstruct two sets of gender-specific sentences: OTSC-Hindi and WinoMT-Hindi\nthat we use to evaluate different Hindi-English (HI-EN) NMT systems\nautomatically for gender bias. Our work highlights the importance of\nconsidering the nature of language when designing such extrinsic bias\nevaluation datasets.\n",
                "链接": "https://arxiv.org/abs/2311.03767"
            },
            {
                "文章ID": "72864",
                "标题": "Wizundry: A Cooperative Wizard of Oz Platform for Simulating Future\n  Speech-based Interfaces with Multiple Wizards",
                "作者": " Siying Hu,  Hen Chen Yen,  Ziwei Yu,  Mingjian Zhao,  Katie Seaborn,  Can Liu",
                "发布日期": "2023-04-19",
                "摘要": "  Wizard of Oz (WoZ) as a prototyping method has been used to simulate\nintelligent user interfaces, particularly for speech-based systems. However, as\nour societies' expectations on artificial intelligence (AI) grows, the question\nremains whether a single Wizard is sufficient for it to simulate smarter\nsystems and more complex interactions. Optimistic visions of 'what artificial\nintelligence (AI) can do' places demands on WoZ platforms to simulate smarter\nsystems and more complex interactions. This raises the question of whether the\ntypical approach of employing a single Wizard is sufficient. Moreover, while\nexisting work has employed multiple Wizards in WoZ studies, a multi-Wizard\napproach has not been systematically studied in terms of feasibility,\neffectiveness, and challenges. We offer Wizundry, a real-time, web-based WoZ\nplatform that allows multiple Wizards to collaboratively operate a\nspeech-to-text based system remotely. We outline the design and technical\nspecifications of our open-source platform, which we iterated over two design\nphases. We report on two studies in which participant-Wizards were tasked with\nnegotiating how to cooperatively simulate an interface that can handle natural\nspeech for dictation and text editing as well as other intelligent text\nprocessing tasks. We offer qualitative findings on the Multi-Wizard experience\nfor Dyads and Triads of Wizards. Our findings reveal the promises and\nchallenges of the multi-Wizard approach and open up new research questions.\n",
                "链接": "https://arxiv.org/abs/2304.08693"
            },
            {
                "文章ID": "68668",
                "标题": "Unleashing ChatGPT on the Metaverse: Savior or Destroyer?",
                "作者": " Pengyuan Zhou",
                "发布日期": "2023-04-13",
                "摘要": "  The incorporation of artificial intelligence (AI) technology, and in\nparticular natural language processing (NLP), is becoming increasingly vital\nfor the development of immersive and interactive metaverse experiences. One\nsuch artificial intelligence tool that is gaining traction in the metaverse is\nChatGPT, a large language model trained by OpenAI. The article delves into the\npros and cons of utilizing ChatGPT for metaverse-based education,\nentertainment, personalization, and support. Dynamic and personalized\nexperiences are possible with this technology, but there are also legitimate\nprivacy, bias, and ethical issues to consider. This article aims to help\nreaders understand the possible influence of ChatGPT on the metaverse and how\nit may be used to effectively create a more immersive and engaging virtual\nenvironment by evaluating these opportunities and obstacles.\n",
                "链接": "https://arxiv.org/abs/2303.13856"
            },
            {
                "文章ID": "105774",
                "标题": "DST-Det: Simple Dynamic Self-Training for Open-Vocabulary Object\n  Detection",
                "作者": " Shilin Xu,  Xiangtai Li,  Size Wu,  Wenwei Zhang,  Yining Li,  Guangliang Cheng,  Yunhai Tong,  Kai Chen,  Chen Change Loy",
                "发布日期": "2023-12-27",
                "摘要": "  Open-vocabulary object detection (OVOD) aims to detect the objects beyond the\nset of classes observed during training. This work presents a simple yet\neffective strategy that leverages the zero-shot classification ability of\npre-trained vision-language models (VLM), such as CLIP, to directly discover\nproposals of possible novel classes. Unlike previous works that ignore novel\nclasses during training and rely solely on the region proposal network (RPN)\nfor novel object detection, our method selectively filters proposals based on\nspecific design criteria. The resulting sets of identified proposals serve as\npseudo-labels of potential novel classes during the training phase. This\nself-training strategy improves the recall and accuracy of novel classes\nwithout requiring additional annotations or datasets. We further propose a\nsimple offline pseudo-label generation strategy to refine the object detector.\nEmpirical evaluations on three datasets, including LVIS, V3Det, and COCO,\ndemonstrate significant improvements over the baseline performance without\nincurring additional parameters or computational costs during inference. In\nparticular, compared with previous F-VLM, our method achieves a 1.7\\%\nimprovement on the LVIS dataset. We also achieve over 6.5\\% improvement on the\nrecent challenging V3Det dataset. When combined with the recent method\nCLIPSelf, our method also achieves 46.7 novel class AP on COCO without\nintroducing extra data for pertaining.\n",
                "链接": "https://arxiv.org/abs/2310.01393"
            },
            {
                "文章ID": "5419",
                "标题": "ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers",
                "作者": " Federico Ruggeri,  Mohsen Mesgar,  Iryna Gurevych",
                "发布日期": "2022-10-14",
                "摘要": "  The applications of conversational agents for scientific disciplines (as\nexpert domains) are understudied due to the lack of dialogue data to train such\nagents. While most data collection frameworks, such as Amazon Mechanical Turk,\nfoster data collection for generic domains by connecting crowd workers and task\ndesigners, these frameworks are not much optimized for data collection in\nexpert domains. Scientists are rarely present in these frameworks due to their\nlimited time budget. Therefore, we introduce a novel framework to collect\ndialogues between scientists as domain experts on scientific papers. Our\nframework lets scientists present their scientific papers as groundings for\ndialogues and participate in dialogue they like its paper title. We use our\nframework to collect a novel argumentative dialogue dataset, ArgSciChat. It\nconsists of 498 messages collected from 41 dialogues on 20 scientific papers.\nAlongside extensive analysis on ArgSciChat, we evaluate a recent conversational\nagent on our dataset. Experimental results show that this agent poorly performs\non ArgSciChat, motivating further research on argumentative scientific agents.\nWe release our framework and the dataset.\n",
                "链接": "https://arxiv.org/abs/2202.06690"
            },
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            }
        ]
    },
    {
        "question": {
            "question": "2023年后利用hotpotqa数据集做问题生成任务的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "8471",
                "标题": "Recent Advances in Neural Text Generation: A Task-Agnostic Survey",
                "作者": " Chen Tang,  Frank Guerin,  Chenghua Lin",
                "发布日期": "2023-06-13",
                "摘要": "  In recent years, considerable research has been dedicated to the application\nof neural models in the field of natural language generation (NLG). The primary\nobjective is to generate text that is both linguistically natural and\nhuman-like, while also exerting control over the generation process. This paper\noffers a comprehensive and task-agnostic survey of the recent advancements in\nneural text generation. These advancements have been facilitated through a\nmultitude of developments, which we categorize into four key areas: data\nconstruction, neural frameworks, training and inference strategies, and\nevaluation metrics. By examining these different aspects, we aim to provide a\nholistic overview of the progress made in the field. Furthermore, we explore\nthe future directions for the advancement of neural text generation, which\nencompass the utilization of neural pipelines and the incorporation of\nbackground knowledge. These avenues present promising opportunities to further\nenhance the capabilities of NLG systems. Overall, this survey serves to\nconsolidate the current state of the art in neural text generation and\nhighlights potential avenues for future research and development in this\ndynamic field.\n",
                "链接": "https://arxiv.org/abs/2203.03047"
            },
            {
                "文章ID": "28757",
                "标题": "A Survey on Table Question Answering: Recent Advances",
                "作者": " Nengzheng Jin,  Joanna Siebert,  Dongfang Li,  Qingcai Chen",
                "发布日期": "2022-07-13",
                "摘要": "  Table Question Answering (Table QA) refers to providing precise answers from\ntables to answer a user's question. In recent years, there have been a lot of\nworks on table QA, but there is a lack of comprehensive surveys on this\nresearch topic. Hence, we aim to provide an overview of available datasets and\nrepresentative methods in table QA. We classify existing methods for table QA\ninto five categories according to their techniques, which include\nsemantic-parsing-based, generative, extractive, matching-based, and\nretriever-reader-based methods. Moreover, as table QA is still a challenging\ntask for existing methods, we also identify and outline several key challenges\nand discuss the potential future directions of table QA.\n",
                "链接": "https://arxiv.org/abs/2207.05270"
            },
            {
                "文章ID": "78463",
                "标题": "Visual Question Answering: A Survey on Techniques and Common Trends in\n  Recent Literature",
                "作者": " Ana Cláudia Akemi Matsuki de Faria,  Felype de Castro Bastos,  José Victor Nogueira Alves da Silva,  Vitor Lopes Fabris,  Valeska de Sousa Uchoa,  Décio Gonçalves de Aguiar Neto,  Claudio Filipi Goncalves dos Santos",
                "发布日期": "2023-06-05",
                "摘要": "  Visual Question Answering (VQA) is an emerging area of interest for\nresearches, being a recent problem in natural language processing and image\nprediction. In this area, an algorithm needs to answer questions about certain\nimages. As of the writing of this survey, 25 recent studies were analyzed.\nBesides, 6 datasets were analyzed and provided their link to download. In this\nwork, several recent pieces of research in this area were investigated and a\ndeeper analysis and comparison among them were provided, including results, the\nstate-of-the-art, common errors, and possible points of improvement for future\nresearchers.\n",
                "链接": "https://arxiv.org/abs/2305.11033"
            },
            {
                "文章ID": "5419",
                "标题": "ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers",
                "作者": " Federico Ruggeri,  Mohsen Mesgar,  Iryna Gurevych",
                "发布日期": "2022-10-14",
                "摘要": "  The applications of conversational agents for scientific disciplines (as\nexpert domains) are understudied due to the lack of dialogue data to train such\nagents. While most data collection frameworks, such as Amazon Mechanical Turk,\nfoster data collection for generic domains by connecting crowd workers and task\ndesigners, these frameworks are not much optimized for data collection in\nexpert domains. Scientists are rarely present in these frameworks due to their\nlimited time budget. Therefore, we introduce a novel framework to collect\ndialogues between scientists as domain experts on scientific papers. Our\nframework lets scientists present their scientific papers as groundings for\ndialogues and participate in dialogue they like its paper title. We use our\nframework to collect a novel argumentative dialogue dataset, ArgSciChat. It\nconsists of 498 messages collected from 41 dialogues on 20 scientific papers.\nAlongside extensive analysis on ArgSciChat, we evaluate a recent conversational\nagent on our dataset. Experimental results show that this agent poorly performs\non ArgSciChat, motivating further research on argumentative scientific agents.\nWe release our framework and the dataset.\n",
                "链接": "https://arxiv.org/abs/2202.06690"
            },
            {
                "文章ID": "76178",
                "标题": "OpenViVQA: Task, Dataset, and Multimodal Fusion Models for Visual\n  Question Answering in Vietnamese",
                "作者": " Nghia Hieu Nguyen,  Duong T. D. Vo,  Kiet Van Nguyen,  Ngan Luu-Thuy Nguyen",
                "发布日期": "2023-10-03",
                "摘要": "  In recent years, visual question answering (VQA) has attracted attention from\nthe research community because of its highly potential applications (such as\nvirtual assistance on intelligent cars, assistant devices for blind people, or\ninformation retrieval from document images using natural language as queries)\nand challenge. The VQA task requires methods that have the ability to fuse the\ninformation from questions and images to produce appropriate answers. Neural\nvisual question answering models have achieved tremendous growth on large-scale\ndatasets which are mostly for resource-rich languages such as English. However,\navailable datasets narrow the VQA task as the answers selection task or answer\nclassification task. We argue that this form of VQA is far from human ability\nand eliminates the challenge of the answering aspect in the VQA task by just\nselecting answers rather than generating them. In this paper, we introduce the\nOpenViVQA (Open-domain Vietnamese Visual Question Answering) dataset, the first\nlarge-scale dataset for VQA with open-ended answers in Vietnamese, consists of\n11,000+ images associated with 37,000+ question-answer pairs (QAs). Moreover,\nwe proposed FST, QuMLAG, and MLPAG which fuse information from images and\nanswers, then use these fused features to construct answers as humans\niteratively. Our proposed methods achieve results that are competitive with\nSOTA models such as SAAA, MCAN, LORA, and M4C. The dataset is available to\nencourage the research community to develop more generalized algorithms\nincluding transformers for low-resource languages such as Vietnamese.\n",
                "链接": "https://arxiv.org/abs/2305.04183"
            },
            {
                "文章ID": "2393",
                "标题": "Question Generation for Evaluating Cross-Dataset Shifts in Multi-modal\n  Grounding",
                "作者": " Arjun R. Akula",
                "发布日期": "2022-01-25",
                "摘要": "  Visual question answering (VQA) is the multi-modal task of answering natural\nlanguage questions about an input image. Through cross-dataset adaptation\nmethods, it is possible to transfer knowledge from a source dataset with larger\ntrain samples to a target dataset where training set is limited. Suppose a VQA\nmodel trained on one dataset train set fails in adapting to another, it is hard\nto identify the underlying cause of domain mismatch as there could exists a\nmultitude of reasons such as image distribution mismatch and question\ndistribution mismatch. At UCLA, we are working on a VQG module that facilitate\nin automatically generating OOD shifts that aid in systematically evaluating\ncross-dataset adaptation capabilities of VQA models.\n",
                "链接": "https://arxiv.org/abs/2201.09639"
            },
            {
                "文章ID": "7957",
                "标题": "Recent, rapid advancement in visual question answering architecture: a\n  review",
                "作者": " Venkat Kodali,  Daniel Berleant",
                "发布日期": "2022-07-12",
                "摘要": "  Understanding visual question answering is going to be crucial for numerous\nhuman activities. However, it presents major challenges at the heart of the\nartificial intelligence endeavor. This paper presents an update on the rapid\nadvancements in visual question answering using images that have occurred in\nthe last couple of years. Tremendous growth in research on improving visual\nquestion answering system architecture has been published recently, showing the\nimportance of multimodal architectures. Several points on the benefits of\nvisual question answering are mentioned in the review paper by Manmadhan et al.\n(2020), on which the present article builds, including subsequent updates in\nthe field.\n",
                "链接": "https://arxiv.org/abs/2203.01322"
            },
            {
                "文章ID": "85260",
                "标题": "The BEA 2023 Shared Task on Generating AI Teacher Responses in\n  Educational Dialogues",
                "作者": " Anaïs Tack,  Ekaterina Kochmar,  Zheng Yuan,  Serge Bibauw,  Chris Piech",
                "发布日期": "2023-06-13",
                "摘要": "  This paper describes the results of the first shared task on the generation\nof teacher responses in educational dialogues. The goal of the task was to\nbenchmark the ability of generative language models to act as AI teachers,\nreplying to a student in a teacher-student dialogue. Eight teams participated\nin the competition hosted on CodaLab. They experimented with a wide variety of\nstate-of-the-art models, including Alpaca, Bloom, DialoGPT, DistilGPT-2,\nFlan-T5, GPT-2, GPT-3, GPT- 4, LLaMA, OPT-2.7B, and T5-base. Their submissions\nwere automatically scored using BERTScore and DialogRPT metrics, and the top\nthree among them were further manually evaluated in terms of pedagogical\nability based on Tack and Piech (2022). The NAISTeacher system, which ranked\nfirst in both automated and human evaluation, generated responses with GPT-3.5\nusing an ensemble of prompts and a DialogRPT-based ranking of responses for\ngiven dialogue contexts. Despite the promising achievements of the\nparticipating teams, the results also highlight the need for evaluation metrics\nbetter suited to educational contexts.\n",
                "链接": "https://arxiv.org/abs/2306.06941"
            },
            {
                "文章ID": "108331",
                "标题": "Visual Question Generation in Bengali",
                "作者": " Mahmud Hasan,  Labiba Islam,  Jannatul Ferdous Ruma,  Tasmiah Tahsin Mayeesha,  Rashedur M. Rahman",
                "发布日期": "2023-10-13",
                "摘要": "  The task of Visual Question Generation (VQG) is to generate human-like\nquestions relevant to the given image. As VQG is an emerging research field,\nexisting works tend to focus only on resource-rich language such as English due\nto the availability of datasets. In this paper, we propose the first Bengali\nVisual Question Generation task and develop a novel transformer-based\nencoder-decoder architecture that generates questions in Bengali when given an\nimage. We propose multiple variants of models - (i) image-only: baseline model\nof generating questions from images without additional information, (ii)\nimage-category and image-answer-category: guided VQG where we condition the\nmodel to generate questions based on the answer and the category of expected\nquestion. These models are trained and evaluated on the translated VQAv2.0\ndataset. Our quantitative and qualitative results establish the first state of\nthe art models for VQG task in Bengali and demonstrate that our models are\ncapable of generating grammatically correct and relevant questions. Our\nquantitative results show that our image-cat model achieves a BLUE-1 score of\n33.12 and BLEU-3 score of 7.56 which is the highest of the other two variants.\nWe also perform a human evaluation to assess the quality of the generation\ntasks. Human evaluation suggests that image-cat model is capable of generating\ngoal-driven and attribute-specific questions and also stays relevant to the\ncorresponding image.\n",
                "链接": "https://arxiv.org/abs/2310.08187"
            },
            {
                "文章ID": "59883",
                "标题": "LIQUID: A Framework for List Question Answering Dataset Generation",
                "作者": " Seongyun Lee,  Hyunjae Kim,  Jaewoo Kang",
                "发布日期": "2023-02-07",
                "摘要": "  Question answering (QA) models often rely on large-scale training datasets,\nwhich necessitates the development of a data generation framework to reduce the\ncost of manual annotations. Although several recent studies have aimed to\ngenerate synthetic questions with single-span answers, no study has been\nconducted on the creation of list questions with multiple, non-contiguous spans\nas answers. To address this gap, we propose LIQUID, an automated framework for\ngenerating list QA datasets from unlabeled corpora. We first convert a passage\nfrom Wikipedia or PubMed into a summary and extract named entities from the\nsummarized text as candidate answers. This allows us to select answers that are\nsemantically correlated in context and is, therefore, suitable for constructing\nlist questions. We then create questions using an off-the-shelf question\ngenerator with the extracted entities and original passage. Finally, iterative\nfiltering and answer expansion are performed to ensure the accuracy and\ncompleteness of the answers. Using our synthetic data, we significantly improve\nthe performance of the previous best list QA models by exact-match F1 scores of\n5.0 on MultiSpanQA, 1.9 on Quoref, and 2.8 averaged across three BioASQ\nbenchmarks.\n",
                "链接": "https://arxiv.org/abs/2302.01691"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下使用机器学习方法进行心理健康疾病研究的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "22134",
                "标题": "Individual health-disease phase diagrams for disease prevention based on\n  machine learning",
                "作者": " Kazuki Nakamura,  Eiichiro Uchino,  Noriaki Sato,  Ayano Araki,  Kei Terayama,  Ryosuke Kojima,  Koichi Murashita,  Ken Itoh,  Tatsuya Mikami,  Yoshinori Tamada,  Yasushi Okuno",
                "发布日期": "2022-07-08",
                "摘要": "  Early disease detection and prevention methods based on effective\ninterventions are gaining attention. Machine learning technology has enabled\nprecise disease prediction by capturing individual differences in multivariate\ndata. Progress in precision medicine has revealed that substantial\nheterogeneity exists in health data at the individual level and that complex\nhealth factors are involved in the development of chronic diseases. However, it\nremains a challenge to identify individual physiological state changes in\ncross-disease onset processes because of the complex relationships among\nmultiple biomarkers. Here, we present the health-disease phase diagram (HDPD),\nwhich represents a personal health state by visualizing the boundary values of\nmultiple biomarkers that fluctuate early in the disease progression process. In\nHDPDs, future onset predictions are represented by perturbing multiple\nbiomarker values while accounting for dependencies among variables. We\nconstructed HDPDs for 11 non-communicable diseases (NCDs) from a longitudinal\nhealth checkup cohort of 3,238 individuals, comprising 3,215 measurement items\nand genetic data. Improvement of biomarker values to the non-onset region in\nHDPD significantly prevented future disease onset in 7 out of 11 NCDs. Our\nresults demonstrate that HDPDs can represent individual physiological states in\nthe onset process and be used as intervention goals for disease prevention.\n",
                "链接": "https://arxiv.org/abs/2205.15598"
            },
            {
                "文章ID": "98633",
                "标题": "Discovering Mental Health Research Topics with Topic Modeling",
                "作者": " Xin Gao,  Cem Sazara",
                "发布日期": "2023-08-29",
                "摘要": "  Mental health significantly influences various aspects of our daily lives,\nand its importance has been increasingly recognized by the research community\nand the general public, particularly in the wake of the COVID-19 pandemic. This\nheightened interest is evident in the growing number of publications dedicated\nto mental health in the past decade. In this study, our goal is to identify\ngeneral trends in the field and pinpoint high-impact research topics by\nanalyzing a large dataset of mental health research papers. To accomplish this,\nwe collected abstracts from various databases and trained a customized\nSentence-BERT based embedding model leveraging the BERTopic framework. Our\ndataset comprises 96,676 research papers pertaining to mental health, enabling\nus to examine the relationships between different topics using their abstracts.\nTo evaluate the effectiveness of the model, we compared it against two other\nstate-of-the-art methods: Top2Vec model and LDA-BERT model. The model\ndemonstrated superior performance in metrics that measure topic diversity and\ncoherence. To enhance our analysis, we also generated word clouds to provide a\ncomprehensive overview of the machine learning models applied in mental health\nresearch, shedding light on commonly utilized techniques and emerging trends.\nFurthermore, we provide a GitHub link* to the dataset used in this paper,\nensuring its accessibility for further research endeavors.\n",
                "链接": "https://arxiv.org/abs/2308.13569"
            },
            {
                "文章ID": "20900",
                "标题": "Bias Discovery in Machine Learning Models for Mental Health",
                "作者": " Pablo Mosteiro,  Jesse Kuiper,  Judith Masthoff,  Floortje Scheepers,  Marco Spruit",
                "发布日期": "2022-05-25",
                "摘要": "  Fairness and bias are crucial concepts in artificial intelligence, yet they\nare relatively ignored in machine learning applications in clinical psychiatry.\nWe computed fairness metrics and present bias mitigation strategies using a\nmodel trained on clinical mental health data. We collected structured data\nrelated to the admission, diagnosis, and treatment of patients in the\npsychiatry department of the University Medical Center Utrecht. We trained a\nmachine learning model to predict future administrations of benzodiazepines on\nthe basis of past data. We found that gender plays an unexpected role in the\npredictions-this constitutes bias. Using the AI Fairness 360 package, we\nimplemented reweighing and discrimination-aware regularization as bias\nmitigation strategies, and we explored their implications for model\nperformance. This is the first application of bias exploration and mitigation\nin a machine learning model trained on real clinical psychiatry data.\n",
                "链接": "https://arxiv.org/abs/2205.12093"
            },
            {
                "文章ID": "32474",
                "标题": "Bias Reducing Multitask Learning on Mental Health Prediction",
                "作者": " Khadija Zanna,  Kusha Sridhar,  Han Yu,  Akane Sano",
                "发布日期": "2022-08-09",
                "摘要": "  There has been an increase in research in developing machine learning models\nfor mental health detection or prediction in recent years due to increased\nmental health issues in society. Effective use of mental health prediction or\ndetection models can help mental health practitioners re-define mental\nillnesses more objectively than currently done, and identify illnesses at an\nearlier stage when interventions may be more effective. However, there is still\na lack of standard in evaluating bias in such machine learning models in the\nfield, which leads to challenges in providing reliable predictions and in\naddressing disparities. This lack of standards persists due to factors such as\ntechnical difficulties, complexities of high dimensional clinical health data,\netc., which are especially true for physiological signals. This along with\nprior evidence of relations between some physiological signals with certain\ndemographic identities restates the importance of exploring bias in mental\nhealth prediction models that utilize physiological signals. In this work, we\naim to perform a fairness analysis and implement a multi-task learning based\nbias mitigation method on anxiety prediction models using ECG data. Our method\nis based on the idea of epistemic uncertainty and its relationship with model\nweights and feature space representation. Our analysis showed that our anxiety\nprediction base model introduced some bias with regards to age, income,\nethnicity, and whether a participant is born in the U.S. or not, and our bias\nmitigation method performed better at reducing the bias in the model, when\ncompared to the reweighting mitigation technique. Our analysis on feature\nimportance also helped identify relationships between heart rate variability\nand multiple demographic groupings.\n",
                "链接": "https://arxiv.org/abs/2208.03621"
            },
            {
                "文章ID": "116611",
                "标题": "Classification Methods Based on Machine Learning for the Analysis of\n  Fetal Health Data",
                "作者": " Binod Regmi,  Chiranjibi Shah",
                "发布日期": "2023-11-21",
                "摘要": "  The persistent battle to decrease childhood mortality serves as a commonly\nemployed benchmark for gauging advancements in the field of medicine. Globally,\nthe under-5 mortality rate stands at approximately 5 million, with a\nsignificant portion of these deaths being avoidable. Given the significance of\nthis problem, Machine learning-based techniques have emerged as a prominent\ntool for assessing fetal health. In this work, we have analyzed the\nclassification performance of various machine learning models for fetal health\nanalysis. Classification performance of various machine learning models, such\nas support vector machine (SVM), random forest(RF), and attentive interpretable\ntabular learning (TabNet) have been assessed on fetal health. Moreover,\ndimensionality reduction techniques, such as Principal component analysis (PCA)\nand Linear discriminant analysis (LDA) have been implemented to obtain better\nclassification performance with less number of features. A TabNet model on a\nfetal health dataset provides a classification accuracy of 94.36%. In general,\nthis technology empowers doctors and healthcare experts to achieve precise\nfetal health classification and identify the most influential features in the\nprocess.\n",
                "链接": "https://arxiv.org/abs/2311.10962"
            },
            {
                "文章ID": "57937",
                "标题": "Deep Learning Mental Health Dialogue System",
                "作者": " Lennart Brocki,  George C. Dyer,  Anna Gładka,  Neo Christopher Chung",
                "发布日期": "2023-01-24",
                "摘要": "  Mental health counseling remains a major challenge in modern society due to\ncost, stigma, fear, and unavailability. We posit that generative artificial\nintelligence (AI) models designed for mental health counseling could help\nimprove outcomes by lowering barriers to access. To this end, we have developed\na deep learning (DL) dialogue system called Serena. The system consists of a\ncore generative model and post-processing algorithms. The core generative model\nis a 2.7 billion parameter Seq2Seq Transformer fine-tuned on thousands of\ntranscripts of person-centered-therapy (PCT) sessions. The series of\npost-processing algorithms detects contradictions, improves coherency, and\nremoves repetitive answers. Serena is implemented and deployed on\n\\url{https://serena.chat}, which currently offers limited free services. While\nthe dialogue system is capable of responding in a qualitatively empathetic and\nengaging manner, occasionally it displays hallucination and long-term\nincoherence. Overall, we demonstrate that a deep learning mental health\ndialogue system has the potential to provide a low-cost and effective\ncomplement to traditional human counselors with less barriers to access.\n",
                "链接": "https://arxiv.org/abs/2301.09412"
            },
            {
                "文章ID": "21978",
                "标题": "Machine Learning Methods for Health-Index Prediction in Coating Chambers",
                "作者": " Clemens Heistracher,  Anahid Jalali,  Jürgen Schneeweiss,  Klaudia Kovacs,  Catherine Laflamme,  Bernhard Haslhofer",
                "发布日期": "2022-05-31",
                "摘要": "  Coating chambers create thin layers that improve the mechanical and optical\nsurface properties in jewelry production using physical vapor deposition. In\nsuch a process, evaporated material condensates on the walls of such chambers\nand, over time, causes mechanical defects and unstable processes. As a result,\nmanufacturers perform extensive maintenance procedures to reduce production\nloss. Current rule-based maintenance strategies neglect the impact of specific\nrecipes and the actual condition of the vacuum chamber. Our overall goal is to\npredict the future condition of the coating chamber to allow cost and quality\noptimized maintenance of the equipment. This paper describes the derivation of\na novel health indicator that serves as a step toward condition-based\nmaintenance for coating chambers. We indirectly use gas emissions of the\nchamber's contamination to evaluate the machine's condition. Our approach\nrelies on process data and does not require additional hardware installation.\nFurther, we evaluated multiple machine learning algorithms for a\ncondition-based forecast of the health indicator that also reflects production\nplanning. Our results show that models based on decision trees are the most\neffective and outperform all three benchmarks, improving at least $0.22$ in the\nmean average error. Our work paves the way for cost and quality optimized\nmaintenance of coating applications.\n",
                "链接": "https://arxiv.org/abs/2205.15145"
            },
            {
                "文章ID": "113411",
                "标题": "Identifying Alzheimer Disease Dementia Levels Using Machine Learning\n  Methods",
                "作者": " Md Gulzar Hussain,  Ye Shiren",
                "发布日期": "2023-11-06",
                "摘要": "  Dementia, a prevalent neurodegenerative condition, is a major manifestation\nof Alzheimer's disease (AD). As the condition progresses from mild to severe,\nit significantly impairs the individual's ability to perform daily tasks\nindependently, necessitating the need for timely and accurate AD\nclassification. Machine learning or deep learning models have emerged as\neffective tools for this purpose. In this study, we suggested an approach for\nclassifying the four stages of dementia using RF, SVM, and CNN algorithms,\naugmented with watershed segmentation for feature extraction from MRI images.\nOur results reveal that SVM with watershed features achieves an impressive\naccuracy of 96.25%, surpassing other classification methods. The ADNI dataset\nis utilized to evaluate the effectiveness of our method, and we observed that\nthe inclusion of watershed segmentation contributes to the enhanced performance\nof the models.\n",
                "链接": "https://arxiv.org/abs/2311.01428"
            },
            {
                "文章ID": "1267",
                "标题": "Mental Health Assessment for the Chatbots",
                "作者": " Yong Shan,  Jinchao Zhang,  Zekang Li,  Yang Feng,  Jie Zhou",
                "发布日期": "2022-01-17",
                "摘要": "  Previous researches on dialogue system assessment usually focus on the\nquality evaluation (e.g. fluency, relevance, etc) of responses generated by the\nchatbots, which are local and technical metrics. For a chatbot which responds\nto millions of online users including minors, we argue that it should have a\nhealthy mental tendency in order to avoid the negative psychological impact on\nthem. In this paper, we establish several mental health assessment dimensions\nfor chatbots (depression, anxiety, alcohol addiction, empathy) and introduce\nthe questionnaire-based mental health assessment methods. We conduct\nassessments on some well-known open-domain chatbots and find that there are\nsevere mental health issues for all these chatbots. We consider that it is due\nto the neglect of the mental health risks during the dataset building and the\nmodel training procedures. We expect to attract researchers' attention to the\nserious mental health problems of chatbots and improve the chatbots' ability in\npositive emotional interaction.\n",
                "链接": "https://arxiv.org/abs/2201.05382"
            },
            {
                "文章ID": "104767",
                "标题": "Identifying Risk Factors for Post-COVID-19 Mental Health Disorders: A\n  Machine Learning Perspective",
                "作者": " Maitham G. Yousif,  Fadhil G. Al-Amran,  Hector J. Castro",
                "发布日期": "2023-09-29",
                "摘要": "  In this study, we leveraged machine learning techniques to identify risk\nfactors associated with post-COVID-19 mental health disorders. Our analysis,\nbased on data collected from 669 patients across various provinces in Iraq,\nyielded valuable insights. We found that age, gender, and geographical region\nof residence were significant demographic factors influencing the likelihood of\ndeveloping mental health disorders in post-COVID-19 patients. Additionally,\ncomorbidities and the severity of COVID-19 illness were important clinical\npredictors. Psychosocial factors, such as social support, coping strategies,\nand perceived stress levels, also played a substantial role. Our findings\nemphasize the complex interplay of multiple factors in the development of\nmental health disorders following COVID-19 recovery. Healthcare providers and\npolicymakers should consider these risk factors when designing targeted\ninterventions and support systems for individuals at risk. Machine\nlearning-based approaches can provide a valuable tool for predicting and\npreventing adverse mental health outcomes in post-COVID-19 patients. Further\nresearch and prospective studies are needed to validate these findings and\nenhance our understanding of the long-term psychological impact of the COVID-19\npandemic. This study contributes to the growing body of knowledge regarding the\nmental health consequences of the COVID-19 pandemic and underscores the\nimportance of a multidisciplinary approach to address the diverse needs of\nindividuals on the path to recovery. Keywords: COVID-19, mental health, risk\nfactors, machine learning, Iraq\n",
                "链接": "https://arxiv.org/abs/2309.16055"
            }
        ]
    },
    {
        "question": {
            "question": "查找关于深度学习在医学影像分析中的最新研究。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "69437",
                "标题": "Medical Image Analysis using Deep Relational Learning",
                "作者": " Zhihua Liu",
                "发布日期": "2023-03-29",
                "摘要": "  In the past ten years, with the help of deep learning, especially the rapid\ndevelopment of deep neural networks, medical image analysis has made remarkable\nprogress. However, how to effectively use the relational information between\nvarious tissues or organs in medical images is still a very challenging\nproblem, and it has not been fully studied. In this thesis, we propose two\nnovel solutions to this problem based on deep relational learning. First, we\npropose a context-aware fully convolutional network that effectively models\nimplicit relation information between features to perform medical image\nsegmentation. The network achieves the state-of-the-art segmentation results on\nthe Multi Modal Brain Tumor Segmentation 2017 (BraTS2017) and Multi Modal Brain\nTumor Segmentation 2018 (BraTS2018) data sets. Subsequently, we propose a new\nhierarchical homography estimation network to achieve accurate medical image\nmosaicing by learning the explicit spatial relationship between adjacent\nframes. We use the UCL Fetoscopy Placenta dataset to conduct experiments and\nour hierarchical homography estimation network outperforms the other\nstate-of-the-art mosaicing methods while generating robust and meaningful\nmosaicing result on unseen frames.\n",
                "链接": "https://arxiv.org/abs/2303.16099"
            },
            {
                "文章ID": "110503",
                "标题": "A comprehensive survey on deep active learning and its applications in\n  medical image analysis",
                "作者": " Haoran Wang,  Qiuye Jin,  Shiman Li,  Siyu Liu,  Manning Wang,  Zhijian Song",
                "发布日期": "2023-10-25",
                "摘要": "  Deep learning has achieved widespread success in medical image analysis,\nleading to an increasing demand for large-scale expert-annotated medical image\ndatasets. Yet, the high cost of annotating medical images severely hampers the\ndevelopment of deep learning in this field. To reduce annotation costs, active\nlearning aims to select the most informative samples for annotation and train\nhigh-performance models with as few labeled samples as possible. In this\nsurvey, we review the core methods of active learning, including the evaluation\nof informativeness and sampling strategy. For the first time, we provide a\ndetailed summary of the integration of active learning with other\nlabel-efficient techniques, such as semi-supervised, self-supervised learning,\nand so on. Additionally, we also highlight active learning works that are\nspecifically tailored to medical image analysis. In the end, we offer our\nperspectives on the future trends and challenges of active learning and its\napplications in medical image analysis.\n",
                "链接": "https://arxiv.org/abs/2310.14230"
            },
            {
                "文章ID": "11766",
                "标题": "Intelligent Masking: Deep Q-Learning for Context Encoding in Medical\n  Image Analysis",
                "作者": " Mojtaba Bahrami,  Mahsa Ghorbani,  Nassir Navab",
                "发布日期": "2022-04-06",
                "摘要": "  The need for a large amount of labeled data in the supervised setting has led\nrecent studies to utilize self-supervised learning to pre-train deep neural\nnetworks using unlabeled data. Many self-supervised training strategies have\nbeen investigated especially for medical datasets to leverage the information\navailable in the much fewer unlabeled data. One of the fundamental strategies\nin image-based self-supervision is context prediction. In this approach, a\nmodel is trained to reconstruct the contents of an arbitrary missing region of\nan image based on its surroundings. However, the existing methods adopt a\nrandom and blind masking approach by focusing uniformly on all regions of the\nimages. This approach results in a lot of unnecessary network updates that\ncause the model to forget the rich extracted features. In this work, we develop\na novel self-supervised approach that occludes targeted regions to improve the\npre-training procedure. To this end, we propose a reinforcement learning-based\nagent which learns to intelligently mask input images through deep Q-learning.\nWe show that training the agent against the prediction model can significantly\nimprove the semantic features extracted for downstream classification tasks. We\nperform our experiments on two public datasets for diagnosing breast cancer in\nthe ultrasound images and detecting lower-grade glioma with MR images. In our\nexperiments, we show that our novel masking strategy advances the learned\nfeatures according to the performance on the classification task in terms of\naccuracy, macro F1, and AUROC.\n",
                "链接": "https://arxiv.org/abs/2203.13865"
            },
            {
                "文章ID": "65067",
                "标题": "Evaluating the Fairness of Deep Learning Uncertainty Estimates in\n  Medical Image Analysis",
                "作者": " Raghav Mehta,  Changjian Shui,  Tal Arbel",
                "发布日期": "2023-03-07",
                "摘要": "  Although deep learning (DL) models have shown great success in many medical\nimage analysis tasks, deployment of the resulting models into real clinical\ncontexts requires: (1) that they exhibit robustness and fairness across\ndifferent sub-populations, and (2) that the confidence in DL model predictions\nbe accurately expressed in the form of uncertainties. Unfortunately, recent\nstudies have indeed shown significant biases in DL models across demographic\nsubgroups (e.g., race, sex, age) in the context of medical image analysis,\nindicating a lack of fairness in the models. Although several methods have been\nproposed in the ML literature to mitigate a lack of fairness in DL models, they\nfocus entirely on the absolute performance between groups without considering\ntheir effect on uncertainty estimation. In this work, we present the first\nexploration of the effect of popular fairness models on overcoming biases\nacross subgroups in medical image analysis in terms of bottom-line performance,\nand their effects on uncertainty quantification. We perform extensive\nexperiments on three different clinically relevant tasks: (i) skin lesion\nclassification, (ii) brain tumour segmentation, and (iii) Alzheimer's disease\nclinical score regression. Our results indicate that popular ML methods, such\nas data-balancing and distributionally robust optimization, succeed in\nmitigating fairness issues in terms of the model performances for some of the\ntasks. However, this can come at the cost of poor uncertainty estimates\nassociated with the model predictions. This tradeoff must be mitigated if\nfairness models are to be adopted in medical image analysis.\n",
                "链接": "https://arxiv.org/abs/2303.03242"
            },
            {
                "文章ID": "68134",
                "标题": "Label-Efficient Deep Learning in Medical Image Analysis: Challenges and\n  Future Directions",
                "作者": " Cheng Jin,  Zhengrui Guo,  Yi Lin,  Luyang Luo,  Hao Chen",
                "发布日期": "2023-12-21",
                "摘要": "  Deep learning has seen rapid growth in recent years and achieved\nstate-of-the-art performance in a wide range of applications. However, training\nmodels typically requires expensive and time-consuming collection of large\nquantities of labeled data. This is particularly true within the scope of\nmedical imaging analysis (MIA), where data are limited and labels are expensive\nto be acquired. Thus, label-efficient deep learning methods are developed to\nmake comprehensive use of the labeled data as well as the abundance of\nunlabeled and weak-labeled data. In this survey, we extensively investigated\nover 300 recent papers to provide a comprehensive overview of recent progress\non label-efficient learning strategies in MIA. We first present the background\nof label-efficient learning and categorize the approaches into different\nschemes. Next, we examine the current state-of-the-art methods in detail\nthrough each scheme. Specifically, we provide an in-depth investigation,\ncovering not only canonical semi-supervised, self-supervised, and\nmulti-instance learning schemes, but also recently emerged active and\nannotation-efficient learning strategies. Moreover, as a comprehensive\ncontribution to the field, this survey not only elucidates the commonalities\nand unique features of the surveyed methods but also presents a detailed\nanalysis of the current challenges in the field and suggests potential avenues\nfor future research.\n",
                "链接": "https://arxiv.org/abs/2303.12484"
            },
            {
                "文章ID": "2860",
                "标题": "An Analysis on Ensemble Learning optimized Medical Image Classification\n  with Deep Convolutional Neural Networks",
                "作者": " Dominik Müller,  Iñaki Soto-Rey,  Frank Kramer",
                "发布日期": "2022-04-14",
                "摘要": "  Novel and high-performance medical image classification pipelines are heavily\nutilizing ensemble learning strategies. The idea of ensemble learning is to\nassemble diverse models or multiple predictions and, thus, boost prediction\nperformance. However, it is still an open question to what extent as well as\nwhich ensemble learning strategies are beneficial in deep learning based\nmedical image classification pipelines. In this work, we proposed a\nreproducible medical image classification pipeline for analyzing the\nperformance impact of the following ensemble learning techniques: Augmenting,\nStacking, and Bagging. The pipeline consists of state-of-the-art preprocessing\nand image augmentation methods as well as 9 deep convolution neural network\narchitectures. It was applied on four popular medical imaging datasets with\nvarying complexity. Furthermore, 12 pooling functions for combining multiple\npredictions were analyzed, ranging from simple statistical functions like\nunweighted averaging up to more complex learning-based functions like support\nvector machines. Our results revealed that Stacking achieved the largest\nperformance gain of up to 13% F1-score increase. Augmenting showed consistent\nimprovement capabilities by up to 4% and is also applicable to single model\nbased pipelines. Cross-validation based Bagging demonstrated significant\nperformance gain close to Stacking, which resulted in an F1-score increase up\nto +11%. Furthermore, we demonstrated that simple statistical pooling functions\nare equal or often even better than more complex pooling functions. We\nconcluded that the integration of ensemble learning techniques is a powerful\nmethod for any medical image classification pipeline to improve robustness and\nboost performance.\n",
                "链接": "https://arxiv.org/abs/2201.11440"
            },
            {
                "文章ID": "52332",
                "标题": "A Trustworthy Framework for Medical Image Analysis with Deep Learning",
                "作者": " Kai Ma,  Siyuan He,  Pengcheng Xi,  Ashkan Ebadi,  Stéphane Tremblay,  Alexander Wong",
                "发布日期": "2022-12-07",
                "摘要": "  Computer vision and machine learning are playing an increasingly important\nrole in computer-assisted diagnosis; however, the application of deep learning\nto medical imaging has challenges in data availability and data imbalance, and\nit is especially important that models for medical imaging are built to be\ntrustworthy. Therefore, we propose TRUDLMIA, a trustworthy deep learning\nframework for medical image analysis, which adopts a modular design, leverages\nself-supervised pre-training, and utilizes a novel surrogate loss function.\nExperimental evaluations indicate that models generated from the framework are\nboth trustworthy and high-performing. It is anticipated that the framework will\nsupport researchers and clinicians in advancing the use of deep learning for\ndealing with public health crises including COVID-19.\n",
                "链接": "https://arxiv.org/abs/2212.02764"
            },
            {
                "文章ID": "107726",
                "标题": "Data efficient deep learning for medical image analysis: A survey",
                "作者": " Suruchi Kumari,  Pravendra Singh",
                "发布日期": "2023-10-11",
                "摘要": "  The rapid evolution of deep learning has significantly advanced the field of\nmedical image analysis. However, despite these achievements, the further\nenhancement of deep learning models for medical image analysis faces a\nsignificant challenge due to the scarcity of large, well-annotated datasets. To\naddress this issue, recent years have witnessed a growing emphasis on the\ndevelopment of data-efficient deep learning methods. This paper conducts a\nthorough review of data-efficient deep learning methods for medical image\nanalysis. To this end, we categorize these methods based on the level of\nsupervision they rely on, encompassing categories such as no supervision,\ninexact supervision, incomplete supervision, inaccurate supervision, and only\nlimited supervision. We further divide these categories into finer\nsubcategories. For example, we categorize inexact supervision into multiple\ninstance learning and learning with weak annotations. Similarly, we categorize\nincomplete supervision into semi-supervised learning, active learning, and\ndomain-adaptive learning and so on. Furthermore, we systematically summarize\ncommonly used datasets for data efficient deep learning in medical image\nanalysis and investigate future research directions to conclude this survey.\n",
                "链接": "https://arxiv.org/abs/2310.06557"
            },
            {
                "文章ID": "56274",
                "标题": "Deep-learning models in medical image analysis: Detection of esophagitis\n  from the Kvasir Dataset",
                "作者": " Kyoka Yoshiok,  Kensuke Tanioka,  Satoru Hiwa,  Tomoyuki Hiroyasu",
                "发布日期": "2023-01-09",
                "摘要": "  Early detection of esophagitis is important because this condition can\nprogress to cancer if left untreated. However, the accuracies of different deep\nlearning models in detecting esophagitis have yet to be compared. Thus, this\nstudy aimed to compare the accuracies of convolutional neural network models\n(GoogLeNet, ResNet-50, MobileNet V2, and MobileNet V3) in detecting esophagitis\nfrom the open Kvasir dataset of endoscopic images. Results showed that among\nthe models, GoogLeNet achieved the highest F1-scores. Based on the average of\ntrue positive rate, MobileNet V3 predicted esophagitis more confidently than\nthe other models. The results obtained using the models were also compared with\nthose obtained using SHapley Additive exPlanations and Gradient-weighted Class\nActivation Mapping.\n",
                "链接": "https://arxiv.org/abs/2301.02390"
            },
            {
                "文章ID": "3541",
                "标题": "Research on Question Classification Methods in the Medical Field",
                "作者": " Jinzhang Liu",
                "发布日期": "2022-02-02",
                "摘要": "  Question classification is one of the important links in the research of\nquestion and answering system. The existing question classification models are\nmore trained on public data sets. At present, there is a lack of question\nclassification data sets in specific fields, especially in the medical field.\nTo make up for this gap, this paper presents a data set for question\nclassification in the medical field. Moreover, this paper proposes a\nmulti-dimensional extraction of the characteristics of the question by\ncombining multiple neural network models, and proposes a question\nclassification model based on multi-dimensional feature extraction. The\nexperimental results show that the proposed method can effectively improve the\nperformance of question classification.\n",
                "链接": "https://arxiv.org/abs/2202.00298"
            }
        ]
    },
    {
        "question": {
            "question": "查找基于优化实现模型越狱的文献",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "47676",
                "标题": "On Optimizing the Communication of Model Parallelism",
                "作者": " Yonghao Zhuang,  Hexu Zhao,  Lianmin Zheng,  Zhuohan Li,  Eric P. Xing,  Qirong Ho,  Joseph E. Gonzalez,  Ion Stoica,  Hao Zhang",
                "发布日期": "2022-11-11",
                "摘要": "  We study a novel and important communication pattern in large-scale\nmodel-parallel deep learning (DL), which we call cross-mesh resharding. This\npattern emerges when the two paradigms of model parallelism - intra-operator\nand inter-operator parallelism - are combined to support large models on large\nclusters. In cross-mesh resharding, a sharded tensor needs to be sent from a\nsource device mesh to a destination device mesh, on which the tensor may be\ndistributed with the same or different layouts. We formalize this as a\nmany-to-many multicast communication problem, and show that existing approaches\neither are sub-optimal or do not generalize to different network topologies or\ntensor layouts, which result from different model architectures and parallelism\nstrategies. We then propose two contributions to address cross-mesh resharding:\nan efficient broadcast-based communication system, and an\n\"overlapping-friendly\" pipeline schedule. On microbenchmarks, our overall\nsystem outperforms existing ones by up to 10x across various tensor and mesh\nlayouts. On end-to-end training of two large models, GPT-3 and U-Transformer,\nwe improve throughput by 10% and 50%, respectively.\n",
                "链接": "https://arxiv.org/abs/2211.05322"
            },
            {
                "文章ID": "9952",
                "标题": "The Design and Implementation of a Broadly Applicable Algorithm for\n  Optimizing Intra-Day Surgical Scheduling",
                "作者": " Jin Xie,  Teng Zhang,  Jose Blanchet,  Peter Glynn,  Matthew Randolph,  David Scheinker",
                "发布日期": "2022-03-17",
                "摘要": "  Surgical scheduling optimization is an active area of research. However, few\nalgorithms to optimize surgical scheduling are implemented and see sustained\nuse. An algorithm is more likely to be implemented, if it allows for surgeon\nautonomy, i.e., requires only limited scheduling centralization, and functions\nin the limited technical infrastructure of widely used electronic medical\nrecords (EMRs). In order for an algorithm to see sustained use, it must be\ncompatible with changes to hospital capacity, patient volumes, and scheduling\npractices. To meet these objectives, we developed the BEDS (better elective day\nof surgery) algorithm, a greedy heuristic for smoothing unit-specific surgical\nadmissions across days. We implemented BEDS in the EMR of a large pediatric\nacademic medical center.\n  The use of BEDS was associated with a reduction in the variability in the\nnumber of admissions. BEDS is freely available as a dashboard in Tableau, a\ncommercial software used by numerous hospitals. BEDS is readily implementable\nwith the limited tools available to most hospitals, does not require reductions\nto surgeon autonomy or centralized scheduling, and is compatible with changes\nto hospital capacity or patient volumes. We present a general algorithmic\nframework from which BEDS is derived based on a particular choice of objectives\nand constraints. We argue that algorithms generated by this framework retain\nmany of the desirable characteristics of BEDS while being compatible with a\nwide range of objectives and constraints.\n",
                "链接": "https://arxiv.org/abs/2203.08146"
            },
            {
                "文章ID": "60419",
                "标题": "The Effect of Metadata on Scientific Literature Tagging: A Cross-Field\n  Cross-Model Study",
                "作者": " Yu Zhang,  Bowen Jin,  Qi Zhu,  Yu Meng,  Jiawei Han",
                "发布日期": "2023-10-24",
                "摘要": "  Due to the exponential growth of scientific publications on the Web, there is\na pressing need to tag each paper with fine-grained topics so that researchers\ncan track their interested fields of study rather than drowning in the whole\nliterature. Scientific literature tagging is beyond a pure multi-label text\nclassification task because papers on the Web are prevalently accompanied by\nmetadata information such as venues, authors, and references, which may serve\nas additional signals to infer relevant tags. Although there have been studies\nmaking use of metadata in academic paper classification, their focus is often\nrestricted to one or two scientific fields (e.g., computer science and\nbiomedicine) and to one specific model. In this work, we systematically study\nthe effect of metadata on scientific literature tagging across 19 fields. We\nselect three representative multi-label classifiers (i.e., a bag-of-words\nmodel, a sequence-based model, and a pre-trained language model) and explore\ntheir performance change in scientific literature tagging when metadata are fed\nto the classifiers as additional features. We observe some ubiquitous patterns\nof metadata's effects across all fields (e.g., venues are consistently\nbeneficial to paper tagging in almost all cases), as well as some unique\npatterns in fields other than computer science and biomedicine, which are not\nexplored in previous studies.\n",
                "链接": "https://arxiv.org/abs/2302.03341"
            },
            {
                "文章ID": "82",
                "标题": "A Systematic Literature Review on Persuasive Technology at the Workplace",
                "作者": " Kilian Wenker",
                "发布日期": "2022-08-15",
                "摘要": "  Employees face decisions every day - in the absence of supervision. The\noutcome of these decisions can be influenced by digital workplace design\nthrough the power of persuasive technology. This paper provides a structured\nliterature review based on recent research on persuasive technology in the\nworkplace. It examines the design and use of persuasive systems from a variety\nof disciplinary perspectives and theories. The reviewed studies were\ncategorized into the research streams of technology design, user-centered\nresearch, and gamification. The purpose of the studies is categorized using a\nmodified definition of the persuasive systems design model. A number of\nexperimental studies show that alignment of the employee's behavior with the\nemployer's agenda can be achieved. A robust finding is the key role of\ninteractivity in granting employees a subjective experience of rapid and\nmeaningful feedback when using the interface.\n",
                "链接": "https://arxiv.org/abs/2201.00329"
            },
            {
                "文章ID": "107289",
                "标题": "Find Your Optimal Assignments On-the-fly: A Holistic Framework for\n  Clustered Federated Learning",
                "作者": " Yongxin Guo,  Xiaoying Tang,  Tao Lin",
                "发布日期": "2023-10-10",
                "摘要": "  Federated Learning (FL) is an emerging distributed machine learning approach\nthat preserves client privacy by storing data on edge devices. However, data\nheterogeneity among clients presents challenges in training models that perform\nwell on all local distributions. Recent studies have proposed clustering as a\nsolution to tackle client heterogeneity in FL by grouping clients with\ndistribution shifts into different clusters. However, the diverse learning\nframeworks used in current clustered FL methods make it challenging to\nintegrate various clustered FL methods, gather their benefits, and make further\nimprovements.\n  To this end, this paper presents a comprehensive investigation into current\nclustered FL methods and proposes a four-tier framework, namely HCFL, to\nencompass and extend existing approaches. Based on the HCFL, we identify the\nremaining challenges associated with current clustering methods in each tier\nand propose an enhanced clustering method called HCFL+ to address these\nchallenges. Through extensive numerical evaluations, we showcase the\neffectiveness of our clustering framework and the improved components. Our code\nwill be publicly available.\n",
                "链接": "https://arxiv.org/abs/2310.05397"
            },
            {
                "文章ID": "85932",
                "标题": "OMS-DPM: Optimizing the Model Schedule for Diffusion Probabilistic\n  Models",
                "作者": " Enshu Liu,  Xuefei Ning,  Zinan Lin,  Huazhong Yang,  Yu Wang",
                "发布日期": "2023-06-16",
                "摘要": "  Diffusion probabilistic models (DPMs) are a new class of generative models\nthat have achieved state-of-the-art generation quality in various domains.\nDespite the promise, one major drawback of DPMs is the slow generation speed\ndue to the large number of neural network evaluations required in the\ngeneration process. In this paper, we reveal an overlooked dimension -- model\nschedule -- for optimizing the trade-off between generation quality and speed.\nMore specifically, we observe that small models, though having worse generation\nquality when used alone, could outperform large models in certain generation\nsteps. Therefore, unlike the traditional way of using a single model, using\ndifferent models in different generation steps in a carefully designed\n\\emph{model schedule} could potentially improve generation quality and speed\n\\emph{simultaneously}. We design OMS-DPM, a predictor-based search algorithm,\nto optimize the model schedule given an arbitrary generation time budget and a\nset of pre-trained models. We demonstrate that OMS-DPM can find model schedules\nthat improve generation quality and speed than prior state-of-the-art methods\nacross CIFAR-10, CelebA, ImageNet, and LSUN datasets. When applied to the\npublic checkpoints of the Stable Diffusion model, we are able to accelerate the\nsampling by 2$\\times$ while maintaining the generation quality.\n",
                "链接": "https://arxiv.org/abs/2306.08860"
            },
            {
                "文章ID": "90198",
                "标题": "Model-Driven Engineering for Artificial Intelligence -- A Systematic\n  Literature Review",
                "作者": " Simon Raedler,  Luca Berardinelli,  Karolin Winter,  Abbas Rahimi,  Stefanie Rinderle-Ma",
                "发布日期": "2023-07-11",
                "摘要": "  Objective: This study aims to investigate the existing body of knowledge in\nthe field of Model-Driven Engineering MDE in support of AI (MDE4AI) to sharpen\nfuture research further and define the current state of the art.\n  Method: We conducted a Systemic Literature Review (SLR), collecting papers\nfrom five major databases resulting in 703 candidate studies, eventually\nretaining 15 primary studies. Each primary study will be evaluated and\ndiscussed with respect to the adoption of (1) MDE principles and practices and\n(2) the phases of AI development support aligned with the stages of the\nCRISP-DM methodology.\n  Results: The study's findings show that the pillar concepts of MDE\n(metamodel, concrete syntax and model transformation), are leveraged to define\ndomain-specific languages (DSL) explicitly addressing AI concerns. Different\nMDE technologies are used, leveraging different language workbenches. The most\nprominent AI-related concerns are training and modeling of the AI algorithm,\nwhile minor emphasis is given to the time-consuming preparation of the data\nsets. Early project phases that support interdisciplinary communication of\nrequirements, such as the CRISP-DM \\textit{Business Understanding} phase, are\nrarely reflected.\n  Conclusion: The study found that the use of MDE for AI is still in its early\nstages, and there is no single tool or method that is widely used.\nAdditionally, current approaches tend to focus on specific stages of\ndevelopment rather than providing support for the entire development process.\nAs a result, the study suggests several research directions to further improve\nthe use of MDE for AI and to guide future research in this area.\n",
                "链接": "https://arxiv.org/abs/2307.04599"
            },
            {
                "文章ID": "83104",
                "标题": "A systematic literature review on the code smells datasets and\n  validation mechanisms",
                "作者": " Morteza Zakeri-Nasrabadi,  Saeed Parsa,  Ehsan Esmaili,  Fabio Palomba",
                "发布日期": "2023-06-05",
                "摘要": "  The accuracy reported for code smell-detecting tools varies depending on the\ndataset used to evaluate the tools. Our survey of 45 existing datasets reveals\nthat the adequacy of a dataset for detecting smells highly depends on relevant\nproperties such as the size, severity level, project types, number of each type\nof smell, number of smells, and the ratio of smelly to non-smelly samples in\nthe dataset. Most existing datasets support God Class, Long Method, and Feature\nEnvy while six smells in Fowler and Beck's catalog are not supported by any\ndatasets. We conclude that existing datasets suffer from imbalanced samples,\nlack of supporting severity level, and restriction to Java language.\n",
                "链接": "https://arxiv.org/abs/2306.01377"
            },
            {
                "文章ID": "106864",
                "标题": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language\n  Models",
                "作者": " Xiaogeng Liu,  Nan Xu,  Muhao Chen,  Chaowei Xiao",
                "发布日期": "2023-10-10",
                "摘要": "  The aligned Large Language Models (LLMs) are powerful language understanding\nand decision-making tools that are created through extensive alignment with\nhuman feedback. However, these large models remain susceptible to jailbreak\nattacks, where adversaries manipulate prompts to elicit malicious outputs that\nshould not be given by aligned LLMs. Investigating jailbreak prompts can lead\nus to delve into the limitations of LLMs and further guide us to secure them.\nUnfortunately, existing jailbreak techniques suffer from either (1) scalability\nissues, where attacks heavily rely on manual crafting of prompts, or (2)\nstealthiness problems, as attacks depend on token-based algorithms to generate\nprompts that are often semantically meaningless, making them susceptible to\ndetection through basic perplexity testing. In light of these challenges, we\nintend to answer this question: Can we develop an approach that can\nautomatically generate stealthy jailbreak prompts? In this paper, we introduce\nAutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can\nautomatically generate stealthy jailbreak prompts by the carefully designed\nhierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN\nnot only automates the process while preserving semantic meaningfulness, but\nalso demonstrates superior attack strength in cross-model transferability, and\ncross-sample universality compared with the baseline. Moreover, we also compare\nAutoDAN with perplexity-based defense methods and show that AutoDAN can bypass\nthem effectively.\n",
                "链接": "https://arxiv.org/abs/2310.04451"
            },
            {
                "文章ID": "95205",
                "标题": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak\n  Prompts on Large Language Models",
                "作者": " Xinyue Shen,  Zeyuan Chen,  Michael Backes,  Yun Shen,  Yang Zhang",
                "发布日期": "2023-08-09",
                "摘要": "  The misuse of large language models (LLMs) has garnered significant attention\nfrom the general public and LLM vendors. In response, efforts have been made to\nalign LLMs with human values and intent use. However, a particular type of\nadversarial prompts, known as jailbreak prompt, has emerged and continuously\nevolved to bypass the safeguards and elicit harmful content from LLMs. In this\npaper, we conduct the first measurement study on jailbreak prompts in the wild,\nwith 6,387 prompts collected from four platforms over six months. Leveraging\nnatural language processing technologies and graph-based community detection\nmethods, we discover unique characteristics of jailbreak prompts and their\nmajor attack strategies, such as prompt injection and privilege escalation. We\nalso observe that jailbreak prompts increasingly shift from public platforms to\nprivate ones, posing new challenges for LLM vendors in proactive detection. To\nassess the potential harm caused by jailbreak prompts, we create a question set\ncomprising 46,800 samples across 13 forbidden scenarios. Our experiments show\nthat current LLMs and safeguards cannot adequately defend jailbreak prompts in\nall scenarios. Particularly, we identify two highly effective jailbreak prompts\nwhich achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and\nthey have persisted online for over 100 days. Our work sheds light on the\nsevere and evolving threat landscape of jailbreak prompts. We hope our study\ncan facilitate the research community and LLM vendors in promoting safer and\nregulated LLMs.\n",
                "链接": "https://arxiv.org/abs/2308.03825"
            }
        ]
    },
    {
        "question": {
            "question": "强化学习在大语言模型领域中应用的相关论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "39445",
                "标题": "Reinforcement Learning with Tensor Networks: Application to Dynamical\n  Large Deviations",
                "作者": " Edward Gillman,  Dominic C. Rose,  Juan P. Garrahan",
                "发布日期": "2022-09-29",
                "摘要": "  We present a framework to integrate tensor network (TN) methods with\nreinforcement learning (RL) for solving dynamical optimisation tasks. We\nconsider the RL actor-critic method, a model-free approach for solving RL\nproblems, and introduce TNs as the approximators for its policy and value\nfunctions. Our \"actor-critic with tensor networks\" (ACTeN) method is especially\nwell suited to problems with large and factorisable state and action spaces. As\nan illustration of the applicability of ACTeN we solve the exponentially hard\ntask of sampling rare trajectories in two paradigmatic stochastic models, the\nEast model of glasses and the asymmetric simple exclusion process (ASEP), the\nlatter being particularly challenging to other methods due to the absence of\ndetailed balance. With substantial potential for further integration with the\nvast array of existing RL methods, the approach introduced here is promising\nboth for applications in physics and to multi-agent RL problems more generally.\n",
                "链接": "https://arxiv.org/abs/2209.14089"
            },
            {
                "文章ID": "107938",
                "标题": "MatChat: A Large Language Model and Application Service Platform for\n  Materials Science",
                "作者": " Ziyi Chen,  Fankai Xie,  Meng Wan,  Yang Yuan,  Miao Liu,  Zongguo Wang,  Sheng Meng,  Yangang Wang",
                "发布日期": "2023-11-03",
                "摘要": "  The prediction of chemical synthesis pathways plays a pivotal role in\nmaterials science research. Challenges, such as the complexity of synthesis\npathways and the lack of comprehensive datasets, currently hinder our ability\nto predict these chemical processes accurately. However, recent advancements in\ngenerative artificial intelligence (GAI), including automated text generation\nand question-answering systems, coupled with fine-tuning techniques, have\nfacilitated the deployment of large-scale AI models tailored to specific\ndomains. In this study, we harness the power of the LLaMA2-7B model and enhance\nit through a learning process that incorporates 13,878 pieces of structured\nmaterial knowledge data. This specialized AI model, named MatChat, focuses on\npredicting inorganic material synthesis pathways. MatChat exhibits remarkable\nproficiency in generating and reasoning with knowledge in materials science.\nAlthough MatChat requires further refinement to meet the diverse material\ndesign needs, this research undeniably highlights its impressive reasoning\ncapabilities and innovative potential in the field of materials science.\nMatChat is now accessible online and open for use, with both the model and its\napplication framework available as open source. This study establishes a robust\nfoundation for collaborative innovation in the integration of generative AI in\nmaterials science.\n",
                "链接": "https://arxiv.org/abs/2310.07197"
            },
            {
                "文章ID": "117475",
                "标题": "Large Language Model is a Good Policy Teacher for Training Reinforcement\n  Learning Agents",
                "作者": " Zihao Zhou,  Bin Hu,  Pu Zhang,  Chenyang Zhao,  Bin Liu",
                "发布日期": "2023-11-30",
                "摘要": "  Recent studies have shown that Large Language Models (LLMs) can be utilized\nfor solving complex sequential decision-making tasks by providing high-level\ninstructions. However, LLM-based agents face limitations in real-time dynamic\nenvironments due to their lack of specialization in solving specific target\nproblems. Moreover, the deployment of such LLM-based agents is both costly and\ntime-consuming in practical scenarios. In this paper, we introduce a novel\nframework that addresses these challenges by training a smaller scale\nspecialized student agent using instructions from an LLM-based teacher agent.\nBy leveraging guided actions provided by the teachers, the prior knowledge of\nthe LLM is distilled into the local student model. Consequently, the student\nagent can be trained with significantly less data. Furthermore, subsequent\ntraining with environment feedback empowers the student agents to surpass the\ncapabilities of their teachers. We conducted experiments on three challenging\nMiniGrid environments to evaluate the effectiveness of our framework. The\nresults demonstrate that our approach enhances sample efficiency and achieves\nsuperior performance compared to baseline methods. Our code is available at\nhttps://github.com/ZJLAB-AMMI/LLM4Teach.\n",
                "链接": "https://arxiv.org/abs/2311.13373"
            },
            {
                "文章ID": "81540",
                "标题": "KoSBi: A Dataset for Mitigating Social Bias Risks Towards Safer Large\n  Language Model Application",
                "作者": " Hwaran Lee,  Seokhee Hong,  Joonsuk Park,  Takyoung Kim,  Gunhee Kim,  Jung-Woo Ha",
                "发布日期": "2023-05-31",
                "摘要": "  Large language models (LLMs) learn not only natural text generation abilities\nbut also social biases against different demographic groups from real-world\ndata. This poses a critical risk when deploying LLM-based applications.\nExisting research and resources are not readily applicable in South Korea due\nto the differences in language and culture, both of which significantly affect\nthe biases and targeted demographic groups. This limitation requires localized\nsocial bias datasets to ensure the safe and effective deployment of LLMs. To\nthis end, we present KO SB I, a new social bias dataset of 34k pairs of\ncontexts and sentences in Korean covering 72 demographic groups in 15\ncategories. We find that through filtering-based moderation, social biases in\ngenerated content can be reduced by 16.47%p on average for HyperCLOVA (30B and\n82B), and GPT-3.\n",
                "链接": "https://arxiv.org/abs/2305.17701"
            },
            {
                "文章ID": "101258",
                "标题": "Large Language Models for Difficulty Estimation of Foreign Language\n  Content with Application to Language Learning",
                "作者": " Michalis Vlachos,  Mircea Lungu,  Yash Raj Shrestha,  Johannes-Rudolf David",
                "发布日期": "2023-09-12",
                "摘要": "  We use large language models to aid learners enhance proficiency in a foreign\nlanguage. This is accomplished by identifying content on topics that the user\nis interested in, and that closely align with the learner's proficiency level\nin that foreign language. Our work centers on French content, but our approach\nis readily transferable to other languages. Our solution offers several\ndistinctive characteristics that differentiate it from existing\nlanguage-learning solutions, such as, a) the discovery of content across topics\nthat the learner cares about, thus increasing motivation, b) a more precise\nestimation of the linguistic difficulty of the content than traditional\nreadability measures, and c) the availability of both textual and video-based\ncontent. The linguistic complexity of video content is derived from the video\ncaptions. It is our aspiration that such technology will enable learners to\nremain engaged in the language-learning process by continuously adapting the\ntopics and the difficulty of the content to align with the learners' evolving\ninterests and learning objectives.\n",
                "链接": "https://arxiv.org/abs/2309.05142"
            },
            {
                "文章ID": "110634",
                "标题": "AlpaCare:Instruction-tuned Large Language Models for Medical Application",
                "作者": " Xinlu Zhang,  Chenxin Tian,  Xianjun Yang,  Lichang Chen,  Zekun Li,  Linda Ruth Petzold",
                "发布日期": "2023-10-24",
                "摘要": "  Large Language Models (LLMs) have demonstrated significant enhancements in\ninstruction-following abilities through instruction tuning, achieving notable\nperformances across various tasks. Previous research has focused on fine-tuning\nmedical domain-specific LLMs using an extensive array of medical-specific data,\nincorporating millions of pieces of biomedical literature to augment their\nmedical capabilities. However, existing medical instruction-tuned LLMs have\nbeen constrained by the limited scope of tasks and instructions available,\nrestricting the efficacy of instruction tuning and adversely affecting\nperformance in the general domain. In this paper, we fine-tune LLaMA-series\nmodels using 52k diverse, machine-generated, medical instruction-following\ndata, MedInstruct-52k, resulting in the model AlpaCare. Comprehensive\nexperimental results on both general and medical-specific domain free-form\ninstruction evaluations showcase AlpaCare's strong medical proficiency and\ngeneralizability compared to previous instruction-tuned models in both medical\nand general domains. We provide public access to our MedInstruct-52k dataset\nand a clinician-crafted free-form instruction test set, MedInstruct-test, along\nwith our codebase, to foster further research and development. Our project page\nis available at https://github.com/XZhang97666/AlpaCare.\n",
                "链接": "https://arxiv.org/abs/2310.14558"
            },
            {
                "文章ID": "32871",
                "标题": "Model-Free Generative Replay for Lifelong Reinforcement Learning:\n  Application to Starcraft-2",
                "作者": " Zachary Daniels,  Aswin Raghavan,  Jesse Hostetler,  Abrar Rahman,  Indranil Sur,  Michael Piacentino,  Ajay Divakaran",
                "发布日期": "2022-08-17",
                "摘要": "  One approach to meet the challenges of deep lifelong reinforcement learning\n(LRL) is careful management of the agent's learning experiences, to learn\n(without forgetting) and build internal meta-models (of the tasks,\nenvironments, agents, and world). Generative replay (GR) is a biologically\ninspired replay mechanism that augments learning experiences with self-labelled\nexamples drawn from an internal generative model that is updated over time. We\npresent a version of GR for LRL that satisfies two desiderata: (a)\nIntrospective density modelling of the latent representations of policies\nlearned using deep RL, and (b) Model-free end-to-end learning. In this paper,\nwe study three deep learning architectures for model-free GR, starting from a\nna\\\"ive GR and adding ingredients to achieve (a) and (b). We evaluate our\nproposed algorithms on three different scenarios comprising tasks from the\nStarcraft-2 and Minigrid domains. We report several key findings showing the\nimpact of the design choices on quantitative metrics that include transfer\nlearning, generalization to unseen tasks, fast adaptation after task change,\nperformance wrt task expert, and catastrophic forgetting. We observe that our\nGR prevents drift in the features-to-action mapping from the latent vector\nspace of a deep RL agent. We also show improvements in established lifelong\nlearning metrics. We find that a small random replay buffer significantly\nincreases the stability of training. Overall, we find that \"hidden replay\" (a\nwell-known architecture for class-incremental classification) is the most\npromising approach that pushes the state-of-the-art in GR for LRL and observe\nthat the architecture of the sleep model might be more important for improving\nperformance than the types of replay used. Our experiments required only 6% of\ntraining samples to achieve 80-90% of expert performance in most Starcraft-2\nscenarios.\n",
                "链接": "https://arxiv.org/abs/2208.05056"
            },
            {
                "文章ID": "76562",
                "标题": "Large Language Model Programs",
                "作者": " Imanol Schlag,  Sainbayar Sukhbaatar,  Asli Celikyilmaz,  Wen-tau Yih,  Jason Weston,  Jürgen Schmidhuber,  Xian Li",
                "发布日期": "2023-05-10",
                "摘要": "  In recent years, large pre-trained language models (LLMs) have demonstrated\nthe ability to follow instructions and perform novel tasks from a few examples.\nThe possibility to parameterise an LLM through such in-context examples widens\ntheir capability at a much lower cost than finetuning. We extend this line of\nreasoning and present a method which further expands the capabilities of an LLM\nby embedding it within an algorithm or program. To demonstrate the benefits of\nthis approach, we present an illustrative example of evidence-supported\nquestion-answering. We obtain a 6.4\\% improvement over the chain of thought\nbaseline through a more algorithmic approach without any finetuning.\nFurthermore, we highlight recent work from this perspective and discuss the\nadvantages and disadvantages in comparison to the standard approaches.\n",
                "链接": "https://arxiv.org/abs/2305.05364"
            },
            {
                "文章ID": "109237",
                "标题": "Large Language Model Unlearning",
                "作者": " Yuanshun Yao,  Xiaojun Xu,  Yang Liu",
                "发布日期": "2023-10-18",
                "摘要": "  We study how to perform unlearning, i.e. forgetting undesirable\n(mis)behaviors, on large language models (LLMs). We show at least three\nscenarios of aligning LLMs with human preferences can benefit from unlearning:\n(1) removing harmful responses, (2) erasing copyright-protected content as\nrequested, and (3) eliminating hallucinations. Unlearning, as an alignment\ntechnique, has three advantages. (1) It only requires negative (e.g. harmful)\nexamples, which are much easier and cheaper to collect (e.g. via red teaming or\nuser reporting) than positive (e.g. helpful and often human-written) examples\nrequired in RLHF (RL from human feedback). (2) It is computationally efficient.\n(3) It is especially effective when we know which training samples cause the\nmisbehavior. To the best of our knowledge, our work is among the first to\nexplore LLM unlearning. We are also among the first to formulate the settings,\ngoals, and evaluations in LLM unlearning. We show that if practitioners only\nhave limited resources, and therefore the priority is to stop generating\nundesirable outputs rather than to try to generate desirable outputs,\nunlearning is particularly appealing. Despite only having negative samples, our\nablation study shows that unlearning can still achieve better alignment\nperformance than RLHF with just 2% of its computational time.\n",
                "链接": "https://arxiv.org/abs/2310.10683"
            },
            {
                "文章ID": "80020",
                "标题": "Language Model Self-improvement by Reinforcement Learning Contemplation",
                "作者": " Jing-Cheng Pang,  Pengyuan Wang,  Kaiyuan Li,  Xiong-Hui Chen,  Jiacheng Xu,  Zongzhang Zhang,  Yang Yu",
                "发布日期": "2023-05-25",
                "摘要": "  Large Language Models (LLMs) have exhibited remarkable performance across\nvarious natural language processing (NLP) tasks. However, fine-tuning these\nmodels often necessitates substantial supervision, which can be expensive and\ntime-consuming to obtain. This paper introduces a novel unsupervised method\ncalled LanguageModel Self-Improvement by Reinforcement Learning Contemplation\n(SIRLC) that improves LLMs without reliance on external labels. Our approach is\ngrounded in the observation that it is simpler for language models to assess\ntext quality than to generate text. Building on this insight, SIRLC assigns\nLLMs dual roles as both student and teacher. As a student, the LLM generates\nanswers to unlabeled questions, while as a teacher, it evaluates the generated\ntext and assigns scores accordingly. The model parameters are updated using\nreinforcement learning to maximize the evaluation score. We demonstrate that\nSIRLC can be applied to various NLP tasks, such as reasoning problems, text\ngeneration, and machine translation. Our experiments show that SIRLC\neffectively improves LLM performance without external supervision, resulting in\na 5.6% increase in answering accuracy for reasoning tasks and a rise in\nBERTScore from 0.82 to 0.86 for translation tasks. Furthermore, SIRLC can be\napplied to models of different sizes, showcasing its broad applicability.\n",
                "链接": "https://arxiv.org/abs/2305.14483"
            }
        ]
    },
    {
        "question": {
            "question": "有关大模型在新任务上面知识迁移的研究",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "117493",
                "标题": "Transfer Attacks and Defenses for Large Language Models on Coding Tasks",
                "作者": " Chi Zhang,  Zifan Wang,  Ravi Mangal,  Matt Fredrikson,  Limin Jia,  Corina Pasareanu",
                "发布日期": "2023-11-23",
                "摘要": "  Modern large language models (LLMs), such as ChatGPT, have demonstrated\nimpressive capabilities for coding tasks including writing and reasoning about\ncode. They improve upon previous neural network models of code, such as\ncode2seq or seq2seq, that already demonstrated competitive results when\nperforming tasks such as code summarization and identifying code\nvulnerabilities. However, these previous code models were shown vulnerable to\nadversarial examples, i.e. small syntactic perturbations that do not change the\nprogram's semantics, such as the inclusion of \"dead code\" through false\nconditions or the addition of inconsequential print statements, designed to\n\"fool\" the models. LLMs can also be vulnerable to the same adversarial\nperturbations but a detailed study on this concern has been lacking so far. In\nthis paper we aim to investigate the effect of adversarial perturbations on\ncoding tasks with LLMs. In particular, we study the transferability of\nadversarial examples, generated through white-box attacks on smaller code\nmodels, to LLMs. Furthermore, to make the LLMs more robust against such\nadversaries without incurring the cost of retraining, we propose prompt-based\ndefenses that involve modifying the prompt to include additional information\nsuch as examples of adversarially perturbed code and explicit instructions for\nreversing adversarial perturbations. Our experiments show that adversarial\nexamples obtained with a smaller code model are indeed transferable, weakening\nthe LLMs' performance. The proposed defenses show promise in improving the\nmodel's resilience, paving the way to more robust defensive solutions for LLMs\nin code-related applications.\n",
                "链接": "https://arxiv.org/abs/2311.13445"
            },
            {
                "文章ID": "110744",
                "标题": "ALCUNA: Large Language Models Meet New Knowledge",
                "作者": " Xunjian Yin,  Baizhou Huang,  Xiaojun Wan",
                "发布日期": "2023-10-24",
                "摘要": "  With the rapid development of NLP, large-scale language models (LLMs) excel\nin various tasks across multiple domains now. However, existing benchmarks may\nnot adequately measure these models' capabilities, especially when faced with\nnew knowledge. In this paper, we address the lack of benchmarks to evaluate\nLLMs' ability to handle new knowledge, an important and challenging aspect in\nthe rapidly evolving world. We propose an approach called KnowGen that\ngenerates new knowledge by altering existing entity attributes and\nrelationships, resulting in artificial entities that are distinct from\nreal-world entities. With KnowGen, we introduce a benchmark named ALCUNA to\nassess LLMs' abilities in knowledge understanding, differentiation, and\nassociation. We benchmark several LLMs, reveals that their performance in face\nof new knowledge is not satisfactory, particularly in reasoning between new and\ninternal knowledge. We also explore the impact of entity similarity on the\nmodel's understanding of entity knowledge and the influence of contextual\nentities. We appeal to the need for caution when using LLMs in new scenarios or\nwith new knowledge, and hope that our benchmarks can help drive the development\nof LLMs in face of new knowledge.\n",
                "链接": "https://arxiv.org/abs/2310.14820"
            },
            {
                "文章ID": "66370",
                "标题": "Algorithmic Ghost in the Research Shell: Large Language Models and\n  Academic Knowledge Creation in Management Research",
                "作者": " Nigel Williams,  Stanislav Ivanov,  Dimitrios Buhalis",
                "发布日期": "2023-03-14",
                "摘要": "  The paper looks at the role of large language models in academic knowledge\ncreation based on a scoping review (2018 to January 2023) of how researchers\nhave previously used the language model GPT to assist in the performance of\nacademic knowledge creation tasks beyond data analysis. These tasks include\nwriting, editing, reviewing, dataset creation and curation, which have been\ndifficult to perform using earlier ML tools. Based on a synthesis of these\npapers, this study identifies pathways for a future academic research landscape\nthat incorporates wider usage of large language models based on the current\nmodes of adoption in published articles as a Co-Writer, Research Assistant and\nRespondent.\n",
                "链接": "https://arxiv.org/abs/2303.07304"
            },
            {
                "文章ID": "114645",
                "标题": "Characterizing Large Language Models as Rationalizers of\n  Knowledge-intensive Tasks",
                "作者": " Aditi Mishra,  Sajjadur Rahman,  Hannah Kim,  Kushan Mitra,  Estevam Hruschka",
                "发布日期": "2023-11-10",
                "摘要": "  Large language models (LLMs) are proficient at generating fluent text with\nminimal task-specific supervision. Yet, their ability to provide well-grounded\nrationalizations for knowledge-intensive tasks remains under-explored. Such\ntasks, like commonsense multiple-choice questions, require rationales based on\nworld knowledge to support predictions and refute alternate options. We\nconsider the task of generating knowledge-guided rationalization in natural\nlanguage by using expert-written examples in a few-shot manner. Surprisingly,\ncrowd-workers preferred knowledge-grounded rationales over crowdsourced\nrationalizations, citing their factuality, sufficiency, and comprehensive\nrefutations. Although LLMs-generated rationales were preferable, further\nimprovements in conciseness and novelty are required. In another study, we show\nhow rationalization of incorrect model predictions erodes humans' trust in\nLLM-generated rationales. Motivated by these observations, we create a\ntwo-stage pipeline to review task predictions and eliminate potential incorrect\ndecisions before rationalization, enabling trustworthy rationale generation.\n",
                "链接": "https://arxiv.org/abs/2311.05085"
            },
            {
                "文章ID": "16746",
                "标题": "Heterogeneous Ensemble Knowledge Transfer for Training Large Models in\n  Federated Learning",
                "作者": " Yae Jee Cho,  Andre Manoel,  Gauri Joshi,  Robert Sim,  Dimitrios Dimitriadis",
                "发布日期": "2022-04-28",
                "摘要": "  Federated learning (FL) enables edge-devices to collaboratively learn a model\nwithout disclosing their private data to a central aggregating server. Most\nexisting FL algorithms require models of identical architecture to be deployed\nacross the clients and server, making it infeasible to train large models due\nto clients' limited system resources. In this work, we propose a novel ensemble\nknowledge transfer method named Fed-ET in which small models (different in\narchitecture) are trained on clients, and used to train a larger model at the\nserver. Unlike in conventional ensemble learning, in FL the ensemble can be\ntrained on clients' highly heterogeneous data. Cognizant of this property,\nFed-ET uses a weighted consensus distillation scheme with diversity\nregularization that efficiently extracts reliable consensus from the ensemble\nwhile improving generalization by exploiting the diversity within the ensemble.\nWe show the generalization bound for the ensemble of weighted models trained on\nheterogeneous datasets that supports the intuition of Fed-ET. Our experiments\non image and language tasks show that Fed-ET significantly outperforms other\nstate-of-the-art FL algorithms with fewer communicated parameters, and is also\nrobust against high data-heterogeneity.\n",
                "链接": "https://arxiv.org/abs/2204.12703"
            },
            {
                "文章ID": "41595",
                "标题": "Knowledge Distillation Transfer Sets and their Impact on Downstream NLU\n  Tasks",
                "作者": " Charith Peris,  Lizhen Tan,  Thomas Gueudre,  Turan Gojayev,  Pan Wei,  Gokmen Oz",
                "发布日期": "2022-10-19",
                "摘要": "  Teacher-student knowledge distillation is a popular technique for compressing\ntoday's prevailing large language models into manageable sizes that fit\nlow-latency downstream applications. Both the teacher and the choice of\ntransfer set used for distillation are crucial ingredients in creating a high\nquality student. Yet, the generic corpora used to pretrain the teacher and the\ncorpora associated with the downstream target domain are often significantly\ndifferent, which raises a natural question: should the student be distilled\nover the generic corpora, so as to learn from high-quality teacher predictions,\nor over the downstream task corpora to align with finetuning? Our study\ninvestigates this trade-off using Domain Classification (DC) and Intent\nClassification/Named Entity Recognition (ICNER) as downstream tasks. We distill\nseveral multilingual students from a larger multilingual LM with varying\nproportions of generic and task-specific datasets, and report their performance\nafter finetuning on DC and ICNER. We observe significant improvements across\ntasks and test sets when only task-specific corpora is used. We also report on\nhow the impact of adding task-specific data to the transfer set correlates with\nthe similarity between generic and task-specific data. Our results clearly\nindicate that, while distillation from a generic LM benefits downstream tasks,\nstudents learn better using target domain data even if it comes at the price of\nnoisier teacher predictions. In other words, target domain data still trumps\nteacher knowledge.\n",
                "链接": "https://arxiv.org/abs/2210.04834"
            },
            {
                "文章ID": "80054",
                "标题": "Sources of Hallucination by Large Language Models on Inference Tasks",
                "作者": " Nick McKenna,  Tianyi Li,  Liang Cheng,  Mohammad Javad Hosseini,  Mark Johnson,  Mark Steedman",
                "发布日期": "2023-10-24",
                "摘要": "  Large Language Models (LLMs) are claimed to be capable of Natural Language\nInference (NLI), necessary for applied tasks like question answering and\nsummarization. We present a series of behavioral studies on several LLM\nfamilies (LLaMA, GPT-3.5, and PaLM) which probe their behavior using controlled\nexperiments. We establish two biases originating from pretraining which predict\nmuch of their behavior, and show that these are major sources of hallucination\nin generative LLMs. First, memorization at the level of sentences: we show\nthat, regardless of the premise, models falsely label NLI test samples as\nentailing when the hypothesis is attested in training data, and that entities\nare used as ``indices'' to access the memorized data. Second, statistical\npatterns of usage learned at the level of corpora: we further show a similar\neffect when the premise predicate is less frequent than that of the hypothesis\nin the training data, a bias following from previous studies. We demonstrate\nthat LLMs perform significantly worse on NLI test samples which do not conform\nto these biases than those which do, and we offer these as valuable controls\nfor future LLM evaluation.\n",
                "链接": "https://arxiv.org/abs/2305.14552"
            },
            {
                "文章ID": "37229",
                "标题": "Learning state correspondence of reinforcement learning tasks for\n  knowledge transfer",
                "作者": " Marko Ruman,  Tatiana V. Guy",
                "发布日期": "2022-09-15",
                "摘要": "  Deep reinforcement learning has shown an ability to achieve super-human\nperformance in solving complex reinforcement learning (RL) tasks only from\nraw-pixels. However, it fails to reuse knowledge from previously learnt tasks\nto solve new, unseen ones. Generalizing and reusing knowledge are the\nfundamental requirements for creating a truly intelligent agent. This work\nproposes a general method for one-to-one transfer learning based on generative\nadversarial network model tailored to RL task.\n",
                "链接": "https://arxiv.org/abs/2209.06604"
            },
            {
                "文章ID": "9744",
                "标题": "Leveraging Visual Knowledge in Language Tasks: An Empirical Study on\n  Intermediate Pre-training for Cross-modal Knowledge Transfer",
                "作者": " Woojeong Jin,  Dong-Ho Lee,  Chenguang Zhu,  Jay Pujara,  Xiang Ren",
                "发布日期": "2022-03-18",
                "摘要": "  Pre-trained language models are still far from human performance in tasks\nthat need understanding of properties (e.g. appearance, measurable quantity)\nand affordances of everyday objects in the real world since the text lacks such\ninformation due to reporting bias. In this work, we study whether integrating\nvisual knowledge into a language model can fill the gap. We investigate two\ntypes of knowledge transfer: (1) text knowledge transfer using image captions\nthat may contain enriched visual knowledge and (2) cross-modal knowledge\ntransfer using both images and captions with vision-language training\nobjectives. On 5 downstream tasks that may need visual knowledge to solve the\nproblem, we perform extensive empirical comparisons over the presented\nobjectives. Our experiments show that visual knowledge transfer can improve\nperformance in both low-resource and fully supervised settings.\n",
                "链接": "https://arxiv.org/abs/2203.07519"
            },
            {
                "文章ID": "110623",
                "标题": "Evaluating Large Language Models on Controlled Generation Tasks",
                "作者": " Jiao Sun,  Yufei Tian,  Wangchunshu Zhou,  Nan Xu,  Qian Hu,  Rahul Gupta,  John Frederick Wieting,  Nanyun Peng,  Xuezhe Ma",
                "发布日期": "2023-10-24",
                "摘要": "  While recent studies have looked into the abilities of large language models\nin various benchmark tasks, including question generation, reading\ncomprehension, multilingual and etc, there have been few studies looking into\nthe controllability of large language models on generation tasks. We present an\nextensive analysis of various benchmarks including a sentence planning\nbenchmark with different granularities. After comparing large language models\nagainst state-of-the-start finetuned smaller models, we present a spectrum\nshowing large language models falling behind, are comparable, or exceed the\nability of smaller models. We conclude that **large language models struggle at\nmeeting fine-grained hard constraints**.\n",
                "链接": "https://arxiv.org/abs/2310.14542"
            }
        ]
    },
    {
        "question": {
            "question": "2023年以后关于大语言模型和人文学科交叉的研究",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "35258",
                "标题": "Labeling of Cultural Heritage Collections on the Intersection of Visual\n  Analytics and Digital Humanities",
                "作者": " Christofer Meinecke",
                "发布日期": "2022-08-30",
                "摘要": "  Engaging in interdisciplinary projects on the intersection between\nvisualization and humanities research can be a challenging endeavor. Challenges\ncan be finding valuable outcomes for both domains, or how to apply\nstate-of-the-art visual analytics methods like supervised machine learning\nalgorithms. We discuss these challenges when working with cultural heritage\ndata. Further, there is a gap in applying these methods to intangible heritage.\nTo give a reflection on some interdisciplinary projects, we present three case\nstudies focusing on the labeling of cultural heritage collections, the problems\nand challenges with the data, the participatory design process, and takeaways\nfor the visualization scholars from these collaborations.\n",
                "链接": "https://arxiv.org/abs/2208.13512"
            },
            {
                "文章ID": "64080",
                "标题": "Beyond the limitations of any imaginable mechanism: large language\n  models and psycholinguistics",
                "作者": " Conor Houghton,  Nina Kazanina,  Priyanka Sukumaran",
                "发布日期": "2023-03-02",
                "摘要": "  Large language models are not detailed models of human linguistic processing.\nThey are, however, extremely successful at their primary task: providing a\nmodel for language. For this reason and because there are no animal models for\nlanguage, large language models are important in psycholinguistics: they are\nuseful as a practical tool, as an illustrative comparative, and\nphilosophically, as a basis for recasting the relationship between language and\nthought.\n",
                "链接": "https://arxiv.org/abs/2303.00077"
            },
            {
                "文章ID": "113893",
                "标题": "On the Intersection of Self-Correction and Trust in Language Models",
                "作者": " Satyapriya Krishna",
                "发布日期": "2023-11-07",
                "摘要": "  Large Language Models (LLMs) have demonstrated remarkable capabilities in\nperforming complex cognitive tasks. However, their complexity and lack of\ntransparency have raised several trustworthiness concerns, including the\npropagation of misinformation and toxicity. Recent research has explored the\nself-correction capabilities of LLMs to enhance their performance. In this\nwork, we investigate whether these self-correction capabilities can be\nharnessed to improve the trustworthiness of LLMs. We conduct experiments\nfocusing on two key aspects of trustworthiness: truthfulness and toxicity. Our\nfindings reveal that self-correction can lead to improvements in toxicity and\ntruthfulness, but the extent of these improvements varies depending on the\nspecific aspect of trustworthiness and the nature of the task. Interestingly,\nour study also uncovers instances of \"self-doubt\" in LLMs during the\nself-correction process, introducing a new set of challenges that need to be\naddressed.\n",
                "链接": "https://arxiv.org/abs/2311.02801"
            },
            {
                "文章ID": "100283",
                "标题": "On the Planning, Search, and Memorization Capabilities of Large Language\n  Models",
                "作者": " Yunhao Yang,  Anshul Tomar",
                "发布日期": "2023-09-06",
                "摘要": "  The rapid advancement of large language models, such as the Generative\nPre-trained Transformer (GPT) series, has had significant implications across\nvarious disciplines. In this study, we investigate the potential of the\nstate-of-the-art large language model (GPT-4) for planning tasks. We explore\nits effectiveness in multiple planning subfields, highlighting both its\nstrengths and limitations. Through a comprehensive examination, we identify\nareas where large language models excel in solving planning problems and reveal\nthe constraints that limit their applicability. Our empirical analysis focuses\non GPT-4's performance in planning domain extraction, graph search path\nplanning, and adversarial planning. We then propose a way of fine-tuning a\ndomain-specific large language model to improve its Chain of Thought (CoT)\ncapabilities for the above-mentioned tasks. The results provide valuable\ninsights into the potential applications of large language models in the\nplanning domain and pave the way for future research to overcome their\nlimitations and expand their capabilities.\n",
                "链接": "https://arxiv.org/abs/2309.01868"
            },
            {
                "文章ID": "70754",
                "标题": "A Bibliometric Review of Large Language Models Research from 2017 to\n  2023",
                "作者": " Lizhou Fan,  Lingyao Li,  Zihui Ma,  Sanggyu Lee,  Huizi Yu,  Libby Hemphill",
                "发布日期": "2023-04-06",
                "摘要": "  Large language models (LLMs) are a class of language models that have\ndemonstrated outstanding performance across a range of natural language\nprocessing (NLP) tasks and have become a highly sought-after research area,\nbecause of their ability to generate human-like language and their potential to\nrevolutionize science and technology. In this study, we conduct bibliometric\nand discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000\npublications, this paper serves as a roadmap for researchers, practitioners,\nand policymakers to navigate the current landscape of LLMs research. We present\nthe research trends from 2017 to early 2023, identifying patterns in research\nparadigms and collaborations. We start with analyzing the core algorithm\ndevelopments and NLP tasks that are fundamental in LLMs research. We then\ninvestigate the applications of LLMs in various fields and domains including\nmedicine, engineering, social science, and humanities. Our review also reveals\nthe dynamic, fast-paced evolution of LLMs research. Overall, this paper offers\nvaluable insights into the current state, impact, and potential of LLMs\nresearch and its applications.\n",
                "链接": "https://arxiv.org/abs/2304.02020"
            },
            {
                "文章ID": "115450",
                "标题": "To Transformers and Beyond: Large Language Models for the Genome",
                "作者": " Micaela E. Consens,  Cameron Dufault,  Michael Wainberg,  Duncan Forster,  Mehran Karimzadeh,  Hani Goodarzi,  Fabian J. Theis,  Alan Moses,  Bo Wang",
                "发布日期": "2023-11-15",
                "摘要": "  In the rapidly evolving landscape of genomics, deep learning has emerged as a\nuseful tool for tackling complex computational challenges. This review focuses\non the transformative role of Large Language Models (LLMs), which are mostly\nbased on the transformer architecture, in genomics. Building on the foundation\nof traditional convolutional neural networks and recurrent neural networks, we\nexplore both the strengths and limitations of transformers and other LLMs for\ngenomics. Additionally, we contemplate the future of genomic modeling beyond\nthe transformer architecture based on current trends in research. The paper\naims to serve as a guide for computational biologists and computer scientists\ninterested in LLMs for genomic data. We hope the paper can also serve as an\neducational introduction and discussion for biologists to a fundamental shift\nin how we will be analyzing genomic data in the future.\n",
                "链接": "https://arxiv.org/abs/2311.07621"
            },
            {
                "文章ID": "123540",
                "标题": "User Modeling in the Era of Large Language Models: Current Research and\n  Future Directions",
                "作者": " Zhaoxuan Tan,  Meng Jiang",
                "发布日期": "2023-12-27",
                "摘要": "  User modeling (UM) aims to discover patterns or learn representations from\nuser data about the characteristics of a specific user, such as profile,\npreference, and personality. The user models enable personalization and\nsuspiciousness detection in many online applications such as recommendation,\neducation, and healthcare. Two common types of user data are text and graph, as\nthe data usually contain a large amount of user-generated content (UGC) and\nonline interactions. The research of text and graph mining is developing\nrapidly, contributing many notable solutions in the past two decades. Recently,\nlarge language models (LLMs) have shown superior performance on generating,\nunderstanding, and even reasoning over text data. The approaches of user\nmodeling have been equipped with LLMs and soon become outstanding. This article\nsummarizes existing research about how and why LLMs are great tools of modeling\nand understanding UGC. Then it reviews a few categories of large language\nmodels for user modeling (LLM-UM) approaches that integrate the LLMs with text\nand graph-based methods in different ways. Then it introduces specific LLM-UM\ntechniques for a variety of UM applications. Finally, it presents remaining\nchallenges and future directions in the LLM-UM research. We maintain the\nreading list at: https://github.com/TamSiuhin/LLM-UM-Reading\n",
                "链接": "https://arxiv.org/abs/2312.11518"
            },
            {
                "文章ID": "117438",
                "标题": "On the Calibration of Large Language Models and Alignment",
                "作者": " Chiwei Zhu,  Benfeng Xu,  Quan Wang,  Yongdong Zhang,  Zhendong Mao",
                "发布日期": "2023-11-23",
                "摘要": "  As large language models attract increasing attention and find widespread\napplication, concurrent challenges of reliability also arise at the same time.\nConfidence calibration, an effective analysis method for gauging the\nreliability of deep models, serves as a crucial tool for assessing and\nimproving their reliability. However, such investigation has been comparatively\nunderexplored. In this work, we conduct a systematic examination of the\ncalibration of aligned language models throughout the entire construction\nprocess, including pretraining and alignment training. At each stage, we\ninvestigate how different training settings, such as parameter scales and\ntraining data, affect model calibration. To thoroughly assess model\ncalibration, we evaluate models on three most concerned aspects: generation,\nfactuality and understanding. Our work sheds light on whether popular LLMs are\nwell-calibrated and how the training process influences model calibration.\n",
                "链接": "https://arxiv.org/abs/2311.13240"
            },
            {
                "文章ID": "103781",
                "标题": "Beyond Traditional Teaching: The Potential of Large Language Models and\n  Chatbots in Graduate Engineering Education",
                "作者": " Mahyar Abedi,  Ibrahem Alshybani,  Muhammad Rubayat Bin Shahadat,  Michael S. Murillo",
                "发布日期": "2023-12-21",
                "摘要": "  In the rapidly evolving landscape of education, digital technologies have\nrepeatedly disrupted traditional pedagogical methods. This paper explores the\nlatest of these disruptions: the potential integration of large language models\n(LLMs) and chatbots into graduate engineering education. We begin by tracing\nhistorical and technological disruptions to provide context and then introduce\nkey terms such as machine learning and deep learning and the underlying\nmechanisms of recent advancements, namely attention/transformer models and\ngraphics processing units. The heart of our investigation lies in the\napplication of an LLM-based chatbot in a graduate fluid mechanics course. We\ndeveloped a question bank from the course material and assessed the chatbot's\nability to provide accurate, insightful responses. The results are encouraging,\ndemonstrating not only the bot's ability to effectively answer complex\nquestions but also the potential advantages of chatbot usage in the classroom,\nsuch as the promotion of self-paced learning, the provision of instantaneous\nfeedback, and the reduction of instructors' workload. The study also examines\nthe transformative effect of intelligent prompting on enhancing the chatbot's\nperformance. Furthermore, we demonstrate how powerful plugins like Wolfram\nAlpha for mathematical problem-solving and code interpretation can\nsignificantly extend the chatbot's capabilities, transforming it into a\ncomprehensive educational tool. While acknowledging the challenges and ethical\nimplications surrounding the use of such AI models in education, we advocate\nfor a balanced approach. The use of LLMs and chatbots in graduate education can\nbe greatly beneficial but requires ongoing evaluation and adaptation to ensure\nethical and efficient use.\n",
                "链接": "https://arxiv.org/abs/2309.13059"
            },
            {
                "文章ID": "66370",
                "标题": "Algorithmic Ghost in the Research Shell: Large Language Models and\n  Academic Knowledge Creation in Management Research",
                "作者": " Nigel Williams,  Stanislav Ivanov,  Dimitrios Buhalis",
                "发布日期": "2023-03-14",
                "摘要": "  The paper looks at the role of large language models in academic knowledge\ncreation based on a scoping review (2018 to January 2023) of how researchers\nhave previously used the language model GPT to assist in the performance of\nacademic knowledge creation tasks beyond data analysis. These tasks include\nwriting, editing, reviewing, dataset creation and curation, which have been\ndifficult to perform using earlier ML tools. Based on a synthesis of these\npapers, this study identifies pathways for a future academic research landscape\nthat incorporates wider usage of large language models based on the current\nmodes of adoption in published articles as a Co-Writer, Research Assistant and\nRespondent.\n",
                "链接": "https://arxiv.org/abs/2303.07304"
            }
        ]
    },
    {
        "question": {
            "question": "2023年以后关于大语言模型和脑科学交叉的研究",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "64080",
                "标题": "Beyond the limitations of any imaginable mechanism: large language\n  models and psycholinguistics",
                "作者": " Conor Houghton,  Nina Kazanina,  Priyanka Sukumaran",
                "发布日期": "2023-03-02",
                "摘要": "  Large language models are not detailed models of human linguistic processing.\nThey are, however, extremely successful at their primary task: providing a\nmodel for language. For this reason and because there are no animal models for\nlanguage, large language models are important in psycholinguistics: they are\nuseful as a practical tool, as an illustrative comparative, and\nphilosophically, as a basis for recasting the relationship between language and\nthought.\n",
                "链接": "https://arxiv.org/abs/2303.00077"
            },
            {
                "文章ID": "113893",
                "标题": "On the Intersection of Self-Correction and Trust in Language Models",
                "作者": " Satyapriya Krishna",
                "发布日期": "2023-11-07",
                "摘要": "  Large Language Models (LLMs) have demonstrated remarkable capabilities in\nperforming complex cognitive tasks. However, their complexity and lack of\ntransparency have raised several trustworthiness concerns, including the\npropagation of misinformation and toxicity. Recent research has explored the\nself-correction capabilities of LLMs to enhance their performance. In this\nwork, we investigate whether these self-correction capabilities can be\nharnessed to improve the trustworthiness of LLMs. We conduct experiments\nfocusing on two key aspects of trustworthiness: truthfulness and toxicity. Our\nfindings reveal that self-correction can lead to improvements in toxicity and\ntruthfulness, but the extent of these improvements varies depending on the\nspecific aspect of trustworthiness and the nature of the task. Interestingly,\nour study also uncovers instances of \"self-doubt\" in LLMs during the\nself-correction process, introducing a new set of challenges that need to be\naddressed.\n",
                "链接": "https://arxiv.org/abs/2311.02801"
            },
            {
                "文章ID": "100283",
                "标题": "On the Planning, Search, and Memorization Capabilities of Large Language\n  Models",
                "作者": " Yunhao Yang,  Anshul Tomar",
                "发布日期": "2023-09-06",
                "摘要": "  The rapid advancement of large language models, such as the Generative\nPre-trained Transformer (GPT) series, has had significant implications across\nvarious disciplines. In this study, we investigate the potential of the\nstate-of-the-art large language model (GPT-4) for planning tasks. We explore\nits effectiveness in multiple planning subfields, highlighting both its\nstrengths and limitations. Through a comprehensive examination, we identify\nareas where large language models excel in solving planning problems and reveal\nthe constraints that limit their applicability. Our empirical analysis focuses\non GPT-4's performance in planning domain extraction, graph search path\nplanning, and adversarial planning. We then propose a way of fine-tuning a\ndomain-specific large language model to improve its Chain of Thought (CoT)\ncapabilities for the above-mentioned tasks. The results provide valuable\ninsights into the potential applications of large language models in the\nplanning domain and pave the way for future research to overcome their\nlimitations and expand their capabilities.\n",
                "链接": "https://arxiv.org/abs/2309.01868"
            },
            {
                "文章ID": "80483",
                "标题": "Science in the Era of ChatGPT, Large Language Models and Generative AI:\n  Challenges for Research Ethics and How to Respond",
                "作者": " Evangelos Pournaras",
                "发布日期": "2023-08-01",
                "摘要": "  Large language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.\n",
                "链接": "https://arxiv.org/abs/2305.15299"
            },
            {
                "文章ID": "70754",
                "标题": "A Bibliometric Review of Large Language Models Research from 2017 to\n  2023",
                "作者": " Lizhou Fan,  Lingyao Li,  Zihui Ma,  Sanggyu Lee,  Huizi Yu,  Libby Hemphill",
                "发布日期": "2023-04-06",
                "摘要": "  Large language models (LLMs) are a class of language models that have\ndemonstrated outstanding performance across a range of natural language\nprocessing (NLP) tasks and have become a highly sought-after research area,\nbecause of their ability to generate human-like language and their potential to\nrevolutionize science and technology. In this study, we conduct bibliometric\nand discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000\npublications, this paper serves as a roadmap for researchers, practitioners,\nand policymakers to navigate the current landscape of LLMs research. We present\nthe research trends from 2017 to early 2023, identifying patterns in research\nparadigms and collaborations. We start with analyzing the core algorithm\ndevelopments and NLP tasks that are fundamental in LLMs research. We then\ninvestigate the applications of LLMs in various fields and domains including\nmedicine, engineering, social science, and humanities. Our review also reveals\nthe dynamic, fast-paced evolution of LLMs research. Overall, this paper offers\nvaluable insights into the current state, impact, and potential of LLMs\nresearch and its applications.\n",
                "链接": "https://arxiv.org/abs/2304.02020"
            },
            {
                "文章ID": "115450",
                "标题": "To Transformers and Beyond: Large Language Models for the Genome",
                "作者": " Micaela E. Consens,  Cameron Dufault,  Michael Wainberg,  Duncan Forster,  Mehran Karimzadeh,  Hani Goodarzi,  Fabian J. Theis,  Alan Moses,  Bo Wang",
                "发布日期": "2023-11-15",
                "摘要": "  In the rapidly evolving landscape of genomics, deep learning has emerged as a\nuseful tool for tackling complex computational challenges. This review focuses\non the transformative role of Large Language Models (LLMs), which are mostly\nbased on the transformer architecture, in genomics. Building on the foundation\nof traditional convolutional neural networks and recurrent neural networks, we\nexplore both the strengths and limitations of transformers and other LLMs for\ngenomics. Additionally, we contemplate the future of genomic modeling beyond\nthe transformer architecture based on current trends in research. The paper\naims to serve as a guide for computational biologists and computer scientists\ninterested in LLMs for genomic data. We hope the paper can also serve as an\neducational introduction and discussion for biologists to a fundamental shift\nin how we will be analyzing genomic data in the future.\n",
                "链接": "https://arxiv.org/abs/2311.07621"
            },
            {
                "文章ID": "51669",
                "标题": "Language models and brain alignment: beyond word-level semantics and\n  prediction",
                "作者": " Gabriele Merlin,  Mariya Toneva",
                "发布日期": "2022-12-02",
                "摘要": "  Pretrained language models that have been trained to predict the next word\nover billions of text documents have been shown to also significantly predict\nbrain recordings of people comprehending language. Understanding the reasons\nbehind the observed similarities between language in machines and language in\nthe brain can lead to more insight into both systems. Recent works suggest that\nthe prediction of the next word is a key mechanism that contributes to the\nalignment between the two. What is not yet understood is whether prediction of\nthe next word is necessary for this observed alignment or simply sufficient,\nand whether there are other shared mechanisms or information that is similarly\nimportant. In this work, we take a first step towards a better understanding\nvia two simple perturbations in a popular pretrained language model. The first\nperturbation is to improve the model's ability to predict the next word in the\nspecific naturalistic stimulus text that the brain recordings correspond to. We\nshow that this indeed improves the alignment with the brain recordings.\nHowever, this improved alignment may also be due to any improved word-level or\nmulti-word level semantics for the specific world that is described by the\nstimulus narrative. We aim to disentangle the contribution of next word\nprediction and semantic knowledge via our second perturbation: scrambling the\nword order at inference time, which reduces the ability to predict the next\nword, but maintains any newly learned word-level semantics. By comparing the\nalignment with brain recordings of these differently perturbed models, we show\nthat improvements in alignment with brain recordings are due to more than\nimprovements in next word prediction and word-level semantics.\n",
                "链接": "https://arxiv.org/abs/2212.00596"
            },
            {
                "文章ID": "123540",
                "标题": "User Modeling in the Era of Large Language Models: Current Research and\n  Future Directions",
                "作者": " Zhaoxuan Tan,  Meng Jiang",
                "发布日期": "2023-12-27",
                "摘要": "  User modeling (UM) aims to discover patterns or learn representations from\nuser data about the characteristics of a specific user, such as profile,\npreference, and personality. The user models enable personalization and\nsuspiciousness detection in many online applications such as recommendation,\neducation, and healthcare. Two common types of user data are text and graph, as\nthe data usually contain a large amount of user-generated content (UGC) and\nonline interactions. The research of text and graph mining is developing\nrapidly, contributing many notable solutions in the past two decades. Recently,\nlarge language models (LLMs) have shown superior performance on generating,\nunderstanding, and even reasoning over text data. The approaches of user\nmodeling have been equipped with LLMs and soon become outstanding. This article\nsummarizes existing research about how and why LLMs are great tools of modeling\nand understanding UGC. Then it reviews a few categories of large language\nmodels for user modeling (LLM-UM) approaches that integrate the LLMs with text\nand graph-based methods in different ways. Then it introduces specific LLM-UM\ntechniques for a variety of UM applications. Finally, it presents remaining\nchallenges and future directions in the LLM-UM research. We maintain the\nreading list at: https://github.com/TamSiuhin/LLM-UM-Reading\n",
                "链接": "https://arxiv.org/abs/2312.11518"
            },
            {
                "文章ID": "117438",
                "标题": "On the Calibration of Large Language Models and Alignment",
                "作者": " Chiwei Zhu,  Benfeng Xu,  Quan Wang,  Yongdong Zhang,  Zhendong Mao",
                "发布日期": "2023-11-23",
                "摘要": "  As large language models attract increasing attention and find widespread\napplication, concurrent challenges of reliability also arise at the same time.\nConfidence calibration, an effective analysis method for gauging the\nreliability of deep models, serves as a crucial tool for assessing and\nimproving their reliability. However, such investigation has been comparatively\nunderexplored. In this work, we conduct a systematic examination of the\ncalibration of aligned language models throughout the entire construction\nprocess, including pretraining and alignment training. At each stage, we\ninvestigate how different training settings, such as parameter scales and\ntraining data, affect model calibration. To thoroughly assess model\ncalibration, we evaluate models on three most concerned aspects: generation,\nfactuality and understanding. Our work sheds light on whether popular LLMs are\nwell-calibrated and how the training process influences model calibration.\n",
                "链接": "https://arxiv.org/abs/2311.13240"
            },
            {
                "文章ID": "103781",
                "标题": "Beyond Traditional Teaching: The Potential of Large Language Models and\n  Chatbots in Graduate Engineering Education",
                "作者": " Mahyar Abedi,  Ibrahem Alshybani,  Muhammad Rubayat Bin Shahadat,  Michael S. Murillo",
                "发布日期": "2023-12-21",
                "摘要": "  In the rapidly evolving landscape of education, digital technologies have\nrepeatedly disrupted traditional pedagogical methods. This paper explores the\nlatest of these disruptions: the potential integration of large language models\n(LLMs) and chatbots into graduate engineering education. We begin by tracing\nhistorical and technological disruptions to provide context and then introduce\nkey terms such as machine learning and deep learning and the underlying\nmechanisms of recent advancements, namely attention/transformer models and\ngraphics processing units. The heart of our investigation lies in the\napplication of an LLM-based chatbot in a graduate fluid mechanics course. We\ndeveloped a question bank from the course material and assessed the chatbot's\nability to provide accurate, insightful responses. The results are encouraging,\ndemonstrating not only the bot's ability to effectively answer complex\nquestions but also the potential advantages of chatbot usage in the classroom,\nsuch as the promotion of self-paced learning, the provision of instantaneous\nfeedback, and the reduction of instructors' workload. The study also examines\nthe transformative effect of intelligent prompting on enhancing the chatbot's\nperformance. Furthermore, we demonstrate how powerful plugins like Wolfram\nAlpha for mathematical problem-solving and code interpretation can\nsignificantly extend the chatbot's capabilities, transforming it into a\ncomprehensive educational tool. While acknowledging the challenges and ethical\nimplications surrounding the use of such AI models in education, we advocate\nfor a balanced approach. The use of LLMs and chatbots in graduate education can\nbe greatly beneficial but requires ongoing evaluation and adaptation to ensure\nethical and efficient use.\n",
                "链接": "https://arxiv.org/abs/2309.13059"
            }
        ]
    },
    {
        "question": {
            "question": "2022年后与AI for Science相关的综述论文",
            "type": "6"
        },
        "results": [
            {
                "文章ID": "110591",
                "标题": "\"Why Should I Review This Paper?\" Unifying Semantic, Topic, and Citation\n  Factors for Paper-Reviewer Matching",
                "作者": " Yu Zhang,  Yanzhen Shen,  Xiusi Chen,  Bowen Jin,  Jiawei Han",
                "发布日期": "2023-10-24",
                "摘要": "  As many academic conferences are overwhelmed by a rapidly increasing number\nof paper submissions, automatically finding appropriate reviewers for each\nsubmission becomes a more urgent need than ever. Various factors have been\nconsidered by previous attempts on this task to measure the expertise relevance\nbetween a paper and a reviewer, including whether the paper is semantically\nclose to, shares topics with, and cites previous papers of the reviewer.\nHowever, the majority of previous studies take only one of these factors into\naccount, leading to an incomprehensive evaluation of paper-reviewer relevance.\nTo bridge this gap, in this paper, we propose a unified model for\npaper-reviewer matching that jointly captures semantic, topic, and citation\nfactors. In the unified model, a contextualized language model backbone is\nshared by all factors to learn common knowledge, while instruction tuning is\nintroduced to characterize the uniqueness of each factor by producing\nfactor-aware paper embeddings. Experiments on four datasets (one of which is\nnewly contributed by us) across different fields, including machine learning,\ncomputer vision, information retrieval, and data mining, consistently validate\nthe effectiveness of our proposed UniPR model in comparison with\nstate-of-the-art paper-reviewer matching methods and scientific pre-trained\nlanguage models.\n",
                "链接": "https://arxiv.org/abs/2310.14483"
            },
            {
                "文章ID": "65767",
                "标题": "Position Paper on Dataset Engineering to Accelerate Science",
                "作者": " Emilio Vital Brazil,  Eduardo Soares,  Lucas Villa Real,  Leonardo Azevedo,  Vinicius Segura,  Luiz Zerkowski,  Renato Cerqueira",
                "发布日期": "2023-03-13",
                "摘要": "  Data is a critical element in any discovery process. In the last decades, we\nobserved exponential growth in the volume of available data and the technology\nto manipulate it. However, data is only practical when one can structure it for\na well-defined task. For instance, we need a corpus of text broken into\nsentences to train a natural language machine-learning model. In this work, we\nwill use the token \\textit{dataset} to designate a structured set of data built\nto perform a well-defined task. Moreover, the dataset will be used in most\ncases as a blueprint of an entity that at any moment can be stored as a table.\nSpecifically, in science, each area has unique forms to organize, gather and\nhandle its datasets. We believe that datasets must be a first-class entity in\nany knowledge-intensive process, and all workflows should have exceptional\nattention to datasets' lifecycle, from their gathering to uses and evolution.\nWe advocate that science and engineering discovery processes are extreme\ninstances of the need for such organization on datasets, claiming for new\napproaches and tooling. Furthermore, these requirements are more evident when\nthe discovery workflow uses artificial intelligence methods to empower the\nsubject-matter expert. In this work, we discuss an approach to bringing\ndatasets as a critical entity in the discovery process in science. We\nillustrate some concepts using material discovery as a use case. We chose this\ndomain because it leverages many significant problems that can be generalized\nto other science fields.\n",
                "链接": "https://arxiv.org/abs/2303.05545"
            },
            {
                "文章ID": "61490",
                "标题": "Relatedly: Scaffolding Literature Reviews with Existing Related Work\n  Sections",
                "作者": " Srishti Palani,  Aakanksha Naik,  Doug Downey,  Amy X. Zhang,  Jonathan Bragg,  Joseph Chee Chang",
                "发布日期": "2023-02-15",
                "摘要": "  Scholars who want to research a scientific topic must take time to read,\nextract meaning, and identify connections across many papers. As scientific\nliterature grows, this becomes increasingly challenging. Meanwhile, authors\nsummarize prior research in papers' related work sections, though this is\nscoped to support a single paper. A formative study found that while reading\nmultiple related work paragraphs helps overview a topic, it is hard to navigate\noverlapping and diverging references and research foci. In this work, we design\na system, Relatedly, that scaffolds exploring and reading multiple related work\nparagraphs on a topic, with features including dynamic re-ranking and\nhighlighting to spotlight unexplored dissimilar information, auto-generated\ndescriptive paragraph headings, and low-lighting of redundant information. From\na within-subjects user study (n=15), we found that scholars generate more\ncoherent, insightful, and comprehensive topic outlines using Relatedly compared\nto a baseline paper list.\n",
                "链接": "https://arxiv.org/abs/2302.06754"
            },
            {
                "文章ID": "20490",
                "标题": "CYRUS Soccer Simulation 2D Team Description Paper 2022",
                "作者": " Nader Zare,  Arad Firouzkouhi,  Omid Amini,  Mahtab Sarvmaili,  Aref Sayareh,  Saba Ramezani Rad,  Stan Matwin,  Amilcar Soares",
                "发布日期": "2022-05-24",
                "摘要": "  Soccer Simulation 2D League is one of the major leagues of RoboCup\ncompetitions. In a Soccer Simulation 2D (SS2D) game, two teams of 11 players\nand one coach compete against each other. The players are only allowed to\ncommunicate with the server that is called Soccer Simulation Server. This paper\nintroduces the previous and current research of the CYRUS soccer simulation\nteam, the champion of RoboCup 2021. We will present our idea about improving\nUnmarking Decisioning and Positioning by using Pass Prediction Deep Neural\nNetwork. Based on our experimental results, this idea proven to be effective on\nincreasing the winning rate of Cyrus against opponents.\n",
                "链接": "https://arxiv.org/abs/2205.10953"
            },
            {
                "文章ID": "24473",
                "标题": "An analysis of retracted papers in Computer Science",
                "作者": " Martin Shepperd,  Leila Yousefi",
                "发布日期": "2023-07-19",
                "摘要": "  Context: The retraction of research papers, for whatever reason, is a growing\nphenomenon. However, although retracted paper information is publicly available\nvia publishers, it is somewhat distributed and inconsistent. Objective: The aim\nis to assess: (i) the extent and nature of retracted research in Computer\nScience (CS) (ii) the post-retraction citation behaviour of retracted works and\n(iii) the potential impact on systematic reviews and mapping studies. Method:\nWe analyse the Retraction Watch database and take citation information from the\nWeb of Science and Google scholar. Results: We find that of the 33,955 entries\nin the Retraction watch database (16 May 2022), 2,816 are classified as CS,\ni.e., approximately 8.3%. For CS, 56% of retracted papers, provide little or no\ninformation as to the reasons. This contrasts with 26% for other disciplines.\nThere is also a remarkable disparity between different publishers, a tendency\nfor multiple versions of a retracted paper over and above the Version of Record\n(VoR), and for new citations long after a paper is officially retracted.\nConclusions: Unfortunately retraction seems to be a sufficiently common outcome\nfor a scientific paper that we as a research community need to take it more\nseriously, e.g., standardising procedures and taxonomies across publishers and\nthe provision of appropriate research tools. Finally, we recommend particular\ncaution when undertaking secondary analyses and meta-analyses which are at risk\nof becoming contaminated by these problem primary studies.\n",
                "链接": "https://arxiv.org/abs/2206.06706"
            },
            {
                "文章ID": "27917",
                "标题": "A Dataset on Malicious Paper Bidding in Peer Review",
                "作者": " Steven Jecmen,  Minji Yoon,  Vincent Conitzer,  Nihar B. Shah,  Fei Fang",
                "发布日期": "2023-03-14",
                "摘要": "  In conference peer review, reviewers are often asked to provide \"bids\" on\neach submitted paper that express their interest in reviewing that paper. A\npaper assignment algorithm then uses these bids (along with other data) to\ncompute a high-quality assignment of reviewers to papers. However, this process\nhas been exploited by malicious reviewers who strategically bid in order to\nunethically manipulate the paper assignment, crucially undermining the peer\nreview process. For example, these reviewers may aim to get assigned to a\nfriend's paper as part of a quid-pro-quo deal. A critical impediment towards\ncreating and evaluating methods to mitigate this issue is the lack of any\npublicly-available data on malicious paper bidding. In this work, we collect\nand publicly release a novel dataset to fill this gap, collected from a mock\nconference activity where participants were instructed to bid either honestly\nor maliciously. We further provide a descriptive analysis of the bidding\nbehavior, including our categorization of different strategies employed by\nparticipants. Finally, we evaluate the ability of each strategy to manipulate\nthe assignment, and also evaluate the performance of some simple algorithms\nmeant to detect malicious bidding. The performance of these detection\nalgorithms can be taken as a baseline for future research on detecting\nmalicious bidding.\n",
                "链接": "https://arxiv.org/abs/2207.02303"
            },
            {
                "文章ID": "103497",
                "标题": "Empowering Precision Medicine: AI-Driven Schizophrenia Diagnosis via EEG\n  Signals: A Comprehensive Review from 2002-2023",
                "作者": " Mahboobeh Jafari,  Delaram Sadeghi,  Afshin Shoeibi,  Hamid Alinejad-Rokny,  Amin Beheshti,  David López García,  Zhaolin Chen,  U. Rajendra Acharya,  Juan M. Gorriz",
                "发布日期": "2023-09-22",
                "摘要": "  Schizophrenia (SZ) is a prevalent mental disorder characterized by cognitive,\nemotional, and behavioral changes. Symptoms of SZ include hallucinations,\nillusions, delusions, lack of motivation, and difficulties in concentration.\nDiagnosing SZ involves employing various tools, including clinical interviews,\nphysical examinations, psychological evaluations, the Diagnostic and\nStatistical Manual of Mental Disorders (DSM), and neuroimaging techniques.\nElectroencephalography (EEG) recording is a significant functional neuroimaging\nmodality that provides valuable insights into brain function during SZ.\nHowever, EEG signal analysis poses challenges for neurologists and scientists\ndue to the presence of artifacts, long-term recordings, and the utilization of\nmultiple channels. To address these challenges, researchers have introduced\nartificial intelligence (AI) techniques, encompassing conventional machine\nlearning (ML) and deep learning (DL) methods, to aid in SZ diagnosis. This\nstudy reviews papers focused on SZ diagnosis utilizing EEG signals and AI\nmethods. The introduction section provides a comprehensive explanation of SZ\ndiagnosis methods and intervention techniques. Subsequently, review papers in\nthis field are discussed, followed by an introduction to the AI methods\nemployed for SZ diagnosis and a summary of relevant papers presented in tabular\nform. Additionally, this study reports on the most significant challenges\nencountered in SZ diagnosis, as identified through a review of papers in this\nfield. Future directions to overcome these challenges are also addressed. The\ndiscussion section examines the specific details of each paper, culminating in\nthe presentation of conclusions and findings.\n",
                "链接": "https://arxiv.org/abs/2309.12202"
            },
            {
                "文章ID": "54650",
                "标题": "evoML Yellow Paper: Evolutionary AI and Optimisation Studio",
                "作者": " Lingbo Li,  Leslie Kanthan,  Michail Basios,  Fan Wu,  Manal Adham,  Vitali Avagyan,  Alexis Butler,  Paul Brookes,  Rafail Giavrimis,  Buhong Liu,  Chrystalla Pavlou,  Matthew Truscott,  Vardan Voskanyan",
                "发布日期": "2022-12-22",
                "摘要": "  Machine learning model development and optimisation can be a rather\ncumbersome and resource-intensive process. Custom models are often more\ndifficult to build and deploy, and they require infrastructure and expertise\nwhich are often costly to acquire and maintain. Machine learning product\ndevelopment lifecycle must take into account the need to navigate the\ndifficulties of developing and deploying machine learning models. evoML is an\nAI-powered tool that provides automated functionalities in machine learning\nmodel development, optimisation, and model code optimisation. Core\nfunctionalities of evoML include data cleaning, exploratory analysis, feature\nanalysis and generation, model optimisation, model evaluation, model code\noptimisation, and model deployment. Additionally, a key feature of evoML is\nthat it embeds code and model optimisation into the model development process,\nand includes multi-objective optimisation capabilities.\n",
                "链接": "https://arxiv.org/abs/2212.10671"
            },
            {
                "文章ID": "37160",
                "标题": "Using Rater and System Metadata to Explain Variance in the VoiceMOS\n  Challenge 2022 Dataset",
                "作者": " Michael Chinen,  Jan Skoglund,  Chandan K A Reddy,  Alessandro Ragano,  Andrew Hines",
                "发布日期": "2022-09-15",
                "摘要": "  Non-reference speech quality models are important for a growing number of\napplications. The VoiceMOS 2022 challenge provided a dataset of synthetic voice\nconversion and text-to-speech samples with subjective labels. This study looks\nat the amount of variance that can be explained in subjective ratings of speech\nquality from metadata and the distribution imbalances of the dataset. Speech\nquality models were constructed using wav2vec 2.0 with additional metadata\nfeatures that included rater groups and system identifiers and obtained\ncompetitive metrics including a Spearman rank correlation coefficient (SRCC) of\n0.934 and MSE of 0.088 at the system-level, and 0.877 and 0.198 at the\nutterance-level. Using data and metadata that the test restricted or blinded\nfurther improved the metrics. A metadata analysis showed that the system-level\nmetrics do not represent the model's system-level prediction as a result of the\nwide variation in the number of utterances used for each system on the\nvalidation and test datasets. We conclude that, in general, conditions should\nhave enough utterances in the test set to bound the sample mean error, and be\nrelatively balanced in utterance count between systems, otherwise the\nutterance-level metrics may be more reliable and interpretable.\n",
                "链接": "https://arxiv.org/abs/2209.06358"
            },
            {
                "文章ID": "102298",
                "标题": "Monitoring Urban Changes in Mariupol/Ukraine in 2022/23",
                "作者": " Georg Zitzlsberger,  Michal Podhoranyi",
                "发布日期": "2023-09-19",
                "摘要": "  The ability to constantly monitor urban changes is of large socio-economic\ninterest. Previous works have already shown approaches in this field with the\nuse of Deep Neural Networks (DNNs) and transfer learning. However, they fell\nshort in demonstrating temporal scale outside of either the training or\ntransfer domain.\n  This work builds on existing research and proves that transfer learning with\nthe use of historic data is a feasible solution, which still allows the urban\nchange monitoring of later years. We considered a case with limited access to\npublic and free Very High Resolution (VHR) imagery to guide the transfer. To\nprovide a high temporal resolution, the core data of our monitoring method\ncomprised multi-modal Synthetic Aperture Radar (SAR) and optical multispectral\nobservations from Sentinel 1 and Sentinel 2, respectively.\n  We chose a practical application of our methods for monitoring urban-related\nchanges in the city of Mariupol in Ukraine during the beginning of the\nRusso-Ukrainian War in 2022/23. During this conflict, availability of VHR data\nwas limited and hence an inexpensive direct transfer to the years 2022/23 was\nrendered impossible. Instead, a transfer was made for the years 2017-2020 that\nprovided sufficient public and free VHR data with an application of the\ntransferred model in the years late 2021 to mid-2023. It was shown that\ntransferring for the years 2017-2020 with this inexpensive historical VHR data\nenabled monitoring during times of war in 2022/23.\n  An ablation study on the impact of the frequency of observations showed our\nmethod as resilient to even a large loss of observations. However, it also\nindicated that our method, despite the multi-modal input, was more dependent on\noptical observations than SAR observations. Neither the indirect transfer, nor\nthe malfunction of Sentinel 1B had a significant impact on the monitoring\ncapabilities of our method.\n",
                "链接": "https://arxiv.org/abs/2309.08607"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下2023年关于LLM-based Agent的综述",
            "type": "6"
        },
        "results": [
            {
                "文章ID": "119996",
                "标题": "A Comprehensive Literature Review on Sweet Orange Leaf Diseases",
                "作者": " Yousuf Rayhan Emon,  Md Golam Rabbani,  Dr. Md. Taimur Ahad,  Faruk Ahmed",
                "发布日期": "2023-12-05",
                "摘要": "  Sweet orange leaf diseases are significant to agricultural productivity. Leaf\ndiseases impact fruit quality in the citrus industry. The apparition of machine\nlearning makes the development of disease finder. Early detection and diagnosis\nare necessary for leaf management. Sweet orange leaf disease-predicting\nautomated systems have already been developed using different image-processing\ntechniques. This comprehensive literature review is systematically based on\nleaf disease and machine learning methodologies applied to the detection of\ndamaged leaves via image classification. The benefits and limitations of\ndifferent machine learning models, including Vision Transformer (ViT), Neural\nNetwork (CNN), CNN with SoftMax and RBF SVM, Hybrid CNN-SVM, HLB-ConvMLP,\nEfficientNet-b0, YOLOv5, YOLOv7, Convolutional, Deep CNN. These machine\nlearning models tested on various datasets and detected the disease. This\ncomprehensive review study related to leaf disease compares the performance of\nthe models; those models' accuracy, precision, recall, etc., were used in the\nsubsisting studies\n",
                "链接": "https://arxiv.org/abs/2312.01756"
            },
            {
                "文章ID": "84465",
                "标题": "A Systematic Literature Review on Client Selection in Federated Learning",
                "作者": " Carl Smestad,  Jingyue Li",
                "发布日期": "2023-06-09",
                "摘要": "  With the arising concerns of privacy within machine learning, federated\nlearning (FL) was invented in 2017, in which the clients, such as mobile\ndevices, train a model and send the update to the centralized server. Choosing\nclients randomly for FL can harm learning performance due to different reasons.\nMany studies have proposed approaches to address the challenges of client\nselection of FL. However, no systematic literature review (SLR) on this topic\nexisted. This SLR investigates the state of the art of client selection in FL\nand answers the challenges, solutions, and metrics to evaluate the solutions.\nWe systematically reviewed 47 primary studies. The main challenges found in\nclient selection are heterogeneity, resource allocation, communication costs,\nand fairness. The client selection schemes aim to improve the original random\nselection algorithm by focusing on one or several of the aforementioned\nchallenges. The most common metric used is testing accuracy versus\ncommunication rounds, as testing accuracy measures the successfulness of the\nlearning and preferably in as few communication rounds as possible, as they are\nvery expensive. Although several possible improvements can be made with the\ncurrent state of client selection, the most beneficial ones are evaluating the\nimpact of unsuccessful clients and gaining a more theoretical understanding of\nthe impact of fairness in FL.\n",
                "链接": "https://arxiv.org/abs/2306.04862"
            },
            {
                "文章ID": "104572",
                "标题": "3D Multiple Object Tracking on Autonomous Driving: A Literature Review",
                "作者": " Peng Zhang,  Xin Li,  Liang He,  Xin Lin",
                "发布日期": "2023-11-06",
                "摘要": "  3D multi-object tracking (3D MOT) stands as a pivotal domain within\nautonomous driving, experiencing a surge in scholarly interest and commercial\npromise over recent years. Despite its paramount significance, 3D MOT confronts\na myriad of formidable challenges, encompassing abrupt alterations in object\nappearances, pervasive occlusion, the presence of diminutive targets, data\nsparsity, missed detections, and the unpredictable initiation and termination\nof object motion trajectories. Countless methodologies have emerged to grapple\nwith these issues, yet 3D MOT endures as a formidable problem that warrants\nfurther exploration. This paper undertakes a comprehensive examination,\nassessment, and synthesis of the research landscape in this domain, remaining\nattuned to the latest developments in 3D MOT while suggesting prospective\navenues for future investigation. Our exploration commences with a systematic\nexposition of key facets of 3D MOT and its associated domains, including\nproblem delineation, classification, methodological approaches, fundamental\nprinciples, and empirical investigations. Subsequently, we categorize these\nmethodologies into distinct groups, dissecting each group meticulously with\nregard to its challenges, underlying rationale, progress, merits, and demerits.\nFurthermore, we present a concise recapitulation of experimental metrics and\noffer an overview of prevalent datasets, facilitating a quantitative comparison\nfor a more intuitive assessment. Lastly, our deliberations culminate in a\ndiscussion of the prevailing research landscape, highlighting extant challenges\nand charting possible directions for 3D MOT research. We present a structured\nand lucid road-map to guide forthcoming endeavors in this field.\n",
                "链接": "https://arxiv.org/abs/2309.15411"
            },
            {
                "文章ID": "79186",
                "标题": "Systematic Literature Review on Application of Machine Learning in\n  Continuous Integration",
                "作者": " Ali Kazemi Arani,  Triet Huynh Minh Le,  Mansooreh Zahedi,  Muhammad Ali Babar",
                "发布日期": "2023-07-18",
                "摘要": "  This research conducted a systematic review of the literature on machine\nlearning (ML)-based methods in the context of Continuous Integration (CI) over\nthe past 22 years. The study aimed to identify and describe the techniques used\nin ML-based solutions for CI and analyzed various aspects such as data\nengineering, feature engineering, hyper-parameter tuning, ML models, evaluation\nmethods, and metrics. In this paper, we have depicted the phases of CI testing,\nthe connection between them, and the employed techniques in training the ML\nmethod phases. We presented nine types of data sources and four taken steps in\nthe selected studies for preparing the data. Also, we identified four feature\ntypes and nine subsets of data features through thematic analysis of the\nselected studies. Besides, five methods for selecting and tuning the\nhyper-parameters are shown. In addition, we summarised the evaluation methods\nused in the literature and identified fifteen different metrics. The most\ncommonly used evaluation methods were found to be precision, recall, and\nF1-score, and we have also identified five methods for evaluating the\nperformance of trained ML models. Finally, we have presented the relationship\nbetween ML model types, performance measurements, and CI phases. The study\nprovides valuable insights for researchers and practitioners interested in\nML-based methods in CI and emphasizes the need for further research in this\narea.\n",
                "链接": "https://arxiv.org/abs/2305.12695"
            },
            {
                "文章ID": "83104",
                "标题": "A systematic literature review on the code smells datasets and\n  validation mechanisms",
                "作者": " Morteza Zakeri-Nasrabadi,  Saeed Parsa,  Ehsan Esmaili,  Fabio Palomba",
                "发布日期": "2023-06-05",
                "摘要": "  The accuracy reported for code smell-detecting tools varies depending on the\ndataset used to evaluate the tools. Our survey of 45 existing datasets reveals\nthat the adequacy of a dataset for detecting smells highly depends on relevant\nproperties such as the size, severity level, project types, number of each type\nof smell, number of smells, and the ratio of smelly to non-smelly samples in\nthe dataset. Most existing datasets support God Class, Long Method, and Feature\nEnvy while six smells in Fowler and Beck's catalog are not supported by any\ndatasets. We conclude that existing datasets suffer from imbalanced samples,\nlack of supporting severity level, and restriction to Java language.\n",
                "链接": "https://arxiv.org/abs/2306.01377"
            },
            {
                "文章ID": "94694",
                "标题": "AI Literature Review Suite",
                "作者": " David A. Tovar",
                "发布日期": "2023-08-07",
                "摘要": "  The process of conducting literature reviews is often time-consuming and\nlabor-intensive. To streamline this process, I present an AI Literature Review\nSuite that integrates several functionalities to provide a comprehensive\nliterature review. This tool leverages the power of open access science, large\nlanguage models (LLMs) and natural language processing to enable the searching,\ndownloading, and organizing of PDF files, as well as extracting content from\narticles. Semantic search queries are used for data retrieval, while text\nembeddings and summarization using LLMs present succinct literature reviews.\nInteraction with PDFs is enhanced through a user-friendly graphical user\ninterface (GUI). The suite also features integrated programs for bibliographic\norganization, interaction and query, and literature review summaries. This tool\npresents a robust solution to automate and optimize the process of literature\nreview in academic and industrial research.\n",
                "链接": "https://arxiv.org/abs/2308.02443"
            },
            {
                "文章ID": "121094",
                "标题": "A Review On Table Recognition Based On Deep Learning",
                "作者": " Shi Jiyuan,  Shi chunqi",
                "发布日期": "2023-12-11",
                "摘要": "  Table recognition is using the computer to automatically understand the\ntable, to detect the position of the table from the document or picture, and to\ncorrectly extract and identify the internal structure and content of the table.\nAfter earlier mainstream approaches based on heuristic rules and machine\nlearning, the development of deep learning techniques has brought a new\nparadigm to this field. This review mainly discusses the table recognition\nproblem from five aspects. The first part introduces data sets, benchmarks, and\ncommonly used evaluation indicators. This section selects representative data\nsets, benchmarks, and evaluation indicators that are frequently used by\nresearchers. The second part introduces the table recognition model. This\nsurvey introduces the development of the table recognition model, especially\nthe table recognition model based on deep learning. It is generally accepted\nthat table recognition is divided into two stages: table detection and table\nstructure recognition. This section introduces the models that follow this\nparadigm (TD and TSR). The third part is the End-to-End method, this section\nintroduces some scholars' attempts to use an end-to-end approach to solve the\ntable recognition problem once and for all and the part are Data-centric\nmethods, such as data augmentation, aligning benchmarks, and other methods. The\nfourth part is the data-centric approach, such as data enhancement, alignment\nbenchmark, and so on. The fifth part summarizes and compares the experimental\ndata in the field of form recognition, and analyzes the mainstream and more\nadvantageous methods. Finally, this paper also discusses the possible\ndevelopment direction and trend of form processing in the future, to provide\nsome ideas for researchers in the field of table recognition. (Resource will be\nreleased at https://github.com/Wa1den-jy/Topic-on-Table-Recognition .)\n",
                "链接": "https://arxiv.org/abs/2312.04808"
            },
            {
                "文章ID": "124934",
                "标题": "Review on Causality Detection Based on Empirical Dynamic Modeling",
                "作者": " Cao Zhihao,  Qu Hongchun",
                "发布日期": "2023-12-27",
                "摘要": "  In contemporary scientific research, understanding the distinction between\ncorrelation and causation is crucial. While correlation is a widely used\nanalytical standard, it does not inherently imply causation. This paper\naddresses the potential for misinterpretation in relying solely on correlation,\nespecially in the context of nonlinear dynamics. Despite the rapid development\nof various correlation research methodologies, including machine learning, the\nexploration into mining causal correlations between variables remains ongoing.\nEmpirical Dynamic Modeling (EDM) emerges as a data-driven framework for\nmodeling dynamic systems, distinguishing itself by eschewing traditional\nformulaic methods in data analysis. Instead, it reconstructs dynamic system\nbehavior directly from time series data. The fundamental premise of EDM is that\ndynamic systems can be conceptualized as processes where a set of states,\ngoverned by specific rules, evolve over time in a high-dimensional space. By\nreconstructing these evolving states, dynamic systems can be effectively\nmodeled. Using EDM, this paper explores the detection of causal relationships\nbetween variables within dynamic systems through their time series data. It\nposits that if variable X causes variable Y, then the information about X is\ninherent in Y and can be extracted from Y's data. This study begins by\nexamining the dialectical relationship between correlation and causation,\nemphasizing that correlation does not equate to causation, and the absence of\ncorrelation does not necessarily indicate a lack of causation.\n",
                "链接": "https://arxiv.org/abs/2312.15919"
            },
            {
                "文章ID": "119756",
                "标题": "On the Effects of Randomness on Stability of Learning with Limited\n  Labelled Data: A Systematic Literature Review",
                "作者": " Branislav Pecher,  Ivan Srba,  Maria Bielikova",
                "发布日期": "2023-12-05",
                "摘要": "  Learning with limited labelled data, such as few-shot learning, meta-learning\nor transfer learning, aims to effectively train a model using only small amount\nof labelled samples. However, these approaches were observed to be excessively\nsensitive to the effects of uncontrolled randomness caused by non-determinism\nin the training process. The randomness negatively affects the stability of the\nmodels, leading to large variance in results across training runs. When such\ninstability is disregarded, it can unintentionally, but unfortunately also\nintentionally, create an imaginary perception of research progress. Recently,\nthis area started to attract a research attention and the number of relevant\nstudies is continuously growing. In this survey, we provide a comprehensive\noverview of 134 papers addressing the effects of randomness on the stability of\nlearning with limited labelled data. We distinguish between four main tasks\naddressed in the papers (investigate/evaluate; determine; mitigate;\nbenchmark/compare/report randomness effects), providing findings for each one.\nFurthermore, we identify and discuss seven challenges and open problems\ntogether with possible directions to facilitate further research. The ultimate\ngoal of this survey is to emphasise the importance of this growing research\narea, which so far has not received appropriate level of attention.\n",
                "链接": "https://arxiv.org/abs/2312.01082"
            },
            {
                "文章ID": "105730",
                "标题": "SPELL: Semantic Prompt Evolution based on a LLM",
                "作者": " Yujian Betterest Li,  Kai Wu",
                "发布日期": "2023-10-03",
                "摘要": "  Prompt engineering is a new paradigm for enhancing the performance of trained\nneural network models. For optimizing text-style prompts, existing methods\nusually individually operate small portions of a text step by step, which\neither breaks the fluency or could not globally adjust a prompt. Since large\nlanguage models (LLMs) have powerful ability of generating coherent texts token\nby token, can we utilize LLMs for improving prompts? Based on this motivation,\nin this paper, considering a trained LLM as a text generator, we attempt to\ndesign a black-box evolution algorithm for automatically optimizing texts,\nnamely SPELL (Semantic Prompt Evolution based on a LLM). The proposed method is\nevaluated with different LLMs and evolution parameters in different text tasks.\nExperimental results show that SPELL could rapidly improve the prompts indeed.\nWe further explore the evolution process and discuss on the limitations,\npotential possibilities and future work.\n",
                "链接": "https://arxiv.org/abs/2310.01260"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下近三个月有关语言模型rlhf的arxiv上的全部文章。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "106830",
                "标题": "Confronting Reward Model Overoptimization with Constrained RLHF",
                "作者": " Ted Moskovitz,  Aaditya K. Singh,  DJ Strouse,  Tuomas Sandholm,  Ruslan Salakhutdinov,  Anca D. Dragan,  Stephen McAleer",
                "发布日期": "2023-10-11",
                "摘要": "  Large language models are typically aligned with human preferences by\noptimizing $\\textit{reward models}$ (RMs) fitted to human feedback. However,\nhuman preferences are multi-faceted, and it is increasingly common to derive\nreward from a composition of simpler reward models which each capture a\ndifferent aspect of language quality. This itself presents a challenge, as it\nis difficult to appropriately weight these component RMs when combining them.\nCompounding this difficulty, because any RM is only a proxy for human\nevaluation, this process is vulnerable to $\\textit{overoptimization}$, wherein\npast a certain point, accumulating higher reward is associated with worse human\nratings. In this paper, we perform, to our knowledge, the first study on\noveroptimization in composite RMs, showing that correlation between component\nRMs has a significant effect on the locations of these points. We then\nintroduce an approach to solve this issue using constrained reinforcement\nlearning as a means of preventing the agent from exceeding each RM's threshold\nof usefulness. Our method addresses the problem of weighting component RMs by\nlearning dynamic weights, naturally expressed by Lagrange multipliers. As a\nresult, each RM stays within the range at which it is an effective proxy,\nimproving evaluation performance. Finally, we introduce an adaptive method\nusing gradient-free optimization to identify and optimize towards these points\nduring a single run.\n",
                "链接": "https://arxiv.org/abs/2310.04373"
            },
            {
                "文章ID": "102832",
                "标题": "Stabilizing RLHF through Advantage Model and Selective Rehearsal",
                "作者": " Baolin Peng,  Linfeng Song,  Ye Tian,  Lifeng Jin,  Haitao Mi,  Dong Yu",
                "发布日期": "2023-09-20",
                "摘要": "  Large Language Models (LLMs) have revolutionized natural language processing,\nyet aligning these models with human values and preferences using RLHF remains\na significant challenge. This challenge is characterized by various\ninstabilities, such as reward hacking and catastrophic forgetting. In this\ntechnical report, we propose two innovations to stabilize RLHF training: 1)\nAdvantage Model, which directly models advantage score i.e., extra reward\ncompared to the expected rewards and regulates score distributions across tasks\nto prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic\nforgetting by strategically selecting data for PPO training and knowledge\nrehearsing. Our experimental analysis on public and proprietary datasets\nreveals that the proposed methods not only increase stability in RLHF training\nbut also achieve higher reward scores and win rates.\n",
                "链接": "https://arxiv.org/abs/2309.10202"
            },
            {
                "文章ID": "63716",
                "标题": "ChatGPT: A Meta-Analysis after 2.5 Months",
                "作者": " Christoph Leiter,  Ran Zhang,  Yanran Chen,  Jonas Belouadi,  Daniil Larionov,  Vivian Fresen,  Steffen Eger",
                "发布日期": "2023-02-28",
                "摘要": "  ChatGPT, a chatbot developed by OpenAI, has gained widespread popularity and\nmedia attention since its release in November 2022. However, little hard\nevidence is available regarding its perception in various sources. In this\npaper, we analyze over 300,000 tweets and more than 150 scientific papers to\ninvestigate how ChatGPT is perceived and discussed. Our findings show that\nChatGPT is generally viewed as of high quality, with positive sentiment and\nemotions of joy dominating in social media. Its perception has slightly\ndecreased since its debut, however, with joy decreasing and (negative) surprise\non the rise, and it is perceived more negatively in languages other than\nEnglish. In recent scientific papers, ChatGPT is characterized as a great\nopportunity across various fields including the medical domain, but also as a\nthreat concerning ethics and receives mixed assessments for education. Our\ncomprehensive meta-analysis of ChatGPT's current perception after 2.5 months\nsince its release can contribute to shaping the public debate and informing its\nfuture development. We make our data available.\n",
                "链接": "https://arxiv.org/abs/2302.13795"
            },
            {
                "文章ID": "92138",
                "标题": "Topics, Authors, and Networks in Large Language Model Research: Trends\n  from a Survey of 17K arXiv Papers",
                "作者": " Rajiv Movva,  Sidhika Balachandar,  Kenny Peng,  Gabriel Agostini,  Nikhil Garg,  Emma Pierson",
                "发布日期": "2023-10-24",
                "摘要": "  Large language model (LLM) research is dramatically impacting society, making\nit essential to understand the topics and values it prioritizes, the authors\nand institutions driving it, and its networks of collaboration. Due to the\nrecent growth of the field, many of these fundamental attributes lack\nsystematic description. We gather, annotate, and analyze a new dataset of\n16,979 LLM-related arXiv papers, focusing on changes in 2023 vs. 2018-2022. We\nshow that LLM research increasingly focuses on societal impacts: the Computers\nand Society sub-arXiv has seen 20x growth in its proportion of LLM-related\npapers in 2023. This change is driven in part by an influx of new authors: a\nmajority of 2023 papers are first-authored by researchers who have not\npreviously written an LLM-related paper, and these papers focus particularly on\napplications and societal considerations. While a handful of companies hold\noutsize influence, academia publishes a much larger fraction of papers than\nindustry overall, and this gap widens in 2023. LLM research is also being\nshaped by social dynamics: there are gender and academic/industry differences\nin the topics authors prioritize, and a stark U.S./China schism in the\ncollaboration network. Overall, our analysis documents how LLM research both\nshapes and is shaped by society, attesting to the necessity of sociotechnical\nlenses; we discuss implications for researchers and policymakers.\n",
                "链接": "https://arxiv.org/abs/2307.10700"
            },
            {
                "文章ID": "90293",
                "标题": "Secrets of RLHF in Large Language Models Part I: PPO",
                "作者": " Rui Zheng,  Shihan Dou,  Songyang Gao,  Yuan Hua,  Wei Shen,  Binghai Wang,  Yan Liu,  Senjie Jin,  Qin Liu,  Yuhao Zhou,  Limao Xiong,  Lu Chen,  Zhiheng Xi,  Nuo Xu,  Wenbin Lai,  Minghao Zhu,  Cheng Chang,  Zhangyue Yin,  Rongxiang Weng,  Wensen Cheng,  Haoran Huang,  Tianxiang Sun,  Hang Yan,  Tao Gui,  Qi Zhang,  Xipeng Qiu,  Xuanjing Huang",
                "发布日期": "2023-07-19",
                "摘要": "  Large language models (LLMs) have formulated a blueprint for the advancement\nof artificial general intelligence. Its primary objective is to function as a\nhuman-centric (helpful, honest, and harmless) assistant. Alignment with humans\nassumes paramount significance, and reinforcement learning with human feedback\n(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.\nCurrent technical routes usually include \\textbf{reward models} to measure\nhuman preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize\npolicy model outputs, and \\textbf{process supervision} to improve step-by-step\nreasoning capabilities. However, due to the challenges of reward design,\nenvironment interaction, and agent training, coupled with huge trial and error\ncost of large language models, there is a significant barrier for AI\nresearchers to motivate the development of technical alignment and safe landing\nof LLMs. The stable training of RLHF has still been a puzzle. In the first\nreport, we dissect the framework of RLHF, re-evaluate the inner workings of\nPPO, and explore how the parts comprising PPO algorithms impact policy agent\ntraining. We identify policy constraints being the key factor for the effective\nimplementation of the PPO algorithm. Therefore, we explore the PPO-max, an\nadvanced version of PPO algorithm, to efficiently improve the training\nstability of the policy model. Based on our main results, we perform a\ncomprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.\nThe absence of open-source implementations has posed significant challenges to\nthe investigation of LLMs alignment. Therefore, we are eager to release\ntechnical reports, reward models and PPO codes, aiming to make modest\ncontributions to the advancement of LLMs.\n",
                "链接": "https://arxiv.org/abs/2307.04964"
            },
            {
                "文章ID": "108320",
                "标题": "Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse\n  Autoencoders",
                "作者": " Luke Marks,  Amir Abdullah,  Luna Mendez,  Rauno Arike,  Philip Torr,  Fazl Barez",
                "发布日期": "2023-11-29",
                "摘要": "  Large language models (LLMs) aligned to human preferences via reinforcement\nlearning from human feedback (RLHF) underpin many commercial applications of\nLLM technology. Despite this, the impacts of RLHF on LLM internals remain\nopaque. We propose a novel method for interpreting implicit reward models\n(IRMs) in LLMs learned through RLHF. Our approach trains pairs of autoencoders\non activations from a base LLM and its RLHF-tuned variant. Through a comparison\nof autoencoder hidden spaces, we identify features that reflect the accuracy of\nthe learned IRM. To illustrate our method, we fine-tune an LLM via RLHF to\nlearn a token-utility mapping and maximize the aggregate utility of generated\ntext. This is the first application of sparse autoencoders to interpreting\nIRMs. Our method provides an abstract approximation of reward integrity and\nholds promise for measuring alignment between specified objectives and learned\nmodel behaviors.\n",
                "链接": "https://arxiv.org/abs/2310.08164"
            },
            {
                "文章ID": "81801",
                "标题": "A Three-regime Model of Network Pruning",
                "作者": " Yefan Zhou,  Yaoqing Yang,  Arin Chang,  Michael W. Mahoney",
                "发布日期": "2023-05-31",
                "摘要": "  Recent work has highlighted the complex influence training hyperparameters,\ne.g., the number of training epochs, can have on the prunability of machine\nlearning models. Perhaps surprisingly, a systematic approach to predict\nprecisely how adjusting a specific hyperparameter will affect prunability\nremains elusive. To address this gap, we introduce a phenomenological model\ngrounded in the statistical mechanics of learning. Our approach uses\ntemperature-like and load-like parameters to model the impact of neural network\n(NN) training hyperparameters on pruning performance. A key empirical result we\nidentify is a sharp transition phenomenon: depending on the value of a\nload-like parameter in the pruned model, increasing the value of a\ntemperature-like parameter in the pre-pruned model may either enhance or impair\nsubsequent pruning performance. Based on this transition, we build a\nthree-regime model by taxonomizing the global structure of the pruned NN loss\nlandscape. Our model reveals that the dichotomous effect of high temperature is\nassociated with transitions between distinct types of global structures in the\npost-pruned model. Based on our results, we present three case-studies: 1)\ndetermining whether to increase or decrease a hyperparameter for improved\npruning; 2) selecting the best model to prune from a family of models; and 3)\ntuning the hyperparameter of the Sharpness Aware Minimization method for better\npruning performance.\n",
                "链接": "https://arxiv.org/abs/2305.18383"
            },
            {
                "文章ID": "119747",
                "标题": "RLHF and IIA: Perverse Incentives",
                "作者": " Wanqiao Xu,  Shi Dong,  Xiuyuan Lu,  Grace Lam,  Zheng Wen,  Benjamin Van Roy",
                "发布日期": "2023-12-22",
                "摘要": "  Existing algorithms for reinforcement learning from human feedback (RLHF) can\nincentivize responses at odds with preferences because they are based on models\nthat assume independence of irrelevant alternatives (IIA). The perverse\nincentives induced by IIA give rise to egregious behavior when innovating on\nquery formats or learning algorithms.\n",
                "链接": "https://arxiv.org/abs/2312.01057"
            },
            {
                "文章ID": "31910",
                "标题": "A comment on Guo et al. [arXiv:2206.11228]",
                "作者": " Ben Lonnqvist,  Harshitha Machiraju,  Michael H. Herzog",
                "发布日期": "2022-08-03",
                "摘要": "  In a recent article, Guo et al. [arXiv:2206.11228] report that adversarially\ntrained neural representations in deep networks may already be as robust as\ncorresponding primate IT neural representations. While we find the paper's\nprimary experiment illuminating, we have doubts about the interpretation and\nphrasing of the results presented in the paper.\n",
                "链接": "https://arxiv.org/abs/2208.01456"
            },
            {
                "文章ID": "61977",
                "标题": "Auditing large language models: a three-layered approach",
                "作者": " Jakob Mökander,  Jonas Schuett,  Hannah Rose Kirk,  Luciano Floridi",
                "发布日期": "2023-06-28",
                "摘要": "  Large language models (LLMs) represent a major advance in artificial\nintelligence (AI) research. However, the widespread use of LLMs is also coupled\nwith significant ethical and social challenges. Previous research has pointed\ntowards auditing as a promising governance mechanism to help ensure that AI\nsystems are designed and deployed in ways that are ethical, legal, and\ntechnically robust. However, existing auditing procedures fail to address the\ngovernance challenges posed by LLMs, which display emergent capabilities and\nare adaptable to a wide range of downstream tasks. In this article, we address\nthat gap by outlining a novel blueprint for how to audit LLMs. Specifically, we\npropose a three-layered approach, whereby governance audits (of technology\nproviders that design and disseminate LLMs), model audits (of LLMs after\npre-training but prior to their release), and application audits (of\napplications based on LLMs) complement and inform each other. We show how\naudits, when conducted in a structured and coordinated manner on all three\nlevels, can be a feasible and effective mechanism for identifying and managing\nsome of the ethical and social risks posed by LLMs. However, it is important to\nremain realistic about what auditing can reasonably be expected to achieve.\nTherefore, we discuss the limitations not only of our three-layered approach\nbut also of the prospect of auditing LLMs at all. Ultimately, this article\nseeks to expand the methodological toolkit available to technology providers\nand policymakers who wish to analyse and evaluate LLMs from technical, ethical,\nand legal perspectives.\n",
                "链接": "https://arxiv.org/abs/2302.08500"
            }
        ]
    },
    {
        "question": {
            "question": "查找大模型结合树搜索方法进行推理的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "109239",
                "标题": "Autonomous Tree-search Ability of Large Language Models",
                "作者": " Zheyu Zhang,  Zhuorui Ye,  Yikang Shen,  Chuang Gan",
                "发布日期": "2023-10-18",
                "摘要": "  Large Language Models have excelled in remarkable reasoning capabilities with\nadvanced prompting techniques, but they fall short on tasks that require\nexploration, strategic foresight, and sequential decision-making. Recent works\npropose to utilize external programs to define search logic, such that LLMs can\nperform passive tree search to solve more challenging reasoning tasks. Though\nimpressive results have been achieved, there are several fundamental\nlimitations of these approaches. First, passive tree searches are not efficient\nas they usually require multiple rounds of LLM API calls to solve one single\nproblem. Moreover, passive search methods are not flexible since they need\ntask-specific program designs. Then a natural question arises: can we maintain\nthe tree-search capability of LLMs without the aid of external programs, and\ncan still generate responses that clearly demonstrate the process of a\ntree-structure search? To this end, we propose a new concept called autonomous\ntree-search ability of LLM, which can automatically generate a response\ncontaining search trajectories for the correct answer. Concretely, we perform\nsearch trajectories using capable LLM API via a fixed system prompt, allowing\nthem to perform autonomous tree-search (ATS) right out of the box. Experiments\non 4 puzzle games demonstrate our method can achieve huge improvements. The\nATS-BFS method outperforms the Chain of Thought approach by achieving an\naverage accuracy improvement of 33%. Compared to Tree of Thoughts, it requires\n65.6% or 47.7% less GPT-api cost to attain a comparable level of accuracy.\nMoreover, we have collected data using the ATS prompt method and fine-tuned\nLLaMA. This approach yield a greater improvement compared to the ones\nfine-tuned on CoT data. Specifically, it outperforms CoT-tuned LLaMAs by an\naverage of 40.6% and 38.5% for LLaMA2-7B and LLaMA2-13B, respectively.\n",
                "链接": "https://arxiv.org/abs/2310.10686"
            },
            {
                "文章ID": "105289",
                "标题": "SocREval: Large Language Models with the Socratic Method for\n  Reference-Free Reasoning Evaluation",
                "作者": " Hangfeng He,  Hongming Zhang,  Dan Roth",
                "发布日期": "2023-10-03",
                "摘要": "  To comprehensively assess the capacity of current models for complex\nreasoning, it is crucial to assess their step-by-step reasoning in a scalable\nmanner. Established reference-based evaluation metrics rely on human-annotated\nreasoning chains to assess the model-derived chains. However, such\n``gold-standard'' human-written reasoning chains may not be unique and their\nacquisition is often labor-intensive. Existing reference-free reasoning metrics\neliminate the need for human-crafted reasoning chains as references, but they\ntypically require fine-tuning on datasets with human-derived reasoning chains,\nwhich complicates the process and raises concerns regarding generalizability\nacross diverse datasets. To address these challenges, we harness GPT-4 to\nautomatically evaluate reasoning chain quality, obviating the need for\nhuman-crafted references. Leveraging the Socratic method, we devise tailored\nprompts to enhance reference-free reasoning evaluation, which we term SocREval\n(Socratic method for Reasoning Evaluation). Empirical results from four human\nannotated datasets reveal that SocREval significantly improves GPT-4's\nperformance, surpassing existing reference-free and reference-based reasoning\nevaluation metrics. Beyond its demonstrated efficacy, our proposed framework,\nlarge language models (LLMs) with the Socratic method, proves to be both\ncost-efficient and robust to prompt writing and example selection, as\nsubstantiated by our in-depth analysis.\n",
                "链接": "https://arxiv.org/abs/2310.00074"
            },
            {
                "文章ID": "105172",
                "标题": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and\n  Training",
                "作者": " Xidong Feng,  Ziyu Wan,  Muning Wen,  Ying Wen,  Weinan Zhang,  Jun Wang",
                "发布日期": "2023-10-02",
                "摘要": "  Large language models (LLMs) typically employ sampling or beam search,\naccompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and\ndecoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via\nPlanning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing\ntree-search algorithms to guide multi-step reasoning. These methods mainly\nfocus on LLMs' reasoning ability during inference and heavily rely on\nhuman-designed prompts to activate LLM as a value function, which lacks general\napplicability and scalability. To address these limitations, we present an\nAlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically\nillustrating how tree-search with a learned value function can guide LLMs'\ndecoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a\nlearned value function, our approach can be generally applied to different\ntasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without\nprompting advanced, large-scale models. (2) It can guide LLM's decoding during\nboth inference and training. Empirical evaluations across reasoning, planning,\nand RLHF alignment tasks validate the effectiveness of TS-LLM, even on trees\nwith a depth of 64.\n",
                "链接": "https://arxiv.org/abs/2309.17179"
            },
            {
                "文章ID": "105928",
                "标题": "Can large language models provide useful feedback on research papers? A\n  large-scale empirical analysis",
                "作者": " Weixin Liang,  Yuhui Zhang,  Hancheng Cao,  Binglu Wang,  Daisy Ding,  Xinyu Yang,  Kailas Vodrahalli,  Siyu He,  Daniel Smith,  Yian Yin,  Daniel McFarland,  James Zou",
                "发布日期": "2023-10-04",
                "摘要": "  Expert feedback lays the foundation of rigorous research. However, the rapid\ngrowth of scholarly production and intricate knowledge specialization challenge\nthe conventional scientific feedback mechanisms. High-quality peer reviews are\nincreasingly difficult to obtain. Researchers who are more junior or from\nunder-resourced settings have especially hard times getting timely feedback.\nWith the breakthrough of large language models (LLM) such as GPT-4, there is\ngrowing interest in using LLMs to generate scientific feedback on research\nmanuscripts. However, the utility of LLM-generated feedback has not been\nsystematically studied. To address this gap, we created an automated pipeline\nusing GPT-4 to provide comments on the full PDFs of scientific papers. We\nevaluated the quality of GPT-4's feedback through two large-scale studies. We\nfirst quantitatively compared GPT-4's generated feedback with human peer\nreviewer feedback in 15 Nature family journals (3,096 papers in total) and the\nICLR machine learning conference (1,709 papers). The overlap in the points\nraised by GPT-4 and by human reviewers (average overlap 30.85% for Nature\njournals, 39.23% for ICLR) is comparable to the overlap between two human\nreviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The\noverlap between GPT-4 and human reviewers is larger for the weaker papers. We\nthen conducted a prospective user study with 308 researchers from 110 US\ninstitutions in the field of AI and computational biology to understand how\nresearchers perceive feedback generated by our GPT-4 system on their own\npapers. Overall, more than half (57.4%) of the users found GPT-4 generated\nfeedback helpful/very helpful and 82.4% found it more beneficial than feedback\nfrom at least some human reviewers. While our findings show that LLM-generated\nfeedback can help researchers, we also identify several limitations.\n",
                "链接": "https://arxiv.org/abs/2310.01783"
            },
            {
                "文章ID": "35864",
                "标题": "Petals: Collaborative Inference and Fine-tuning of Large Models",
                "作者": " Alexander Borzunov,  Dmitry Baranchuk,  Tim Dettmers,  Max Ryabinin,  Younes Belkada,  Artem Chumachenko,  Pavel Samygin,  Colin Raffel",
                "发布日期": "2023-03-06",
                "摘要": "  Many NLP tasks benefit from using large language models (LLMs) that often\nhave more than 100 billion parameters. With the release of BLOOM-176B and\nOPT-175B, everyone can download pretrained models of this scale. Still, using\nthese models requires high-end hardware unavailable to many researchers. In\nsome cases, LLMs can be used more affordably via RAM offloading or hosted APIs.\nHowever, these techniques have innate limitations: offloading is too slow for\ninteractive inference, while APIs are not flexible enough for research that\nrequires access to weights, attention or logits. In this work, we propose\nPetals - a system for inference and fine-tuning of large models collaboratively\nby joining the resources of multiple parties. We demonstrate that this strategy\noutperforms offloading for very large models, running inference of BLOOM-176B\non consumer GPUs with $\\approx$ 1 step per second, which is enough for many\ninteractive LLM applications. Unlike most inference APIs, Petals also natively\nexposes hidden states of served models, allowing to train and share custom\nmodel extensions based on efficient fine-tuning methods.\n",
                "链接": "https://arxiv.org/abs/2209.01188"
            },
            {
                "文章ID": "71550",
                "标题": "Inference with Reference: Lossless Acceleration of Large Language Models",
                "作者": " Nan Yang,  Tao Ge,  Liang Wang,  Binxing Jiao,  Daxin Jiang,  Linjun Yang,  Rangan Majumder,  Furu Wei",
                "发布日期": "2023-04-11",
                "摘要": "  We propose LLMA, an LLM accelerator to losslessly speed up Large Language\nModel (LLM) inference with references. LLMA is motivated by the observation\nthat there are abundant identical text spans between the decoding result by an\nLLM and the reference that is available in many real world scenarios (e.g.,\nretrieved documents). LLMA first selects a text span from the reference and\ncopies its tokens to the decoder and then efficiently checks the tokens'\nappropriateness as the decoding result in parallel within one decoding step.\nThe improved computational parallelism allows LLMA to achieve over 2x speed-up\nfor LLMs with identical generation results as greedy decoding in many practical\ngeneration scenarios where significant overlap between in-context reference and\noutputs exists (e.g., search engines and multi-turn conversations).\n",
                "链接": "https://arxiv.org/abs/2304.04487"
            },
            {
                "文章ID": "81190",
                "标题": "Levin Tree Search with Context Models",
                "作者": " Laurent Orseau,  Marcus Hutter,  Levi H. S. Lelis",
                "发布日期": "2023-06-28",
                "摘要": "  Levin Tree Search (LTS) is a search algorithm that makes use of a policy (a\nprobability distribution over actions) and comes with a theoretical guarantee\non the number of expansions before reaching a goal node, depending on the\nquality of the policy. This guarantee can be used as a loss function, which we\ncall the LTS loss, to optimize neural networks representing the policy\n(LTS+NN). In this work we show that the neural network can be substituted with\nparameterized context models originating from the online compression literature\n(LTS+CM). We show that the LTS loss is convex under this new model, which\nallows for using standard convex optimization tools, and obtain convergence\nguarantees to the optimal parameters in an online setting for a given set of\nsolution trajectories -- guarantees that cannot be provided for neural\nnetworks. The new LTS+CM algorithm compares favorably against LTS+NN on several\nbenchmarks: Sokoban (Boxoban), The Witness, and the 24-Sliding Tile puzzle\n(STP). The difference is particularly large on STP, where LTS+NN fails to solve\nmost of the test instances while LTS+CM solves each test instance in a fraction\nof a second. Furthermore, we show that LTS+CM is able to learn a policy that\nsolves the Rubik's cube in only a few hundred expansions, which considerably\nimproves upon previous machine learning techniques.\n",
                "链接": "https://arxiv.org/abs/2305.16945"
            },
            {
                "文章ID": "107843",
                "标题": "Sparse Fine-tuning for Inference Acceleration of Large Language Models",
                "作者": " Eldar Kurtic,  Denis Kuznedelev,  Elias Frantar,  Michael Goin,  Dan Alistarh",
                "发布日期": "2023-10-16",
                "摘要": "  We consider the problem of accurate sparse fine-tuning of large language\nmodels (LLMs), that is, fine-tuning pretrained LLMs on specialized tasks, while\ninducing sparsity in their weights. On the accuracy side, we observe that\nstandard loss-based fine-tuning may fail to recover accuracy, especially at\nhigh sparsities. To address this, we perform a detailed study of\ndistillation-type losses, determining an L2-based distillation approach we term\nSquareHead which enables accurate recovery even at higher sparsities, across\nall model types. On the practical efficiency side, we show that sparse LLMs can\nbe executed with speedups by taking advantage of sparsity, for both CPU and GPU\nruntimes. While the standard approach is to leverage sparsity for computational\nreduction, we observe that in the case of memory-bound LLMs sparsity can also\nbe leveraged for reducing memory bandwidth. We exhibit end-to-end results\nshowing speedups due to sparsity, while recovering accuracy, on T5 (language\ntranslation), Whisper (speech translation), and open GPT-type (MPT for text\ngeneration). For MPT text generation, we show for the first time that sparse\nfine-tuning can reach 75% sparsity without accuracy drops, provide notable\nend-to-end speedups for both CPU and GPU inference, and highlight that sparsity\nis also compatible with quantization approaches. Models and software for\nreproducing our results are provided in Section 6.\n",
                "链接": "https://arxiv.org/abs/2310.06927"
            },
            {
                "文章ID": "22877",
                "标题": "Adaptive Tree Backup Algorithms for Temporal-Difference Reinforcement\n  Learning",
                "作者": " Brett Daley,  Isaac Chan",
                "发布日期": "2022-06-07",
                "摘要": "  Q($\\sigma$) is a recently proposed temporal-difference learning method that\ninterpolates between learning from expected backups and sampled backups. It has\nbeen shown that intermediate values for the interpolation parameter $\\sigma \\in\n[0,1]$ perform better in practice, and therefore it is commonly believed that\n$\\sigma$ functions as a bias-variance trade-off parameter to achieve these\nimprovements. In our work, we disprove this notion, showing that the choice of\n$\\sigma=0$ minimizes variance without increasing bias. This indicates that\n$\\sigma$ must have some other effect on learning that is not fully understood.\nAs an alternative, we hypothesize the existence of a new trade-off: larger\n$\\sigma$-values help overcome poor initializations of the value function, at\nthe expense of higher statistical variance. To automatically balance these\nconsiderations, we propose Adaptive Tree Backup (ATB) methods, whose weighted\nbackups evolve as the agent gains experience. Our experiments demonstrate that\nadaptive strategies can be more effective than relying on fixed or\ntime-annealed $\\sigma$-values.\n",
                "链接": "https://arxiv.org/abs/2206.01896"
            }
        ]
    },
    {
        "question": {
            "question": "2023年以后关于通过prompt经验性研究大语言模型行为的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "76839",
                "标题": "Privacy-Preserving Prompt Tuning for Large Language Model Services",
                "作者": " Yansong Li,  Zhixing Tan,  Yang Liu",
                "发布日期": "2023-05-11",
                "摘要": "  Prompt tuning provides an efficient way for users to customize Large Language\nModels (LLMs) with their private data in the emerging LLM service scenario.\nHowever, the sensitive nature of private data brings the need for privacy\npreservation in LLM service customization. Based on prompt tuning, we propose\nPrivacy-Preserving Prompt Tuning (RAPT), a framework that provides privacy\nguarantees for LLM services. \\textsc{rapt} adopts a local privacy setting,\nallowing users to privatize their data locally with local differential privacy.\nAs prompt tuning performs poorly when directly trained on privatized data, we\nintroduce a novel privatized token reconstruction task that is trained jointly\nwith the downstream task, allowing LLMs to learn better task-dependent\nrepresentations. Despite the simplicity of our framework, experiments show that\nRAPT achieves competitive performance across tasks while providing privacy\nguarantees against adversaries.\n",
                "链接": "https://arxiv.org/abs/2305.06212"
            },
            {
                "文章ID": "83568",
                "标题": "When Large Language Model based Agent Meets User Behavior Analysis: A\n  Novel User Simulation Paradigm",
                "作者": " Lei Wang,  Jingsen Zhang,  Hao Yang,  Zhiyuan Chen,  Jiakai Tang,  Zeyu Zhang,  Xu Chen,  Yankai Lin,  Ruihua Song,  Wayne Xin Zhao,  Jun Xu,  Zhicheng Dou,  Jun Wang,  Ji-Rong Wen",
                "发布日期": "2023-09-19",
                "摘要": "  User behavior analysis is crucial in human-centered AI applications. In this\nfield, the collection of sufficient and high-quality user behavior data has\nalways been a fundamental yet challenging problem. An intuitive idea to address\nthis problem is automatically simulating the user behaviors. However, due to\nthe subjective and complex nature of human cognitive processes, reliably\nsimulating the user behavior is difficult. Recently, large language models\n(LLM) have obtained remarkable successes, showing great potential to achieve\nhuman-like intelligence. We argue that these models present significant\nopportunities for reliable user simulation, and have the potential to\nrevolutionize traditional study paradigms in user behavior analysis. In this\npaper, we take recommender system as an example to explore the potential of\nusing LLM for user simulation. Specifically, we regard each user as an\nLLM-based autonomous agent, and let different agents freely communicate, behave\nand evolve in a virtual simulator called RecAgent. For comprehensively\nsimulation, we not only consider the behaviors within the recommender system\n(\\emph{e.g.}, item browsing and clicking), but also accounts for external\ninfluential factors, such as, friend chatting and social advertisement. Our\nsimulator contains at most 1000 agents, and each agent is composed of a\nprofiling module, a memory module and an action module, enabling it to behave\nconsistently, reasonably and reliably. In addition, to more flexibly operate\nour simulator, we also design two global functions including real-human playing\nand system intervention. To evaluate the effectiveness of our simulator, we\nconduct extensive experiments from both agent and system perspectives. In order\nto advance this direction, we have released our project at\n{https://github.com/RUC-GSAI/YuLan-Rec}.\n",
                "链接": "https://arxiv.org/abs/2306.02552"
            },
            {
                "文章ID": "95315",
                "标题": "Large Language Model Prompt Chaining for Long Legal Document\n  Classification",
                "作者": " Dietrich Trautmann",
                "发布日期": "2023-08-09",
                "摘要": "  Prompting is used to guide or steer a language model in generating an\nappropriate response that is consistent with the desired outcome. Chaining is a\nstrategy used to decompose complex tasks into smaller, manageable components.\nIn this study, we utilize prompt chaining for extensive legal document\nclassification tasks, which present difficulties due to their intricate\ndomain-specific language and considerable length. Our approach begins with the\ncreation of a concise summary of the original document, followed by a semantic\nsearch for related exemplar texts and their corresponding annotations from a\ntraining corpus. Finally, we prompt for a label - based on the task - to\nassign, by leveraging the in-context learning from the few-shot prompt. We\ndemonstrate that through prompt chaining, we can not only enhance the\nperformance over zero-shot, but also surpass the micro-F1 score achieved by\nlarger models, such as ChatGPT zero-shot, using smaller models.\n",
                "链接": "https://arxiv.org/abs/2308.04138"
            },
            {
                "文章ID": "63456",
                "标题": "Robot Behavior-Tree-Based Task Generation with Large Language Models",
                "作者": " Yue Cao,  C. S. George Lee",
                "发布日期": "2023-02-28",
                "摘要": "  Nowadays, the behavior tree is gaining popularity as a representation for\nrobot tasks due to its modularity and reusability. Designing behavior-tree\ntasks manually is time-consuming for robot end-users, thus there is a need for\ninvestigating automatic behavior-tree-based task generation. Prior\nbehavior-tree-based task generation approaches focus on fixed primitive tasks\nand lack generalizability to new task domains. To cope with this issue, we\npropose a novel behavior-tree-based task generation approach that utilizes\nstate-of-the-art large language models. We propose a Phase-Step prompt design\nthat enables a hierarchical-structured robot task generation and further\nintegrate it with behavior-tree-embedding-based search to set up the\nappropriate prompt. In this way, we enable an automatic and cross-domain\nbehavior-tree task generation. Our behavior-tree-based task generation approach\ndoes not require a set of pre-defined primitive tasks. End-users only need to\ndescribe an abstract desired task and our proposed approach can swiftly\ngenerate the corresponding behavior tree. A full-process case study is provided\nto demonstrate our proposed approach. An ablation study is conducted to\nevaluate the effectiveness of our Phase-Step prompts. Assessment on Phase-Step\nprompts and the limitation of large language models are presented and\ndiscussed.\n",
                "链接": "https://arxiv.org/abs/2302.12927"
            },
            {
                "文章ID": "109833",
                "标题": "PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models",
                "作者": " Hongwei Yao,  Jian Lou,  Zhan Qin",
                "发布日期": "2023-12-19",
                "摘要": "  Prompts have significantly improved the performance of pretrained Large\nLanguage Models (LLMs) on various downstream tasks recently, making them\nincreasingly indispensable for a diverse range of LLM application scenarios.\nHowever, the backdoor vulnerability, a serious security threat that can\nmaliciously alter the victim model's normal predictions, has not been\nsufficiently explored for prompt-based LLMs. In this paper, we present\nPOISONPROMPT, a novel backdoor attack capable of successfully compromising both\nhard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and\nrobustness of POISONPROMPT through extensive experiments on three popular\nprompt methods, using six datasets and three widely used LLMs. Our findings\nhighlight the potential security threats posed by backdoor attacks on\nprompt-based LLMs and emphasize the need for further research in this area.\n",
                "链接": "https://arxiv.org/abs/2310.12439"
            },
            {
                "文章ID": "69159",
                "标题": "Prompt Tuning based Adapter for Vision-Language Model Adaption",
                "作者": " Jingchen Sun,  Jiayu Qin,  Zihao Lin,  Changyou Chen",
                "发布日期": "2023-03-28",
                "摘要": "  Large pre-trained vision-language (VL) models have shown significant promise\nin adapting to various downstream tasks. However, fine-tuning the entire\nnetwork is challenging due to the massive number of model parameters. To\naddress this issue, efficient adaptation methods such as prompt tuning have\nbeen proposed. We explore the idea of prompt tuning with multi-task pre-trained\ninitialization and find it can significantly improve model performance. Based\non our findings, we introduce a new model, termed Prompt-Adapter, that combines\npre-trained prompt tunning with an efficient adaptation network. Our approach\nbeat the state-of-the-art methods in few-shot image classification on the\npublic 11 datasets, especially in settings with limited data instances such as\n1 shot, 2 shots, 4 shots, and 8 shots images. Our proposed method demonstrates\nthe promise of combining prompt tuning and parameter-efficient networks for\nefficient vision-language model adaptation. The code is publicly available at:\nhttps://github.com/Jingchensun/prompt_adapter.\n",
                "链接": "https://arxiv.org/abs/2303.15234"
            },
            {
                "文章ID": "97348",
                "标题": "PACE: Improving Prompt with Actor-Critic Editing for Large Language\n  Model",
                "作者": " Yihong Dong,  Kangcheng Luo,  Xue Jiang,  Zhi Jin,  Ge Li",
                "发布日期": "2023-08-22",
                "摘要": "  Large language models (LLMs) have showcased remarkable potential across\nvarious tasks by conditioning on prompts. However, the quality of different\nhuman-written prompts leads to substantial discrepancies in LLMs' performance,\nand improving prompts usually necessitates considerable human effort and\nexpertise. To this end, this paper proposes Prompt with Actor-Critic Editing\n(PACE) for LLMs to enable automatic prompt editing. Drawing inspiration from\nthe actor-critic algorithm in reinforcement learning, PACE leverages LLMs as\nthe dual roles of actors and critics, conceptualizing prompt as a type of\npolicy. PACE refines prompt, taking into account the feedback from both actors\nperforming prompt and critics criticizing response. This process helps LLMs\nbetter align prompt to a specific task, thanks to real responses and thinking\nfrom LLMs. We conduct extensive experiments on 24 instruction induction tasks\nand 21 big-bench tasks. Experimental results indicate that PACE elevates the\nrelative performance of medium/low-quality human-written prompts by up to 98\\%,\nwhich has comparable performance to high-quality human-written prompts.\nMoreover, PACE also exhibits notable efficacy for prompt generation.\n",
                "链接": "https://arxiv.org/abs/2308.10088"
            },
            {
                "文章ID": "114329",
                "标题": "Black-Box Prompt Optimization: Aligning Large Language Models without\n  Model Training",
                "作者": " Jiale Cheng,  Xiao Liu,  Kehan Zheng,  Pei Ke,  Hongning Wang,  Yuxiao Dong,  Jie Tang,  Minlie Huang",
                "发布日期": "2023-11-09",
                "摘要": "  Large language models (LLMs) have shown impressive success in various\napplications. However, these models are often not well aligned with human\nintents, which calls for additional treatments on them, that is, the alignment\nproblem. To make LLMs better follow user instructions, existing alignment\nmethods mostly focus on further training them. However, the extra training of\nLLMs are usually expensive in terms of GPU compute; worse still, LLMs of\ninterest are oftentimes not accessible for user-demanded training, such as\nGPTs. In this work, we take a different perspective -- Black-Box Prompt\nOptimization (BPO) -- to perform alignments. The idea is to optimize user\nprompts to suit LLMs' input understanding, so as to best realize users' intents\nwithout updating LLMs' parameters. BPO is model-agnostic and the empirical\nresults demonstrate that the BPO-aligned ChatGPT yields a 22% increase in the\nwin rate against its original version, and 10% for GPT-4. Importantly, the\nBPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it\nalso brings additional performance gains when combining BPO with PPO or DPO.\nCode and datasets are released at https://github.com/thu-coai/BPO.\n",
                "链接": "https://arxiv.org/abs/2311.04155"
            },
            {
                "文章ID": "120691",
                "标题": "DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt\n  Engineer",
                "作者": " Junyuan Hong,  Jiachen T. Wang,  Chenhui Zhang,  Zhangheng Li,  Bo Li,  Zhangyang Wang",
                "发布日期": "2023-12-08",
                "摘要": "  Large Language Models (LLMs) have emerged as dominant tools for various\ntasks, particularly when tailored for a specific target by prompt tuning.\nNevertheless, concerns surrounding data privacy present obstacles due to the\ntuned prompts' dependency on sensitive private information. A practical\nsolution is to host a local LLM and optimize a soft prompt privately using\ndata. Yet, hosting a local model becomes problematic when model ownership is\nprotected. Alternative methods, like sending data to the model's provider for\ntraining, intensify these privacy issues facing an untrusted provider. In this\npaper, we present a novel solution called Differentially-Private Offsite Prompt\nTuning (DP-OPT) to address this challenge. Our approach involves tuning a\ndiscrete prompt on the client side and then applying it to the desired cloud\nmodels. We demonstrate that prompts suggested by LLMs themselves can be\ntransferred without compromising performance significantly. To ensure that the\nprompts do not leak private information, we introduce the first private prompt\ngeneration mechanism, by a differentially-private (DP) ensemble of in-context\nlearning with private demonstrations. With DP-OPT, generating\nprivacy-preserving prompts by Vicuna-7b can yield competitive performance\ncompared to non-private in-context learning on GPT3.5 or local private prompt\ntuning. Codes are available at https://github.com/VITA-Group/DP-OPT .\n",
                "链接": "https://arxiv.org/abs/2312.03724"
            },
            {
                "文章ID": "79552",
                "标题": "A Study of Generative Large Language Model for Medical Research and\n  Healthcare",
                "作者": " Cheng Peng,  Xi Yang,  Aokun Chen,  Kaleb E Smith,  Nima PourNejatian,  Anthony B Costa,  Cheryl Martin,  Mona G Flores,  Ying Zhang,  Tanja Magoc,  Gloria Lipori,  Duane A Mitchell,  Naykky S Ospina,  Mustafa M Ahmed,  William R Hogan,  Elizabeth A Shenkman,  Yi Guo,  Jiang Bian,  Yonghui Wu",
                "发布日期": "2023-11-20",
                "摘要": "  There is enormous enthusiasm and concerns in using large language models\n(LLMs) in healthcare, yet current assumptions are all based on general-purpose\nLLMs such as ChatGPT. This study develops a clinical generative LLM,\nGatorTronGPT, using 277 billion words of mixed clinical and English text with a\nGPT-3 architecture of 20 billion parameters. GatorTronGPT improves biomedical\nnatural language processing for medical research. Synthetic NLP models trained\nusing GatorTronGPT generated text outperform NLP models trained using\nreal-world clinical text. Physicians Turing test using 1 (worst) to 9 (best)\nscale shows that there is no significant difference in linguistic readability\n(p = 0.22; 6.57 of GatorTronGPT compared with 6.93 of human) and clinical\nrelevance (p = 0.91; 7.0 of GatorTronGPT compared with 6.97 of human) and that\nphysicians cannot differentiate them (p < 0.001). This study provides insights\non the opportunities and challenges of LLMs for medical research and\nhealthcare.\n",
                "链接": "https://arxiv.org/abs/2305.13523"
            }
        ]
    },
    {
        "question": {
            "question": "查找多模态大模型理解和生成统一建模、端到端训练相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "94301",
                "标题": "Interpretable End-to-End Driving Model for Implicit Scene Understanding",
                "作者": " Yiyang Sun,  Xiaonian Wang,  Yangyang Zhang,  Jiagui Tang,  Xiaqiang Tang,  Jing Yao",
                "发布日期": "2023-08-03",
                "摘要": "  Driving scene understanding is to obtain comprehensive scene information\nthrough the sensor data and provide a basis for downstream tasks, which is\nindispensable for the safety of self-driving vehicles. Specific perception\ntasks, such as object detection and scene graph generation, are commonly used.\nHowever, the results of these tasks are only equivalent to the characterization\nof sampling from high-dimensional scene features, which are not sufficient to\nrepresent the scenario. In addition, the goal of perception tasks is\ninconsistent with human driving that just focuses on what may affect the\nego-trajectory. Therefore, we propose an end-to-end Interpretable Implicit\nDriving Scene Understanding (II-DSU) model to extract implicit high-dimensional\nscene features as scene understanding results guided by a planning module and\nto validate the plausibility of scene understanding using auxiliary perception\ntasks for visualization. Experimental results on CARLA benchmarks show that our\napproach achieves the new state-of-the-art and is able to obtain scene features\nthat embody richer scene information relevant to driving, enabling superior\nperformance of the downstream planning.\n",
                "链接": "https://arxiv.org/abs/2308.01180"
            },
            {
                "文章ID": "21046",
                "标题": "End-to-End Multimodal Fact-Checking and Explanation Generation: A\n  Challenging Dataset and Models",
                "作者": "Virginia Tech  Barry Menglong Yao, Virginia Tech  Aditya Shah, Lehigh University  Lichao Sun, Virginia Tech  Jin-Hee Cho, Virginia Tech  Lifu Huang",
                "发布日期": "2023-07-10",
                "摘要": "  We propose end-to-end multimodal fact-checking and explanation generation,\nwhere the input is a claim and a large collection of web sources, including\narticles, images, videos, and tweets, and the goal is to assess the\ntruthfulness of the claim by retrieving relevant evidence and predicting a\ntruthfulness label (e.g., support, refute or not enough information), and to\ngenerate a statement to summarize and explain the reasoning and ruling process.\nTo support this research, we construct Mocheg, a large-scale dataset consisting\nof 15,601 claims where each claim is annotated with a truthfulness label and a\nruling statement, and 33,880 textual paragraphs and 12,112 images in total as\nevidence. To establish baseline performances on Mocheg, we experiment with\nseveral state-of-the-art neural architectures on the three pipelined subtasks:\nmultimodal evidence retrieval, claim verification, and explanation generation,\nand demonstrate that the performance of the state-of-the-art end-to-end\nmultimodal fact-checking does not provide satisfactory outcomes. To the best of\nour knowledge, we are the first to build the benchmark dataset and solutions\nfor end-to-end multimodal fact-checking and explanation generation. The\ndataset, source code and model checkpoints are available at\nhttps://github.com/VT-NLP/Mocheg.\n",
                "链接": "https://arxiv.org/abs/2205.12487"
            },
            {
                "文章ID": "12682",
                "标题": "End-to-end Document Recognition and Understanding with Dessurt",
                "作者": " Brian Davis,  Bryan Morse,  Bryan Price,  Chris Tensmeyer,  Curtis Wigington,  Vlad Morariu",
                "发布日期": "2022-06-17",
                "摘要": "  We introduce Dessurt, a relatively simple document understanding transformer\ncapable of being fine-tuned on a greater variety of document tasks than prior\nmethods. It receives a document image and task string as input and generates\narbitrary text autoregressively as output. Because Dessurt is an end-to-end\narchitecture that performs text recognition in addition to the document\nunderstanding, it does not require an external recognition model as prior\nmethods do. Dessurt is a more flexible model than prior methods and is able to\nhandle a variety of document domains and tasks. We show that this model is\neffective at 9 different dataset-task combinations.\n",
                "链接": "https://arxiv.org/abs/2203.16618"
            },
            {
                "文章ID": "75571",
                "标题": "End-to-end Training and Decoding for Pivot-based Cascaded Translation\n  Model",
                "作者": " Hao Cheng,  Meng Zhang,  Liangyou Li,  Qun Liu,  Zhihua Zhang",
                "发布日期": "2023-05-04",
                "摘要": "  Utilizing pivot language effectively can significantly improve low-resource\nmachine translation. Usually, the two translation models, source-pivot and\npivot-target, are trained individually and do not utilize the limited (source,\ntarget) parallel data. This work proposes an end-to-end training method for the\ncascaded translation model and configures an improved decoding algorithm. The\ninput of the pivot-target model is modified to weighted pivot embedding based\non the probability distribution output by the source-pivot model. This allows\nthe model to be trained end-to-end. In addition, we mitigate the inconsistency\nbetween tokens and probability distributions while using beam search in pivot\ndecoding. Experiments demonstrate that our method enhances the quality of\ntranslation.\n",
                "链接": "https://arxiv.org/abs/2305.02261"
            },
            {
                "文章ID": "106028",
                "标题": "Tuning Large language model for End-to-end Speech Translation",
                "作者": " Hao Zhang,  Nianwen Si,  Yaqi Chen,  Wenlin Zhang,  Xukui Yang,  Dan Qu,  Xiaolin Jiao",
                "发布日期": "2023-10-04",
                "摘要": "  With the emergence of large language models (LLMs), multimodal models based\non LLMs have demonstrated significant potential. Models such as LLaSM, X-LLM,\nand SpeechGPT exhibit an impressive ability to comprehend and generate human\ninstructions. However, their performance often falters when faced with complex\ntasks like end-to-end speech translation (E2E-ST), a cross-language and\ncross-modal translation task. In comparison to single-modal models, multimodal\nmodels lag behind in these scenarios. This paper introduces LST, a Large\nmultimodal model designed to excel at the E2E-ST task. LST consists of a speech\nfrontend, an adapter, and a LLM backend. The training of LST consists of two\nstages: (1) Modality adjustment, where the adapter is tuned to align speech\nrepresentation with text embedding space, and (2) Downstream task fine-tuning,\nwhere both the adapter and LLM model are trained to optimize performance on the\nE2EST task. Experimental results on the MuST-C speech translation benchmark\ndemonstrate that LST-13B achieves BLEU scores of 30.39/41.55/35.33 on\nEn-De/En-Fr/En-Es language pairs, surpassing previous models and establishing a\nnew state-of-the-art. Additionally, we conduct an in-depth analysis of\nsingle-modal model selection and the impact of training strategies, which lays\nthe foundation for future research. We will open up our code and models after\nreview.\n",
                "链接": "https://arxiv.org/abs/2310.02050"
            },
            {
                "文章ID": "92569",
                "标题": "Modality Confidence Aware Training for Robust End-to-End Spoken Language\n  Understanding",
                "作者": " Suyoun Kim,  Akshat Shrivastava,  Duc Le,  Ju Lin,  Ozlem Kalinli,  Michael L. Seltzer",
                "发布日期": "2023-07-25",
                "摘要": "  End-to-end (E2E) spoken language understanding (SLU) systems that generate a\nsemantic parse from speech have become more promising recently. This approach\nuses a single model that utilizes audio and text representations from\npre-trained speech recognition models (ASR), and outperforms traditional\npipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems\nstill show weakness when text representation quality is low due to ASR\ntranscription errors. To overcome this issue, we propose a novel E2E SLU system\nthat enhances robustness to ASR errors by fusing audio and text representations\nbased on the estimated modality confidence of ASR hypotheses. We introduce two\nnovel techniques: 1) an effective method to encode the quality of ASR\nhypotheses and 2) an effective approach to integrate them into E2E SLU models.\nWe show accuracy improvements on STOP dataset and share the analysis to\ndemonstrate the effectiveness of our approach.\n",
                "链接": "https://arxiv.org/abs/2307.12134"
            },
            {
                "文章ID": "62000",
                "标题": "JEIT: Joint End-to-End Model and Internal Language Model Training for\n  Speech Recognition",
                "作者": " Zhong Meng,  Weiran Wang,  Rohit Prabhavalkar,  Tara N. Sainath,  Tongzhou Chen,  Ehsan Variani,  Yu Zhang,  Bo Li,  Andrew Rosenberg,  Bhuvana Ramabhadran",
                "发布日期": "2023-02-20",
                "摘要": "  We propose JEIT, a joint end-to-end (E2E) model and internal language model\n(ILM) training method to inject large-scale unpaired text into ILM during E2E\ntraining which improves rare-word speech recognition. With JEIT, the E2E model\ncomputes an E2E loss on audio-transcript pairs while its ILM estimates a\ncross-entropy loss on unpaired text. The E2E model is trained to minimize a\nweighted sum of E2E and ILM losses. During JEIT, ILM absorbs knowledge from\nunpaired text while the E2E training serves as regularization. Unlike ILM\nadaptation methods, JEIT does not require a separate adaptation step and avoids\nthe need for Kullback-Leibler divergence regularization of ILM. We also show\nthat modular hybrid autoregressive transducer (MHAT) performs better than HAT\nin the JEIT framework, and is much more robust than HAT during ILM adaptation.\nTo push the limit of unpaired text injection, we further propose a combined\nJEIT and JOIST training (CJJT) that benefits from modality matching, encoder\ntext injection and ILM training. Both JEIT and CJJT can foster a more effective\nLM fusion. With 100B unpaired sentences, JEIT/CJJT improves rare-word\nrecognition accuracy by up to 16.4% over a model trained without unpaired text.\n",
                "链接": "https://arxiv.org/abs/2302.08583"
            },
            {
                "文章ID": "100055",
                "标题": "End-to-End Learning on Multimodal Knowledge Graphs",
                "作者": " W. X. Wilcke,  P. Bloem,  V. de Boer,  R. H. van t Veer",
                "发布日期": "2023-09-06",
                "摘要": "  Knowledge graphs enable data scientists to learn end-to-end on heterogeneous\nknowledge. However, most end-to-end models solely learn from the relational\ninformation encoded in graphs' structure: raw values, encoded as literal nodes,\nare either omitted completely or treated as regular nodes without consideration\nfor their values. In either case we lose potentially relevant information which\ncould have otherwise been exploited by our learning methods. We propose a\nmultimodal message passing network which not only learns end-to-end from the\nstructure of graphs, but also from their possibly divers set of multimodal node\nfeatures. Our model uses dedicated (neural) encoders to naturally learn\nembeddings for node features belonging to five different types of modalities,\nincluding numbers, texts, dates, images and geometries, which are projected\ninto a joint representation space together with their relational information.\nWe implement and demonstrate our model on node classification and link\nprediction for artificial and real-worlds datasets, and evaluate the effect\nthat each modality has on the overall performance in an inverse ablation study.\nOur results indicate that end-to-end multimodal learning from any arbitrary\nknowledge graph is indeed possible, and that including multimodal information\ncan significantly affect performance, but that much depends on the\ncharacteristics of the data.\n",
                "链接": "https://arxiv.org/abs/2309.01169"
            },
            {
                "文章ID": "104271",
                "标题": "LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language\n  Models",
                "作者": " Ahmad Faiz,  Sotaro Kaneda,  Ruhan Wang,  Rita Osi,  Parteek Sharma,  Fan Chen,  Lei Jiang",
                "发布日期": "2023-09-27",
                "摘要": "  The carbon footprint associated with large language models (LLMs) is a\nsignificant concern, encompassing emissions from their training, inference,\nexperimentation, and storage processes, including operational and embodied\ncarbon emissions. An essential aspect is accurately estimating the carbon\nimpact of emerging LLMs even before their training, which heavily relies on GPU\nusage. Existing studies have reported the carbon footprint of LLM training, but\nonly one tool, mlco2, can predict the carbon footprint of new neural networks\nprior to physical training. However, mlco2 has several serious limitations. It\ncannot extend its estimation to dense or mixture-of-experts (MoE) LLMs,\ndisregards critical architectural parameters, focuses solely on GPUs, and\ncannot model embodied carbon footprints. Addressing these gaps, we introduce\n\\textit{LLMCarbon}, an end-to-end carbon footprint projection model designed\nfor both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly\nenhances the accuracy of carbon footprint estimations for various LLMs.\n",
                "链接": "https://arxiv.org/abs/2309.14393"
            },
            {
                "文章ID": "12174",
                "标题": "Towards End-to-End Unified Scene Text Detection and Layout Analysis",
                "作者": " Shangbang Long,  Siyang Qin,  Dmitry Panteleev,  Alessandro Bissacco,  Yasuhisa Fujii,  Michalis Raptis",
                "发布日期": "2022-06-06",
                "摘要": "  Scene text detection and document layout analysis have long been treated as\ntwo separate tasks in different image domains. In this paper, we bring them\ntogether and introduce the task of unified scene text detection and layout\nanalysis. The first hierarchical scene text dataset is introduced to enable\nthis novel research task. We also propose a novel method that is able to\nsimultaneously detect scene text and form text clusters in a unified way.\nComprehensive experiments show that our unified model achieves better\nperformance than multiple well-designed baseline methods. Additionally, this\nmodel achieves state-of-the-art results on multiple scene text detection\ndatasets without the need of complex post-processing. Dataset and code:\nhttps://github.com/google-research-datasets/hiertext and\nhttps://github.com/tensorflow/models/tree/master/official/projects/unified_detector.\n",
                "链接": "https://arxiv.org/abs/2203.15143"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下近两年关于语言模型奖励建模评估的文章。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "105659",
                "标题": "Tool-Augmented Reward Modeling",
                "作者": " Lei Li,  Yekun Chai,  Shuohuan Wang,  Yu Sun,  Hao Tian,  Ningyu Zhang,  Hua Wu",
                "发布日期": "2023-10-03",
                "摘要": "  Reward modeling (a.k.a., preference modeling) is instrumental for aligning\nlarge language models with human preferences, particularly within the context\nof reinforcement learning from human feedback (RLHF). While conventional reward\nmodels (RMs) have exhibited remarkable scalability, they oft struggle with\nfundamental functionality such as arithmetic computation, code execution, and\nfactual lookup. In this paper, we propose a tool-augmented preference modeling\napproach, named \\name, to address these limitations by empowering RMs with\naccess to external environments, including calculators and search engines. This\napproach not only fosters synergy between tool utilization and reward grading\nbut also enhances interpretive capacity and scoring reliability. Our study\ndelves into the integration of external tools into RMs, enabling them to\ninteract with diverse external sources and construct task-specific tool\nengagement and reasoning traces in an autoregressive manner. We validate our\napproach across a wide range of domains, incorporating seven distinct external\ntools. Our experimental results demonstrate a noteworthy overall improvement of\n17.7% across eight tasks in preference ranking. Furthermore, our approach\noutperforms Gopher 280B by 7.3% on TruthfulQA task in zero-shot evaluation. In\nhuman evaluations, RLHF trained with Themis attains an average win rate of 32%\nwhen compared to baselines across four distinct tasks. Additionally, we provide\na comprehensive collection of tool-related RM datasets, incorporating data from\nseven distinct tool APIs, totaling 15,000 instances. We anticipate that this\npublicly available dataset will facilitate and inspire further research\nadvancements in the field.\n",
                "链接": "https://arxiv.org/abs/2310.01045"
            },
            {
                "文章ID": "6241",
                "标题": "Reward Modeling for Mitigating Toxicity in Transformer-based Language\n  Models",
                "作者": " Farshid Faal,  Ketra Schmitt,  Jia Yuan Yu",
                "发布日期": "2022-07-28",
                "摘要": "  Transformer-based language models are able to generate fluent text and be\nefficiently adapted across various natural language generation tasks. However,\nlanguage models that are pretrained on large unlabeled web text corpora have\nbeen shown to suffer from degenerating toxic content and social bias behaviors,\nconsequently hindering their safe deployment. Various detoxification methods\nwere proposed to mitigate the language model's toxicity; however, these methods\nstruggled to detoxify language models when conditioned on prompts that contain\nspecific social identities related to gender, race, or religion. In this study,\nwe propose Reinforce-Detoxify; A reinforcement learning-based method for\nmitigating toxicity in language models. We address the challenge of safety in\nlanguage models and propose a new reward model that is able to detect toxic\ncontent and mitigate unintended bias towards social identities in toxicity\nprediction. The experiments demonstrate that the Reinforce-Detoxify method for\nlanguage model detoxification outperforms existing detoxification approaches in\nautomatic evaluation metrics, indicating the ability of our approach in\nlanguage model detoxification and less prone to unintended bias toward social\nidentities in generated content.\n",
                "链接": "https://arxiv.org/abs/2202.09662"
            },
            {
                "文章ID": "81738",
                "标题": "Direct Preference Optimization: Your Language Model is Secretly a Reward\n  Model",
                "作者": " Rafael Rafailov,  Archit Sharma,  Eric Mitchell,  Stefano Ermon,  Christopher D. Manning,  Chelsea Finn",
                "发布日期": "2023-12-14",
                "摘要": "  While large-scale unsupervised language models (LMs) learn broad world\nknowledge and some reasoning skills, achieving precise control of their\nbehavior is difficult due to the completely unsupervised nature of their\ntraining. Existing methods for gaining such steerability collect human labels\nof the relative quality of model generations and fine-tune the unsupervised LM\nto align with these preferences, often with reinforcement learning from human\nfeedback (RLHF). However, RLHF is a complex and often unstable procedure, first\nfitting a reward model that reflects the human preferences, and then\nfine-tuning the large unsupervised LM using reinforcement learning to maximize\nthis estimated reward without drifting too far from the original model. In this\npaper we introduce a new parameterization of the reward model in RLHF that\nenables extraction of the corresponding optimal policy in closed form, allowing\nus to solve the standard RLHF problem with only a simple classification loss.\nThe resulting algorithm, which we call Direct Preference Optimization (DPO), is\nstable, performant, and computationally lightweight, eliminating the need for\nsampling from the LM during fine-tuning or performing significant\nhyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align\nwith human preferences as well as or better than existing methods. Notably,\nfine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of\ngenerations, and matches or improves response quality in summarization and\nsingle-turn dialogue while being substantially simpler to implement and train.\n",
                "链接": "https://arxiv.org/abs/2305.18290"
            },
            {
                "文章ID": "110451",
                "标题": "Learning Reward for Physical Skills using Large Language Model",
                "作者": " Yuwei Zeng,  Yiqing Xu",
                "发布日期": "2023-10-24",
                "摘要": "  Learning reward functions for physical skills are challenging due to the vast\nspectrum of skills, the high-dimensionality of state and action space, and\nnuanced sensory feedback. The complexity of these tasks makes acquiring expert\ndemonstration data both costly and time-consuming. Large Language Models (LLMs)\ncontain valuable task-related knowledge that can aid in learning these reward\nfunctions. However, the direct application of LLMs for proposing reward\nfunctions has its limitations such as numerical instability and inability to\nincorporate the environment feedback. We aim to extract task knowledge from\nLLMs using environment feedback to create efficient reward functions for\nphysical skills. Our approach consists of two components. We first use the LLM\nto propose features and parameterization of the reward function. Next, we\nupdate the parameters of this proposed reward function through an iterative\nself-alignment process. In particular, this process minimizes the ranking\ninconsistency between the LLM and our learned reward functions based on the new\nobservations. We validated our method by testing it on three simulated physical\nskill learning tasks, demonstrating effective support for our design choices.\n",
                "链接": "https://arxiv.org/abs/2310.14092"
            },
            {
                "文章ID": "40287",
                "标题": "Reward Learning with Trees: Methods and Evaluation",
                "作者": " Tom Bewley,  Jonathan Lawry,  Arthur Richards,  Rachel Craddock,  Ian Henderson",
                "发布日期": "2022-10-04",
                "摘要": "  Recent efforts to learn reward functions from human feedback have tended to\nuse deep neural networks, whose lack of transparency hampers our ability to\nexplain agent behaviour or verify alignment. We explore the merits of learning\nintrinsically interpretable tree models instead. We develop a recently proposed\nmethod for learning reward trees from preference labels, and show it to be\nbroadly competitive with neural networks on challenging high-dimensional tasks,\nwith good robustness to limited or corrupted data. Having found that reward\ntree learning can be done effectively in complex settings, we then consider why\nit should be used, demonstrating that the interpretable reward structure gives\nsignificant scope for traceability, verification and explanation.\n",
                "链接": "https://arxiv.org/abs/2210.01007"
            },
            {
                "文章ID": "58975",
                "标题": "Direct Preference-based Policy Optimization without Reward Modeling",
                "作者": " Gaon An,  Junhyeok Lee,  Xingdong Zuo,  Norio Kosaka,  Kyung-Min Kim,  Hyun Oh Song",
                "发布日期": "2023-10-30",
                "摘要": "  Preference-based reinforcement learning (PbRL) is an approach that enables RL\nagents to learn from preference, which is particularly useful when formulating\na reward function is challenging. Existing PbRL methods generally involve a\ntwo-step procedure: they first learn a reward model based on given preference\ndata and then employ off-the-shelf reinforcement learning algorithms using the\nlearned reward model. However, obtaining an accurate reward model solely from\npreference information, especially when the preference is from human teachers,\ncan be difficult. Instead, we propose a PbRL algorithm that directly learns\nfrom preference without requiring any reward modeling. To achieve this, we\nadopt a contrastive learning framework to design a novel policy scoring metric\nthat assigns a high score to policies that align with the given preferences. We\napply our algorithm to offline RL tasks with actual human preference labels and\nshow that our algorithm outperforms or is on par with the existing PbRL\nmethods. Notably, on high-dimensional control tasks, our algorithm surpasses\noffline RL methods that learn with ground-truth reward information. Finally, we\nshow that our algorithm can be successfully applied to fine-tune large language\nmodels.\n",
                "链接": "https://arxiv.org/abs/2301.12842"
            },
            {
                "文章ID": "59850",
                "标题": "Controlling for Stereotypes in Multimodal Language Model Evaluation",
                "作者": " Manuj Malik,  Richard Johansson",
                "发布日期": "2023-02-06",
                "摘要": "  We propose a methodology and design two benchmark sets for measuring to what\nextent language-and-vision language models use the visual signal in the\npresence or absence of stereotypes. The first benchmark is designed to test for\nstereotypical colors of common objects, while the second benchmark considers\ngender stereotypes. The key idea is to compare predictions when the image\nconforms to the stereotype to predictions when it does not.\n  Our results show that there is significant variation among multimodal models:\nthe recent Transformer-based FLAVA seems to be more sensitive to the choice of\nimage and less affected by stereotypes than older CNN-based models such as\nVisualBERT and LXMERT. This effect is more discernible in this type of\ncontrolled setting than in traditional evaluations where we do not know whether\nthe model relied on the stereotype or the visual signal.\n",
                "链接": "https://arxiv.org/abs/2302.01582"
            },
            {
                "文章ID": "109754",
                "标题": "Pseudointelligence: A Unifying Framework for Language Model Evaluation",
                "作者": " Shikhar Murty,  Orr Paradise,  Pratyusha Sharma",
                "发布日期": "2023-10-19",
                "摘要": "  With large language models surpassing human performance on an increasing\nnumber of benchmarks, we must take a principled approach for targeted\nevaluation of model capabilities. Inspired by pseudorandomness, we propose\npseudointelligence, which captures the maxim that \"(perceived) intelligence\nlies in the eye of the beholder\". That is, that claims of intelligence are\nmeaningful only when their evaluator is taken into account. Concretely, we\npropose a complexity-theoretic framework of model evaluation cast as a dynamic\ninteraction between a model and a learned evaluator. We demonstrate that this\nframework can be used to reason about two case studies in language model\nevaluation, as well as analyze existing evaluation methods.\n",
                "链接": "https://arxiv.org/abs/2310.12135"
            },
            {
                "文章ID": "64063",
                "标题": "Reward Design with Language Models",
                "作者": " Minae Kwon,  Sang Michael Xie,  Kalesha Bullard,  Dorsa Sadigh",
                "发布日期": "2023-03-02",
                "摘要": "  Reward design in reinforcement learning (RL) is challenging since specifying\nhuman notions of desired behavior may be difficult via reward functions or\nrequire many expert demonstrations. Can we instead cheaply design rewards using\na natural language interface? This paper explores how to simplify reward design\nby prompting a large language model (LLM) such as GPT-3 as a proxy reward\nfunction, where the user provides a textual prompt containing a few examples\n(few-shot) or a description (zero-shot) of the desired behavior. Our approach\nleverages this proxy reward function in an RL framework. Specifically, users\nspecify a prompt once at the beginning of training. During training, the LLM\nevaluates an RL agent's behavior against the desired behavior described by the\nprompt and outputs a corresponding reward signal. The RL agent then uses this\nreward to update its behavior. We evaluate whether our approach can train\nagents aligned with user objectives in the Ultimatum Game, matrix games, and\nthe DealOrNoDeal negotiation task. In all three tasks, we show that RL agents\ntrained with our framework are well-aligned with the user's objectives and\noutperform RL agents trained with reward functions learned via supervised\nlearning\n",
                "链接": "https://arxiv.org/abs/2303.00001"
            },
            {
                "文章ID": "53814",
                "标题": "Attributed Question Answering: Evaluation and Modeling for Attributed\n  Large Language Models",
                "作者": " Bernd Bohnet,  Vinh Q. Tran,  Pat Verga,  Roee Aharoni,  Daniel Andor,  Livio Baldini Soares,  Massimiliano Ciaramita,  Jacob Eisenstein,  Kuzman Ganchev,  Jonathan Herzig,  Kai Hui,  Tom Kwiatkowski,  Ji Ma,  Jianmo Ni,  Lierni Sestorain Saralegui,  Tal Schuster,  William W. Cohen,  Michael Collins,  Dipanjan Das,  Donald Metzler,  Slav Petrov,  Kellie Webster",
                "发布日期": "2023-02-14",
                "摘要": "  Large language models (LLMs) have shown impressive results while requiring\nlittle or no direct supervision. Further, there is mounting evidence that LLMs\nmay have potential in information-seeking scenarios. We believe the ability of\nan LLM to attribute the text that it generates is likely to be crucial in this\nsetting. We formulate and study Attributed QA as a key first step in the\ndevelopment of attributed LLMs. We propose a reproducible evaluation framework\nfor the task and benchmark a broad set of architectures. We take human\nannotations as a gold standard and show that a correlated automatic metric is\nsuitable for development. Our experimental work gives concrete answers to two\nkey questions (How to measure attribution?, and How well do current\nstate-of-the-art methods perform on attribution?), and give some hints as to\nhow to address a third (How to build LLMs with attribution?).\n",
                "链接": "https://arxiv.org/abs/2212.08037"
            }
        ]
    },
    {
        "question": {
            "question": "请帮我找到从位置编码角度改善模型长序列建模能力的相关论文。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "9782",
                "标题": "Efficient Long Sequence Encoding via Synchronization",
                "作者": " Xiangyang Mou,  Mo Yu,  Bingsheng Yao,  Lifu Huang",
                "发布日期": "2022-03-16",
                "摘要": "  Pre-trained Transformer models have achieved successes in a wide range of NLP\ntasks, but are inefficient when dealing with long input sequences. Existing\nstudies try to overcome this challenge via segmenting the long sequence\nfollowed by hierarchical encoding or post-hoc aggregation. We propose a\nsynchronization mechanism for hierarchical encoding. Our approach first\nidentifies anchor tokens across segments and groups them by their roles in the\noriginal input sequence. Then inside Transformer layer, anchor embeddings are\nsynchronized within their group via a self-attention module. Our approach is a\ngeneral framework with sufficient flexibility -- when adapted to a new task, it\nis easy to be enhanced with the task-specific anchor definitions. Experiments\non two representative tasks with different types of long input texts,\nNarrativeQA summary setting and wild multi-hop reasoning from HotpotQA,\ndemonstrate that our approach is able to improve the global information\nexchange among segments while maintaining efficiency.\n",
                "链接": "https://arxiv.org/abs/2203.07644"
            },
            {
                "文章ID": "61325",
                "标题": "A Unified View of Long-Sequence Models towards Modeling Million-Scale\n  Dependencies",
                "作者": " Hongyu Hè,  Marko Kabic",
                "发布日期": "2023-02-17",
                "摘要": "  Ever since their conception, Transformers have taken over traditional\nsequence models in many tasks, such as NLP, image classification, and\nvideo/audio processing, for their fast training and superior performance. Much\nof the merit is attributable to positional encoding and multi-head attention.\nHowever, Transformers fall short in learning long-range dependencies mainly due\nto the quadratic complexity scaled with context length, in terms of both time\nand space. Consequently, over the past five years, a myriad of methods has been\nproposed to make Transformers more efficient. In this work, we first take a\nstep back, study and compare existing solutions to long-sequence modeling in\nterms of their pure mathematical formulation. Specifically, we summarize them\nusing a unified template, given their shared nature of token mixing. Through\nbenchmarks, we then demonstrate that long context length does yield better\nperformance, albeit application-dependent, and traditional Transformer models\nfall short in taking advantage of long-range dependencies. Next, inspired by\nemerging sparse models of huge capacity, we propose a machine learning system\nfor handling million-scale dependencies. As a proof of concept, we evaluate the\nperformance of one essential component of this system, namely, the distributed\nmulti-head attention. We show that our algorithm can scale up attention\ncomputation by almost $40\\times$ using four GeForce RTX 4090 GPUs, compared to\nvanilla multi-head attention mechanism. We believe this study is an\ninstrumental step towards modeling million-scale dependencies.\n",
                "链接": "https://arxiv.org/abs/2302.06218"
            },
            {
                "文章ID": "40615",
                "标题": "WavSpA: Wavelet Space Attention for Boosting Transformers' Long Sequence\n  Learning Ability",
                "作者": " Yufan Zhuang,  Zihan Wang,  Fangbo Tao,  Jingbo Shang",
                "发布日期": "2023-05-24",
                "摘要": "  Transformer and its variants are fundamental neural architectures in deep\nlearning. Recent works show that learning attention in the Fourier space can\nimprove the long sequence learning capability of Transformers. We argue that\nwavelet transform shall be a better choice because it captures both position\nand frequency information with linear time complexity. Therefore, in this\npaper, we systematically study the synergy between wavelet transform and\nTransformers. We propose Wavelet Space Attention (WavSpA) that facilitates\nattention learning in a learnable wavelet coefficient space which replaces the\nattention in Transformers by (1) applying forward wavelet transform to project\nthe input sequences to multi-resolution bases, (2) conducting attention\nlearning in the wavelet coefficient space, and (3) reconstructing the\nrepresentation in input space via backward wavelet transform. Extensive\nexperiments on the Long Range Arena demonstrate that learning attention in the\nwavelet space using either fixed or adaptive wavelets can consistently improve\nTransformer's performance and also significantly outperform learning in Fourier\nspace. We further show our method can enhance Transformer's reasoning\nextrapolation capability over distance on the LEGO chain-of-reasoning task.\n",
                "链接": "https://arxiv.org/abs/2210.01989"
            },
            {
                "文章ID": "92203",
                "标题": "Variational Point Encoding Deformation for Dental Modeling",
                "作者": " Johan Ziruo Ye,  Thomas Ørkild,  Peter Lempel Søndergaard,  Søren Hauberg",
                "发布日期": "2023-07-21",
                "摘要": "  Digital dentistry has made significant advancements in recent years, yet\nnumerous challenges remain to be addressed. In this study, we release a new\nextensive dataset of tooth meshes to encourage further research. Additionally,\nwe propose Variational FoldingNet (VF-Net), which extends FoldingNet to enable\nprobabilistic learning of point cloud representations. A key challenge in\nexisting latent variable models for point clouds is the lack of a 1-to-1\nmapping between input points and output points. Instead, they must rely on\noptimizing Chamfer distances, a metric that does not have a normalized\ndistributional counterpart, preventing its usage in probabilistic models. We\ndemonstrate that explicit minimization of Chamfer distances can be replaced by\na suitable encoder, which allows us to increase computational efficiency while\nsimplifying the probabilistic extension. Our experimental findings present\nempirical evidence demonstrating the superior performance of VF-Net over\nexisting models in terms of dental scan reconstruction and extrapolation.\nAdditionally, our investigation highlights the robustness of VF-Net's latent\nrepresentations. These results underscore the promising prospects of VF-Net as\nan effective and reliable method for point cloud reconstruction and analysis.\n",
                "链接": "https://arxiv.org/abs/2307.10895"
            },
            {
                "文章ID": "6738",
                "标题": "FastRPB: a Scalable Relative Positional Encoding for Long Sequence Tasks",
                "作者": " Maksim Zubkov,  Daniil Gavrilov",
                "发布日期": "2022-02-24",
                "摘要": "  Transformers achieve remarkable performance in various domains, including\nNLP, CV, audio processing, and graph analysis. However, they do not scale well\non long sequence tasks due to their quadratic complexity w.r.t. the inputs\nlength. Linear Transformers were proposed to address this limitation. However,\nthese models have shown weaker performance on the long sequence tasks comparing\nto the original one. In this paper, we explore Linear Transformer models,\nrethinking their two core components. Firstly, we improved Linear Transformer\nwith Shift-Invariant Kernel Function SIKF, which achieve higher accuracy\nwithout loss in speed. Secondly, we introduce FastRPB which stands for Fast\nRelative Positional Bias, which efficiently adds positional information to\nself-attention using Fast Fourier Transformation. FastRPB is independent of the\nself-attention mechanism and can be combined with an original self-attention\nand all its efficient variants. FastRPB has O(N log(N)) computational\ncomplexity, requiring O(N) memory w.r.t. input sequence length N.\n",
                "链接": "https://arxiv.org/abs/2202.11364"
            },
            {
                "文章ID": "42698",
                "标题": "CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling",
                "作者": " Jun Zhang,  Shuyang Jiang,  Jiangtao Feng,  Lin Zheng,  Lingpeng Kong",
                "发布日期": "2023-07-04",
                "摘要": "  Transformer has achieved remarkable success in language, image, and speech\nprocessing. Recently, various efficient attention architectures have been\nproposed to improve transformer's efficiency while largely preserving its\nefficacy, especially in modeling long sequences. A widely-used benchmark to\ntest these efficient methods' capability on long-range modeling is Long Range\nArena (LRA). However, LRA only focuses on the standard bidirectional (or\nnoncausal) self attention, and completely ignores cross attentions and\nunidirectional (or causal) attentions, which are equally important to\ndownstream applications. In this paper, we propose Comprehensive Attention\nBenchmark (CAB) under a fine-grained attention taxonomy with four\ndistinguishable attention patterns, namely, noncausal self, causal self,\nnoncausal cross, and causal cross attentions. CAB collects seven real-world\ntasks from different research areas to evaluate efficient attentions under the\nfour attention patterns. Among these tasks, CAB validates efficient attentions\nin eight backbone networks to show their generalization across neural\narchitectures. We conduct exhaustive experiments to benchmark the performances\nof nine widely-used efficient attention architectures designed with different\nphilosophies on CAB. Extensive experimental results also shed light on the\nfundamental problems of efficient attentions, such as efficiency length against\nvanilla attention, performance consistency across attention patterns, the\nbenefit of attention mechanisms, and interpolation/extrapolation on\nlong-context language modeling.\n",
                "链接": "https://arxiv.org/abs/2210.07661"
            },
            {
                "文章ID": "61456",
                "标题": "Simple Hardware-Efficient Long Convolutions for Sequence Modeling",
                "作者": " Daniel Y. Fu,  Elliot L. Epstein,  Eric Nguyen,  Armin W. Thomas,  Michael Zhang,  Tri Dao,  Atri Rudra,  Christopher Ré",
                "发布日期": "2023-02-15",
                "摘要": "  State space models (SSMs) have high performance on long sequence modeling but\nrequire sophisticated initialization techniques and specialized implementations\nfor high quality and runtime performance. We study whether a simple alternative\ncan match SSMs in performance and efficiency: directly learning long\nconvolutions over the sequence. We find that a key requirement to achieving\nhigh performance is keeping the convolution kernels smooth. We find that simple\ninterventions--such as squashing the kernel weights--result in smooth kernels\nand recover SSM performance on a range of tasks including the long range arena,\nimage classification, language modeling, and brain data modeling. Next, we\ndevelop FlashButterfly, an IO-aware algorithm to improve the runtime\nperformance of long convolutions. FlashButterfly appeals to classic Butterfly\ndecompositions of the convolution to reduce GPU memory IO and increase FLOP\nutilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows\nus to train on Path256, a challenging task with sequence length 64K, where we\nset state-of-the-art by 29.1 points while training 7.2$\\times$ faster than\nprior work. Lastly, we introduce an extension to FlashButterfly that learns the\ncoefficients of the Butterfly decomposition, increasing expressivity without\nincreasing runtime. Using this extension, we outperform a Transformer on\nWikiText103 by 0.2 PPL with 30% fewer parameters.\n",
                "链接": "https://arxiv.org/abs/2302.06646"
            },
            {
                "文章ID": "83622",
                "标题": "Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition\n  and Constraints",
                "作者": " Chao Lou,  Kewei Tu",
                "发布日期": "2023-06-06",
                "摘要": "  Neural QCFG is a grammar-based sequence-tosequence (seq2seq) model with\nstrong inductive biases on hierarchical structures. It excels in\ninterpretability and generalization but suffers from expensive inference. In\nthis paper, we study two low-rank variants of Neural QCFG for faster inference\nwith different trade-offs between efficiency and expressiveness. Furthermore,\nutilizing the symbolic interface provided by the grammar, we introduce two soft\nconstraints over tree hierarchy and source coverage. We experiment with various\ndatasets and find that our models outperform vanilla Neural QCFG in most\nsettings.\n",
                "链接": "https://arxiv.org/abs/2306.02671"
            },
            {
                "文章ID": "105912",
                "标题": "Stack Attention: Improving the Ability of Transformers to Model\n  Hierarchical Patterns",
                "作者": " Brian DuSell,  David Chiang",
                "发布日期": "2023-10-04",
                "摘要": "  Attention, specifically scaled dot-product attention, has proven effective\nfor natural language, but it does not have a mechanism for handling\nhierarchical patterns of arbitrary nesting depth, which limits its ability to\nrecognize certain syntactic structures. To address this shortcoming, we propose\nstack attention: an attention operator that incorporates stacks, inspired by\ntheir theoretical connections to context-free languages (CFLs). We show that\nstack attention is analogous to standard attention, but with a latent model of\nsyntax that requires no syntactic supervision. We propose two variants: one\nrelated to deterministic pushdown automata (PDAs) and one based on\nnondeterministic PDAs, which allows transformers to recognize arbitrary CFLs.\nWe show that transformers with stack attention are very effective at learning\nCFLs that standard transformers struggle on, achieving strong results on a CFL\nwith theoretically maximal parsing difficulty. We also show that stack\nattention is more effective at natural language modeling under a constrained\nparameter budget, and we include results on machine translation.\n",
                "链接": "https://arxiv.org/abs/2310.01749"
            },
            {
                "文章ID": "43292",
                "标题": "What Makes Convolutional Models Great on Long Sequence Modeling?",
                "作者": " Yuhong Li,  Tianle Cai,  Yi Zhang,  Deming Chen,  Debadeepta Dey",
                "发布日期": "2022-10-18",
                "摘要": "  Convolutional models have been widely used in multiple domains. However, most\nexisting models only use local convolution, making the model unable to handle\nlong-range dependency efficiently. Attention overcomes this problem by\naggregating global information but also makes the computational complexity\nquadratic to the sequence length. Recently, Gu et al. [2021] proposed a model\ncalled S4 inspired by the state space model. S4 can be efficiently implemented\nas a global convolutional model whose kernel size equals the input sequence\nlength. S4 can model much longer sequences than Transformers and achieve\nsignificant gains over SoTA on several long-range tasks. Despite its empirical\nsuccess, S4 is involved. It requires sophisticated parameterization and\ninitialization schemes. As a result, S4 is less intuitive and hard to use. Here\nwe aim to demystify S4 and extract basic principles that contribute to the\nsuccess of S4 as a global convolutional model. We focus on the structure of the\nconvolution kernel and identify two critical but intuitive principles enjoyed\nby S4 that are sufficient to make up an effective global convolutional model:\n1) The parameterization of the convolutional kernel needs to be efficient in\nthe sense that the number of parameters should scale sub-linearly with sequence\nlength. 2) The kernel needs to satisfy a decaying structure that the weights\nfor convolving with closer neighbors are larger than the more distant ones.\nBased on the two principles, we propose a simple yet effective convolutional\nmodel called Structured Global Convolution (SGConv). SGConv exhibits strong\nempirical performance over several tasks: 1) With faster speed, SGConv\nsurpasses S4 on Long Range Arena and Speech Command datasets. 2) When plugging\nSGConv into standard language and vision models, it shows the potential to\nimprove both efficiency and performance.\n",
                "链接": "https://arxiv.org/abs/2210.09298"
            }
        ]
    },
    {
        "question": {
            "question": "利用大模型做数学题生成的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "75161",
                "标题": "Automated Paper Screening for Clinical Reviews Using Large Language\n  Models",
                "作者": " Eddie Guo,  Mehul Gupta,  Jiawen Deng,  Ye-Jean Park,  Mike Paget,  Christopher Naugler",
                "发布日期": "2023-10-09",
                "摘要": "  Objective: To assess the performance of the OpenAI GPT API in accurately and\nefficiently identifying relevant titles and abstracts from real-world clinical\nreview datasets and compare its performance against ground truth labelling by\ntwo independent human reviewers.\n  Methods: We introduce a novel workflow using the OpenAI GPT API for screening\ntitles and abstracts in clinical reviews. A Python script was created to make\ncalls to the GPT API with the screening criteria in natural language and a\ncorpus of title and abstract datasets that have been filtered by a minimum of\ntwo human reviewers. We compared the performance of our model against\nhuman-reviewed papers across six review papers, screening over 24,000 titles\nand abstracts.\n  Results: Our results show an accuracy of 0.91, a sensitivity of excluded\npapers of 0.91, and a sensitivity of included papers of 0.76. On a randomly\nselected subset of papers, the GPT API demonstrated the ability to provide\nreasoning for its decisions and corrected its initial decision upon being asked\nto explain its reasoning for a subset of incorrect classifications.\n  Conclusion: The GPT API has the potential to streamline the clinical review\nprocess, save valuable time and effort for researchers, and contribute to the\noverall quality of clinical reviews. By prioritizing the workflow and acting as\nan aid rather than a replacement for researchers and reviewers, the GPT API can\nenhance efficiency and lead to more accurate and reliable conclusions in\nmedical research.\n",
                "链接": "https://arxiv.org/abs/2305.00844"
            },
            {
                "文章ID": "42549",
                "标题": "Bootstrapping Multilingual Semantic Parsers using Large Language Models",
                "作者": " Abhijeet Awasthi,  Nitish Gupta,  Bidisha Samanta,  Shachi Dave,  Sunita Sarawagi,  Partha Talukdar",
                "发布日期": "2023-02-14",
                "摘要": "  Despite cross-lingual generalization demonstrated by pre-trained multilingual\nmodels, the translate-train paradigm of transferring English datasets across\nmultiple languages remains to be a key mechanism for training task-specific\nmultilingual models. However, for many low-resource languages, the availability\nof a reliable translation service entails significant amounts of costly\nhuman-annotated translation pairs. Further, translation services may continue\nto be brittle due to domain mismatch between task-specific input text and\ngeneral-purpose text used for training translation models. For multilingual\nsemantic parsing, we demonstrate the effectiveness and flexibility offered by\nlarge language models (LLMs) for translating English datasets into several\nlanguages via few-shot prompting. Through extensive comparisons on two public\ndatasets, MTOP and MASSIVE, spanning 50 languages and several domains, we show\nthat our method of translating data using LLMs outperforms a strong\ntranslate-train baseline on 41 out of 50 languages. We study the key design\nchoices that enable more effective multilingual data translation via prompted\nLLMs.\n",
                "链接": "https://arxiv.org/abs/2210.07313"
            },
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "82791",
                "标题": "ReviewerGPT? An Exploratory Study on Using Large Language Models for\n  Paper Reviewing",
                "作者": " Ryan Liu,  Nihar B. Shah",
                "发布日期": "2023-06-02",
                "摘要": "  Given the rapid ascent of large language models (LLMs), we study the\nquestion: (How) can large language models help in reviewing of scientific\npapers or proposals? We first conduct some pilot studies where we find that (i)\nGPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly,\nOpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to\nidentify errors) outperforms prompting to simply write a review. With these\ninsights, we study the use of LLMs (specifically, GPT-4) for three tasks:\n  1. Identifying errors: We construct 13 short computer science papers each\nwith a deliberately inserted error, and ask the LLM to check for the\ncorrectness of these papers. We observe that the LLM finds errors in 7 of them,\nspanning both mathematical and conceptual errors.\n  2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist\nquestions in the respective sections of 15 NeurIPS 2022 papers. We find that\nacross 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy.\n  3. Choosing the \"better\" paper: We generate 10 pairs of abstracts,\ndeliberately designing each pair in such a way that one abstract was clearly\nsuperior than the other. The LLM, however, struggled to discern these\nrelatively straightforward distinctions accurately, committing errors in its\nevaluations for 6 out of the 10 pairs.\n  Based on these experiments, we think that LLMs have a promising use as\nreviewing assistants for specific reviewing tasks, but not (yet) for complete\nevaluations of papers or proposals.\n",
                "链接": "https://arxiv.org/abs/2306.00622"
            },
            {
                "文章ID": "83805",
                "标题": "Synthesizing Affective Neurophysiological Signals Using Generative\n  Models: A Review Paper",
                "作者": " Alireza F. Nia,  Vanessa Tang,  Gonzalo Maso Talou,  Mark Billinghurst",
                "发布日期": "2023-06-07",
                "摘要": "  The integration of emotional intelligence in machines is an important step in\nadvancing human-computer interaction. This demands the development of reliable\nend-to-end emotion recognition systems. However, the scarcity of public\naffective datasets presents a challenge. In this literature review, we\nemphasize the use of generative models to address this issue in\nneurophysiological signals, particularly Electroencephalogram (EEG) and\nFunctional Near-Infrared Spectroscopy (fNIRS). We provide a comprehensive\nanalysis of different generative models used in the field, examining their\ninput formulation, deployment strategies, and methodologies for evaluating the\nquality of synthesized data. This review serves as a comprehensive overview,\noffering insights into the advantages, challenges, and promising future\ndirections in the application of generative models in emotion recognition\nsystems. Through this review, we aim to facilitate the progression of\nneurophysiological data augmentation, thereby supporting the development of\nmore efficient and reliable emotion recognition systems.\n",
                "链接": "https://arxiv.org/abs/2306.03112"
            },
            {
                "文章ID": "38346",
                "标题": "Seeking Diverse Reasoning Logic: Controlled Equation Expression\n  Generation for Solving Math Word Problems",
                "作者": " Yibin Shen,  Qianying Liu,  Zhuoyuan Mao,  Zhen Wan,  Fei Cheng,  Sadao Kurohashi",
                "发布日期": "2022-12-01",
                "摘要": "  To solve Math Word Problems, human students leverage diverse reasoning logic\nthat reaches different possible equation solutions. However, the mainstream\nsequence-to-sequence approach of automatic solvers aims to decode a fixed\nsolution equation supervised by human annotation. In this paper, we propose a\ncontrolled equation generation solver by leveraging a set of control codes to\nguide the model to consider certain reasoning logic and decode the\ncorresponding equations expressions transformed from the human reference. The\nempirical results suggest that our method universally improves the performance\non single-unknown (Math23K) and multiple-unknown (DRAW1K, HMWP) benchmarks,\nwith substantial improvements up to 13.2% accuracy on the challenging\nmultiple-unknown datasets.\n",
                "链接": "https://arxiv.org/abs/2209.10310"
            },
            {
                "文章ID": "83265",
                "标题": "Conceptual Design Generation Using Large Language Models",
                "作者": " Kevin Ma,  Daniele Grandi,  Christopher McComb,  Kosa Goucher-Lambert",
                "发布日期": "2023-06-06",
                "摘要": "  Concept generation is a creative step in the conceptual design phase, where\ndesigners often turn to brainstorming, mindmapping, or crowdsourcing design\nideas to complement their own knowledge of the domain. Recent advances in\nnatural language processing (NLP) and machine learning (ML) have led to the\nrise of Large Language Models (LLMs) capable of generating seemingly creative\noutputs from textual prompts. The success of these models has led to their\nintegration and application across a variety of domains, including art,\nentertainment, and other creative work. In this paper, we leverage LLMs to\ngenerate solutions for a set of 12 design problems and compare them to a\nbaseline of crowdsourced solutions. We evaluate the differences between\ngenerated and crowdsourced design solutions through multiple perspectives,\nincluding human expert evaluations and computational metrics. Expert\nevaluations indicate that the LLM-generated solutions have higher average\nfeasibility and usefulness while the crowdsourced solutions have more novelty.\nWe experiment with prompt engineering and find that leveraging few-shot\nlearning can lead to the generation of solutions that are more similar to the\ncrowdsourced solutions. These findings provide insight into the quality of\ndesign solutions generated with LLMs and begins to evaluate prompt engineering\ntechniques that could be leveraged by practitioners to generate higher-quality\ndesign solutions synergistically with LLMs.\n",
                "链接": "https://arxiv.org/abs/2306.01779"
            },
            {
                "文章ID": "14278",
                "标题": "DISK: Domain-constrained Instance Sketch for Math Word Problem\n  Generation",
                "作者": " Tianyang Cao,  Shuang Zeng,  Xiaodan Xu,  Mairgup Mansur,  Baobao Chang",
                "发布日期": "2022-04-12",
                "摘要": "  A math word problem (MWP) is a coherent narrative which reflects the\nunderlying logic of math equations. Successful MWP generation can automate the\nwriting of mathematics questions. Previous methods mainly generate MWP text\nbased on inflexible pre-defined templates. In this paper, we propose a neural\nmodel for generating MWP text from math equations. Firstly, we incorporate a\nmatching model conditioned on the domain knowledge to retrieve a MWP instance\nwhich is most consistent with the ground-truth, where the domain is a latent\nvariable extracted with a domain summarizer. Secondly, by constructing a\nQuantity Cell Graph (QCG) from the retrieved MWP instance and reasoning over\nit, we improve the model's comprehension of real-world scenarios and derive a\ndomain-constrained instance sketch to guide the generation. Besides, the QCG\nalso interacts with the equation encoder to enhance the alignment between math\ntokens (e.g., quantities and variables) and MWP text. Experiments and empirical\nanalysis on educational MWP set show that our model achieves impressive\nperformance in both automatic evaluation metrics and human evaluation metrics.\n",
                "链接": "https://arxiv.org/abs/2204.04686"
            },
            {
                "文章ID": "97564",
                "标题": "Exploring Equation as a Better Intermediate Meaning Representation for\n  Numerical Reasoning",
                "作者": " Dingzirui Wang,  Longxu Dou,  Wenbin Zhang,  Junyu Zeng,  Wanxiang Che",
                "发布日期": "2023-08-22",
                "摘要": "  Numerical reasoning is vital for natural language processing models to\nunderstand and process numerical information in real-world scenarios. Most\ncurrent methods first generate the Intermediate Meaning Representations (IMRs)\nof questions and then generate answers. Current SOTA methods generate programs\nas IMRs with large language models (LLMs). Intuitively, equations have fewer\nrestrictions and closer semantics to the question than programs, leading to\nhigher generation accuracy. However, current LLMs generate equations worse than\nprograms, where we assume that the equation data is rare in pre-training data\ncompared to programs. So in this paper, we try to use equations as IMRs to\nsolve the numerical reasoning task by addressing two problems: (1)\nTheoretically, how to prove that the equation is an IMR with higher generation\naccuracy than programs; (2) Empirically, how to improve the generation accuracy\nof equations with LLMs. For the first problem, we propose and prove a\nproposition to theoretically compare the generation accuracy of different IMRs.\nFor the second problem, we present a method called Boosting Numerical\nReason\\textbfing by Decomposing the Generation of Equations (Bridge), which can\nimprove the accuracy of LLMs in generating equations as IMRs by reducing the\ntendency of generating constant expressions and programs. Our method improves\nthe performance by 2.2%, 0.9%, and 1.7% on GSM8K, SVAMP, and Algebra datasets\ncompared to the previous state-of-the-art methods under the single reasoning\npath setting. Our codes and prompts are released in\nhttps://github.com/zirui-HIT/Bridge_for_Numerical_Reasoning.\n",
                "链接": "https://arxiv.org/abs/2308.10585"
            },
            {
                "文章ID": "81272",
                "标题": "Large Language Models as Tool Makers",
                "作者": " Tianle Cai,  Xuezhi Wang,  Tengyu Ma,  Xinyun Chen,  Denny Zhou",
                "发布日期": "2023-05-29",
                "摘要": "  Recent research shows the potential of enhancing the problem-solving ability\nof large language models (LLMs) through the use of external tools. However,\nprior work along this line depends on the availability of existing tools. In\nthis work, we take an initial step towards removing this dependency by\nproposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM),\nwhere LLMs create their own reusable tools for problem-solving. Our approach\nconsists of two key phases: 1) tool making: an LLM acts as the tool maker that\ncrafts tools for given tasks, where a tool is implemented as a Python utility\nfunction. 2) tool using: an LLM acts as the tool user, which applies the tool\nbuilt by the tool maker for problem-solving. The tool user can be either the\nsame or a different LLM from the tool maker. Tool-making enables an LLM to\ncontinually generate tools that can be applied to different requests so that\nfuture requests can call the corresponding APIs when beneficial for solving the\ntasks. Furthermore, the division of labor among LLMs for tool-making and\ntool-using phases introduces the opportunity to achieve cost effectiveness\nwithout degrading the quality of generated tools and problem solutions. For\nexample, recognizing that tool-making demands more sophisticated capabilities\nthan tool-using, we can apply a powerful yet resource-intensive model as the\ntool maker, and a lightweight while cost-effective model as the tool user. We\nvalidate the effectiveness of our approach across a variety of complex\nreasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and\nGPT-3.5 as the tool user, LATM can achieve performance that is on par with\nusing GPT-4 for both tool making and tool using, while the inference cost is\nsignificantly reduced.\n",
                "链接": "https://arxiv.org/abs/2305.17126"
            }
        ]
    },
    {
        "question": {
            "question": "查找可以用于验证模型推理能力的数据集论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "51545",
                "标题": "Distilling Reasoning Capabilities into Smaller Language Models",
                "作者": " Kumar Shridhar,  Alessandro Stolfo,  Mrinmaya Sachan",
                "发布日期": "2023-05-19",
                "摘要": "  Step-by-step reasoning approaches like chain of thought (CoT) have proved to\nbe very effective in inducing reasoning capabilities in large language models.\nHowever, the success of the CoT approach is fundamentally tied to the model\nsize, and billion parameter-scale models are often needed to get CoT to work.\nIn this paper, we propose a knowledge distillation approach that leverages the\nstep-by-step CoT reasoning capabilities of larger models and distills these\nabilities into smaller models.\n  In this work, we propose an alternative reasoning scheme, Socratic CoT, that\nlearns a decomposition of the original problem into a sequence of subproblems\nand uses it to guide the intermediate reasoning steps. We use Socratic CoT to\ntrain a combination of two small distilled models: a problem decomposer and a\nsubproblem solver. In practice, given a new problem, the two distilled models\nwork in sync to decompose and solve complex problems. On multiple reasoning\ndatasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies\nboosts the performance of smaller models over 70% compared to the baselines.\nFinally, we investigate when Socratic CoT is an effective alternative to CoT,\ndemonstrating cases where a much smaller model (GPT-2 large) can outperform a\n10X larger model (GPT-3 6B). Our code is available here:\nhttps://github.com/kumar-shridhar/Distiiling-LM\n",
                "链接": "https://arxiv.org/abs/2212.00193"
            },
            {
                "文章ID": "43954",
                "标题": "Disentangling Reasoning Capabilities from Language Models with\n  Compositional Reasoning Transformers",
                "作者": " Wanjun Zhong,  Tingting Ma,  Jiahai Wang,  Jian Yin,  Tiejun Zhao,  Chin-Yew Lin,  Nan Duan",
                "发布日期": "2022-12-08",
                "摘要": "  This paper presents ReasonFormer, a unified reasoning framework for mirroring\nthe modular and compositional reasoning process of humans in complex\ndecision-making. Inspired by dual-process theory in cognitive science, the\nrepresentation module (automatic thinking) and reasoning modules (controlled\nthinking) are decoupled to capture different levels of cognition. Upon the top\nof the representation module, the pre-trained reasoning modules are modular and\nprofessional in specific and fundamental reasoning skills (e.g., logic, simple\nQA, etc). To mimic the controlled compositional thinking process, different\nreasoning modules are dynamically activated and composed in both parallel and\ncascaded manners to control what reasoning skills are activated and how deep\nthe reasoning process will be reached to solve the current problems. The\nunified reasoning framework solves multiple tasks with a single model, and is\ntrained and inferred in an end-to-end manner. Evaluated on 11 datasets\nrequiring different reasoning skills and complexity, ReasonFormer demonstrates\nsubstantial performance boosts, revealing the compositional reasoning ability.\nFew-shot experiments exhibit better generalization ability by learning to\ncompose pre-trained skills for new tasks with limited data, and decoupling the\nrepresentation module and the reasoning modules. Further analysis shows the\nmodularity of reasoning modules as different tasks activate distinct reasoning\nskills at different reasoning depths.\n",
                "链接": "https://arxiv.org/abs/2210.11265"
            },
            {
                "文章ID": "83002",
                "标题": "Evaluating the Capabilities of Multi-modal Reasoning Models with\n  Synthetic Task Data",
                "作者": " Nathan Vaska,  Victoria Helus",
                "发布日期": "2023-06-05",
                "摘要": "  The impressive advances and applications of large language and joint\nlanguage-and-visual understanding models has led to an increased need for\nmethods of probing their potential reasoning capabilities. However, the\ndifficulty of gather naturally-occurring data for complex multi-modal reasoning\ntasks bottlenecks the evaluation of AI methods on tasks which are not already\ncovered by an academic dataset. In this work, we leverage recent advances in\nhigh resolution text-to-image generation to develop a framework for generating\nevaluation data for multi-modal reasoning tasks. We apply this framework to\ngenerate context-dependent anomaly data, creating a synthetic dataset on a\nchallenging task which is not well covered by existing datasets. We benchmark\nthe performance of a state-of-the-art visual question answering (VQA) model\nagainst data generated with this method, and demonstrate that while the task is\ntractable, the model performs significantly worse on the context-dependent\nanomaly detection task than on standard VQA tasks.\n",
                "链接": "https://arxiv.org/abs/2306.01144"
            },
            {
                "文章ID": "107512",
                "标题": "Measuring reasoning capabilities of ChatGPT",
                "作者": " Adrian Groza",
                "发布日期": "2023-10-11",
                "摘要": "  I shall quantify the logical faults generated by ChatGPT when applied to\nreasoning tasks. For experiments, I use the 144 puzzles from the library\n\\url{https://users.utcluj.ro/~agroza/puzzles/maloga}~\\cite{groza:fol}. The\nlibrary contains puzzles of various types, including arithmetic puzzles,\nlogical equations, Sudoku-like puzzles, zebra-like puzzles, truth-telling\npuzzles, grid puzzles, strange numbers, or self-reference puzzles. The correct\nsolutions for these puzzles were checked using the theorem prover\nProver9~\\cite{mccune2005release} and the finite models finder\nMace4~\\cite{mccune2003mace4} based on human-modelling in Equational First Order\nLogic. A first output of this study is the benchmark of 100 logical puzzles.\nFor this dataset ChatGPT provided both correct answer and justification for 7\\%\nonly. %, while BARD for 5\\%. Since the dataset seems challenging, the\nresearchers are invited to test the dataset on more advanced or tuned models\nthan ChatGPT3.5 with more crafted prompts. A second output is the\nclassification of reasoning faults conveyed by ChatGPT. This classification\nforms a basis for a taxonomy of reasoning faults generated by large language\nmodels. I have identified 67 such logical faults, among which: inconsistencies,\nimplication does not hold, unsupported claim, lack of commonsense, wrong\njustification. The 100 solutions generated by ChatGPT contain 698 logical\nfaults. That is on average, 7 fallacies for each reasoning task. A third ouput\nis the annotated answers of the ChatGPT with the corresponding logical faults.\nEach wrong statement within the ChatGPT answer was manually annotated, aiming\nto quantify the amount of faulty text generated by the language model. On\naverage, 26.03\\% from the generated text was a logical fault.\n",
                "链接": "https://arxiv.org/abs/2310.05993"
            },
            {
                "文章ID": "123621",
                "标题": "Assessing Logical Reasoning Capabilities of Encoder-Only Transformer\n  Models",
                "作者": " Paulo Pirozelli,  Marcos M. José,  Paulo de Tarso P. Filho,  Anarosa A. F. Brandão,  Fabio G. Cozman",
                "发布日期": "2023-12-20",
                "摘要": "  Logical reasoning is central to complex human activities, such as thinking,\ndebating, and planning; it is also a central component of many AI systems as\nwell. In this paper, we investigate the extent to which encoder-only\ntransformer language models (LMs) can reason according to logical rules. We ask\nwhether those LMs can deduce theorems in propositional calculus and first-order\nlogic; if their relative success in these problems reflects general logical\ncapabilities; and which layers contribute the most to the task. First, we show\nfor several encoder-only LMs that they can be trained, to a reasonable degree,\nto determine logical validity on various datasets. Next, by cross-probing\nfine-tuned models on these datasets, we show that LMs have difficulty in\ntransferring their putative logical reasoning ability, which suggests that they\nmay have learned dataset-specific features, instead of a general capability.\nFinally, we conduct a layerwise probing experiment, which shows that the\nhypothesis classification task is mostly solved through higher layers.\n",
                "链接": "https://arxiv.org/abs/2312.11720"
            },
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "107805",
                "标题": "Advancing Transformer's Capabilities in Commonsense Reasoning",
                "作者": " Yu Zhou,  Yunqiu Han,  Hanyu Zhou,  Yulun Wu",
                "发布日期": "2023-10-11",
                "摘要": "  Recent advances in general purpose pre-trained language models have shown\ngreat potential in commonsense reasoning. However, current works still perform\npoorly on standard commonsense reasoning benchmarks including the Com2Sense\nDataset. We argue that this is due to a disconnect with current cutting-edge\nmachine learning methods. In this work, we aim to bridge the gap by introducing\ncurrent ML-based methods to improve general purpose pre-trained language models\nin the task of commonsense reasoning. Specifically, we experiment with and\nsystematically evaluate methods including knowledge transfer, model ensemble,\nand introducing an additional pairwise contrastive objective. Our best model\noutperforms the strongest previous works by ~15\\% absolute gains in Pairwise\nAccuracy and ~8.7\\% absolute gains in Standard Accuracy.\n",
                "链接": "https://arxiv.org/abs/2310.06803"
            },
            {
                "文章ID": "4133",
                "标题": "Verifying Inverse Model Neural Networks",
                "作者": " Chelsea Sidrane,  Sydney Katz,  Anthony Corso,  Mykel J. Kochenderfer",
                "发布日期": "2023-01-06",
                "摘要": "  Inverse problems exist in a wide variety of physical domains from aerospace\nengineering to medical imaging. The goal is to infer the underlying state from\na set of observations. When the forward model that produced the observations is\nnonlinear and stochastic, solving the inverse problem is very challenging.\nNeural networks are an appealing solution for solving inverse problems as they\ncan be trained from noisy data and once trained are computationally efficient\nto run. However, inverse model neural networks do not have guarantees of\ncorrectness built-in, which makes them unreliable for use in safety and\naccuracy-critical contexts. In this work we introduce a method for verifying\nthe correctness of inverse model neural networks. Our approach is to\noverapproximate a nonlinear, stochastic forward model with piecewise linear\nconstraints and encode both the overapproximate forward model and the neural\nnetwork inverse model as a mixed-integer program. We demonstrate this\nverification procedure on a real-world airplane fuel gauge case study. The\nability to verify and consequently trust inverse model neural networks allows\ntheir use in a wide variety of contexts, from aerospace to medicine.\n",
                "链接": "https://arxiv.org/abs/2202.02429"
            },
            {
                "文章ID": "92654",
                "标题": "CommonsenseVIS: Visualizing and Understanding Commonsense Reasoning\n  Capabilities of Natural Language Models",
                "作者": " Xingbo Wang,  Renfei Huang,  Zhihua Jin,  Tianqing Fang,  Huamin Qu",
                "发布日期": "2023-11-01",
                "摘要": "  Recently, large pretrained language models have achieved compelling\nperformance on commonsense benchmarks. Nevertheless, it is unclear what\ncommonsense knowledge the models learn and whether they solely exploit spurious\npatterns. Feature attributions are popular explainability techniques that\nidentify important input concepts for model outputs. However, commonsense\nknowledge tends to be implicit and rarely explicitly presented in inputs. These\nmethods cannot infer models' implicit reasoning over mentioned concepts. We\npresent CommonsenseVIS, a visual explanatory system that utilizes external\ncommonsense knowledge bases to contextualize model behavior for commonsense\nquestion-answering. Specifically, we extract relevant commonsense knowledge in\ninputs as references to align model behavior with human knowledge. Our system\nfeatures multi-level visualization and interactive model probing and editing\nfor different concepts and their underlying relations. Through a user study, we\nshow that CommonsenseVIS helps NLP experts conduct a systematic and scalable\nvisual analysis of models' relational reasoning over concepts in different\nsituations.\n",
                "链接": "https://arxiv.org/abs/2307.12382"
            },
            {
                "文章ID": "120925",
                "标题": "CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language\n  Models",
                "作者": " Zhijing Jin,  Yuen Chen,  Felix Leeb,  Luigi Gresele,  Ojasv Kamal,  Zhiheng Lyu,  Kevin Blin,  Fernando Gonzalez Adauto,  Max Kleiman-Weiner,  Mrinmaya Sachan,  Bernhard Schölkopf",
                "发布日期": "2023-12-08",
                "摘要": "  The ability to perform causal reasoning is widely considered a core feature\nof intelligence. In this work, we investigate whether large language models\n(LLMs) can coherently reason about causality. Much of the existing work in\nnatural language processing (NLP) focuses on evaluating commonsense causal\nreasoning in LLMs, thus failing to assess whether a model can perform causal\ninference in accordance with a set of well-defined formal rules. To address\nthis, we propose a new NLP task, causal inference in natural language, inspired\nby the \"causal inference engine\" postulated by Judea Pearl et al. We compose a\nlarge dataset, CLadder, with 10K samples: based on a collection of causal\ngraphs and queries (associational, interventional, and counterfactual), we\nobtain symbolic questions and ground-truth answers, through an oracle causal\ninference engine. These are then translated into natural language. We evaluate\nmultiple LLMs on our dataset, and we introduce and evaluate a bespoke\nchain-of-thought prompting strategy, CausalCoT. We show that our task is highly\nchallenging for LLMs, and we conduct an in-depth analysis to gain deeper\ninsight into the causal reasoning abilities of LLMs. Our data is open-sourced\nat https://huggingface.co/datasets/causalNLP/cladder, and our code can be found\nat https://github.com/causalNLP/cladder.\n",
                "链接": "https://arxiv.org/abs/2312.04350"
            }
        ]
    },
    {
        "question": {
            "question": "帮我找一下用大模型进行论文查找的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "71243",
                "标题": "On Efficient Training of Large-Scale Deep Learning Models: A Literature\n  Review",
                "作者": " Li Shen,  Yan Sun,  Zhiyuan Yu,  Liang Ding,  Xinmei Tian,  Dacheng Tao",
                "发布日期": "2023-04-10",
                "摘要": "  The field of deep learning has witnessed significant progress, particularly\nin computer vision (CV), natural language processing (NLP), and speech. The use\nof large-scale models trained on vast amounts of data holds immense promise for\npractical applications, enhancing industrial productivity and facilitating\nsocial development. With the increasing demands on computational capacity,\nthough numerous studies have explored the efficient training, a comprehensive\nsummarization on acceleration techniques of training deep learning models is\nstill much anticipated. In this survey, we present a detailed review for\ntraining acceleration. We consider the fundamental update formulation and split\nits basic components into five main perspectives: (1) data-centric: including\ndataset regularization, data sampling, and data-centric curriculum learning\ntechniques, which can significantly reduce the computational complexity of the\ndata samples; (2) model-centric, including acceleration of basic modules,\ncompression training, model initialization and model-centric curriculum\nlearning techniques, which focus on accelerating the training via reducing the\ncalculations on parameters; (3) optimization-centric, including the selection\nof learning rate, the employment of large batchsize, the designs of efficient\nobjectives, and model average techniques, which pay attention to the training\npolicy and improving the generality for the large-scale models; (4) budgeted\ntraining, including some distinctive acceleration methods on source-constrained\nsituations; (5) system-centric, including some efficient open-source\ndistributed libraries/systems which provide adequate hardware support for the\nimplementation of acceleration algorithms. By presenting this comprehensive\ntaxonomy, our survey presents a comprehensive review to understand the general\nmechanisms within each component and their joint interaction.\n",
                "链接": "https://arxiv.org/abs/2304.03589"
            },
            {
                "文章ID": "97576",
                "标题": "Large Language Models for Software Engineering: A Systematic Literature\n  Review",
                "作者": " Xinyi Hou,  Yanjie Zhao,  Yue Liu,  Zhou Yang,  Kailong Wang,  Li Li,  Xiapu Luo,  David Lo,  John Grundy,  Haoyu Wang",
                "发布日期": "2023-09-13",
                "摘要": "  Large Language Models (LLMs) have significantly impacted numerous domains,\nincluding Software Engineering (SE). Many recent publications have explored\nLLMs applied to various SE tasks. Nevertheless, a comprehensive understanding\nof the application, effects, and possible limitations of LLMs on SE is still in\nits early stages. To bridge this gap, we conducted a systematic literature\nreview on LLM4SE, with a particular focus on understanding how LLMs can be\nexploited to optimize processes and outcomes. We collect and analyze 229\nresearch papers from 2017 to 2023 to answer four key research questions (RQs).\nIn RQ1, we categorize different LLMs that have been employed in SE tasks,\ncharacterizing their distinctive features and uses. In RQ2, we analyze the\nmethods used in data collection, preprocessing, and application highlighting\nthe role of well-curated datasets for successful LLM for SE implementation. RQ3\ninvestigates the strategies employed to optimize and evaluate the performance\nof LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have\nshown success to date, illustrating their practical contributions to the field.\nFrom the answers to these RQs, we discuss the current state-of-the-art and\ntrends, identifying gaps in existing research, and flagging promising areas for\nfuture study.\n",
                "链接": "https://arxiv.org/abs/2308.10620"
            },
            {
                "文章ID": "82",
                "标题": "A Systematic Literature Review on Persuasive Technology at the Workplace",
                "作者": " Kilian Wenker",
                "发布日期": "2022-08-15",
                "摘要": "  Employees face decisions every day - in the absence of supervision. The\noutcome of these decisions can be influenced by digital workplace design\nthrough the power of persuasive technology. This paper provides a structured\nliterature review based on recent research on persuasive technology in the\nworkplace. It examines the design and use of persuasive systems from a variety\nof disciplinary perspectives and theories. The reviewed studies were\ncategorized into the research streams of technology design, user-centered\nresearch, and gamification. The purpose of the studies is categorized using a\nmodified definition of the persuasive systems design model. A number of\nexperimental studies show that alignment of the employee's behavior with the\nemployer's agenda can be achieved. A robust finding is the key role of\ninteractivity in granting employees a subjective experience of rapid and\nmeaningful feedback when using the interface.\n",
                "链接": "https://arxiv.org/abs/2201.00329"
            },
            {
                "文章ID": "80431",
                "标题": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review\n  Generation",
                "作者": " Tetsu Kasanishi,  Masaru Isonuma,  Junichiro Mori,  Ichiro Sakata",
                "发布日期": "2023-05-25",
                "摘要": "  Automatic literature review generation is one of the most challenging tasks\nin natural language processing. Although large language models have tackled\nliterature review generation, the absence of large-scale datasets has been a\nstumbling block to the progress. We release SciReviewGen, consisting of over\n10,000 literature reviews and 690,000 papers cited in the reviews. Based on the\ndataset, we evaluate recent transformer-based summarization models on the\nliterature review generation task, including Fusion-in-Decoder extended for\nliterature review generation. Human evaluation results show that some\nmachine-generated summaries are comparable to human-written reviews, while\nrevealing the challenges of automatic literature review generation such as\nhallucinations and a lack of detailed information. Our dataset and code are\navailable at https://github.com/tetsu9923/SciReviewGen.\n",
                "链接": "https://arxiv.org/abs/2305.15186"
            },
            {
                "文章ID": "90483",
                "标题": "The Ethical Implications of Generative Audio Models: A Systematic\n  Literature Review",
                "作者": " Julia Barnett",
                "发布日期": "2023-07-13",
                "摘要": "  Generative audio models typically focus their applications in music and\nspeech generation, with recent models having human-like quality in their audio\noutput. This paper conducts a systematic literature review of 884 papers in the\narea of generative audio models in order to both quantify the degree to which\nresearchers in the field are considering potential negative impacts and\nidentify the types of ethical implications researchers in this area need to\nconsider. Though 65% of generative audio research papers note positive\npotential impacts of their work, less than 10% discuss any negative impacts.\nThis jarringly small percentage of papers considering negative impact is\nparticularly worrying because the issues brought to light by the few papers\ndoing so are raising serious ethical implications and concerns relevant to the\nbroader field such as the potential for fraud, deep-fakes, and copyright\ninfringement. By quantifying this lack of ethical consideration in generative\naudio research and identifying key areas of potential harm, this paper lays the\ngroundwork for future work in the field at a critical point in time in order to\nguide more conscientious research as this field progresses.\n",
                "链接": "https://arxiv.org/abs/2307.05527"
            },
            {
                "文章ID": "56592",
                "标题": "How Data Scientists Review the Scholarly Literature",
                "作者": " Sheshera Mysore,  Mahmood Jasim,  Haoru Song,  Sarah Akbar,  Andre Kenneth Chase Randall,  Narges Mahyar",
                "发布日期": "2023-01-11",
                "摘要": "  Keeping up with the research literature plays an important role in the\nworkflow of scientists - allowing them to understand a field, formulate the\nproblems they focus on, and develop the solutions that they contribute, which\nin turn shape the nature of the discipline. In this paper, we examine the\nliterature review practices of data scientists. Data science represents a field\nseeing an exponential rise in papers, and increasingly drawing on and being\napplied in numerous diverse disciplines. Recent efforts have seen the\ndevelopment of several tools intended to help data scientists cope with a\ndeluge of research and coordinated efforts to develop AI tools intended to\nuncover the research frontier. Despite these trends indicative of the\ninformation overload faced by data scientists, no prior work has examined the\nspecific practices and challenges faced by these scientists in an\ninterdisciplinary field with evolving scholarly norms. In this paper, we close\nthis gap through a set of semi-structured interviews and think-aloud protocols\nof industry and academic data scientists (N = 20). Our results while\ncorroborating other knowledge workers' practices uncover several novel\nfindings: individuals (1) are challenged in seeking and sensemaking of papers\nbeyond their disciplinary bubbles, (2) struggle to understand papers in the\nface of missing details and mathematical content, (3) grapple with the deluge\nby leveraging the knowledge context in code, blogs, and talks, and (4) lean on\ntheir peers online and in-person. Furthermore, we outline future directions\nlikely to help data scientists cope with the burgeoning research literature.\n",
                "链接": "https://arxiv.org/abs/2301.03774"
            },
            {
                "文章ID": "94694",
                "标题": "AI Literature Review Suite",
                "作者": " David A. Tovar",
                "发布日期": "2023-08-07",
                "摘要": "  The process of conducting literature reviews is often time-consuming and\nlabor-intensive. To streamline this process, I present an AI Literature Review\nSuite that integrates several functionalities to provide a comprehensive\nliterature review. This tool leverages the power of open access science, large\nlanguage models (LLMs) and natural language processing to enable the searching,\ndownloading, and organizing of PDF files, as well as extracting content from\narticles. Semantic search queries are used for data retrieval, while text\nembeddings and summarization using LLMs present succinct literature reviews.\nInteraction with PDFs is enhanced through a user-friendly graphical user\ninterface (GUI). The suite also features integrated programs for bibliographic\norganization, interaction and query, and literature review summaries. This tool\npresents a robust solution to automate and optimize the process of literature\nreview in academic and industrial research.\n",
                "链接": "https://arxiv.org/abs/2308.02443"
            },
            {
                "文章ID": "17522",
                "标题": "Distributed intelligence on the Edge-to-Cloud Continuum: A systematic\n  literature review",
                "作者": "KerData  Daniel Rosendo, KerData  Alexandru Costan, ZENITH  Patrick Valduriez, KerData  Gabriel Antoniu",
                "发布日期": "2022-05-03",
                "摘要": "  The explosion of data volumes generated by an increasing number of\napplications is strongly impacting the evolution of distributed digital\ninfrastructures for data analytics and machine learning (ML). While data\nanalytics used to be mainly performed on cloud infrastructures, the rapid\ndevelopment of IoT infrastructures and the requirements for low-latency, secure\nprocessing has motivated the development of edge analytics. Today, to balance\nvarious trade-offs, ML-based analytics tends to increasingly leverage an\ninterconnected ecosystem that allows complex applications to be executed on\nhybrid infrastructures where IoT Edge devices are interconnected to Cloud/HPC\nsystems in what is called the Computing Continuum, the Digital Continuum, or\nthe Transcontinuum.Enabling learning-based analytics on such complex\ninfrastructures is challenging. The large scale and optimized deployment of\nlearning-based workflows across the Edge-to-Cloud Continuum requires extensive\nand reproducible experimental analysis of the application execution on\nrepresentative testbeds. This is necessary to help understand the performance\ntrade-offs that result from combining a variety of learning paradigms and\nsupportive frameworks. A thorough experimental analysis requires the assessment\nof the impact of multiple factors, such as: model accuracy, training time,\nnetwork overhead, energy consumption, processing latency, among others.This\nreview aims at providing a comprehensive vision of the main state-of-the-art\nlibraries and frameworks for machine learning and data analytics available\ntoday. It describes the main learning paradigms enabling learning-based\nanalytics on the Edge-to-Cloud Continuum. The main simulation, emulation,\ndeployment systems, and testbeds for experimental research on the Edge-to-Cloud\nContinuum available today are also surveyed. Furthermore, we analyze how the\nselected systems provide support for experiment reproducibility. We conclude\nour review with a detailed discussion of relevant open research challenges and\nof future directions in this domain such as: holistic understanding of\nperformance; performance optimization of applications;efficient deployment of\nArtificial Intelligence (AI) workflows on highly heterogeneous infrastructures;\nand reproducible analysis of experiments on the Computing Continuum.\n",
                "链接": "https://arxiv.org/abs/2205.01081"
            },
            {
                "文章ID": "83104",
                "标题": "A systematic literature review on the code smells datasets and\n  validation mechanisms",
                "作者": " Morteza Zakeri-Nasrabadi,  Saeed Parsa,  Ehsan Esmaili,  Fabio Palomba",
                "发布日期": "2023-06-05",
                "摘要": "  The accuracy reported for code smell-detecting tools varies depending on the\ndataset used to evaluate the tools. Our survey of 45 existing datasets reveals\nthat the adequacy of a dataset for detecting smells highly depends on relevant\nproperties such as the size, severity level, project types, number of each type\nof smell, number of smells, and the ratio of smelly to non-smelly samples in\nthe dataset. Most existing datasets support God Class, Long Method, and Feature\nEnvy while six smells in Fowler and Beck's catalog are not supported by any\ndatasets. We conclude that existing datasets suffer from imbalanced samples,\nlack of supporting severity level, and restriction to Java language.\n",
                "链接": "https://arxiv.org/abs/2306.01377"
            },
            {
                "文章ID": "90669",
                "标题": "Deep Generative Models for Physiological Signals: A Systematic\n  Literature Review",
                "作者": " Nour Neifar,  Afef Mdhaffar,  Achraf Ben-Hamadou,  Mohamed Jmaiel",
                "发布日期": "2023-07-13",
                "摘要": "  In this paper, we present a systematic literature review on deep generative\nmodels for physiological signals, particularly electrocardiogram,\nelectroencephalogram, photoplethysmogram and electromyogram. Compared to the\nexisting review papers, we present the first review that summarizes the recent\nstate-of-the-art deep generative models. By analysing the state-of-the-art\nresearch related to deep generative models along with their main applications\nand challenges, this review contributes to the overall understanding of these\nmodels applied to physiological signals. Additionally, by highlighting the\nemployed evaluation protocol and the most used physiological databases, this\nreview facilitates the assessment and benchmarking of deep generative models.\n",
                "链接": "https://arxiv.org/abs/2307.06162"
            }
        ]
    },
    {
        "question": {
            "question": "请帮我找到，最早提出分布式词表示的论文是哪一篇？",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "16842",
                "标题": "Extremal GloVe: Theoretically Accurate Distributed Word Embedding by\n  Tail Inference",
                "作者": " Hao Wang",
                "发布日期": "2022-04-28",
                "摘要": "  Distributed word embeddings such as Word2Vec and GloVe have been widely\nadopted in industrial context settings. Major technical applications of GloVe\ninclude recommender systems and natural language processing. The fundamental\ntheory behind GloVe relies on the selection of a weighting function in the\nweighted least squres formulation that computes the powered ratio of word\noccurrence count and the maximum word count in the corpus. However, the initial\nformulation of GloVe is not theoretically sound in two aspects, namely the\nselection of the weighting function and its power exponent is ad-hoc. In this\npaper, we utilize the theory of extreme value analysis and propose a\ntheoretically accurate version of GloVe. By reformulating the weighted least\nsquares loss function as the expected loss function and accurately choosing the\npower exponent, we create a theoretically accurate version of GloVe. We\ndemonstrate the competitiveness of our algorithm and show that the initial\nformulation of GloVe with the suggested optimal parameter can be viewed as a\nspecial case of our paradigm.\n",
                "链接": "https://arxiv.org/abs/2204.13009"
            },
            {
                "文章ID": "42603",
                "标题": "Multilingual Word Sense Disambiguation with Unified Sense Representation",
                "作者": " Ying Su,  Hongming Zhang,  Yangqiu Song,  Tong Zhang",
                "发布日期": "2022-10-17",
                "摘要": "  As a key natural language processing (NLP) task, word sense disambiguation\n(WSD) evaluates how well NLP models can understand the lexical semantics of\nwords under specific contexts. Benefited from the large-scale annotation,\ncurrent WSD systems have achieved impressive performances in English by\ncombining supervised learning with lexical knowledge. However, such success is\nhard to be replicated in other languages, where we only have limited\nannotations.In this paper, based on the multilingual lexicon BabelNet\ndescribing the same set of concepts across languages, we propose building\nknowledge and supervised-based Multilingual Word Sense Disambiguation (MWSD)\nsystems. We build unified sense representations for multiple languages and\naddress the annotation scarcity problem for MWSD by transferring annotations\nfrom rich-sourced languages to poorer ones. With the unified sense\nrepresentations, annotations from multiple languages can be jointly trained to\nbenefit the MWSD tasks. Evaluations of SemEval-13 and SemEval-15 datasets\ndemonstrate the effectiveness of our methodology.\n",
                "链接": "https://arxiv.org/abs/2210.07447"
            },
            {
                "文章ID": "49974",
                "标题": "Word-Level Representation From Bytes For Language Modeling",
                "作者": " Chu-Tak Lee,  Qipeng Guo,  Xipeng Qiu",
                "发布日期": "2022-11-24",
                "摘要": "  Modern language models mostly take sub-words as input, a design that balances\nthe trade-off between vocabulary size, number of parameters, and performance.\nHowever, sub-word tokenization still has disadvantages like not being robust to\nnoise and difficult to generalize to new languages. Also, the current trend of\nscaling up models reveals that larger models require larger embeddings but that\nmakes parallelization hard. Previous work on image classification proves\nsplitting raw input into a sequence of chucks is a strong, model-agnostic\ninductive bias. Based on this observation, we rethink the existing\ncharacter-aware method that takes character-level inputs but makes word-level\nsequence modeling and prediction. We overhaul this method by introducing a\ncross-attention network that builds word-level representation directly from\nbytes, and a sub-word level prediction based on word-level hidden states to\navoid the time and space requirement of word-level prediction. With these two\nimprovements combined, we have a token free model with slim input embeddings\nfor downstream tasks. We name our method Byte2Word and perform evaluations on\nlanguage modeling and text classification. Experiments show that Byte2Word is\non par with the strong sub-word baseline BERT but only takes up 10\\% of\nembedding size. We further test our method on synthetic noise and cross-lingual\ntransfer and find it competitive to baseline methods on both settings.\n",
                "链接": "https://arxiv.org/abs/2211.12677"
            },
            {
                "文章ID": "19560",
                "标题": "KGNN: Distributed Framework for Graph Neural Knowledge Representation",
                "作者": " Binbin Hu,  Zhiyang Hu,  Zhiqiang Zhang,  Jun Zhou,  Chuan Shi",
                "发布日期": "2022-05-18",
                "摘要": "  Knowledge representation learning has been commonly adopted to incorporate\nknowledge graph (KG) into various online services. Although existing knowledge\nrepresentation learning methods have achieved considerable performance\nimprovement, they ignore high-order structure and abundant attribute\ninformation, resulting unsatisfactory performance on semantics-rich KGs.\nMoreover, they fail to make prediction in an inductive manner and cannot scale\nto large industrial graphs. To address these issues, we develop a novel\nframework called KGNN to take full advantage of knowledge data for\nrepresentation learning in the distributed learning system. KGNN is equipped\nwith GNN based encoder and knowledge aware decoder, which aim to jointly\nexplore high-order structure and attribute information together in a\nfine-grained fashion and preserve the relation patterns in KGs, respectively.\nExtensive experiments on three datasets for link prediction and triplet\nclassification task demonstrate the effectiveness and scalability of KGNN\nframework.\n",
                "链接": "https://arxiv.org/abs/2205.08285"
            },
            {
                "文章ID": "73435",
                "标题": "Distributed Neural Representation for Reactive in situ Visualization",
                "作者": " Qi Wu,  Joseph A. Insley,  Victor A. Mateevitsi,  Silvio Rizzi,  Michael E. Papka,  Kwan-Liu Ma",
                "发布日期": "2023-04-21",
                "摘要": "  In situ visualization and steering of computational modeling can be\neffectively achieved using reactive programming, which leverages temporal\nabstraction and data caching mechanisms to create dynamic workflows. However,\nimplementing a temporal cache for large-scale simulations can be challenging.\nImplicit neural networks have proven effective in compressing large volume\ndata. However, their application to distributed data has yet to be fully\nexplored. In this work, we develop an implicit neural representation for\ndistributed volume data and incorporate it into the DIVA reactive programming\nsystem. This implementation enables us to build an in situ temporal caching\nsystem with a capacity 100 times larger than previously achieved. We integrate\nour implementation into the Ascent infrastructure and evaluate its performance\nusing real-world simulations.\n",
                "链接": "https://arxiv.org/abs/2304.10516"
            },
            {
                "文章ID": "3597",
                "标题": "Towards a Theoretical Understanding of Word and Relation Representation",
                "作者": " Carl Allen",
                "发布日期": "2022-02-02",
                "摘要": "  Representing words by vectors, or embeddings, enables computational reasoning\nand is foundational to automating natural language tasks. For example, if word\nembeddings of similar words contain similar values, word similarity can be\nreadily assessed, whereas judging that from their spelling is often impossible\n(e.g. cat /feline) and to predetermine and store similarities between all words\nis prohibitively time-consuming, memory intensive and subjective. We focus on\nword embeddings learned from text corpora and knowledge graphs. Several\nwell-known algorithms learn word embeddings from text on an unsupervised basis\nby learning to predict those words that occur around each word, e.g. word2vec\nand GloVe. Parameters of such word embeddings are known to reflect word\nco-occurrence statistics, but how they capture semantic meaning has been\nunclear. Knowledge graph representation models learn representations both of\nentities (words, people, places, etc.) and relations between them, typically by\ntraining a model to predict known facts in a supervised manner. Despite steady\nimprovements in fact prediction accuracy, little is understood of the latent\nstructure that enables this.\n  The limited understanding of how latent semantic structure is encoded in the\ngeometry of word embeddings and knowledge graph representations makes a\nprincipled means of improving their performance, reliability or\ninterpretability unclear. To address this:\n  1. we theoretically justify the empirical observation that particular\ngeometric relationships between word embeddings learned by algorithms such as\nword2vec and GloVe correspond to semantic relations between words; and\n  2. we extend this correspondence between semantics and geometry to the\nentities and relations of knowledge graphs, providing a model for the latent\nstructure of knowledge graph representation linked to that of word embeddings.\n",
                "链接": "https://arxiv.org/abs/2202.00486"
            },
            {
                "文章ID": "80919",
                "标题": "A Distributed Automatic Domain-Specific Multi-Word Term Recognition\n  Architecture using Spark Ecosystem",
                "作者": " Ciprian-Octavian Truică,  Neculai-Ovidiu Istrate,  Elena-Simona Apostol",
                "发布日期": "2023-05-29",
                "摘要": "  Automatic Term Recognition is used to extract domain-specific terms that\nbelong to a given domain. In order to be accurate, these corpus and\nlanguage-dependent methods require large volumes of textual data that need to\nbe processed to extract candidate terms that are afterward scored according to\na given metric. To improve text preprocessing and candidate terms extraction\nand scoring, we propose a distributed Spark-based architecture to automatically\nextract domain-specific terms. The main contributions are as follows: (1)\npropose a novel distributed automatic domain-specific multi-word term\nrecognition architecture built on top of the Spark ecosystem; (2) perform an\nin-depth analysis of our architecture in terms of accuracy and scalability; (3)\ndesign an easy-to-integrate Python implementation that enables the use of Big\nData processing in fields such as Computational Linguistics and Natural\nLanguage Processing. We prove empirically the feasibility of our architecture\nby performing experiments on two real-world datasets.\n",
                "链接": "https://arxiv.org/abs/2305.16343"
            },
            {
                "文章ID": "116434",
                "标题": "Collaborative Word-based Pre-trained Item Representation for\n  Transferable Recommendation",
                "作者": " Shenghao Yang,  Chenyang Wang,  Yankai Liu,  Kangping Xu,  Weizhi Ma,  Yiqun Liu,  Min Zhang,  Haitao Zeng,  Junlan Feng,  Chao Deng",
                "发布日期": "2023-12-22",
                "摘要": "  Item representation learning (IRL) plays an essential role in recommender\nsystems, especially for sequential recommendation. Traditional sequential\nrecommendation models usually utilize ID embeddings to represent items, which\nare not shared across different domains and lack the transferable ability.\nRecent studies use pre-trained language models (PLM) for item text embeddings\n(text-based IRL) that are universally applicable across domains. However, the\nexisting text-based IRL is unaware of the important collaborative filtering\n(CF) information. In this paper, we propose CoWPiRec, an approach of\nCollaborative Word-based Pre-trained item representation for Recommendation. To\neffectively incorporate CF information into text-based IRL, we convert the\nitem-level interaction data to a word graph containing word-level\ncollaborations. Subsequently, we design a novel pre-training task to align the\nword-level semantic- and CF-related item representation. Extensive experimental\nresults on multiple public datasets demonstrate that compared to\nstate-of-the-art transferable sequential recommenders, CoWPiRec achieves\nsignificantly better performances in both fine-tuning and zero-shot settings\nfor cross-scenario recommendation and effectively alleviates the cold-start\nissue. The code is available at: https://github.com/ysh-1998/CoWPiRec.\n",
                "链接": "https://arxiv.org/abs/2311.10501"
            },
            {
                "文章ID": "118082",
                "标题": "Probabilistic Transformer: A Probabilistic Dependency Model for\n  Contextual Word Representation",
                "作者": " Haoyi Wu,  Kewei Tu",
                "发布日期": "2023-11-28",
                "摘要": "  Syntactic structures used to play a vital role in natural language processing\n(NLP), but since the deep learning revolution, NLP has been gradually dominated\nby neural models that do not consider syntactic structures in their design. One\nvastly successful class of neural models is transformers. When used as an\nencoder, a transformer produces contextual representation of words in the input\nsentence. In this work, we propose a new model of contextual word\nrepresentation, not from a neural perspective, but from a purely syntactic and\nprobabilistic perspective. Specifically, we design a conditional random field\nthat models discrete latent representations of all words in a sentence as well\nas dependency arcs between them; and we use mean field variational inference\nfor approximate inference. Strikingly, we find that the computation graph of\nour model resembles transformers, with correspondences between dependencies and\nself-attention and between distributions over latent representations and\ncontextual embeddings of words. Experiments show that our model performs\ncompetitively to transformers on small to medium sized datasets. We hope that\nour work could help bridge the gap between traditional syntactic and\nprobabilistic approaches and cutting-edge neural approaches to NLP, and inspire\nmore linguistically-principled neural approaches in the future.\n",
                "链接": "https://arxiv.org/abs/2311.15211"
            },
            {
                "文章ID": "686",
                "标题": "Coherence-Based Distributed Document Representation Learning for\n  Scientific Documents",
                "作者": " Shicheng Tan,  Shu Zhao,  Yanping Zhang",
                "发布日期": "2022-01-11",
                "摘要": "  Distributed document representation is one of the basic problems in natural\nlanguage processing. Currently distributed document representation methods\nmainly consider the context information of words or sentences. These methods do\nnot take into account the coherence of the document as a whole, e.g., a\nrelation between the paper title and abstract, headline and description, or\nadjacent bodies in the document. The coherence shows whether a document is\nmeaningful, both logically and syntactically, especially in scientific\ndocuments (papers or patents, etc.). In this paper, we propose a coupled text\npair embedding (CTPE) model to learn the representation of scientific\ndocuments, which maintains the coherence of the document with coupled text\npairs formed by segmenting the document. First, we divide the document into two\nparts (e.g., title and abstract, etc) which construct a coupled text pair.\nThen, we adopt negative sampling to construct uncoupled text pairs whose two\nparts are from different documents. Finally, we train the model to judge\nwhether the text pair is coupled or uncoupled and use the obtained embedding of\ncoupled text pairs as the embedding of documents. We perform experiments on\nthree datasets for one information retrieval task and two recommendation tasks.\nThe experimental results verify the effectiveness of the proposed CTPE model.\n",
                "链接": "https://arxiv.org/abs/2201.02846"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下nips 2023 paper list",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "16003",
                "标题": "The 2021 NIST Speaker Recognition Evaluation",
                "作者": " Seyed Omid Sadjadi,  Craig Greenberg,  Elliot Singer,  Lisa Mason,  Douglas Reynolds",
                "发布日期": "2022-04-22",
                "摘要": "  The 2021 Speaker Recognition Evaluation (SRE21) was the latest cycle of the\nongoing evaluation series conducted by the U.S. National Institute of Standards\nand Technology (NIST) since 1996. It was the second large-scale multimodal\nspeaker/person recognition evaluation organized by NIST (the first one being\nSRE19). Similar to SRE19, it featured two core evaluation tracks, namely audio\nand audio-visual, as well as an optional visual track. In addition to offering\nfixed and open training conditions, it also introduced new challenges for the\ncommunity, thanks to a new multimodal (i.e., audio, video, and selfie images)\nand multilingual (i.e., with multilingual speakers) corpus, termed WeCanTalk,\ncollected outside North America by the Linguistic Data Consortium (LDC). These\nchallenges included: 1) trials (target and non-target) with enrollment and test\nsegments originating from different domains (i.e., telephony versus video), and\n2) trials (target and non-target) with enrollment and test segments spoken in\ndifferent languages (i.e., cross-lingual trials). This paper presents an\noverview of SRE21 including the tasks, performance metric, data, evaluation\nprotocol, results and system performance analyses. A total of 23 organizations\n(forming 15 teams) from academia and industry participated in SRE21 and\nsubmitted 158 valid system outputs. Evaluation results indicate: audio-visual\nfusion produce substantial gains in performance over audio-only or visual-only\nsystems; top performing speaker and face recognition systems exhibited\ncomparable performance under the matched domain conditions present in this\nevaluation; and, the use of complex neural network architectures (e.g., ResNet)\nalong with angular losses with margin, data augmentation, as well as long\nduration fine-tuning contributed to notable performance improvements for the\naudio-only speaker recognition task.\n",
                "链接": "https://arxiv.org/abs/2204.10242"
            },
            {
                "文章ID": "63988",
                "标题": "The 2022 NIST Language Recognition Evaluation",
                "作者": " Yooyoung Lee,  Craig Greenberg,  Eliot Godard,  Asad A. Butt,  Elliot Singer,  Trang Nguyen,  Lisa Mason,  Douglas Reynolds",
                "发布日期": "2023-03-01",
                "摘要": "  In 2022, the U.S. National Institute of Standards and Technology (NIST)\nconducted the latest Language Recognition Evaluation (LRE) in an ongoing series\nadministered by NIST since 1996 to foster research in language recognition and\nto measure state-of-the-art technology. Similar to previous LREs, LRE22 focused\non conversational telephone speech (CTS) and broadcast narrowband speech (BNBS)\ndata. LRE22 also introduced new evaluation features, such as an emphasis on\nAfrican languages, including low resource languages, and a test set consisting\nof segments containing between 3s and 35s of speech randomly sampled and\nextracted from longer recordings. A total of 21 research organizations, forming\n16 teams, participated in this 3-month long evaluation and made a total of 65\nvalid system submissions to be evaluated. This paper presents an overview of\nLRE22 and an analysis of system performance over different evaluation\nconditions. The evaluation results suggest that Oromo and Tigrinya are easier\nto detect while Xhosa and Zulu are more challenging. A greater confusability is\nseen for some language pairs. When speech duration increased, system\nperformance significantly increased up to a certain duration, and then a\ndiminishing return on system performance is observed afterward.\n",
                "链接": "https://arxiv.org/abs/2302.14624"
            },
            {
                "文章ID": "20490",
                "标题": "CYRUS Soccer Simulation 2D Team Description Paper 2022",
                "作者": " Nader Zare,  Arad Firouzkouhi,  Omid Amini,  Mahtab Sarvmaili,  Aref Sayareh,  Saba Ramezani Rad,  Stan Matwin,  Amilcar Soares",
                "发布日期": "2022-05-24",
                "摘要": "  Soccer Simulation 2D League is one of the major leagues of RoboCup\ncompetitions. In a Soccer Simulation 2D (SS2D) game, two teams of 11 players\nand one coach compete against each other. The players are only allowed to\ncommunicate with the server that is called Soccer Simulation Server. This paper\nintroduces the previous and current research of the CYRUS soccer simulation\nteam, the champion of RoboCup 2021. We will present our idea about improving\nUnmarking Decisioning and Positioning by using Pass Prediction Deep Neural\nNetwork. Based on our experimental results, this idea proven to be effective on\nincreasing the winning rate of Cyrus against opponents.\n",
                "链接": "https://arxiv.org/abs/2205.10953"
            },
            {
                "文章ID": "91898",
                "标题": "Rob\\^oCIn Small Size League Extended Team Description Paper for RoboCup\n  2023",
                "作者": " Aline Lima de Oliveira,  Cauê Addae da Silva Gomes,  Cecília Virginia Santos da Silva,  Charles Matheus de Sousa Alves,  Danilo Andrade Martins de Souza,  Driele Pires Ferreira Araújo Xavier,  Edgleyson Pereira da Silva,  Felipe Bezerra Martins,  Lucas Henrique Cavalcanti Santos,  Lucas Dias Maciel,  Matheus Paixão Gumercindo dos Santos,  Matheus Lafayette Vasconcelos,  Matheus Vinícius Teotonio do Nascimento Andrade,  João Guilherme Oliveira Carvalho de Melo,  João Pedro Souza Pereira de Moura,  José Ronald da Silva,  José Victor Silva Cruz,  Pedro Henrique Santana de Morais,  Pedro Paulo Salman de Oliveira,  Riei Joaquim Matos Rodrigues,  Roberto Costa Fernandes,  Ryan Vinicius Santos Morais,  Tamara Mayara Ramos Teobaldo,  Washington Igor dos Santos Silva,  Edna Natividade Silva Barros",
                "发布日期": "2023-07-20",
                "摘要": "  Rob\\^oCIn has participated in RoboCup Small Size League since 2019, won its\nfirst world title in 2022 (Division B), and is currently a three-times\nLatin-American champion. This paper presents our improvements to defend the\nSmall Size League (SSL) division B title in RoboCup 2023 in Bordeaux, France.\nThis paper aims to share some of the academic research that our team developed\nover the past year. Our team has successfully published 2 articles related to\nSSL at two high-impact conferences: the 25th RoboCup International Symposium\nand the 19th IEEE Latin American Robotics Symposium (LARS 2022). Over the last\nyear, we have been continuously migrating from our past codebase to\nUnification. We will describe the new architecture implemented and some points\nof software and AI refactoring. In addition, we discuss the process of\nintegrating machined components into the mechanical system, our development for\nparticipating in the vision blackout challenge last year and what we are\npreparing for this year.\n",
                "链接": "https://arxiv.org/abs/2307.10018"
            },
            {
                "文章ID": "103411",
                "标题": "Scaling up COMETKIWI: Unbabel-IST 2023 Submission for the Quality\n  Estimation Shared Task",
                "作者": " Ricardo Rei,  Nuno M. Guerreiro,  José Pombal,  Daan van Stigt,  Marcos Treviso,  Luisa Coheur,  José G. C. de Souza,  André F. T. Martins",
                "发布日期": "2023-09-22",
                "摘要": "  We present the joint contribution of Unbabel and Instituto Superior T\\'ecnico\nto the WMT 2023 Shared Task on Quality Estimation (QE). Our team participated\non all tasks: sentence- and word-level quality prediction (task 1) and\nfine-grained error span detection (task 2). For all tasks, we build on the\nCOMETKIWI-22 model (Rei et al., 2022b). Our multilingual approaches are ranked\nfirst for all tasks, reaching state-of-the-art performance for quality\nestimation at word-, span- and sentence-level granularity. Compared to the\nprevious state-of-the-art COMETKIWI-22, we show large improvements in\ncorrelation with human judgements (up to 10 Spearman points). Moreover, we\nsurpass the second-best multilingual submission to the shared-task with up to\n3.8 absolute points.\n",
                "链接": "https://arxiv.org/abs/2309.11925"
            },
            {
                "文章ID": "7583",
                "标题": "Paper Plain: Making Medical Research Papers Approachable to Healthcare\n  Consumers with Natural Language Processing",
                "作者": " Tal August,  Lucy Lu Wang,  Jonathan Bragg,  Marti A. Hearst,  Andrew Head,  Kyle Lo",
                "发布日期": "2022-03-02",
                "摘要": "  When seeking information not covered in patient-friendly documents, like\nmedical pamphlets, healthcare consumers may turn to the research literature.\nReading medical papers, however, can be a challenging experience. To improve\naccess to medical papers, we introduce a novel interactive interface-Paper\nPlain-with four features powered by natural language processing: definitions of\nunfamiliar terms, in-situ plain language section summaries, a collection of key\nquestions that guide readers to answering passages, and plain language\nsummaries of the answering passages. We evaluate Paper Plain, finding that\nparticipants who use Paper Plain have an easier time reading and understanding\nresearch papers without a loss in paper comprehension compared to those who use\na typical PDF reader. Altogether, the study results suggest that guiding\nreaders to relevant passages and providing plain language summaries, or\n\"gists,\" alongside the original paper content can make reading medical papers\neasier and give readers more confidence to approach these papers.\n",
                "链接": "https://arxiv.org/abs/2203.00130"
            },
            {
                "文章ID": "99360",
                "标题": "SharpSAT-TD in Model Counting Competitions 2021-2023",
                "作者": " Tuukka Korhonen,  Matti Järvisalo",
                "发布日期": "2023-08-31",
                "摘要": "  We describe SharpSAT-TD, our submission to the unweighted and weighted tracks\nof the Model Counting Competition in 2021-2023, which has won in total $6$\nfirst places in different tracks of the competition. SharpSAT-TD is based on\nSharpSAT [Thurley, SAT 2006], with the primary novel modification being the use\nof tree decompositions in the variable selection heuristic as introduced by the\nauthors in [CP 2021]. Unlike the version of SharpSAT-TD evaluated in [CP 2021],\nthe current version that is available in https://github.com/Laakeri/sharpsat-td\nfeatures also other significant modifications compared to the original\nSharpSAT, for example, a new preprocessor.\n",
                "链接": "https://arxiv.org/abs/2308.15819"
            },
            {
                "文章ID": "37129",
                "标题": "CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared\n  Task",
                "作者": " Ricardo Rei,  Marcos Treviso,  Nuno M. Guerreiro,  Chrysoula Zerva,  Ana C. Farinha,  Christine Maroti,  José G. C. de Souza,  Taisiya Glushkova,  Duarte M. Alves,  Alon Lavie,  Luisa Coheur,  André F. T. Martins",
                "发布日期": "2022-09-15",
                "摘要": "  We present the joint contribution of IST and Unbabel to the WMT 2022 Shared\nTask on Quality Estimation (QE). Our team participated on all three subtasks:\n(i) Sentence and Word-level Quality Prediction; (ii) Explainable QE; and (iii)\nCritical Error Detection. For all tasks we build on top of the COMET framework,\nconnecting it with the predictor-estimator architecture of OpenKiwi, and\nequipping it with a word-level sequence tagger and an explanation extractor.\nOur results suggest that incorporating references during pretraining improves\nperformance across several language pairs on downstream tasks, and that jointly\ntraining with sentence and word-level objectives yields a further boost.\nFurthermore, combining attention and gradient information proved to be the top\nstrategy for extracting good explanations of sentence-level QE models. Overall,\nour submissions achieved the best results for all three tasks for almost all\nlanguage pairs by a considerable margin.\n",
                "链接": "https://arxiv.org/abs/2209.06243"
            },
            {
                "文章ID": "56242",
                "标题": "LostNet: A smart way for lost and find",
                "作者": " Meihua Zhou,  Ivan Fung,  Li Yang,  Nan Wan,  Keke Di,  Tingting Wang",
                "发布日期": "2023-01-09",
                "摘要": "  Due to the enormous population growth of cities in recent years, objects are\nfrequently lost and unclaimed on public transportation, in restaurants, or any\nother public areas. While services like Find My iPhone can easily identify lost\nelectronic devices, more valuable objects cannot be tracked in an intelligent\nmanner, making it impossible for administrators to reclaim a large number of\nlost and found items in a timely manner. We present a method that significantly\nreduces the complexity of searching by comparing previous images of lost and\nrecovered things provided by the owner with photos taken when registered lost\nand found items are received. In this research, we will primarily design a\nphoto matching network by combining the fine-tuning method of MobileNetv2 with\nCBAM Attention and using the Internet framework to develop an online lost and\nfound image identification system. Our implementation gets a testing accuracy\nof 96.8% using only 665.12M GLFOPs and 3.5M training parameters. It can\nrecognize practice images and can be run on a regular laptop.\n",
                "链接": "https://arxiv.org/abs/2301.02277"
            },
            {
                "文章ID": "108003",
                "标题": "Preliminary Results of a Scientometric Analysis of the German\n  Information Retrieval Community 2020-2023",
                "作者": " Philipp Schaer,  Svetlana Myshkina,  Jüri Keller",
                "发布日期": "2023-10-12",
                "摘要": "  The German Information Retrieval community is located in two different\nsub-fields: Information and computer science. There are no current studies that\ninvestigate these communities on a scientometric level. Available studies only\nfocus on the information scientific part of the community. We generated a data\nset of 401 recent IR-related publications extracted from six core IR\nconferences from a mainly computer scientific background. We analyze this data\nset at the institutional and researcher level. The data set is publicly\nreleased, and we also demonstrate a mapping use case.\n",
                "链接": "https://arxiv.org/abs/2310.07346"
            }
        ]
    },
    {
        "question": {
            "question": "闭源大模型用户数据隐私保护研究",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "106181",
                "标题": "Large Language Models Can Be Good Privacy Protection Learners",
                "作者": " Yijia Xiao,  Yiqiao Jin,  Yushi Bai,  Yue Wu,  Xianjun Yang,  Xiao Luo,  Wenchao Yu,  Xujiang Zhao,  Yanchi Liu,  Haifeng Chen,  Wei Wang,  Wei Cheng",
                "发布日期": "2023-10-10",
                "摘要": "  The proliferation of Large Language Models (LLMs) has driven considerable\ninterest in fine-tuning them with domain-specific data to create specialized\nlanguage models. Nevertheless, such domain-specific fine-tuning data often\ncontains sensitive personally identifiable information (PII). Direct\nfine-tuning LLMs on this data without privacy protection poses a risk of\nleakage. To address this challenge, we introduce Privacy Protection Language\nModels (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects\ndomain-specific knowledge while safeguarding data privacy. Our work offers a\ntheoretical analysis for model design and delves into various techniques such\nas corpus curation, penalty-based unlikelihood in training loss, and\ninstruction-based tuning, etc. Extensive experiments across diverse datasets\nand scenarios demonstrate the effectiveness of our approaches. In particular,\ninstruction tuning with both positive and negative examples, stands out as a\npromising method, effectively protecting private data while enhancing the\nmodel's knowledge. Our work underscores the potential for Large Language Models\nas robust privacy protection learners.\n",
                "链接": "https://arxiv.org/abs/2310.02469"
            },
            {
                "文章ID": "85712",
                "标题": "Protecting User Privacy in Remote Conversational Systems: A\n  Privacy-Preserving framework based on text sanitization",
                "作者": " Zhigang Kan,  Linbo Qiao,  Hao Yu,  Liwen Peng,  Yifu Gao,  Dongsheng Li",
                "发布日期": "2023-06-16",
                "摘要": "  Large Language Models (LLMs) are gaining increasing attention due to their\nexceptional performance across numerous tasks. As a result, the general public\nutilize them as an influential tool for boosting their productivity while\nnatural language processing researchers endeavor to employ them in solving\nexisting or new research problems. Unfortunately, individuals can only access\nsuch powerful AIs through APIs, which ultimately leads to the transmission of\nraw data to the models' providers and increases the possibility of privacy data\nleakage. Current privacy-preserving methods for cloud-deployed language models\naim to protect privacy information in the pre-training dataset or during the\nmodel training phase. However, they do not meet the specific challenges\npresented by the remote access approach of new large-scale language models.\n  This paper introduces a novel task, \"User Privacy Protection for Dialogue\nModels,\" which aims to safeguard sensitive user information from any possible\ndisclosure while conversing with chatbots. We also present an evaluation scheme\nfor this task, which covers evaluation metrics for privacy protection, data\navailability, and resistance to simulation attacks. Moreover, we propose the\nfirst framework for this task, namely privacy protection through text\nsanitization. Before sending the input to remote large models, it filters out\nthe sensitive information, using several rounds of text sanitization based on\nprivacy types that users define. Upon receiving responses from the larger\nmodel, our framework automatically restores privacy to ensure that the\nconversation goes smoothly, without intervention from the privacy filter.\nExperiments based on real-world datasets demonstrate the efficacy of our\nprivacy-preserving approach against eavesdropping from potential attackers.\n",
                "链接": "https://arxiv.org/abs/2306.08223"
            },
            {
                "文章ID": "5282",
                "标题": "Privacy protection based on mask template",
                "作者": " Hao Wang,  Yu Bai,  Guangmin Sun,  Jie Liu",
                "发布日期": "2022-02-15",
                "摘要": "  Powerful recognition algorithms are widely used in the Internet or important\nmedical systems, which poses a serious threat to personal privacy. Although the\nlaw provides for diversity protection, e.g. The General Data Protection\nRegulation (GDPR) in Europe and Articles 1032 to 1039 of the civil code in\nChina. However, as an important privacy disclosure event, biometric data is\noften hidden, which is difficult for the owner to detect and trace to the\nsource. Human biometrics generally exist in images. In order to avoid the\ndisclosure of personal privacy, we should prevent unauthorized recognition\nalgorithms from acquiring the real features of the original image.\n",
                "链接": "https://arxiv.org/abs/2202.06250"
            },
            {
                "文章ID": "24967",
                "标题": "Adversarial Privacy Protection on Speech Enhancement",
                "作者": " Mingyu Dong,  Diqun Yan,  Rangding Wang",
                "发布日期": "2022-06-17",
                "摘要": "  Speech is easily leaked imperceptibly, such as being recorded by mobile\nphones in different situations. Private content in speech may be maliciously\nextracted through speech enhancement technology. Speech enhancement technology\nhas developed rapidly along with deep neural networks (DNNs), but adversarial\nexamples can cause DNNs to fail. In this work, we propose an adversarial method\nto degrade speech enhancement systems. Experimental results show that generated\nadversarial examples can erase most content information in original examples or\nreplace it with target speech content through speech enhancement. The word\nerror rate (WER) between an enhanced original example and enhanced adversarial\nexample recognition result can reach 89.0%. WER of target attack between\nenhanced adversarial example and target example is low to 33.75% . Adversarial\nperturbation can bring the rate of change to the original example to more than\n1.4430. This work can prevent the malicious extraction of speech.\n",
                "链接": "https://arxiv.org/abs/2206.08170"
            },
            {
                "文章ID": "1657",
                "标题": "Tutela: An Open-Source Tool for Assessing User-Privacy on Ethereum and\n  Tornado Cash",
                "作者": " Mike Wu,  Will McTighe,  Kaili Wang,  Istvan A. Seres,  Nick Bax,  Manuel Puebla,  Mariano Mendez,  Federico Carrone,  Tomás De Mattey,  Herman O. Demaestri,  Mariano Nicolini,  Pedro Fontana",
                "发布日期": "2022-01-19",
                "摘要": "  A common misconception among blockchain users is that pseudonymity guarantees\nprivacy. The reality is almost the opposite. Every transaction one makes is\nrecorded on a public ledger and reveals information about one's identity.\nMixers, such as Tornado Cash, were developed to preserve privacy through\n\"mixing\" transactions with those of others in an anonymity pool, making it\nharder to link deposits and withdrawals from the pool. Unfortunately, it is\nstill possible to reveal information about those in the anonymity pool if users\nare not careful. We introduce Tutela, an application built on expert heuristics\nto report the true anonymity of an Ethereum address. In particular, Tutela has\nthree functionalities: first, it clusters together Ethereum addresses based on\ninteraction history such that for an Ethereum address, we can identify other\naddresses likely owned by the same entity; second, it shows Ethereum users\ntheir potentially compromised transactions; third, Tutela computes the true\nsize of the anonymity pool of each Tornado Cash mixer by excluding potentially\ncompromised transactions. A public implementation of Tutela can be found at\nhttps://github.com/TutelaLabs/tutela-app. To use Tutela, visit\nhttps://www.tutela.xyz.\n",
                "链接": "https://arxiv.org/abs/2201.06811"
            },
            {
                "文章ID": "71001",
                "标题": "Protecting User Privacy in Online Settings via Supervised Learning",
                "作者": " Alexandru Rusescu,  Brooke Lampe,  Weizhi Meng",
                "发布日期": "2023-04-07",
                "摘要": "  Companies that have an online presence-in particular, companies that are\nexclusively digital-often subscribe to this business model: collect data from\nthe user base, then expose the data to advertisement agencies in order to turn\na profit. Such companies routinely market a service as \"free\", while\nobfuscating the fact that they tend to \"charge\" users in the currency of\npersonal information rather than money. However, online companies also gather\nuser data for more principled purposes, such as improving the user experience\nand aggregating statistics. The problem is the sale of user data to third\nparties. In this work, we design an intelligent approach to online privacy\nprotection that leverages supervised learning. By detecting and blocking data\ncollection that might infringe on a user's privacy, we can restore a degree of\ndigital privacy to the user. In our evaluation, we collect a dataset of network\nrequests and measure the performance of several classifiers that adhere to the\nsupervised learning paradigm. The results of our evaluation demonstrate the\nfeasibility and potential of our approach.\n",
                "链接": "https://arxiv.org/abs/2304.02870"
            },
            {
                "文章ID": "108690",
                "标题": "User Inference Attacks on Large Language Models",
                "作者": " Nikhil Kandpal,  Krishna Pillutla,  Alina Oprea,  Peter Kairouz,  Christopher A. Choquette-Choo,  Zheng Xu",
                "发布日期": "2023-10-16",
                "摘要": "  Fine-tuning is a common and effective method for tailoring large language\nmodels (LLMs) to specialized tasks and applications. In this paper, we study\nthe privacy implications of fine-tuning LLMs on user data. To this end, we\ndefine a realistic threat model, called user inference, wherein an attacker\ninfers whether or not a user's data was used for fine-tuning. We implement\nattacks for this threat model that require only a small set of samples from a\nuser (possibly different from the samples used for training) and black-box\naccess to the fine-tuned LLM. We find that LLMs are susceptible to user\ninference attacks across a variety of fine-tuning datasets, at times with near\nperfect attack success rates. Further, we investigate which properties make\nusers vulnerable to user inference, finding that outlier users (i.e. those with\ndata distributions sufficiently different from other users) and users who\ncontribute large quantities of data are most susceptible to attack. Finally, we\nexplore several heuristics for mitigating privacy attacks. We find that\ninterventions in the training algorithm, such as batch or per-example gradient\nclipping and early stopping fail to prevent user inference. However, limiting\nthe number of fine-tuning samples from a single user can reduce attack\neffectiveness, albeit at the cost of reducing the total amount of fine-tuning\ndata.\n",
                "链接": "https://arxiv.org/abs/2310.09266"
            },
            {
                "文章ID": "111984",
                "标题": "$\\alpha$-Mutual Information: A Tunable Privacy Measure for Privacy\n  Protection in Data Sharing",
                "作者": " MirHamed Jafarzadeh Asl,  Mohammadhadi Shateri,  Fabrice Labeau",
                "发布日期": "2023-10-30",
                "摘要": "  This paper adopts Arimoto's $\\alpha$-Mutual Information as a tunable privacy\nmeasure, in a privacy-preserving data release setting that aims to prevent\ndisclosing private data to adversaries. By fine-tuning the privacy metric, we\ndemonstrate that our approach yields superior models that effectively thwart\nattackers across various performance dimensions. We formulate a general\ndistortion-based mechanism that manipulates the original data to offer privacy\nprotection. The distortion metrics are determined according to the data\nstructure of a specific experiment. We confront the problem expressed in the\nformulation by employing a general adversarial deep learning framework that\nconsists of a releaser and an adversary, trained with opposite goals. This\nstudy conducts empirical experiments on images and time-series data to verify\nthe functionality of $\\alpha$-Mutual Information. We evaluate the\nprivacy-utility trade-off of customized models and compare them to mutual\ninformation as the baseline measure. Finally, we analyze the consequence of an\nattacker's access to side information about private data and witness that\nadapting the privacy measure results in a more refined model than the\nstate-of-the-art in terms of resiliency against side information.\n",
                "链接": "https://arxiv.org/abs/2310.18241"
            },
            {
                "文章ID": "97796",
                "标题": "Federated Learning on Patient Data for Privacy-Protecting Polycystic\n  Ovary Syndrome Treatment",
                "作者": " Lucia Morris,  Tori Qiu,  Nikhil Raghuraman",
                "发布日期": "2023-08-23",
                "摘要": "  The field of women's endocrinology has trailed behind data-driven medical\nsolutions, largely due to concerns over the privacy of patient data. Valuable\ndatapoints about hormone levels or menstrual cycling could expose patients who\nsuffer from comorbidities or terminate a pregnancy, violating their privacy. We\nexplore the application of Federated Learning (FL) to predict the optimal drug\nfor patients with polycystic ovary syndrome (PCOS). PCOS is a serious hormonal\ndisorder impacting millions of women worldwide, yet it's poorly understood and\nits research is stunted by a lack of patient data. We demonstrate that a\nvariety of FL approaches succeed on a synthetic PCOS patient dataset. Our\nproposed FL models are a tool to access massive quantities of diverse data and\nidentify the most effective treatment option while providing PCOS patients with\nprivacy guarantees.\n",
                "链接": "https://arxiv.org/abs/2308.11220"
            },
            {
                "文章ID": "111764",
                "标题": "An Open Source Data Contamination Report for Large Language Models",
                "作者": " Yucheng Li",
                "发布日期": "2023-12-19",
                "摘要": "  Data contamination in language model evaluation is increasingly prevalent as\nthe popularity of large language models. It allows models to \"cheat\" via\nmemorisation instead of displaying true capabilities. Therefore, contamination\nanalysis has became an crucial part of reliable model evaluation to validate\nresults. However, existing contamination analysis is usually conducted\ninternally by LLM developers and often lacks transparency and completeness.\nThis paper present an open source data contamination reports for the Llama\nseries models. We analyse six popular multi-choice QA benchmarks and quantify\ntheir overlapping with the training set of Llama. Various levels of\ncontamination ranging from 1\\% to 8.7\\% are found across benchmarks. Our\ncomparison also reveals that Llama models can gain over 5\\% higher accuracy on\ncontaminated subsets versus clean subsets. Data and code are available at:\nhttps://github.com/liyucheng09/Contamination_Detector.\n",
                "链接": "https://arxiv.org/abs/2310.17589"
            }
        ]
    },
    {
        "question": {
            "question": "推荐与AutoGPT相似的10篇文献",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "86417",
                "标题": "AD-AutoGPT: An Autonomous GPT for Alzheimer's Disease Infodemiology",
                "作者": " Haixing Dai,  Yiwei Li,  Zhengliang Liu,  Lin Zhao,  Zihao Wu,  Suhang Song,  Ye Shen,  Dajiang Zhu,  Xiang Li,  Sheng Li,  Xiaobai Yao,  Lu Shi,  Quanzheng Li,  Zhuo Chen,  Donglan Zhang,  Gengchen Mai,  Tianming Liu",
                "发布日期": "2023-06-21",
                "摘要": "  In this pioneering study, inspired by AutoGPT, the state-of-the-art\nopen-source application based on the GPT-4 large language model, we develop a\nnovel tool called AD-AutoGPT which can conduct data collection, processing, and\nanalysis about complex health narratives of Alzheimer's Disease in an\nautonomous manner via users' textual prompts. We collated comprehensive data\nfrom a variety of news sources, including the Alzheimer's Association, BBC,\nMayo Clinic, and the National Institute on Aging since June 2022, leading to\nthe autonomous execution of robust trend analyses, intertopic distance maps\nvisualization, and identification of salient terms pertinent to Alzheimer's\nDisease. This approach has yielded not only a quantifiable metric of relevant\ndiscourse but also valuable insights into public focus on Alzheimer's Disease.\nThis application of AD-AutoGPT in public health signifies the transformative\npotential of AI in facilitating a data-rich understanding of complex health\nnarratives like Alzheimer's Disease in an autonomous manner, setting the\ngroundwork for future AI-driven investigations in global health landscapes.\n",
                "链接": "https://arxiv.org/abs/2306.10095"
            },
            {
                "文章ID": "123126",
                "标题": "Do Similar Entities have Similar Embeddings?",
                "作者": " Nicolas Hubert,  Heiko Paulheim,  Armelle Brun,  Davy Monticolo",
                "发布日期": "2023-12-19",
                "摘要": "  Knowledge graph embedding models (KGEMs) developed for link prediction learn\nvector representations for graph entities, known as embeddings. A common tacit\nassumption is the KGE entity similarity assumption, which states that these\nKGEMs retain the graph's structure within their embedding space, i.e., position\nsimilar entities close to one another. This desirable property make KGEMs\nwidely used in downstream tasks such as recommender systems or drug\nrepurposing. Yet, the alignment of graph similarity with embedding space\nsimilarity has rarely been formally evaluated. Typically, KGEMs are assessed\nbased on their sole link prediction capabilities, using ranked-based metrics\nsuch as Hits@K or Mean Rank. This paper challenges the prevailing assumption\nthat entity similarity in the graph is inherently mirrored in the embedding\nspace. Therefore, we conduct extensive experiments to measure the capability of\nKGEMs to cluster similar entities together, and investigate the nature of the\nunderlying factors. Moreover, we study if different KGEMs expose a different\nnotion of similarity. Datasets, pre-trained embeddings and code are available\nat: https://github.com/nicolas-hbt/similar-embeddings.\n",
                "链接": "https://arxiv.org/abs/2312.10370"
            },
            {
                "文章ID": "49001",
                "标题": "Learning to Counterfactually Explain Recommendations",
                "作者": " Yuanshun Yao,  Chong Wang,  Hang Li",
                "发布日期": "2023-02-10",
                "摘要": "  Recommender system practitioners are facing increasing pressure to explain\nrecommendations. We explore how to explain recommendations using counterfactual\nlogic, i.e. \"Had you not interacted with the following items, we would not\nrecommend it.\" Compared to the traditional explanation logic, counterfactual\nexplanations are easier to understand, more technically verifiable, and more\ninformative in terms of giving users control over recommendations. The major\nchallenge of generating such explanations is the computational cost because it\nrequires repeatedly retraining the models to obtain the effect on a\nrecommendation caused by the absence of user history. We propose a\nlearning-based framework to generate counterfactual explanations. The key idea\nis to train a surrogate model to learn the effect of removing a subset of user\nhistory on the recommendation. To this end, we first artificially simulate the\ncounterfactual outcomes on the recommendation after deleting subsets of\nhistory. Then we train a surrogate model to learn the mapping between a history\ndeletion and the corresponding change of the recommendation caused by the\ndeletion. Finally, to generate an explanation, we find the history subset\npredicted by the surrogate model that is most likely to remove the\nrecommendation. Through offline experiments and online user studies, we show\nour method, compared to baselines, can generate explanations that are more\ncounterfactually valid and more satisfactory considered by users.\n",
                "链接": "https://arxiv.org/abs/2211.09752"
            },
            {
                "文章ID": "100452",
                "标题": "Doppelgangers: Learning to Disambiguate Images of Similar Structures",
                "作者": " Ruojin Cai,  Joseph Tung,  Qianqian Wang,  Hadar Averbuch-Elor,  Bharath Hariharan,  Noah Snavely",
                "发布日期": "2023-09-06",
                "摘要": "  We consider the visual disambiguation task of determining whether a pair of\nvisually similar images depict the same or distinct 3D surfaces (e.g., the same\nor opposite sides of a symmetric building). Illusory image matches, where two\nimages observe distinct but visually similar 3D surfaces, can be challenging\nfor humans to differentiate, and can also lead 3D reconstruction algorithms to\nproduce erroneous results. We propose a learning-based approach to visual\ndisambiguation, formulating it as a binary classification task on image pairs.\nTo that end, we introduce a new dataset for this problem, Doppelgangers, which\nincludes image pairs of similar structures with ground truth labels. We also\ndesign a network architecture that takes the spatial distribution of local\nkeypoints and matches as input, allowing for better reasoning about both local\nand global cues. Our evaluation shows that our method can distinguish illusory\nmatches in difficult cases, and can be integrated into SfM pipelines to produce\ncorrect, disambiguated 3D reconstructions. See our project page for our code,\ndatasets, and more results: http://doppelgangers-3d.github.io/.\n",
                "链接": "https://arxiv.org/abs/2309.02420"
            },
            {
                "文章ID": "11104",
                "标题": "CNNs and Transformers Perceive Hybrid Images Similar to Humans",
                "作者": " Ali Borji",
                "发布日期": "2022-03-23",
                "摘要": "  Hybrid images is a technique to generate images with two interpretations that\nchange as a function of viewing distance. It has been utilized to study\nmultiscale processing of images by the human visual system. Using 63,000 hybrid\nimages across 10 fruit categories, here we show that predictions of deep\nlearning vision models qualitatively matches with the human perception of these\nimages. Our results provide yet another evidence in support of the hypothesis\nthat Convolutional Neural Networks (CNNs) and Transformers are good at modeling\nthe feedforward sweep of information in the ventral stream of visual cortex.\nCode and data is available at https://github.com/aliborji/hybrid_images.git.\n",
                "链接": "https://arxiv.org/abs/2203.11678"
            },
            {
                "文章ID": "41506",
                "标题": "Visually Similar Products Retrieval for Shopsy",
                "作者": " Prajit Nadkarni,  Narendra Varma Dasararaju",
                "发布日期": "2022-10-11",
                "摘要": "  Visual search is of great assistance in reseller commerce, especially for\nnon-tech savvy users with affinity towards regional languages. It allows\nresellers to accurately locate the products that they seek, unlike textual\nsearch which recommends products from head brands. Product attributes available\nin e-commerce have a great potential for building better visual search systems\nas they capture fine grained relations between data points. In this work, we\ndesign a visual search system for reseller commerce using a multi-task learning\napproach. We also highlight and address the challenges like image compression,\ncropping, scribbling on the image, etc, faced in reseller commerce. Our model\nconsists of three different tasks: attribute classification, triplet ranking\nand variational autoencoder (VAE). Masking technique is used for designing the\nattribute classification. Next, we introduce an offline triplet mining\ntechnique which utilizes information from multiple attributes to capture\nrelative order within the data. This technique displays a better performance\ncompared to the traditional triplet mining baseline, which uses single\nlabel/attribute information. We also compare and report incremental gain\nachieved by our unified multi-task model over each individual task separately.\nThe effectiveness of our method is demonstrated using the in-house dataset of\nproduct images from the Lifestyle business-unit of Flipkart, India's largest\ne-commerce company. To efficiently retrieve the images in production, we use\nthe Approximate Nearest Neighbor (ANN) index. Finally, we highlight our\nproduction environment constraints and present the design choices and\nexperiments conducted to select a suitable ANN index.\n",
                "链接": "https://arxiv.org/abs/2210.04560"
            },
            {
                "文章ID": "119654",
                "标题": "A Turing Test: Are AI Chatbots Behaviorally Similar to Humans?",
                "作者": " Qiaozhu Mei,  Yutong Xie,  Walter Yuan,  Matthew O. Jackson",
                "发布日期": "2023-12-05",
                "摘要": "  We administer a Turing Test to AI Chatbots. We examine how Chatbots behave in\na suite of classic behavioral games that are designed to elicit characteristics\nsuch as trust, fairness, risk-aversion, cooperation, \\textit{etc.}; as well as\na traditional Big-5 psychological survey that measures personality traits.\nChatGPT-4 passes the Turing Test in that it consistently exhibits human-like\nbehavioral and personality traits based on a comparison to the behavior of\nhundreds of thousands of humans from more than 50 countries. Chatbots also\nmodify their behavior based on previous experience and contexts ``as if'' they\nwere learning from the interactions, and change their behavior in response to\ndifferent framings of the same strategic situation. Their behaviors are often\ndistinct from average and modal human behaviors, in which case they tend to\nbehave on the more altruistic and cooperative end of the distribution. We\nestimate that they act as if they are maximizing an average of their own and\npartner's payoff.\n",
                "链接": "https://arxiv.org/abs/2312.00798"
            },
            {
                "文章ID": "3640",
                "标题": "Rewiring What-to-Watch-Next Recommendations to Reduce Radicalization\n  Pathways",
                "作者": " Francesco Fabbri,  Yanhao Wang,  Francesco Bonchi,  Carlos Castillo,  Michael Mathioudakis",
                "发布日期": "2023-04-27",
                "摘要": "  Recommender systems typically suggest to users content similar to what they\nconsumed in the past. If a user happens to be exposed to strongly polarized\ncontent, she might subsequently receive recommendations which may steer her\ntowards more and more radicalized content, eventually being trapped in what we\ncall a \"radicalization pathway\". In this paper, we study the problem of\nmitigating radicalization pathways using a graph-based approach. Specifically,\nwe model the set of recommendations of a \"what-to-watch-next\" recommender as a\nd-regular directed graph where nodes correspond to content items, links to\nrecommendations, and paths to possible user sessions. We measure the\n\"segregation\" score of a node representing radicalized content as the expected\nlength of a random walk from that node to any node representing non-radicalized\ncontent. High segregation scores are associated to larger chances to get users\ntrapped in radicalization pathways. Hence, we define the problem of reducing\nthe prevalence of radicalization pathways by selecting a small number of edges\nto \"rewire\", so to minimize the maximum of segregation scores among all\nradicalized nodes, while maintaining the relevance of the recommendations. We\nprove that the problem of finding the optimal set of recommendations to rewire\nis NP-hard and NP-hard to approximate within any factor. Therefore, we turn our\nattention to heuristics, and propose an efficient yet effective greedy\nalgorithm based on the absorbing random walk theory. Our experiments on\nreal-world datasets in the context of video and news recommendations confirm\nthe effectiveness of our proposal.\n",
                "链接": "https://arxiv.org/abs/2202.00640"
            },
            {
                "文章ID": "88929",
                "标题": "Lottery and Sprint: Generate a Board Game with Design Sprint Method on\n  AutoGPT",
                "作者": " Maya Grace Torii,  Takahito Murakami,  Yoichi Ochiai",
                "发布日期": "2023-12-25",
                "摘要": "  In this paper, we present a novel approach using the Auto GPT system\nalongside Design Sprint methodology to facilitate board game creation for\ninexperienced users. We introduce the implementation of Auto GPT for generating\ndiverse board games and the subsequent optimization process through a\ncustomized Design Sprint. A user study is conducted to investigate the\nplayability and enjoyment of the generated games, revealing both successes and\nchallenges in employing systems like Auto GPT for board game design. Insights\nand future research directions are proposed to overcome identified limitations\nand enhance computational-driven game creation.\n",
                "链接": "https://arxiv.org/abs/2307.00348"
            },
            {
                "文章ID": "2781",
                "标题": "Rapid solution for searching similar audio items",
                "作者": " Kastriot Kadriu",
                "发布日期": "2022-01-28",
                "摘要": "  A naive approach for finding similar audio items would be to compare each\nentry from the feature vector of the test example with each feature vector of\nthe candidates in a k-nearest neighbors fashion. There are already two problems\nwith this approach: audio signals are represented by high dimensional vectors\nand the number of candidates can be very large - think thousands. The search\nprocess would have a high complexity. Our paper will treat this problem through\nhashing methodologies more specifically the Locality Sensitive Hashing. This\nproject will be in the spirit of classification and clustering problems. The\ncomputer sound production principles will be used to determine which features\nthat describe an audio signal are the most useful. That will down-sample the\nsize of the feature vectors and speed up the process subsequently.\n",
                "链接": "https://arxiv.org/abs/2201.11178"
            }
        ]
    },
    {
        "question": {
            "question": "请搜索近一年发表的有关多模态大模型与产业相结合、与具体应用场景相结合的论文",
            "type": "2"
        },
        "results": [
            {
                "文章ID": "90924",
                "标题": "Digital Health Discussion Through Articles Published Until the Year\n  2021: A Digital Topic Modeling Approach",
                "作者": " Junhyoun Sung,  Hyungsook Kim",
                "发布日期": "2023-09-20",
                "摘要": "  The digital health industry has grown in popularity since the 2010s, but\nthere has been limited analysis of the topics discussed in the field across\nacademic disciplines. This study aims to analyze the research trends of digital\nhealth-related articles published on the Web of Science until 2021, in order to\nunderstand the concentration, scope, and characteristics of the research.\n15,950 digital health-related papers from the top 10 academic fields were\nanalyzed using the Web of Science. The papers were grouped into three domains:\npublic health, medicine, and electrical engineering and computer science\n(EECS). Two time periods (2012-2016 and 2017-2021) were compared using Latent\nDirichlet Allocation (LDA) for topic modeling. The number of topics was\ndetermined based on coherence score, and topic compositions were compared using\na homogeneity test. The number of optimal topics varied across domains and time\nperiods. For public health, the first and second halves had 13 and 19 topics,\nrespectively. Medicine had 14 and 25 topics, and EECS had 7 and 21 topics. Text\nanalysis revealed shared topics among the domains, but with variations in\ncomposition. The homogeneity test confirmed significant differences between the\ngroups (adjusted p-value<0.05). Six dominant themes emerged, including journal\narticle methodology, information technology, medical issues, population\ndemographics, social phenomena, and healthcare. Digital health research is\nexpanding and evolving, particularly in relation to Covid-19, where topics such\nas depression and mental disorders, education, and physical activity have\ngained prominence. There was no bias in topic composition among the three\ndomains, but other fields like kinesiology or psychology could contribute to\nfuture digital health research. Exploring expanded topics that reflect people's\nneeds for digital health over time will be crucial.\n",
                "链接": "https://arxiv.org/abs/2307.07130"
            },
            {
                "文章ID": "97795",
                "标题": "Federated Learning in Big Model Era: Domain-Specific Multimodal Large\n  Models",
                "作者": " Zengxiang Li,  Zhaoxiang Hou,  Hui Liu,  Ying Wang,  Tongzhi Li,  Longfei Xie,  Chao Shi,  Chengyi Yang,  Weishan Zhang,  Zelei Liu,  Liang Xu",
                "发布日期": "2023-08-25",
                "摘要": "  Multimodal data, which can comprehensively perceive and recognize the\nphysical world, has become an essential path towards general artificial\nintelligence. However, multimodal large models trained on public datasets often\nunderperform in specific industrial domains. This paper proposes a multimodal\nfederated learning framework that enables multiple enterprises to utilize\nprivate domain data to collaboratively train large models for vertical domains,\nachieving intelligent services across scenarios. The authors discuss in-depth\nthe strategic transformation of federated learning in terms of intelligence\nfoundation and objectives in the era of big model, as well as the new\nchallenges faced in heterogeneous data, model aggregation, performance and cost\ntrade-off, data privacy, and incentive mechanism. The paper elaborates a case\nstudy of leading enterprises contributing multimodal data and expert knowledge\nto city safety operation management , including distributed deployment and\nefficient coordination of the federated learning platform, technical\ninnovations on data quality improvement based on large model capabilities and\nefficient joint fine-tuning approaches. Preliminary experiments show that\nenterprises can enhance and accumulate intelligent capabilities through\nmultimodal model federated learning, thereby jointly creating an smart city\nmodel that provides high-quality intelligent services covering energy\ninfrastructure safety, residential community security, and urban operation\nmanagement. The established federated learning cooperation ecosystem is\nexpected to further aggregate industry, academia, and research resources,\nrealize large models in multiple vertical domains, and promote the large-scale\nindustrial application of artificial intelligence and cutting-edge research on\nmultimodal federated learning.\n",
                "链接": "https://arxiv.org/abs/2308.11217"
            },
            {
                "文章ID": "95924",
                "标题": "Large Language Models for Telecom: Forthcoming Impact on the Industry",
                "作者": " Ali Maatouk,  Nicola Piovesan,  Fadhel Ayed,  Antonio De Domenico,  Merouane Debbah",
                "发布日期": "2023-08-14",
                "摘要": "  Large Language Models (LLMs) have emerged as a transformative force,\nrevolutionizing numerous fields well beyond the conventional domain of Natural\nLanguage Processing (NLP) and garnering unprecedented attention. As LLM\ntechnology continues to progress, the telecom industry is facing the prospect\nof its potential impact on its landscape. To elucidate these implications, we\ndelve into the inner workings of LLMs, providing insights into their current\ncapabilities and limitations. We also examine the use cases that can be readily\nimplemented in the telecom industry, streamlining numerous tasks that currently\nhinder operational efficiency and demand significant manpower and engineering\nexpertise. Furthermore, we uncover essential research directions that deal with\nthe distinctive challenges of utilizing the LLMs within the telecom domain.\nAddressing these challenges represents a significant stride towards fully\nharnessing the potential of LLMs and unlocking their capabilities to the\nfullest extent within the telecom domain.\n",
                "链接": "https://arxiv.org/abs/2308.06013"
            },
            {
                "文章ID": "102877",
                "标题": "Investigating the Catastrophic Forgetting in Multimodal Large Language\n  Models",
                "作者": " Yuexiang Zhai,  Shengbang Tong,  Xiao Li,  Mu Cai,  Qing Qu,  Yong Jae Lee,  Yi Ma",
                "发布日期": "2023-12-06",
                "摘要": "  Following the success of GPT4, there has been a surge in interest in\nmultimodal large language model (MLLM) research. This line of research focuses\non developing general-purpose LLMs through fine-tuning pre-trained LLMs and\nvision models. However, catastrophic forgetting, a notorious phenomenon where\nthe fine-tuned model fails to retain similar performance compared to the\npre-trained model, still remains an inherent problem in multimodal LLMs (MLLM).\nIn this paper, we introduce EMT: Evaluating MulTimodality for evaluating the\ncatastrophic forgetting in MLLMs, by treating each MLLM as an image classifier.\nWe first apply EMT to evaluate several open-source fine-tuned MLLMs and we\ndiscover that almost all evaluated MLLMs fail to retain the same performance\nlevels as their vision encoders on standard image classification tasks.\nMoreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess\nperformance throughout the fine-tuning. Interestingly, our results suggest that\nearly-stage fine-tuning on an image dataset improves performance across other\nimage datasets, by enhancing the alignment of text and visual features.\nHowever, as fine-tuning proceeds, the MLLMs begin to hallucinate, resulting in\na significant loss of generalizability, even when the image encoder remains\nfrozen. Our results suggest that MLLMs have yet to demonstrate performance on\npar with their vision models on standard image classification tasks and the\ncurrent MLLM fine-tuning procedure still has room for improvement.\n",
                "链接": "https://arxiv.org/abs/2309.10313"
            },
            {
                "文章ID": "105595",
                "标题": "Application of frozen large-scale models to multimodal task-oriented\n  dialogue",
                "作者": " Tatsuki Kawamoto,  Takuma Suzuki,  Ko Miyama,  Takumi Meguro,  Tomohiro Takagi",
                "发布日期": "2023-10-03",
                "摘要": "  In this study, we use the existing Large Language Models ENnhanced to See\nFramework (LENS Framework) to test the feasibility of multimodal task-oriented\ndialogues. The LENS Framework has been proposed as a method to solve computer\nvision tasks without additional training and with fixed parameters of\npre-trained models. We used the Multimodal Dialogs (MMD) dataset, a multimodal\ntask-oriented dialogue benchmark dataset from the fashion field, and for the\nevaluation, we used the ChatGPT-based G-EVAL, which only accepts textual\nmodalities, with arrangements to handle multimodal data. Compared to\nTransformer-based models in previous studies, our method demonstrated an\nabsolute lift of 10.8% in fluency, 8.8% in usefulness, and 5.2% in relevance\nand coherence. The results show that using large-scale models with fixed\nparameters rather than using models trained on a dataset from scratch improves\nperformance in multimodal task-oriented dialogues. At the same time, we show\nthat Large Language Models (LLMs) are effective for multimodal task-oriented\ndialogues. This is expected to lead to efficient applications to existing\nsystems.\n",
                "链接": "https://arxiv.org/abs/2310.00845"
            },
            {
                "文章ID": "100666",
                "标题": "Gender-specific Machine Translation with Large Language Models",
                "作者": " Eduardo Sánchez,  Pierre Andrews,  Pontus Stenetorp,  Mikel Artetxe,  Marta R. Costa-jussà",
                "发布日期": "2023-09-07",
                "摘要": "  Decoder-only Large Language Models (LLMs) have demonstrated potential in\nmachine translation (MT), albeit with performance slightly lagging behind\ntraditional encoder-decoder Neural Machine Translation (NMT) systems. However,\nLLMs offer a unique advantage: the ability to control the properties of the\noutput through prompts. In this study, we harness this flexibility to explore\nLLaMa's capability to produce gender-specific translations for languages with\ngrammatical gender. Our results indicate that LLaMa can generate\ngender-specific translations with competitive accuracy and gender bias\nmitigation when compared to NLLB, a state-of-the-art multilingual NMT system.\nFurthermore, our experiments reveal that LLaMa's translations are robust,\nshowing significant performance drops when evaluated against opposite-gender\nreferences in gender-ambiguous datasets but maintaining consistency in less\nambiguous contexts. This research provides insights into the potential and\nchallenges of using LLMs for gender-specific translations and highlights the\nimportance of in-context learning to elicit new tasks in LLMs.\n",
                "链接": "https://arxiv.org/abs/2309.03175"
            },
            {
                "文章ID": "108101",
                "标题": "The Past, Present and Better Future of Feedback Learning in Large\n  Language Models for Subjective Human Preferences and Values",
                "作者": " Hannah Rose Kirk,  Andrew M. Bean,  Bertie Vidgen,  Paul Röttger,  Scott A. Hale",
                "发布日期": "2023-10-12",
                "摘要": "  Human feedback is increasingly used to steer the behaviours of Large Language\nModels (LLMs). However, it is unclear how to collect and incorporate feedback\nin a way that is efficient, effective and unbiased, especially for highly\nsubjective human preferences and values. In this paper, we survey existing\napproaches for learning from human feedback, drawing on 95 papers primarily\nfrom the ACL and arXiv repositories.First, we summarise the past, pre-LLM\ntrends for integrating human feedback into language models. Second, we give an\noverview of present techniques and practices, as well as the motivations for\nusing feedback; conceptual frameworks for defining values and preferences; and\nhow feedback is collected and from whom. Finally, we encourage a better future\nof feedback learning in LLMs by raising five unresolved conceptual and\npractical challenges.\n",
                "链接": "https://arxiv.org/abs/2310.07629"
            },
            {
                "文章ID": "77352",
                "标题": "On the Hidden Mystery of OCR in Large Multimodal Models",
                "作者": " Yuliang Liu,  Zhang Li,  Hongliang Li,  Wenwen Yu,  Yang Liu,  Biao Yang,  Mingxin Huang,  Dezhi Peng,  Mingyu Liu,  Mingrui Chen,  Chunyuan Li,  Xucheng Yin,  Cheng-lin Liu,  Lianwen Jin,  Xiang Bai",
                "发布日期": "2023-06-21",
                "摘要": "  Large models have recently played a dominant role in natural language\nprocessing and multimodal vision-language learning. It remains less explored\nabout their efficacy in text-related visual tasks. We conducted a comprehensive\nstudy of existing publicly available multimodal models, evaluating their\nperformance in text recognition (document text, artistic text, handwritten\ntext, scene text), text-based visual question answering (document text, scene\ntext, and bilingual text), key information extraction (receipts, documents, and\nnutrition facts) and handwritten mathematical expression recognition. Our\nfindings reveal strengths and weaknesses in these models, which primarily rely\non semantic understanding for word recognition and exhibit inferior perception\nof individual character shapes. They also display indifference towards text\nlength and have limited capabilities in detecting finegrained features in\nimages. Consequently, these results demonstrate that even the current most\npowerful large multimodal models cannot match domain-specific methods in\ntraditional text tasks and face greater challenges in more complex tasks. Most\nimportantly, the baseline results showcased in this study could provide a\nfoundational framework for the conception and assessment of innovative\nstrategies targeted at enhancing zero-shot multimodal techniques. Evaluation\npipeline is available at https://github.com/Yuliang-Liu/MultimodalOCR.\n",
                "链接": "https://arxiv.org/abs/2305.07895"
            },
            {
                "文章ID": "105512",
                "标题": "Beyond Task Performance: Evaluating and Reducing the Flaws of Large\n  Multimodal Models with In-Context Learning",
                "作者": " Mustafa Shukor,  Alexandre Rame,  Corentin Dancette,  Matthieu Cord",
                "发布日期": "2023-10-03",
                "摘要": "  Following the success of Large Language Models (LLMs), Large Multimodal\nModels (LMMs), such as the Flamingo model and its subsequent competitors, have\nstarted to emerge as natural steps towards generalist agents. However,\ninteracting with recent LMMs reveals major limitations that are hardly captured\nby the current evaluation benchmarks. Indeed, task performances (e.g., VQA\naccuracy) alone do not provide enough clues to understand their real\ncapabilities, limitations, and to which extent such models are aligned to human\nexpectations. To refine our understanding of those flaws, we deviate from the\ncurrent evaluation paradigm and propose the EvALign-ICL framework, in which we\n(1) evaluate 8 recent open-source LMMs (based on the Flamingo architecture such\nas OpenFlamingo and IDEFICS) on 5 different axes; hallucinations, abstention,\ncompositionality, explainability and instruction following. Our evaluation on\nthese axes reveals major flaws in LMMs. To efficiently address these problems,\nand inspired by the success of in-context learning (ICL) in LLMs, (2) we\nexplore ICL as a solution and study how it affects these limitations. Based on\nour ICL study, (3) we push ICL further and propose new multimodal ICL\napproaches such as; Multitask-ICL, Chain-of-Hindsight-ICL, and\nSelf-Correcting-ICL. Our findings are as follows; (1) Despite their success,\nLMMs have flaws that remain unsolved with scaling alone. (2) The effect of ICL\non LMMs flaws is nuanced; despite its effectiveness for improved\nexplainability, abstention, and instruction following, ICL does not improve\ncompositional abilities, and actually even amplifies hallucinations. (3) The\nproposed ICL variants are promising as post-hoc approaches to efficiently\ntackle some of those flaws. The code is available here:\nhttps://evalign-icl.github.io/\n",
                "链接": "https://arxiv.org/abs/2310.00647"
            },
            {
                "文章ID": "71871",
                "标题": "The MONET dataset: Multimodal drone thermal dataset recorded in rural\n  scenarios",
                "作者": " Luigi Riz,  Andrea Caraffa,  Matteo Bortolon,  Mohamed Lamine Mekhalfi,  Davide Boscaini,  André Moura,  José Antunes,  André Dias,  Hugo Silva,  Andreas Leonidou,  Christos Constantinides,  Christos Keleshis,  Dante Abate,  Fabio Poiesi",
                "发布日期": "2023-07-20",
                "摘要": "  We present MONET, a new multimodal dataset captured using a thermal camera\nmounted on a drone that flew over rural areas, and recorded human and vehicle\nactivities. We captured MONET to study the problem of object localisation and\nbehaviour understanding of targets undergoing large-scale variations and being\nrecorded from different and moving viewpoints. Target activities occur in two\ndifferent land sites, each with unique scene structures and cluttered\nbackgrounds. MONET consists of approximately 53K images featuring 162K manually\nannotated bounding boxes. Each image is timestamp-aligned with drone metadata\nthat includes information about attitudes, speed, altitude, and GPS\ncoordinates. MONET is different from previous thermal drone datasets because it\nfeatures multimodal data, including rural scenes captured with thermal cameras\ncontaining both person and vehicle targets, along with trajectory information\nand metadata. We assessed the difficulty of the dataset in terms of transfer\nlearning between the two sites and evaluated nine object detection algorithms\nto identify the open challenges associated with this type of data. Project\npage: https://github.com/fabiopoiesi/monet_dataset.\n",
                "链接": "https://arxiv.org/abs/2304.05417"
            }
        ]
    },
    {
        "question": {
            "question": "请找出最近一年内发表的关于自然语言处理领域中，使用Transformer模型并在大规模数据集上进行预训练的论文。特别关注模型结构和性能指标。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "85409",
                "标题": "EriBERTa: A Bilingual Pre-Trained Language Model for Clinical Natural\n  Language Processing",
                "作者": " Iker de la Iglesia,  Aitziber Atutxa,  Koldo Gojenola,  Ander Barrena",
                "发布日期": "2023-06-14",
                "摘要": "  The utilization of clinical reports for various secondary purposes, including\nhealth research and treatment monitoring, is crucial for enhancing patient\ncare. Natural Language Processing (NLP) tools have emerged as valuable assets\nfor extracting and processing relevant information from these reports. However,\nthe availability of specialized language models for the clinical domain in\nSpanish has been limited.\n  In this paper, we introduce EriBERTa, a bilingual domain-specific language\nmodel pre-trained on extensive medical and clinical corpora. We demonstrate\nthat EriBERTa outperforms previous Spanish language models in the clinical\ndomain, showcasing its superior capabilities in understanding medical texts and\nextracting meaningful information. Moreover, EriBERTa exhibits promising\ntransfer learning abilities, allowing for knowledge transfer from one language\nto another. This aspect is particularly beneficial given the scarcity of\nSpanish clinical data.\n",
                "链接": "https://arxiv.org/abs/2306.07373"
            },
            {
                "文章ID": "109322",
                "标题": "Enhanced Transformer Architecture for Natural Language Processing",
                "作者": " Woohyeon Moon,  Taeyoung Kim,  Bumgeun Park,  Dongsoo Har",
                "发布日期": "2023-10-18",
                "摘要": "  Transformer is a state-of-the-art model in the field of natural language\nprocessing (NLP). Current NLP models primarily increase the number of\ntransformers to improve processing performance. However, this technique\nrequires a lot of training resources such as computing capacity. In this paper,\na novel structure of Transformer is proposed. It is featured by full layer\nnormalization, weighted residual connection, positional encoding exploiting\nreinforcement learning, and zero masked self-attention. The proposed\nTransformer model, which is called Enhanced Transformer, is validated by the\nbilingual evaluation understudy (BLEU) score obtained with the Multi30k\ntranslation dataset. As a result, the Enhanced Transformer achieves 202.96%\nhigher BLEU score as compared to the original transformer with the translation\ndataset.\n",
                "链接": "https://arxiv.org/abs/2310.10930"
            },
            {
                "文章ID": "109416",
                "标题": "ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text\n  Processing",
                "作者": " Quoc-Nam Nguyen,  Thang Chau Phan,  Duc-Vu Nguyen,  Kiet Van Nguyen",
                "发布日期": "2023-10-31",
                "摘要": "  English and Chinese, known as resource-rich languages, have witnessed the\nstrong development of transformer-based language models for natural language\nprocessing tasks. Although Vietnam has approximately 100M people speaking\nVietnamese, several pre-trained models, e.g., PhoBERT, ViBERT, and vELECTRA,\nperformed well on general Vietnamese NLP tasks, including POS tagging and named\nentity recognition. These pre-trained language models are still limited to\nVietnamese social media tasks. In this paper, we present the first monolingual\npre-trained language model for Vietnamese social media texts, ViSoBERT, which\nis pre-trained on a large-scale corpus of high-quality and diverse Vietnamese\nsocial media texts using XLM-R architecture. Moreover, we explored our\npre-trained model on five important natural language downstream tasks on\nVietnamese social media texts: emotion recognition, hate speech detection,\nsentiment analysis, spam reviews detection, and hate speech spans detection.\nOur experiments demonstrate that ViSoBERT, with far fewer parameters, surpasses\nthe previous state-of-the-art models on multiple Vietnamese social media tasks.\nOur ViSoBERT model is available only for research purposes.\n",
                "链接": "https://arxiv.org/abs/2310.11166"
            },
            {
                "文章ID": "93014",
                "标题": "Evaluating Large Language Models for Radiology Natural Language\n  Processing",
                "作者": " Zhengliang Liu,  Tianyang Zhong,  Yiwei Li,  Yutong Zhang,  Yi Pan,  Zihao Zhao,  Peixin Dong,  Chao Cao,  Yuxiao Liu,  Peng Shu,  Yaonai Wei,  Zihao Wu,  Chong Ma,  Jiaqi Wang,  Sheng Wang,  Mengyue Zhou,  Zuowei Jiang,  Chunlin Li,  Jason Holmes,  Shaochen Xu,  Lu Zhang,  Haixing Dai,  Kai Zhang,  Lin Zhao,  Yuanhao Chen,  Xu Liu,  Peilong Wang,  Pingkun Yan,  Jun Liu,  Bao Ge,  Lichao Sun,  Dajiang Zhu,  Xiang Li,  Wei Liu,  Xiaoyan Cai,  Xintao Hu,  Xi Jiang,  Shu Zhang,  Xin Zhang,  Tuo Zhang,  Shijie Zhao,  Quanzheng Li,  Hongtu Zhu,  Dinggang Shen,  Tianming Liu",
                "发布日期": "2023-07-28",
                "摘要": "  The rise of large language models (LLMs) has marked a pivotal shift in the\nfield of natural language processing (NLP). LLMs have revolutionized a\nmultitude of domains, and they have made a significant impact in the medical\nfield. Large language models are now more abundant than ever, and many of these\nmodels exhibit bilingual capabilities, proficient in both English and Chinese.\nHowever, a comprehensive evaluation of these models remains to be conducted.\nThis lack of assessment is especially apparent within the context of radiology\nNLP. This study seeks to bridge this gap by critically evaluating thirty two\nLLMs in interpreting radiology reports, a crucial component of radiology NLP.\nSpecifically, the ability to derive impressions from radiologic findings is\nassessed. The outcomes of this evaluation provide key insights into the\nperformance, strengths, and weaknesses of these LLMs, informing their practical\napplications within the medical domain.\n",
                "链接": "https://arxiv.org/abs/2307.13693"
            },
            {
                "文章ID": "123681",
                "标题": "Sparse is Enough in Fine-tuning Pre-trained Large Language Model",
                "作者": " Weixi Song,  Zuchao Li,  Lefei Zhang,  Hai Zhao,  Bo Du",
                "发布日期": "2023-12-20",
                "摘要": "  With the prevalence of pre-training-fine-tuning paradigm, how to efficiently\nadapt the pre-trained model to the downstream tasks has been an intriguing\nissue. Parameter-Efficient Fine-Tuning (PEFT) methods have been proposed for\nlow-cost adaptation, including Adapters, Bia-only, and the recently widely used\nLow-Rank Adaptation. Although these methods have demonstrated their\neffectiveness to some extent and have been widely applied, the underlying\nprinciples are still unclear. In this paper, we reveal the transition of loss\nlandscape in the downstream domain from random initialization to pre-trained\ninitialization, that is, from low-amplitude oscillation to high-amplitude\noscillation. The parameter gradients exhibit a property akin to sparsity, where\na small fraction of components dominate the total gradient norm, for instance,\n1% of the components account for 99% of the gradient. This property ensures\nthat the pre-trained model can easily find a flat minimizer which guarantees\nthe model's ability to generalize even with a low number of trainable\nparameters. Based on this, we propose a gradient-based sparse fine-tuning\nalgorithm, named Sparse Increment Fine-Tuning (SIFT), and validate its\neffectiveness on a range of tasks including the GLUE Benchmark and\nInstruction-tuning. The code is accessible at https://github.com/song-wx/SIFT/.\n",
                "链接": "https://arxiv.org/abs/2312.11875"
            },
            {
                "文章ID": "102542",
                "标题": "Performance of the Pre-Trained Large Language Model GPT-4 on Automated\n  Short Answer Grading",
                "作者": " Gerd Kortemeyer",
                "发布日期": "2023-09-19",
                "摘要": "  Automated Short Answer Grading (ASAG) has been an active area of\nmachine-learning research for over a decade. It promises to let educators grade\nand give feedback on free-form responses in large-enrollment courses in spite\nof limited availability of human graders. Over the years, carefully trained\nmodels have achieved increasingly higher levels of performance. More recently,\npre-trained Large Language Models (LLMs) emerged as a commodity, and an\nintriguing question is how a general-purpose tool without additional training\ncompares to specialized models. We studied the performance of GPT-4 on the\nstandard benchmark 2-way and 3-way datasets SciEntsBank and Beetle, where in\naddition to the standard task of grading the alignment of the student answer\nwith a reference answer, we also investigated withholding the reference answer.\nWe found that overall, the performance of the pre-trained general-purpose GPT-4\nLLM is comparable to hand-engineered models, but worse than pre-trained LLMs\nthat had specialized training.\n",
                "链接": "https://arxiv.org/abs/2309.09338"
            },
            {
                "文章ID": "95493",
                "标题": "Efficient Bayesian Optimization with Deep Kernel Learning and\n  Transformer Pre-trained on Multiple Heterogeneous Datasets",
                "作者": " Wenlong Lyu,  Shoubo Hu,  Jie Chuai,  Zhitang Chen",
                "发布日期": "2023-08-10",
                "摘要": "  Bayesian optimization (BO) is widely adopted in black-box optimization\nproblems and it relies on a surrogate model to approximate the black-box\nresponse function. With the increasing number of black-box optimization tasks\nsolved and even more to solve, the ability to learn from multiple prior tasks\nto jointly pre-train a surrogate model is long-awaited to further boost\noptimization efficiency. In this paper, we propose a simple approach to\npre-train a surrogate, which is a Gaussian process (GP) with a kernel defined\non deep features learned from a Transformer-based encoder, using datasets from\nprior tasks with possibly heterogeneous input spaces. In addition, we provide a\nsimple yet effective mix-up initialization strategy for input tokens\ncorresponding to unseen input variables and therefore accelerate new tasks'\nconvergence. Experiments on both synthetic and real benchmark problems\ndemonstrate the effectiveness of our proposed pre-training and transfer BO\nstrategy over existing methods.\n",
                "链接": "https://arxiv.org/abs/2308.04660"
            },
            {
                "文章ID": "85449",
                "标题": "PauseSpeech: Natural Speech Synthesis via Pre-trained Language Model and\n  Pause-based Prosody Modeling",
                "作者": " Ji-Sang Hwang,  Sang-Hoon Lee,  Seong-Whan Lee",
                "发布日期": "2023-06-14",
                "摘要": "  Although text-to-speech (TTS) systems have significantly improved, most TTS\nsystems still have limitations in synthesizing speech with appropriate\nphrasing. For natural speech synthesis, it is important to synthesize the\nspeech with a phrasing structure that groups words into phrases based on\nsemantic information. In this paper, we propose PuaseSpeech, a speech synthesis\nsystem with a pre-trained language model and pause-based prosody modeling.\nFirst, we introduce a phrasing structure encoder that utilizes a context\nrepresentation from the pre-trained language model. In the phrasing structure\nencoder, we extract a speaker-dependent syntactic representation from the\ncontext representation and then predict a pause sequence that separates the\ninput text into phrases. Furthermore, we introduce a pause-based word encoder\nto model word-level prosody based on pause sequence. Experimental results show\nPauseSpeech outperforms previous models in terms of naturalness. Furthermore,\nin terms of objective evaluations, we can observe that our proposed methods\nhelp the model decrease the distance between ground-truth and synthesized\nspeech. Audio samples are available at\nhttps://jisang93.github.io/pausespeech-demo/.\n",
                "链接": "https://arxiv.org/abs/2306.07489"
            },
            {
                "文章ID": "76320",
                "标题": "Putting Natural in Natural Language Processing",
                "作者": " Grzegorz Chrupała",
                "发布日期": "2023-05-24",
                "摘要": "  Human language is firstly spoken and only secondarily written. Text, however,\nis a very convenient and efficient representation of language, and modern\ncivilization has made it ubiquitous. Thus the field of NLP has overwhelmingly\nfocused on processing written rather than spoken language. Work on spoken\nlanguage, on the other hand, has been siloed off within the largely separate\nspeech processing community which has been inordinately preoccupied with\ntranscribing speech into text. Recent advances in deep learning have led to a\nfortuitous convergence in methods between speech processing and mainstream NLP.\nArguably, the time is ripe for a unification of these two fields, and for\nstarting to take spoken language seriously as the primary mode of human\ncommunication. Truly natural language processing could lead to better\nintegration with the rest of language science and could lead to systems which\nare more data-efficient and more human-like, and which can communicate beyond\nthe textual modality.\n",
                "链接": "https://arxiv.org/abs/2305.04572"
            },
            {
                "文章ID": "94916",
                "标题": "Pre-Trained Large Language Models for Industrial Control",
                "作者": " Lei Song,  Chuheng Zhang,  Li Zhao,  Jiang Bian",
                "发布日期": "2023-08-08",
                "摘要": "  For industrial control, developing high-performance controllers with few\nsamples and low technical debt is appealing. Foundation models, possessing rich\nprior knowledge obtained from pre-training with Internet-scale corpus, have the\npotential to be a good controller with proper prompts. In this paper, we take\nHVAC (Heating, Ventilation, and Air Conditioning) building control as an\nexample to examine the ability of GPT-4 (one of the first-tier foundation\nmodels) as the controller. To control HVAC, we wrap the task as a language game\nby providing text including a short description for the task, several selected\ndemonstrations, and the current observation to GPT-4 on each step and execute\nthe actions responded by GPT-4. We conduct series of experiments to answer the\nfollowing questions: 1)~How well can GPT-4 control HVAC? 2)~How well can GPT-4\ngeneralize to different scenarios for HVAC control? 3) How different parts of\nthe text context affect the performance? In general, we found GPT-4 achieves\nthe performance comparable to RL methods with few samples and low technical\ndebt, indicating the potential of directly applying foundation models to\nindustrial control tasks.\n",
                "链接": "https://arxiv.org/abs/2308.03028"
            }
        ]
    },
    {
        "question": {
            "question": "请查找关于智能体的研究进展的综述或调查论文。重点关注不同领域中智能体的方法和应用，以及相关的挑战和未来方向。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "36627",
                "标题": "Metaverse for Healthcare: A Survey on Potential Applications, Challenges\n  and Future Directions",
                "作者": " Rajeswari Chengoden,  Nancy Victor,  Thien Huynh-The,  Gokul Yenduri,  Rutvij H. Jhaveri,  Mamoun Alazab,  Sweta Bhattacharya,  Pawan Hegde,  Praveen Kumar Reddy Maddikunta,  Thippa Reddy Gadekallu",
                "发布日期": "2022-09-12",
                "摘要": "  The rapid progress in digitalization and automation have led to an\naccelerated growth in healthcare, generating novel models that are creating new\nchannels for rendering treatment with reduced cost. The Metaverse is an\nemerging technology in the digital space which has huge potential in\nhealthcare, enabling realistic experiences to the patients as well as the\nmedical practitioners. The Metaverse is a confluence of multiple enabling\ntechnologies such as artificial intelligence, virtual reality, augmented\nreality, internet of medical devices, robotics, quantum computing, etc. through\nwhich new directions for providing quality healthcare treatment and services\ncan be explored. The amalgamation of these technologies ensures immersive,\nintimate and personalized patient care. It also provides adaptive intelligent\nsolutions that eliminates the barriers between healthcare providers and\nreceivers. This article provides a comprehensive review of the Metaverse for\nhealthcare, emphasizing on the state of the art, the enabling technologies for\nadopting the Metaverse for healthcare, the potential applications and the\nrelated projects. The issues in the adaptation of the Metaverse for healthcare\napplications are also identified and the plausible solutions are highlighted as\npart of future research directions.\n",
                "链接": "https://arxiv.org/abs/2209.04160"
            },
            {
                "文章ID": "77270",
                "标题": "A Comprehensive Survey on Affective Computing; Challenges, Trends,\n  Applications, and Future Directions",
                "作者": " Sitara Afzal,  Haseeb Ali Khan,  Imran Ullah Khan,  Md. Jalil Piran,  Jong Weon Lee",
                "发布日期": "2023-05-16",
                "摘要": "  As the name suggests, affective computing aims to recognize human emotions,\nsentiments, and feelings. There is a wide range of fields that study affective\ncomputing, including languages, sociology, psychology, computer science, and\nphysiology. However, no research has ever been done to determine how machine\nlearning (ML) and mixed reality (XR) interact together. This paper discusses\nthe significance of affective computing, as well as its ideas, conceptions,\nmethods, and outcomes. By using approaches of ML and XR, we survey and discuss\nrecent methodologies in affective computing. We survey the state-of-the-art\napproaches along with current affective data resources. Further, we discuss\nvarious applications where affective computing has a significant impact, which\nwill aid future scholars in gaining a better understanding of its significance\nand practical relevance.\n",
                "链接": "https://arxiv.org/abs/2305.07665"
            },
            {
                "文章ID": "64747",
                "标题": "Backdoor Attacks and Defenses in Federated Learning: Survey, Challenges\n  and Future Research Directions",
                "作者": " Thuy Dung Nguyen,  Tuan Nguyen,  Phi Le Nguyen,  Hieu H. Pham,  Khoa Doan,  Kok-Seng Wong",
                "发布日期": "2023-03-07",
                "摘要": "  Federated learning (FL) is a machine learning (ML) approach that allows the\nuse of distributed data without compromising personal privacy. However, the\nheterogeneous distribution of data among clients in FL can make it difficult\nfor the orchestration server to validate the integrity of local model updates,\nmaking FL vulnerable to various threats, including backdoor attacks. Backdoor\nattacks involve the insertion of malicious functionality into a targeted model\nthrough poisoned updates from malicious clients. These attacks can cause the\nglobal model to misbehave on specific inputs while appearing normal in other\ncases. Backdoor attacks have received significant attention in the literature\ndue to their potential to impact real-world deep learning applications.\nHowever, they have not been thoroughly studied in the context of FL. In this\nsurvey, we provide a comprehensive survey of current backdoor attack strategies\nand defenses in FL, including a comprehensive analysis of different approaches.\nWe also discuss the challenges and potential future directions for attacks and\ndefenses in the context of FL.\n",
                "链接": "https://arxiv.org/abs/2303.02213"
            },
            {
                "文章ID": "55278",
                "标题": "A Survey on Table-and-Text HybridQA: Concepts, Methods, Challenges and\n  Future Directions",
                "作者": " Dingzirui Wang,  Longxu Dou,  Wanxiang Che",
                "发布日期": "2023-02-03",
                "摘要": "  Table-and-text hybrid question answering (HybridQA) is a widely used and\nchallenging NLP task commonly applied in the financial and scientific domain.\nThe early research focuses on migrating other QA task methods to HybridQA,\nwhile with further research, more and more HybridQA-specific methods have been\npresent. With the rapid development of HybridQA, the systematic survey is still\nunder-explored to summarize the main techniques and advance further research.\nSo we present this work to summarize the current HybridQA benchmarks and\nmethods, then analyze the challenges and future directions of this task. The\ncontributions of this paper can be summarized in three folds: (1) first survey,\nto our best knowledge, including benchmarks, methods and challenges for\nHybridQA; (2) systematic investigation with the reasonable comparison of the\nexisting systems to articulate their advantages and shortcomings; (3) detailed\nanalysis of challenges in four important dimensions to shed light on future\ndirections.\n",
                "链接": "https://arxiv.org/abs/2212.13465"
            },
            {
                "文章ID": "68556",
                "标题": "TinyML: Tools, Applications, Challenges, and Future Research Directions",
                "作者": " Rakhee Kallimani,  Krishna Pai,  Prasoon Raghuwanshi,  Sridhar Iyer,  Onel L. A. López",
                "发布日期": "2023-09-08",
                "摘要": "  In recent years, Artificial Intelligence (AI) and Machine learning (ML) have\ngained significant interest from both, industry and academia. Notably,\nconventional ML techniques require enormous amounts of power to meet the\ndesired accuracy, which has limited their use mainly to high-capability devices\nsuch as network nodes. However, with many advancements in technologies such as\nthe Internet of Things (IoT) and edge computing, it is desirable to incorporate\nML techniques into resource-constrained embedded devices for distributed and\nubiquitous intelligence. This has motivated the emergence of the TinyML\nparadigm which is an embedded ML technique that enables ML applications on\nmultiple cheap, resource- and power-constrained devices. However, during this\ntransition towards appropriate implementation of the TinyML technology,\nmultiple challenges such as processing capacity optimization, improved\nreliability, and maintenance of learning models' accuracy require timely\nsolutions. In this article, various avenues available for TinyML implementation\nare reviewed. Firstly, a background of TinyML is provided, followed by detailed\ndiscussions on various tools supporting TinyML. Then, state-of-art applications\nof TinyML using advanced technologies are detailed. Lastly, various research\nchallenges and future directions are identified.\n",
                "链接": "https://arxiv.org/abs/2303.13569"
            },
            {
                "文章ID": "70279",
                "标题": "A Survey on Federated Learning for the Healthcare Metaverse: Concepts,\n  Applications, Challenges, and Future Directions",
                "作者": " Ali Kashif Bashir,  Nancy Victor,  Sweta Bhattacharya,  Thien Huynh-The,  Rajeswari Chengoden,  Gokul Yenduri,  Praveen Kumar Reddy Maddikunta,  Quoc-Viet Pham,  Thippa Reddy Gadekallu,  Madhusanka Liyanage",
                "发布日期": "2023-04-06",
                "摘要": "  Recent technological advancements have considerately improved healthcare\nsystems to provide various intelligent healthcare services and improve the\nquality of life. Federated learning (FL), a new branch of artificial\nintelligence (AI), opens opportunities to deal with privacy issues in\nhealthcare systems and exploit data and computing resources available at\ndistributed devices. Additionally, the Metaverse, through integrating emerging\ntechnologies, such as AI, cloud edge computing, Internet of Things (IoT),\nblockchain, and semantic communications, has transformed many vertical domains\nin general and the healthcare sector in particular. Obviously, FL shows many\nbenefits and provides new opportunities for conventional and Metaverse\nhealthcare, motivating us to provide a survey on the usage of FL for Metaverse\nhealthcare systems. First, we present preliminaries to IoT-based healthcare\nsystems, FL in conventional healthcare, and Metaverse healthcare. The benefits\nof FL in Metaverse healthcare are then discussed, from improved privacy and\nscalability, better interoperability, better data management, and extra\nsecurity to automation and low-latency healthcare services. Subsequently, we\ndiscuss several applications pertaining to FL-enabled Metaverse healthcare,\nincluding medical diagnosis, patient monitoring, medical education, infectious\ndisease, and drug discovery. Finally, we highlight significant challenges and\npotential solutions toward the realization of FL in Metaverse healthcare.\n",
                "链接": "https://arxiv.org/abs/2304.00524"
            },
            {
                "文章ID": "75487",
                "标题": "A Survey on Dataset Distillation: Approaches, Applications and Future\n  Directions",
                "作者": " Jiahui Geng,  Zongxiong Chen,  Yuandou Wang,  Herbert Woisetschlaeger,  Sonja Schimmler,  Ruben Mayer,  Zhiming Zhao,  Chunming Rong",
                "发布日期": "2023-08-25",
                "摘要": "  Dataset distillation is attracting more attention in machine learning as\ntraining sets continue to grow and the cost of training state-of-the-art models\nbecomes increasingly high. By synthesizing datasets with high information\ndensity, dataset distillation offers a range of potential applications,\nincluding support for continual learning, neural architecture search, and\nprivacy protection. Despite recent advances, we lack a holistic understanding\nof the approaches and applications. Our survey aims to bridge this gap by first\nproposing a taxonomy of dataset distillation, characterizing existing\napproaches, and then systematically reviewing the data modalities, and related\napplications. In addition, we summarize the challenges and discuss future\ndirections for this field of research.\n",
                "链接": "https://arxiv.org/abs/2305.01975"
            },
            {
                "文章ID": "115918",
                "标题": "Applications of Computer Vision in Autonomous Vehicles: Methods,\n  Challenges and Future Directions",
                "作者": " Xingshuai Dong,  Massimiliano L. Cappuccio",
                "发布日期": "2023-11-17",
                "摘要": "  Autonomous vehicle refers to a vehicle capable of perceiving its surrounding\nenvironment and driving with little or no human driver input. The perception\nsystem is a fundamental component which enables the autonomous vehicle to\ncollect data and extract relevant information from the environment to drive\nsafely. Benefit from the recent advances in computer vision, the perception\ntask can be achieved by using sensors, such as camera, LiDAR, radar, and\nultrasonic sensor. This paper reviews publications on computer vision and\nautonomous driving that are published during the last ten years. In particular,\nwe first investigate the development of autonomous driving systems and\nsummarize these systems that are developed by the major automotive\nmanufacturers from different countries. Second, we investigate the sensors and\nbenchmark data sets that are commonly utilized for autonomous driving. Then, a\ncomprehensive overview of computer vision applications for autonomous driving\nsuch as depth estimation, object detection, lane detection, and traffic sign\nrecognition are discussed. Additionally, we review public opinions and concerns\non autonomous vehicles. Based on the discussion, we analyze the current\ntechnological challenges that autonomous vehicles meet with. Finally, we\npresent our insights and point out some promising directions for future\nresearch. This paper will help the reader to understand autonomous vehicles\nfrom the perspectives of academia and industry.\n",
                "链接": "https://arxiv.org/abs/2311.09093"
            },
            {
                "文章ID": "24751",
                "标题": "A Comprehensive Survey on Deep Clustering: Taxonomy, Challenges, and\n  Future Directions",
                "作者": " Sheng Zhou,  Hongjia Xu,  Zhuonan Zheng,  Jiawei Chen,  Zhao li,  Jiajun Bu,  Jia Wu,  Xin Wang,  Wenwu Zhu,  Martin Ester",
                "发布日期": "2022-06-16",
                "摘要": "  Clustering is a fundamental machine learning task which has been widely\nstudied in the literature. Classic clustering methods follow the assumption\nthat data are represented as features in a vectorized form through various\nrepresentation learning techniques. As the data become increasingly complicated\nand complex, the shallow (traditional) clustering methods can no longer handle\nthe high-dimensional data type. With the huge success of deep learning,\nespecially the deep unsupervised learning, many representation learning\ntechniques with deep architectures have been proposed in the past decade.\nRecently, the concept of Deep Clustering, i.e., jointly optimizing the\nrepresentation learning and clustering, has been proposed and hence attracted\ngrowing attention in the community. Motivated by the tremendous success of deep\nlearning in clustering, one of the most fundamental machine learning tasks, and\nthe large number of recent advances in this direction, in this paper we conduct\na comprehensive survey on deep clustering by proposing a new taxonomy of\ndifferent state-of-the-art approaches. We summarize the essential components of\ndeep clustering and categorize existing methods by the ways they design\ninteractions between deep representation learning and clustering. Moreover,\nthis survey also provides the popular benchmark datasets, evaluation metrics\nand open-source implementations to clearly illustrate various experimental\nsettings. Last but not least, we discuss the practical applications of deep\nclustering and suggest challenging topics deserving further investigations as\nfuture directions.\n",
                "链接": "https://arxiv.org/abs/2206.07579"
            },
            {
                "文章ID": "117114",
                "标题": "A Survey of Graph Meets Large Language Model: Progress and Future\n  Directions",
                "作者": " Yuhan Li,  Zhixun Li,  Peisong Wang,  Jia Li,  Xiangguo Sun,  Hong Cheng,  Jeffrey Xu Yu",
                "发布日期": "2023-11-29",
                "摘要": "  Graph plays a significant role in representing and analyzing complex\nrelationships in real-world applications such as citation networks, social\nnetworks, and biological data. Recently, Large Language Models (LLMs), which\nhave achieved tremendous success in various domains, have also been leveraged\nin graph-related tasks to surpass traditional Graph Neural Networks (GNNs)\nbased methods and yield state-of-the-art performance. In this survey, we first\npresent a comprehensive review and analysis of existing methods that integrate\nLLMs with graphs. First of all, we propose a new taxonomy, which organizes\nexisting methods into three categories based on the role (i.e., enhancer,\npredictor, and alignment component) played by LLMs in graph-related tasks. Then\nwe systematically survey the representative methods along the three categories\nof the taxonomy. Finally, we discuss the remaining limitations of existing\nstudies and highlight promising avenues for future research. The relevant\npapers are summarized and will be consistently updated at:\nhttps://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.\n",
                "链接": "https://arxiv.org/abs/2311.12399"
            }
        ]
    }
]