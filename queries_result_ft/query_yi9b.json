[
    {
        "question": {
            "question": "与大模型工具学习相关论文",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找关于大模型Agent应用的最新发表论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "11318",
                "标题": "Multi-agent Searching System for Medical Information",
                "作者": " Mariya Evtimova-Gardair",
                "发布日期": "2022-03-24",
                "摘要": "  In the paper is proposed a model of multi-agent security system for searching\na medical information in Internet. The advantages when using mobile agent are\ndescribed, so that to perform searching in Internet. Nowadays, multi-agent\nsystems found their application into distribution of decisions. For modeling\nthe proposed multi-agent medical system is used JADE. Finally, the results when\nusing mobile agent are generated that could reflect performance when working\nwith BIG DATA. The proposed system is having also relatively high precision\n96%.\n",
                "链接": "https://arxiv.org/abs/2203.12465"
            },
            {
                "文章ID": "122182",
                "标题": "Large Language Model Enhanced Multi-Agent Systems for 6G Communications",
                "作者": " Feibo Jiang,  Li Dong,  Yubo Peng,  Kezhi Wang,  Kun Yang,  Cunhua Pan,  Dusit Niyato,  Octavia A. Dobre",
                "发布日期": "2023-12-14",
                "摘要": "  The rapid development of the Large Language Model (LLM) presents huge\nopportunities for 6G communications, e.g., network optimization and management\nby allowing users to input task requirements to LLMs by nature language.\nHowever, directly applying native LLMs in 6G encounters various challenges,\nsuch as a lack of private communication data and knowledge, limited logical\nreasoning, evaluation, and refinement abilities. Integrating LLMs with the\ncapabilities of retrieval, planning, memory, evaluation and reflection in\nagents can greatly enhance the potential of LLMs for 6G communications. To this\nend, we propose a multi-agent system with customized communication knowledge\nand tools for solving communication related tasks using natural language,\ncomprising three components: (1) Multi-agent Data Retrieval (MDR), which\nemploys the condensate and inference agents to refine and summarize\ncommunication knowledge from the knowledge base, expanding the knowledge\nboundaries of LLMs in 6G communications; (2) Multi-agent Collaborative Planning\n(MCP), which utilizes multiple planning agents to generate feasible solutions\nfor the communication related task from different perspectives based on the\nretrieved knowledge; (3) Multi-agent Evaluation and Reflecxion (MER), which\nutilizes the evaluation agent to assess the solutions, and applies the\nreflexion agent and refinement agent to provide improvement suggestions for\ncurrent solutions. Finally, we validate the effectiveness of the proposed\nmulti-agent system by designing a semantic communication system, as a case\nstudy of 6G communications.\n",
                "链接": "https://arxiv.org/abs/2312.07850"
            },
            {
                "文章ID": "120758",
                "标题": "LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent\n  Ecosystem",
                "作者": " Yingqiang Ge,  Yujie Ren,  Wenyue Hua,  Shuyuan Xu,  Juntao Tan,  Yongfeng Zhang",
                "发布日期": "2023-12-12",
                "摘要": "  This paper envisions a revolutionary AIOS-Agent ecosystem, where Large\nLanguage Model (LLM) serves as the (Artificial) Intelligent Operating System\n(IOS, or AIOS)--an operating system \"with soul\". Upon this foundation, a\ndiverse range of LLM-based AI Agent Applications (Agents, or AAPs) are\ndeveloped, enriching the AIOS-Agent ecosystem and signaling a paradigm shift\nfrom the traditional OS-APP ecosystem. We envision that LLM's impact will not\nbe limited to the AI application level, instead, it will in turn revolutionize\nthe design and implementation of computer system, architecture, software, and\nprogramming language, featured by several main concepts: LLM as OS\n(system-level), Agents as Applications (application-level), Natural Language as\nProgramming Interface (user-level), and Tools as Devices/Libraries\n(hardware/middleware-level). We begin by introducing the architecture of\ntraditional OS. Then we formalize a conceptual framework for AIOS through \"LLM\nas OS (LLMOS)\", drawing analogies between AIOS and traditional OS: LLM is\nlikened to OS kernel, context window to memory, external storage to file\nsystem, hardware tools to peripheral devices, software tools to programming\nlibraries, and user prompts to user commands. Subsequently, we introduce the\nnew AIOS-Agent Ecosystem, where users can easily program Agent Applications\n(AAPs) using natural language, democratizing the development of software, which\nis different from the traditional OS-APP ecosystem. Following this, we explore\nthe diverse scope of Agent Applications. We delve into both single-agent and\nmulti-agent systems, as well as human-agent interaction. Lastly, drawing on the\ninsights from traditional OS-APP ecosystem, we propose a roadmap for the\nevolution of the AIOS-Agent ecosystem. This roadmap is designed to guide the\nfuture research and development, suggesting systematic progresses of AIOS and\nits Agent applications.\n",
                "链接": "https://arxiv.org/abs/2312.03815"
            },
            {
                "文章ID": "112741",
                "标题": "Multi-Agent Consensus Seeking via Large Language Models",
                "作者": " Huaben Chen,  Wenkang Ji,  Lufeng Xu,  Shiyu Zhao",
                "发布日期": "2023-11-01",
                "摘要": "  Multi-agent systems driven by large language models (LLMs) have shown\npromising abilities for solving complex tasks in a collaborative manner. This\nwork considers a fundamental problem in multi-agent collaboration: consensus\nseeking. When multiple agents work together, we are interested in how they can\nreach a consensus through inter-agent negotiation. To that end, this work\nstudies a consensus-seeking task where the state of each agent is a numerical\nvalue and they negotiate with each other to reach a consensus value. It is\nrevealed that when not explicitly directed on which strategy should be adopted,\nthe LLM-driven agents primarily use the average strategy for consensus seeking\nalthough they may occasionally use some other strategies. Moreover, this work\nanalyzes the impact of the agent number, agent personality, and network\ntopology on the negotiation process. The findings reported in this work can\npotentially lay the foundations for understanding the behaviors of LLM-driven\nmulti-agent systems for solving more complex tasks. Furthermore, LLM-driven\nconsensus seeking is applied to a multi-robot aggregation task. This\napplication demonstrates the potential of LLM-driven agents to achieve\nzero-shot autonomous planning for multi-robot collaboration tasks. Project\nwebsite: westlakeintelligentrobotics.github.io/ConsensusLLM/.\n",
                "链接": "https://arxiv.org/abs/2310.20151"
            },
            {
                "文章ID": "98891",
                "标题": "RecMind: Large Language Model Powered Agent For Recommendation",
                "作者": " Yancheng Wang,  Ziyan Jiang,  Zheng Chen,  Fan Yang,  Yingxue Zhou,  Eunah Cho,  Xing Fan,  Xiaojiang Huang,  Yanbin Lu,  Yingzhen Yang",
                "发布日期": "2023-08-29",
                "摘要": "  Recent advancements in instructing Large Language Models (LLMs) to utilize\nexternal tools and execute multi-step plans have significantly enhanced their\nability to solve intricate tasks, ranging from mathematical problems to\ncreative writing. Yet, there remains a notable gap in studying the capacity of\nLLMs in responding to personalized queries such as a recommendation request. To\nbridge this gap, we have designed an LLM-powered autonomous recommender agent,\nRecMind, which is capable of providing precise personalized recommendations\nthrough careful planning, utilizing tools for obtaining external knowledge, and\nleveraging individual data. We propose a novel algorithm, Self-Inspiring, to\nimprove the planning ability of the LLM agent. At each intermediate planning\nstep, the LLM 'self-inspires' to consider all previously explored states to\nplan for next step. This mechanism greatly improves the model's ability to\ncomprehend and utilize historical planning information for recommendation. We\nevaluate RecMind's performance in various recommendation scenarios, including\nrating prediction, sequential recommendation, direct recommendation,\nexplanation generation, and review summarization. Our experiment shows that\nRecMind outperforms existing zero/few-shot LLM-based recommendation methods in\ndifferent recommendation tasks and achieves competitive performance to a recent\nmodel P5, which requires fully pre-train for the recommendation tasks.\n",
                "链接": "https://arxiv.org/abs/2308.14296"
            },
            {
                "文章ID": "1080",
                "标题": "The Recurrent Reinforcement Learning Crypto Agent",
                "作者": " Gabriel Borrageiro,  Nick Firoozye,  Paolo Barucca",
                "发布日期": "2022-05-24",
                "摘要": "  We demonstrate a novel application of online transfer learning for a digital\nassets trading agent. This agent uses a powerful feature space representation\nin the form of an echo state network, the output of which is made available to\na direct, recurrent reinforcement learning agent. The agent learns to trade the\nXBTUSD (Bitcoin versus US Dollars) perpetual swap derivatives contract on\nBitMEX on an intraday basis. By learning from the multiple sources of impact on\nthe quadratic risk-adjusted utility that it seeks to maximise, the agent avoids\nexcessive over-trading, captures a funding profit, and can predict the market's\ndirection. Overall, our crypto agent realises a total return of 350\\%, net of\ntransaction costs, over roughly five years, 71\\% of which is down to funding\nprofit. The annualised information ratio that it achieves is 1.46.\n",
                "链接": "https://arxiv.org/abs/2201.04699"
            },
            {
                "文章ID": "34376",
                "标题": "A simple learning agent interacting with an agent-based market model",
                "作者": " Matthew Dicks,  Andrew Paskaramoorthy,  Tim Gebbie",
                "发布日期": "2023-11-23",
                "摘要": "  We consider the learning dynamics of a single reinforcement learning optimal\nexecution trading agent when it interacts with an event driven agent-based\nfinancial market model. Trading takes place asynchronously through a matching\nengine in event time. The optimal execution agent is considered at different\nlevels of initial order-sizes and differently sized state spaces. The resulting\nimpact on the agent-based model and market are considered using a calibration\napproach that explores changes in the empirical stylised facts and price impact\ncurves. Convergence, volume trajectory and action trace plots are used to\nvisualise the learning dynamics. Here the smaller state space agents had the\nnumber of states they visited converge much faster than the larger state space\nagents, and they were able to start learning to trade intuitively using the\nspread and volume states. We find that the moments of the model are robust to\nthe impact of the learning agents except for the Hurst exponent, which was\nlowered by the introduction of strategic order-splitting. The introduction of\nthe learning agent preserves the shape of the price impact curves but can\nreduce the trade-sign auto-correlations when their trading volumes increase.\n",
                "链接": "https://arxiv.org/abs/2208.10434"
            },
            {
                "文章ID": "106067",
                "标题": "Dynamic LLM-Agent Network: An LLM-agent Collaboration Framework with\n  Agent Team Optimization",
                "作者": " Zijun Liu,  Yanzhe Zhang,  Peng Li,  Yang Liu,  Diyi Yang",
                "发布日期": "2023-10-04",
                "摘要": "  Large language model (LLM) agents have been shown effective on a wide range\nof tasks, and by ensembling multiple LLM agents, their performances could be\nfurther improved. Existing approaches employ a fixed set of agents to interact\nwith each other in a static architecture, which limits their generalizability\nto various tasks and requires strong human prior in designing these agents. In\nthis work, we propose to construct a strategic team of agents communicating in\na dynamic interaction architecture based on the task query. Specifically, we\nbuild a framework named Dynamic LLM-Agent Network ($\\textbf{DyLAN}$) for\nLLM-agent collaboration on complicated tasks like reasoning and code\ngeneration. DyLAN enables agents to interact for multiple rounds in a dynamic\narchitecture with inference-time agent selection and an early-stopping\nmechanism to improve performance and efficiency. We further design an automatic\nagent team optimization algorithm based on an unsupervised metric termed\n$\\textit{Agent Importance Score}$, enabling the selection of best agents based\non the contribution each agent makes. Empirically, we demonstrate that DyLAN\nperforms well in both reasoning and code generation tasks with reasonable\ncomputational cost. DyLAN achieves 13.0% and 13.3% improvement on MATH and\nHumanEval, respectively, compared to a single execution on GPT-35-turbo. On\nspecific subjects of MMLU, agent team optimization in DyLAN increases accuracy\nby up to 25.0%.\n",
                "链接": "https://arxiv.org/abs/2310.02170"
            },
            {
                "文章ID": "42384",
                "标题": "Multi-agent Dynamic Algorithm Configuration",
                "作者": " Ke Xue,  Jiacheng Xu,  Lei Yuan,  Miqing Li,  Chao Qian,  Zongzhang Zhang,  Yang Yu",
                "发布日期": "2022-10-14",
                "摘要": "  Automated algorithm configuration relieves users from tedious,\ntrial-and-error tuning tasks. A popular algorithm configuration tuning paradigm\nis dynamic algorithm configuration (DAC), in which an agent learns dynamic\nconfiguration policies across instances by reinforcement learning (RL).\nHowever, in many complex algorithms, there may exist different types of\nconfiguration hyperparameters, and such heterogeneity may bring difficulties\nfor classic DAC which uses a single-agent RL policy. In this paper, we aim to\naddress this issue and propose multi-agent DAC (MA-DAC), with one agent working\nfor one type of configuration hyperparameter. MA-DAC formulates the dynamic\nconfiguration of a complex algorithm with multiple types of hyperparameters as\na contextual multi-agent Markov decision process and solves it by a cooperative\nmulti-agent RL (MARL) algorithm. To instantiate, we apply MA-DAC to a\nwell-known optimization algorithm for multi-objective optimization problems.\nExperimental results show the effectiveness of MA-DAC in not only achieving\nsuperior performance compared with other configuration tuning approaches based\non heuristic rules, multi-armed bandits, and single-agent RL, but also being\ncapable of generalizing to different problem classes. Furthermore, we release\nthe environments in this paper as a benchmark for testing MARL algorithms, with\nthe hope of facilitating the application of MARL.\n",
                "链接": "https://arxiv.org/abs/2210.06835"
            },
            {
                "文章ID": "99980",
                "标题": "ModelScope-Agent: Building Your Customizable Agent System with\n  Open-source Large Language Models",
                "作者": " Chenliang Li,  Hehong Chen,  Ming Yan,  Weizhou Shen,  Haiyang Xu,  Zhikai Wu,  Zhicheng Zhang,  Wenmeng Zhou,  Yingda Chen,  Chen Cheng,  Hongzhu Shi,  Ji Zhang,  Fei Huang,  Jingren Zhou",
                "发布日期": "2023-09-06",
                "摘要": "  Large language models (LLMs) have recently demonstrated remarkable\ncapabilities to comprehend human intentions, engage in reasoning, and design\nplanning-like behavior. To further unleash the power of LLMs to accomplish\ncomplex tasks, there is a growing trend to build agent framework that equips\nLLMs, such as ChatGPT, with tool-use abilities to connect with massive external\nAPIs. In this work, we introduce ModelScope-Agent, a general and customizable\nagent framework for real-world applications, based on open-source LLMs as\ncontrollers. It provides a user-friendly system library, with customizable\nengine design to support model training on multiple open-source LLMs, while\nalso enabling seamless integration with both model APIs and common APIs in a\nunified way. To equip the LLMs with tool-use abilities, a comprehensive\nframework has been proposed spanning over tool-use data collection, tool\nretrieval, tool registration, memory control, customized model training, and\nevaluation for practical real-world applications. Finally, we showcase\nModelScopeGPT, a real-world intelligent assistant of ModelScope Community based\non the ModelScope-Agent framework, which is able to connect open-source LLMs\nwith more than 1000 public AI models and localized community knowledge in\nModelScope. The ModelScope-Agent\nlibrary\\footnote{https://github.com/modelscope/modelscope-agent} and online\ndemo\\footnote{https://modelscope.cn/studios/damo/ModelScopeGPT/summary} are now\npublicly available.\n",
                "链接": "https://arxiv.org/abs/2309.00986"
            }
        ]
    },
    {
        "question": {
            "question": "查找OCR文本识别最新进展。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "97589",
                "标题": "bbOCR: An Open-source Multi-domain OCR Pipeline for Bengali Documents",
                "作者": " Imam Mohammad Zulkarnain,  Shayekh Bin Islam,  Md. Zami Al Zunaed Farabe,  Md. Mehedi Hasan Shawon,  Jawaril Munshad Abedin,  Beig Rajibul Hasan,  Marsia Haque,  Istiak Shihab,  Syed Mobassir,  MD. Nazmuddoha Ansary,  Asif Sushmit,  Farig Sadeque",
                "发布日期": "2023-08-23",
                "摘要": "  Despite the existence of numerous Optical Character Recognition (OCR) tools,\nthe lack of comprehensive open-source systems hampers the progress of document\ndigitization in various low-resource languages, including Bengali. Low-resource\nlanguages, especially those with an alphasyllabary writing system, suffer from\nthe lack of large-scale datasets for various document OCR components such as\nword-level OCR, document layout extraction, and distortion correction; which\nare available as individual modules in high-resource languages. In this paper,\nwe introduce Bengali$.$AI-BRACU-OCR (bbOCR): an open-source scalable document\nOCR system that can reconstruct Bengali documents into a structured searchable\ndigitized format that leverages a novel Bengali text recognition model and two\nnovel synthetic datasets. We present extensive component-level and system-level\nevaluation: both use a novel diversified evaluation dataset and comprehensive\nevaluation metrics. Our extensive evaluation suggests that our proposed\nsolution is preferable over the current state-of-the-art Bengali OCR systems.\nThe source codes and datasets are available here:\nhttps://bengaliai.github.io/bbocr.\n",
                "链接": "https://arxiv.org/abs/2308.10647"
            },
            {
                "文章ID": "115320",
                "标题": "What Large Language Models Bring to Text-rich VQA?",
                "作者": " Xuejing Liu,  Wei Tang,  Xinzhe Ni,  Jinghui Lu,  Rui Zhao,  Zechao Li,  Fei Tan",
                "发布日期": "2023-11-14",
                "摘要": "  Text-rich VQA, namely Visual Question Answering based on text recognition in\nthe images, is a cross-modal task that requires both image comprehension and\ntext recognition. In this work, we focus on investigating the advantages and\nbottlenecks of LLM-based approaches in addressing this problem. To address the\nabove concern, we separate the vision and language modules, where we leverage\nexternal OCR models to recognize texts in the image and Large Language Models\n(LLMs) to answer the question given texts. The whole framework is training-free\nbenefiting from the in-context ability of LLMs. This pipeline achieved superior\nperformance compared to the majority of existing Multimodal Large Language\nModels (MLLM) on four text-rich VQA datasets. Besides, based on the ablation\nstudy, we find that LLM brings stronger comprehension ability and may introduce\nhelpful knowledge for the VQA problem. The bottleneck for LLM to address\ntext-rich VQA problems may primarily lie in visual part. We also combine the\nOCR module with MLLMs and pleasantly find that the combination of OCR module\nwith MLLM also works. It's worth noting that not all MLLMs can comprehend the\nOCR information, which provides insights into how to train an MLLM that\npreserves the abilities of LLM.\n",
                "链接": "https://arxiv.org/abs/2311.07306"
            },
            {
                "文章ID": "111490",
                "标题": "Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and\n  In-depth Evaluation",
                "作者": " Yongxin Shi,  Dezhi Peng,  Wenhui Liao,  Zening Lin,  Xinhong Chen,  Chongyu Liu,  Yuyi Zhang,  Lianwen Jin",
                "发布日期": "2023-10-31",
                "摘要": "  This paper presents a comprehensive evaluation of the Optical Character\nRecognition (OCR) capabilities of the recently released GPT-4V(ision), a Large\nMultimodal Model (LMM). We assess the model's performance across a range of OCR\ntasks, including scene text recognition, handwritten text recognition,\nhandwritten mathematical expression recognition, table structure recognition,\nand information extraction from visually-rich document. The evaluation reveals\nthat GPT-4V performs well in recognizing and understanding Latin contents, but\nstruggles with multilingual scenarios and complex tasks. Specifically, it\nshowed limitations when dealing with non-Latin languages and complex tasks such\nas handwriting mathematical expression recognition, table structure\nrecognition, and end-to-end semantic entity recognition and pair extraction\nfrom document image. Based on these observations, we affirm the necessity and\ncontinued research value of specialized OCR models. In general, despite its\nversatility in handling diverse OCR tasks, GPT-4V does not outperform existing\nstate-of-the-art OCR models. How to fully utilize pre-trained general-purpose\nLMMs such as GPT-4V for OCR downstream tasks remains an open problem. The study\noffers a critical reference for future research in OCR with LMMs. Evaluation\npipeline and results are available at\nhttps://github.com/SCUT-DLVCLab/GPT-4V_OCR.\n",
                "链接": "https://arxiv.org/abs/2310.16809"
            },
            {
                "文章ID": "115997",
                "标题": "Reading Between the Mud: A Challenging Motorcycle Racer Number Dataset",
                "作者": " Jacob Tyo,  Youngseog Chung,  Motolani Olarinre,  Zachary C. Lipton",
                "发布日期": "2023-11-17",
                "摘要": "  This paper introduces the off-road motorcycle Racer number Dataset (RnD), a\nnew challenging dataset for optical character recognition (OCR) research. RnD\ncontains 2,411 images from professional motorsports photographers that depict\nmotorcycle racers in off-road competitions. The images exhibit a wide variety\nof factors that make OCR difficult, including mud occlusions, motion blur,\nnon-standard fonts, glare, complex backgrounds, etc. The dataset has 5,578\nmanually annotated bounding boxes around visible motorcycle numbers, along with\ntranscribed digits and letters. Our experiments benchmark leading OCR\nalgorithms and reveal an end-to-end F1 score of only 0.527 on RnD, even after\nfine-tuning. Analysis of performance on different occlusion types shows mud as\nthe primary challenge, degrading accuracy substantially compared to normal\nconditions. But the models struggle with other factors including glare, blur,\nshadows, and dust. Analysis exposes substantial room for improvement and\nhighlights failure cases of existing models. RnD represents a valuable new\nbenchmark to drive innovation in real-world OCR capabilities. The authors hope\nthe community will build upon this dataset and baseline experiments to make\nprogress on the open problem of robustly recognizing text in unconstrained\nnatural environments. The dataset is available at\nhttps://github.com/JacobTyo/SwinTextSpotter.\n",
                "链接": "https://arxiv.org/abs/2311.09256"
            },
            {
                "文章ID": "123651",
                "标题": "Advancements and Challenges in Arabic Optical Character Recognition: A\n  Comprehensive Survey",
                "作者": " Mahmoud SalahEldin Kasem,  Mohamed Mahmoud,  Hyun-Soo Kang",
                "发布日期": "2023-12-20",
                "摘要": "  Optical character recognition (OCR) is a vital process that involves the\nextraction of handwritten or printed text from scanned or printed images,\nconverting it into a format that can be understood and processed by machines.\nThis enables further data processing activities such as searching and editing.\nThe automatic extraction of text through OCR plays a crucial role in digitizing\ndocuments, enhancing productivity, improving accessibility, and preserving\nhistorical records. This paper seeks to offer an exhaustive review of\ncontemporary applications, methodologies, and challenges associated with Arabic\nOptical Character Recognition (OCR). A thorough analysis is conducted on\nprevailing techniques utilized throughout the OCR process, with a dedicated\neffort to discern the most efficacious approaches that demonstrate enhanced\noutcomes. To ensure a thorough evaluation, a meticulous keyword-search\nmethodology is adopted, encompassing a comprehensive analysis of articles\nrelevant to Arabic OCR, including both backward and forward citation reviews.\nIn addition to presenting cutting-edge techniques and methods, this paper\ncritically identifies research gaps within the realm of Arabic OCR. By\nhighlighting these gaps, we shed light on potential areas for future\nexploration and development, thereby guiding researchers toward promising\navenues in the field of Arabic OCR. The outcomes of this study provide valuable\ninsights for researchers, practitioners, and stakeholders involved in Arabic\nOCR, ultimately fostering advancements in the field and facilitating the\ncreation of more accurate and efficient OCR systems for the Arabic language.\n",
                "链接": "https://arxiv.org/abs/2312.11812"
            },
            {
                "文章ID": "90438",
                "标题": "Handwritten Text Recognition Using Convolutional Neural Network",
                "作者": " Atman Mishra,  A. Sharath Ram,  Kavyashree C",
                "发布日期": "2023-07-12",
                "摘要": "  OCR (Optical Character Recognition) is a technology that offers comprehensive\nalphanumeric recognition of handwritten and printed characters at electronic\nspeed by merely scanning the document. Recently, the understanding of visual\ndata has been termed Intelligent Character Recognition (ICR). Intelligent\nCharacter Recognition (ICR) is the OCR module that can convert scans of\nhandwritten or printed characters into ASCII text. ASCII data is the standard\nformat for data encoding in electronic communication. ASCII assigns standard\nnumeric values to letters, numeral, symbols, white-spaces and other characters.\nIn more technical terms, OCR is the process of using an electronic device to\ntransform 2-Dimensional textual information into machine-encoded text. Anything\nthat contains text both machine written or handwritten can be scanned either\nthrough a scanner or just simply a picture of the text is enough for the\nrecognition system to distinguish the text. The goal of this papers is to show\nthe results of a Convolutional Neural Network model which has been trained on\nNational Institute of Science and Technology (NIST) dataset containing over a\n100,000 images. The network learns from the features extracted from the images\nand use it to generate the probability of each class to which the picture\nbelongs to. We have achieved an accuracy of 90.54% with a loss of 2.53%.\n",
                "链接": "https://arxiv.org/abs/2307.05396"
            },
            {
                "文章ID": "53136",
                "标题": "Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned\n  Receipt Images",
                "作者": " Hongkuan Zhang,  Edward Whittaker,  Ikuo Kitagishi",
                "发布日期": "2023-10-17",
                "摘要": "  Digitization of scanned receipts aims to extract text from receipt images and\nsave it into structured documents. This is usually split into two sub-tasks:\ntext localization and optical character recognition (OCR). Most existing OCR\nmodels only focus on the cropped text instance images, which require the\nbounding box information provided by a text region detection model. Introducing\nan additional detector to identify the text instance images in advance adds\ncomplexity, however instance-level OCR models have very low accuracy when\nprocessing the whole image for the document-level OCR, such as receipt images\ncontaining multiple text lines arranged in various layouts. To this end, we\npropose a localization-free document-level OCR model for transcribing all the\ncharacters in a receipt image into an ordered sequence end-to-end.\nSpecifically, we finetune the pretrained instance-level model TrOCR with\nrandomly cropped image chunks, and gradually increase the image chunk size to\ngeneralize the recognition ability from instance images to full-page images. In\nour experiments on the SROIE receipt OCR dataset, the model finetuned with our\nstrategy achieved 64.4 F1-score and a 22.8% character error rate (CER),\nrespectively, which outperforms the baseline results with 48.5 F1-score and\n50.6% CER. The best model, which splits the full image into 15 equally sized\nchunks, gives 87.8 F1-score and 4.98% CER with minimal additional pre or\npost-processing of the output. Moreover, the characters in the generated\ndocument-level sequences are arranged in the reading order, which is practical\nfor real-world applications.\n",
                "链接": "https://arxiv.org/abs/2212.05525"
            },
            {
                "文章ID": "118261",
                "标题": "Optimization of Image Processing Algorithms for Character Recognition in\n  Cultural Typewritten Documents",
                "作者": " Mariana Dias,  Carla Teixeira Lopes",
                "发布日期": "2023-11-28",
                "摘要": "  Linked Data is used in various fields as a new way of structuring and\nconnecting data. Cultural heritage institutions have been using linked data to\nimprove archival descriptions and facilitate the discovery of information. Most\narchival records have digital representations of physical artifacts in the form\nof scanned images that are non-machine-readable. Optical Character Recognition\n(OCR) recognizes text in images and translates it into machine-encoded text.\nThis paper evaluates the impact of image processing methods and parameter\ntuning in OCR applied to typewritten cultural heritage documents. The approach\nuses a multi-objective problem formulation to minimize Levenshtein edit\ndistance and maximize the number of words correctly identified with a\nnon-dominated sorting genetic algorithm (NSGA-II) to tune the methods'\nparameters. Evaluation results show that parameterization by digital\nrepresentation typology benefits the performance of image pre-processing\nalgorithms in OCR. Furthermore, our findings suggest that employing image\npre-processing algorithms in OCR might be more suitable for typologies where\nthe text recognition task without pre-processing does not produce good results.\nIn particular, Adaptive Thresholding, Bilateral Filter, and Opening are the\nbest-performing algorithms for the theatre plays' covers, letters, and overall\ndataset, respectively, and should be applied before OCR to improve its\nperformance.\n",
                "链接": "https://arxiv.org/abs/2311.15740"
            },
            {
                "文章ID": "73951",
                "标题": "ICDAR 2023 Competition on Reading the Seal Title",
                "作者": " Wenwen Yu,  Mingyu Liu,  Mingrui Chen,  Ning Lu,  Yinlong Wen,  Yuliang Liu,  Dimosthenis Karatzas,  Xiang Bai",
                "发布日期": "2023-06-07",
                "摘要": "  Reading seal title text is a challenging task due to the variable shapes of\nseals, curved text, background noise, and overlapped text. However, this\nimportant element is commonly found in official and financial scenarios, and\nhas not received the attention it deserves in the field of OCR technology. To\npromote research in this area, we organized ICDAR 2023 competition on reading\nthe seal title (ReST), which included two tasks: seal title text detection\n(Task 1) and end-to-end seal title recognition (Task 2). We constructed a\ndataset of 10,000 real seal data, covering the most common classes of seals,\nand labeled all seal title texts with text polygons and text contents. The\ncompetition opened on 30th December, 2022 and closed on 20th March, 2023. The\ncompetition attracted 53 participants from academia and industry including 28\nsubmissions for Task 1 and 25 submissions for Task 2, which demonstrated\nsignificant interest in this challenging task. In this report, we present an\noverview of the competition, including the organization, challenges, and\nresults. We describe the dataset and tasks, and summarize the submissions and\nevaluation results. The results show that significant progress has been made in\nthe field of seal title text reading, and we hope that this competition will\ninspire further research and development in this important area of OCR\ntechnology.\n",
                "链接": "https://arxiv.org/abs/2304.11966"
            },
            {
                "文章ID": "100439",
                "标题": "STEP -- Towards Structured Scene-Text Spotting",
                "作者": " Sergi Garcia-Bordils,  Dimosthenis Karatzas,  Marçal Rusiñol",
                "发布日期": "2023-12-12",
                "摘要": "  We introduce the structured scene-text spotting task, which requires a\nscene-text OCR system to spot text in the wild according to a query regular\nexpression. Contrary to generic scene text OCR, structured scene-text spotting\nseeks to dynamically condition both scene text detection and recognition on\nuser-provided regular expressions. To tackle this task, we propose the\nStructured TExt sPotter (STEP), a model that exploits the provided text\nstructure to guide the OCR process. STEP is able to deal with regular\nexpressions that contain spaces and it is not bound to detection at the\nword-level granularity. Our approach enables accurate zero-shot structured text\nspotting in a wide variety of real-world reading scenarios and is solely\ntrained on publicly available data. To demonstrate the effectiveness of our\napproach, we introduce a new challenging test dataset that contains several\ntypes of out-of-vocabulary structured text, reflecting important reading\napplications of fields such as prices, dates, serial numbers, license plates\netc. We demonstrate that STEP can provide specialised OCR performance on demand\nin all tested scenarios.\n",
                "链接": "https://arxiv.org/abs/2309.02356"
            }
        ]
    },
    {
        "question": {
            "question": "与大模型安全相关论文",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "近几个月自然语言处理相关的文章。",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找多语言情感分析的最新论文。",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "近一个月与多模态大模型相关论文",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找近六个月工具学习评测数据集的论文",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "对比解码相关论文",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找中文ner常用的数据集论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "5948",
                "标题": "AISHELL-NER: Named Entity Recognition from Chinese Speech",
                "作者": " Boli Chen,  Guangwei Xu,  Xiaobin Wang,  Pengjun Xie,  Meishan Zhang,  Fei Huang",
                "发布日期": "2022-02-18",
                "摘要": "  Named Entity Recognition (NER) from speech is among Spoken Language\nUnderstanding (SLU) tasks, aiming to extract semantic information from the\nspeech signal. NER from speech is usually made through a two-step pipeline that\nconsists of (1) processing the audio using an Automatic Speech Recognition\n(ASR) system and (2) applying an NER tagger to the ASR outputs. Recent works\nhave shown the capability of the End-to-End (E2E) approach for NER from English\nand French speech, which is essentially entity-aware ASR. However, due to the\nmany homophones and polyphones that exist in Chinese, NER from Chinese speech\nis effectively a more challenging task. In this paper, we introduce a new\ndataset AISEHLL-NER for NER from Chinese speech. Extensive experiments are\nconducted to explore the performance of several state-of-the-art methods. The\nresults demonstrate that the performance could be improved by combining\nentity-aware ASR and pretrained NER tagger, which can be easily applied to the\nmodern SLU pipeline. The dataset is publicly available at\ngithub.com/Alibaba-NLP/AISHELL-NER.\n",
                "链接": "https://arxiv.org/abs/2202.08533"
            },
            {
                "文章ID": "12836",
                "标题": "$k$NN-NER: Named Entity Recognition with Nearest Neighbor Search",
                "作者": " Shuhe Wang,  Xiaoya Li,  Yuxian Meng,  Tianwei Zhang,  Rongbin Ouyang,  Jiwei Li,  Guoyin Wang",
                "发布日期": "2022-04-01",
                "摘要": "  Inspired by recent advances in retrieval augmented methods in\nNLP~\\citep{khandelwal2019generalization,khandelwal2020nearest,meng2021gnn}, in\nthis paper, we introduce a $k$ nearest neighbor NER ($k$NN-NER) framework,\nwhich augments the distribution of entity labels by assigning $k$ nearest\nneighbors retrieved from the training set. This strategy makes the model more\ncapable of handling long-tail cases, along with better few-shot learning\nabilities. $k$NN-NER requires no additional operation during the training\nphase, and by interpolating $k$ nearest neighbors search into the vanilla NER\nmodel, $k$NN-NER consistently outperforms its vanilla counterparts: we achieve\na new state-of-the-art F1-score of 72.03 (+1.25) on the Chinese Weibo dataset\nand improved results on a variety of widely used NER benchmarks. Additionally,\nwe show that $k$NN-NER can achieve comparable results to the vanilla NER model\nwith 40\\% less amount of training data. Code available at\n\\url{https://github.com/ShannonAI/KNN-NER}.\n",
                "链接": "https://arxiv.org/abs/2203.17103"
            },
            {
                "文章ID": "73406",
                "标题": "GPT-NER: Named Entity Recognition via Large Language Models",
                "作者": " Shuhe Wang,  Xiaofei Sun,  Xiaoya Li,  Rongbin Ouyang,  Fei Wu,  Tianwei Zhang,  Jiwei Li,  Guoyin Wang",
                "发布日期": "2023-10-10",
                "摘要": "  Despite the fact that large-scale Language Models (LLM) have achieved SOTA\nperformances on a variety of NLP tasks, its performance on NER is still\nsignificantly below supervised baselines. This is due to the gap between the\ntwo tasks the NER and LLMs: the former is a sequence labeling task in nature\nwhile the latter is a text-generation model.\n  In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the\ngap by transforming the sequence labeling task to a generation task that can be\neasily adapted by LLMs e.g., the task of finding location entities in the input\ntext \"Columbus is a city\" is transformed to generate the text sequence\n\"@@Columbus## is a city\", where special tokens @@## marks the entity to\nextract. To efficiently address the \"hallucination\" issue of LLMs, where LLMs\nhave a strong inclination to over-confidently label NULL inputs as entities, we\npropose a self-verification strategy by prompting LLMs to ask itself whether\nthe extracted entities belong to a labeled entity tag.\n  We conduct experiments on five widely adopted NER datasets, and GPT-NER\nachieves comparable performances to fully supervised baselines, which is the\nfirst time as far as we are concerned. More importantly, we find that GPT-NER\nexhibits a greater ability in the low-resource and few-shot setups, when the\namount of training data is extremely scarce, GPT-NER performs significantly\nbetter than supervised models. This demonstrates the capabilities of GPT-NER in\nreal-world NER applications where the number of labeled examples is limited.\n",
                "链接": "https://arxiv.org/abs/2304.10428"
            },
            {
                "文章ID": "81587",
                "标题": "E-NER: Evidential Deep Learning for Trustworthy Named Entity Recognition",
                "作者": " Zhen Zhang,  Mengting Hu,  Shiwan Zhao,  Minlie Huang,  Haotian Wang,  Lemao Liu,  Zhirui Zhang,  Zhe Liu,  Bingzhe Wu",
                "发布日期": "2023-05-30",
                "摘要": "  Most named entity recognition (NER) systems focus on improving model\nperformance, ignoring the need to quantify model uncertainty, which is critical\nto the reliability of NER systems in open environments. Evidential deep\nlearning (EDL) has recently been proposed as a promising solution to explicitly\nmodel predictive uncertainty for classification tasks. However, directly\napplying EDL to NER applications faces two challenges, i.e., the problems of\nsparse entities and OOV/OOD entities in NER tasks. To address these challenges,\nwe propose a trustworthy NER framework named E-NER by introducing two\nuncertainty-guided loss terms to the conventional EDL, along with a series of\nuncertainty-guided training strategies. Experiments show that E-NER can be\napplied to multiple NER paradigms to obtain accurate uncertainty estimation.\nFurthermore, compared to state-of-the-art baselines, the proposed method\nachieves a better OOV/OOD detection performance and better generalization\nability on OOV entities.\n",
                "链接": "https://arxiv.org/abs/2305.17854"
            },
            {
                "文章ID": "108977",
                "标题": "Empirical Study of Zero-Shot NER with ChatGPT",
                "作者": " Tingyu Xie,  Qi Li,  Jian Zhang,  Yan Zhang,  Zuozhu Liu,  Hongwei Wang",
                "发布日期": "2023-10-17",
                "摘要": "  Large language models (LLMs) exhibited powerful capability in various natural\nlanguage processing tasks. This work focuses on exploring LLM performance on\nzero-shot information extraction, with a focus on the ChatGPT and named entity\nrecognition (NER) task. Inspired by the remarkable reasoning capability of LLM\non symbolic and arithmetic reasoning, we adapt the prevalent reasoning methods\nto NER and propose reasoning strategies tailored for NER. First, we explore a\ndecomposed question-answering paradigm by breaking down the NER task into\nsimpler subproblems by labels. Second, we propose syntactic augmentation to\nstimulate the model's intermediate thinking in two ways: syntactic prompting,\nwhich encourages the model to analyze the syntactic structure itself, and tool\naugmentation, which provides the model with the syntactic information generated\nby a parsing tool. Besides, we adapt self-consistency to NER by proposing a\ntwo-stage majority voting strategy, which first votes for the most consistent\nmentions, then the most consistent types. The proposed methods achieve\nremarkable improvements for zero-shot NER across seven benchmarks, including\nChinese and English datasets, and on both domain-specific and general-domain\nscenarios. In addition, we present a comprehensive analysis of the error types\nwith suggestions for optimization directions. We also verify the effectiveness\nof the proposed methods on the few-shot setting and other LLMs.\n",
                "链接": "https://arxiv.org/abs/2310.10035"
            },
            {
                "文章ID": "81674",
                "标题": "Extrinsic Factors Affecting the Accuracy of Biomedical NER",
                "作者": " Zhiyi Li,  Shengjie Zhang,  Yujie Song,  Jungyeul Park",
                "发布日期": "2023-05-30",
                "摘要": "  Biomedical named entity recognition (NER) is a critial task that aims to\nidentify structured information in clinical text, which is often replete with\ncomplex, technical terms and a high degree of variability. Accurate and\nreliable NER can facilitate the extraction and analysis of important biomedical\ninformation, which can be used to improve downstream applications including the\nhealthcare system. However, NER in the biomedical domain is challenging due to\nlimited data availability, as the high expertise, time, and expenses are\nrequired to annotate its data. In this paper, by using the limited data, we\nexplore various extrinsic factors including the corpus annotation scheme, data\naugmentation techniques, semi-supervised learning and Brill transformation, to\nimprove the performance of a NER model on a clinical text dataset (i2b2 2012,\n\\citet{sun-rumshisky-uzuner:2013}). Our experiments demonstrate that these\napproaches can significantly improve the model's F1 score from original 73.74\nto 77.55. Our findings suggest that considering different extrinsic factors and\ncombining these techniques is a promising approach for improving NER\nperformance in the biomedical domain where the size of data is limited.\n",
                "链接": "https://arxiv.org/abs/2305.18152"
            },
            {
                "文章ID": "81620",
                "标题": "ContrastNER: Contrastive-based Prompt Tuning for Few-shot NER",
                "作者": " Amirhossein Layegh,  Amir H. Payberah,  Ahmet Soylu,  Dumitru Roman,  Mihhail Matskin",
                "发布日期": "2023-08-08",
                "摘要": "  Prompt-based language models have produced encouraging results in numerous\napplications, including Named Entity Recognition (NER) tasks. NER aims to\nidentify entities in a sentence and provide their types. However, the strong\nperformance of most available NER approaches is heavily dependent on the design\nof discrete prompts and a verbalizer to map the model-predicted outputs to\nentity categories, which are complicated undertakings. To address these\nchallenges, we present ContrastNER, a prompt-based NER framework that employs\nboth discrete and continuous tokens in prompts and uses a contrastive learning\napproach to learn the continuous prompts and forecast entity types. The\nexperimental results demonstrate that ContrastNER obtains competitive\nperformance to the state-of-the-art NER methods in high-resource settings and\noutperforms the state-of-the-art models in low-resource circumstances without\nrequiring extensive manual prompt engineering and verbalizer design.\n",
                "链接": "https://arxiv.org/abs/2305.17951"
            },
            {
                "文章ID": "54218",
                "标题": "E-NER -- An Annotated Named Entity Recognition Corpus of Legal Text",
                "作者": " Ting Wai Terence Au,  Ingemar J. Cox,  Vasileios Lampos",
                "发布日期": "2022-12-20",
                "摘要": "  Identifying named entities such as a person, location or organization, in\ndocuments can highlight key information to readers. Training Named Entity\nRecognition (NER) models requires an annotated data set, which can be a\ntime-consuming labour-intensive task. Nevertheless, there are publicly\navailable NER data sets for general English. Recently there has been interest\nin developing NER for legal text. However, prior work and experimental results\nreported here indicate that there is a significant degradation in performance\nwhen NER methods trained on a general English data set are applied to legal\ntext. We describe a publicly available legal NER data set, called E-NER, based\non legal company filings available from the US Securities and Exchange\nCommission's EDGAR data set. Training a number of different NER algorithms on\nthe general English CoNLL-2003 corpus but testing on our test collection\nconfirmed significant degradations in accuracy, as measured by the F1-score, of\nbetween 29.4\\% and 60.4\\%, compared to training and testing on the E-NER\ncollection.\n",
                "链接": "https://arxiv.org/abs/2212.09306"
            },
            {
                "文章ID": "82089",
                "标题": "A Multilingual Evaluation of NER Robustness to Adversarial Inputs",
                "作者": " Akshay Srinivasan,  Sowmya Vajjala",
                "发布日期": "2023-05-31",
                "摘要": "  Adversarial evaluations of language models typically focus on English alone.\nIn this paper, we performed a multilingual evaluation of Named Entity\nRecognition (NER) in terms of its robustness to small perturbations in the\ninput. Our results showed the NER models we explored across three languages\n(English, German and Hindi) are not very robust to such changes, as indicated\nby the fluctuations in the overall F1 score as well as in a more fine-grained\nevaluation. With that knowledge, we further explored whether it is possible to\nimprove the existing NER models using a part of the generated adversarial data\nsets as augmented training data to train a new NER model or as fine-tuning data\nto adapt an existing NER model. Our results showed that both these approaches\nimprove performance on the original as well as adversarial test sets. While\nthere is no significant difference between the two approaches for English,\nre-training is significantly better than fine-tuning for German and Hindi.\n",
                "链接": "https://arxiv.org/abs/2305.18933"
            },
            {
                "文章ID": "78785",
                "标题": "Enhancing Few-shot NER with Prompt Ordering based Data Augmentation",
                "作者": " Huiming Wang,  Liying Cheng,  Wenxuan Zhang,  De Wen Soh,  Lidong Bing",
                "发布日期": "2023-05-22",
                "摘要": "  Recently, data augmentation (DA) methods have been proven to be effective for\npre-trained language models (PLMs) in low-resource settings, including few-shot\nnamed entity recognition (NER). However, conventional NER DA methods are mostly\naimed at sequence labeling models, i.e., token-level classification, and few\nare compatible with unified autoregressive generation frameworks, which can\nhandle a wider range of NER tasks, such as nested NER. Furthermore, these\ngeneration frameworks have a strong assumption that the entities will appear in\nthe target sequence with the same left-to-right order as the source sequence.\nIn this paper, we claim that there is no need to keep this strict order, and\nmore diversified but reasonable target entity sequences can be provided during\nthe training stage as a novel DA method. Nevertheless, a naive mixture of\naugmented data can confuse the model since one source sequence will then be\npaired with different target sequences. Therefore, we propose a simple but\neffective Prompt Ordering based Data Augmentation (PODA) method to improve the\ntraining of unified autoregressive generation frameworks under few-shot NER\nscenarios. Experimental results on three public NER datasets and further\nanalyses demonstrate the effectiveness of our approach.\n",
                "链接": "https://arxiv.org/abs/2305.11791"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下利用gpt4做评测指标优缺点的文章",
            "type": "5"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找使用GPT4v完成多模态智能体的论文",
            "type": "5"
        },
        "results": []
    },
    {
        "question": {
            "question": "请找出使用Transformer模型并在大规模数据集上进行预训练的论文。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "113210",
                "标题": "Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in\n  Transformer Models",
                "作者": " Steve Yadlowsky,  Lyric Doshi,  Nilesh Tripuraneni",
                "发布日期": "2023-11-03",
                "摘要": "  Transformer models, notably large language models (LLMs), have the remarkable\nability to perform in-context learning (ICL) -- to perform new tasks when\nprompted with unseen input-output examples without any explicit model training.\nIn this work, we study how effectively transformers can bridge between their\npretraining data mixture, comprised of multiple distinct task families, to\nidentify and learn new tasks in-context which are both inside and outside the\npretraining distribution. Building on previous work, we investigate this\nquestion in a controlled setting, where we study transformer models trained on\nsequences of $(x, f(x))$ pairs rather than natural language. Our empirical\nresults show transformers demonstrate near-optimal unsupervised model selection\ncapabilities, in their ability to first in-context identify different task\nfamilies and in-context learn within them when the task families are\nwell-represented in their pretraining data. However when presented with tasks\nor functions which are out-of-domain of their pretraining data, we demonstrate\nvarious failure modes of transformers and degradation of their generalization\nfor even simple extrapolation tasks. Together our results highlight that the\nimpressive ICL abilities of high-capacity sequence models may be more closely\ntied to the coverage of their pretraining data mixtures than inductive biases\nthat create fundamental generalization capabilities.\n",
                "链接": "https://arxiv.org/abs/2311.00871"
            },
            {
                "文章ID": "35894",
                "标题": "TransPolymer: a Transformer-based language model for polymer property\n  predictions",
                "作者": " Changwen Xu,  Yuyang Wang,  Amir Barati Farimani",
                "发布日期": "2023-04-27",
                "摘要": "  Accurate and efficient prediction of polymer properties is of great\nsignificance in polymer design. Conventionally, expensive and time-consuming\nexperiments or simulations are required to evaluate polymer functions.\nRecently, Transformer models, equipped with self-attention mechanisms, have\nexhibited superior performance in natural language processing. However, such\nmethods have not been investigated in polymer sciences. Herein, we report\nTransPolymer, a Transformer-based language model for polymer property\nprediction. Our proposed polymer tokenizer with chemical awareness enables\nlearning representations from polymer sequences. Rigorous experiments on ten\npolymer property prediction benchmarks demonstrate the superior performance of\nTransPolymer. Moreover, we show that TransPolymer benefits from pretraining on\nlarge unlabeled dataset via Masked Language Modeling. Experimental results\nfurther manifest the important role of self-attention in modeling polymer\nsequences. We highlight this model as a promising computational tool for\npromoting rational polymer design and understanding structure-property\nrelationships from a data science view.\n",
                "链接": "https://arxiv.org/abs/2209.01307"
            },
            {
                "文章ID": "75455",
                "标题": "A Lightweight CNN-Transformer Model for Learning Traveling Salesman\n  Problems",
                "作者": " Minseop Jung,  Jaeseung Lee,  Jibum Kim",
                "发布日期": "2023-05-04",
                "摘要": "  Transformer-based models show state-of-the-art performance even for\nlarge-scale Traveling Salesman Problems (TSPs). However, they are based on\nfully-connected attention models and suffer from large computational complexity\nand GPU memory usage. We propose a lightweight CNN-Transformer model based on a\nCNN embedding layer and partial self-attention. Our CNN-Transformer model is\nable to better learn spatial features from input data using a CNN embedding\nlayer compared with the standard Transformer models. It also removes\nconsiderable redundancy in fully connected attention models using the proposed\npartial self-attention. Experiments show that the proposed model outperforms\nother state-of-the-art Transformer-based models in terms of TSP solution\nquality, GPU memory usage, and inference time. Our model consumes approximately\n20% less GPU memory usage and has 45% faster inference time compared with other\nstate-of-the-art Transformer-based models. Our code is publicly available at\nhttps://github.com/cm8908/CNN_Transformer3\n",
                "链接": "https://arxiv.org/abs/2305.01883"
            },
            {
                "文章ID": "102081",
                "标题": "Large-Vocabulary 3D Diffusion Model with Transformer",
                "作者": " Ziang Cao,  Fangzhou Hong,  Tong Wu,  Liang Pan,  Ziwei Liu",
                "发布日期": "2023-09-18",
                "摘要": "  Creating diverse and high-quality 3D assets with an automatic generative\nmodel is highly desirable. Despite extensive efforts on 3D generation, most\nexisting works focus on the generation of a single category or a few\ncategories. In this paper, we introduce a diffusion-based feed-forward\nframework for synthesizing massive categories of real-world 3D objects with a\nsingle generative model. Notably, there are three major challenges for this\nlarge-vocabulary 3D generation: a) the need for expressive yet efficient 3D\nrepresentation; b) large diversity in geometry and texture across categories;\nc) complexity in the appearances of real-world objects. To this end, we propose\na novel triplane-based 3D-aware Diffusion model with TransFormer, DiffTF, for\nhandling challenges via three aspects. 1) Considering efficiency and\nrobustness, we adopt a revised triplane representation and improve the fitting\nspeed and accuracy. 2) To handle the drastic variations in geometry and\ntexture, we regard the features of all 3D objects as a combination of\ngeneralized 3D knowledge and specialized 3D features. To extract generalized 3D\nknowledge from diverse categories, we propose a novel 3D-aware transformer with\nshared cross-plane attention. It learns the cross-plane relations across\ndifferent planes and aggregates the generalized 3D knowledge with specialized\n3D features. 3) In addition, we devise the 3D-aware encoder/decoder to enhance\nthe generalized 3D knowledge in the encoded triplanes for handling categories\nwith complex appearances. Extensive experiments on ShapeNet and OmniObject3D\n(over 200 diverse real-world categories) convincingly demonstrate that a single\nDiffTF model achieves state-of-the-art large-vocabulary 3D object generation\nperformance with large diversity, rich semantics, and high quality.\n",
                "链接": "https://arxiv.org/abs/2309.07920"
            },
            {
                "文章ID": "106729",
                "标题": "Quantized Transformer Language Model Implementations on Edge Devices",
                "作者": " Mohammad Wali Ur Rahman,  Murad Mehrab Abrar,  Hunter Gibbons Copening,  Salim Hariri,  Sicong Shao,  Pratik Satam,  Soheil Salehi",
                "发布日期": "2023-10-09",
                "摘要": "  Large-scale transformer-based models like the Bidirectional Encoder\nRepresentations from Transformers (BERT) are widely used for Natural Language\nProcessing (NLP) applications, wherein these models are initially pre-trained\nwith a large corpus with millions of parameters and then fine-tuned for a\ndownstream NLP task. One of the major limitations of these large-scale models\nis that they cannot be deployed on resource-constrained devices due to their\nlarge model size and increased inference latency. In order to overcome these\nlimitations, such large-scale models can be converted to an optimized\nFlatBuffer format, tailored for deployment on resource-constrained edge\ndevices. Herein, we evaluate the performance of such FlatBuffer transformed\nMobileBERT models on three different edge devices, fine-tuned for Reputation\nanalysis of English language tweets in the RepLab 2013 dataset. In addition,\nthis study encompassed an evaluation of the deployed models, wherein their\nlatency, performance, and resource efficiency were meticulously assessed. Our\nexperiment results show that, compared to the original BERT large model, the\nconverted and quantized MobileBERT models have 160$\\times$ smaller footprints\nfor a 4.1% drop in accuracy while analyzing at least one tweet per second on\nedge devices. Furthermore, our study highlights the privacy-preserving aspect\nof TinyML systems as all data is processed locally within a serverless\nenvironment.\n",
                "链接": "https://arxiv.org/abs/2310.03971"
            },
            {
                "文章ID": "10273",
                "标题": "ODE Transformer: An Ordinary Differential Equation-Inspired Model for\n  Sequence Generation",
                "作者": " Bei Li,  Quan Du,  Tao Zhou,  Yi Jing,  Shuhan Zhou,  Xin Zeng,  Tong Xiao,  JingBo Zhu,  Xuebo Liu,  Min Zhang",
                "发布日期": "2022-04-13",
                "摘要": "  Residual networks are an Euler discretization of solutions to Ordinary\nDifferential Equations (ODE). This paper explores a deeper relationship between\nTransformer and numerical ODE methods. We first show that a residual block of\nlayers in Transformer can be described as a higher-order solution to ODE.\nInspired by this, we design a new architecture, {\\it ODE Transformer}, which is\nanalogous to the Runge-Kutta method that is well motivated in ODE. As a natural\nextension to Transformer, ODE Transformer is easy to implement and efficient to\nuse. Experimental results on the large-scale machine translation, abstractive\nsummarization, and grammar error correction tasks demonstrate the high\ngenericity of ODE Transformer. It can gain large improvements in model\nperformance over strong baselines (e.g., 30.77 and 44.11 BLEU scores on the\nWMT'14 English-German and English-French benchmarks) at a slight cost in\ninference efficiency.\n",
                "链接": "https://arxiv.org/abs/2203.09176"
            },
            {
                "文章ID": "56812",
                "标题": "NarrowBERT: Accelerating Masked Language Model Pretraining and Inference",
                "作者": " Haoxin Li,  Phillip Keung,  Daniel Cheng,  Jungo Kasai,  Noah A. Smith",
                "发布日期": "2023-06-07",
                "摘要": "  Large-scale language model pretraining is a very successful form of\nself-supervised learning in natural language processing, but it is increasingly\nexpensive to perform as the models and pretraining corpora have become larger\nover time. We propose NarrowBERT, a modified transformer encoder that increases\nthe throughput for masked language model pretraining by more than $2\\times$.\nNarrowBERT sparsifies the transformer model such that the self-attention\nqueries and feedforward layers only operate on the masked tokens of each\nsentence during pretraining, rather than all of the tokens as with the usual\ntransformer encoder. We also show that NarrowBERT increases the throughput at\ninference time by as much as $3.5\\times$ with minimal (or no) performance\ndegradation on sentence encoding tasks like MNLI. Finally, we examine the\nperformance of NarrowBERT on the IMDB and Amazon reviews classification and\nCoNLL NER tasks and show that it is also comparable to standard BERT\nperformance.\n",
                "链接": "https://arxiv.org/abs/2301.04761"
            },
            {
                "文章ID": "73751",
                "标题": "Transformer-Based Language Model Surprisal Predicts Human Reading Times\n  Best with About Two Billion Training Tokens",
                "作者": " Byung-Doh Oh,  William Schuler",
                "发布日期": "2023-10-24",
                "摘要": "  Recent psycholinguistic studies have drawn conflicting conclusions about the\nrelationship between the quality of a language model and the ability of its\nsurprisal estimates to predict human reading times, which has been speculated\nto be due to the large gap in both the amount of training data and model\ncapacity across studies. The current work aims to consolidate these findings by\nevaluating surprisal estimates from Transformer-based language model variants\nthat vary systematically in the amount of training data and model capacity on\ntheir ability to predict human reading times. The results show that surprisal\nestimates from most variants with contemporary model capacities provide the\nbest fit after seeing about two billion training tokens, after which they begin\nto diverge from humanlike expectations. Additionally, newly-trained smaller\nmodel variants reveal a 'tipping point' at convergence, after which the\ndecrease in language model perplexity begins to result in poorer fits to human\nreading times. These results suggest that the massive amount of training data\nis mainly responsible for the poorer fit achieved by surprisal from larger\npre-trained language models, and that a certain degree of model capacity is\nnecessary for Transformer-based language models to capture humanlike\nexpectations.\n",
                "链接": "https://arxiv.org/abs/2304.11389"
            },
            {
                "文章ID": "111908",
                "标题": "FaultSeg Swin-UNETR: Transformer-Based Self-Supervised Pretraining Model\n  for Fault Recognition",
                "作者": " Zeren Zhang,  Ran Chen,  Jinwen Ma",
                "发布日期": "2023-10-30",
                "摘要": "  This paper introduces an approach to enhance seismic fault recognition\nthrough self-supervised pretraining. Seismic fault interpretation holds great\nsignificance in the fields of geophysics and geology. However, conventional\nmethods for seismic fault recognition encounter various issues, including\ndependence on data quality and quantity, as well as susceptibility to\ninterpreter subjectivity. Currently, automated fault recognition methods\nproposed based on small synthetic datasets experience performance degradation\nwhen applied to actual seismic data. To address these challenges, we have\nintroduced the concept of self-supervised learning, utilizing a substantial\namount of relatively easily obtainable unlabeled seismic data for pretraining.\nSpecifically, we have employed the Swin Transformer model as the core network\nand employed the SimMIM pretraining task to capture unique features related to\ndiscontinuities in seismic data. During the fine-tuning phase, inspired by edge\ndetection techniques, we have also refined the structure of the Swin-UNETR\nmodel, enabling multiscale decoding and fusion for more effective fault\ndetection. Experimental results demonstrate that our proposed method attains\nstate-of-the-art performance on the Thebe dataset, as measured by the OIS and\nODS metrics.\n",
                "链接": "https://arxiv.org/abs/2310.17974"
            },
            {
                "文章ID": "53733",
                "标题": "Sim-to-Real Transfer for Quadrupedal Locomotion via Terrain Transformer",
                "作者": " Hang Lai,  Weinan Zhang,  Xialin He,  Chen Yu,  Zheng Tian,  Yong Yu,  Jun Wang",
                "发布日期": "2023-03-22",
                "摘要": "  Deep reinforcement learning has recently emerged as an appealing alternative\nfor legged locomotion over multiple terrains by training a policy in physical\nsimulation and then transferring it to the real world (i.e., sim-to-real\ntransfer). Despite considerable progress, the capacity and scalability of\ntraditional neural networks are still limited, which may hinder their\napplications in more complex environments. In contrast, the Transformer\narchitecture has shown its superiority in a wide range of large-scale sequence\nmodeling tasks, including natural language processing and decision-making\nproblems. In this paper, we propose Terrain Transformer (TERT), a high-capacity\nTransformer model for quadrupedal locomotion control on various terrains.\nFurthermore, to better leverage Transformer in sim-to-real scenarios, we\npresent a novel two-stage training framework consisting of an offline\npretraining stage and an online correction stage, which can naturally integrate\nTransformer with privileged training. Extensive experiments in simulation\ndemonstrate that TERT outperforms state-of-the-art baselines on different\nterrains in terms of return, energy consumption and control smoothness. In\nfurther real-world validation, TERT successfully traverses nine challenging\nterrains, including sand pit and stair down, which can not be accomplished by\nstrong baselines.\n",
                "链接": "https://arxiv.org/abs/2212.07740"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下对使用GPT生成数据集的训练步骤进行改进的论文。",
            "type": "5"
        },
        "results": []
    },
    {
        "question": {
            "question": "请找到利用clip做开放词汇检测的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "69202",
                "标题": "EVA-CLIP: Improved Training Techniques for CLIP at Scale",
                "作者": " Quan Sun,  Yuxin Fang,  Ledell Wu,  Xinlong Wang,  Yue Cao",
                "发布日期": "2023-03-28",
                "摘要": "  Contrastive language-image pre-training, CLIP for short, has gained\nincreasing attention for its potential in various scenarios. In this paper, we\npropose EVA-CLIP, a series of models that significantly improve the efficiency\nand effectiveness of CLIP training. Our approach incorporates new techniques\nfor representation learning, optimization, and augmentation, enabling EVA-CLIP\nto achieve superior performance compared to previous CLIP models with the same\nnumber of parameters but significantly smaller training costs. Notably, our\nlargest 5.0B-parameter EVA-02-CLIP-E/14+ with only 9 billion seen samples\nachieves 82.0 zero-shot top-1 accuracy on ImageNet-1K val. A smaller\nEVA-02-CLIP-L/14+ with only 430 million parameters and 6 billion seen samples\nachieves 80.4 zero-shot top-1 accuracy on ImageNet-1K val. To facilitate open\naccess and open research, we release the complete suite of EVA-CLIP to the\ncommunity at https://github.com/baaivision/EVA/tree/master/EVA-CLIP.\n",
                "链接": "https://arxiv.org/abs/2303.15389"
            },
            {
                "文章ID": "1347",
                "标题": "CLIP-TD: CLIP Targeted Distillation for Vision-Language Tasks",
                "作者": " Zhecan Wang,  Noel Codella,  Yen-Chun Chen,  Luowei Zhou,  Jianwei Yang,  Xiyang Dai,  Bin Xiao,  Haoxuan You,  Shih-Fu Chang,  Lu Yuan",
                "发布日期": "2023-01-02",
                "摘要": "  Contrastive language-image pretraining (CLIP) links vision and language\nmodalities into a unified embedding space, yielding the tremendous potential\nfor vision-language (VL) tasks. While early concurrent works have begun to\nstudy this potential on a subset of tasks, important questions remain: 1) What\nis the benefit of CLIP on unstudied VL tasks? 2) Does CLIP provide benefit in\nlow-shot or domain-shifted scenarios? 3) Can CLIP improve existing approaches\nwithout impacting inference or pretraining complexity? In this work, we seek to\nanswer these questions through two key contributions. First, we introduce an\nevaluation protocol that includes Visual Commonsense Reasoning (VCR), Visual\nEntailment (SNLI-VE), and Visual Question Answering (VQA), across a variety of\ndata availability constraints and conditions of domain shift. Second, we\npropose an approach, named CLIP Targeted Distillation (CLIP-TD), to\nintelligently distill knowledge from CLIP into existing architectures using a\ndynamically weighted objective applied to adaptively selected tokens per\ninstance. Experiments demonstrate that our proposed CLIP-TD leads to\nexceptional gains in the low-shot (up to 51.9%) and domain-shifted (up to\n71.3%) conditions of VCR, while simultaneously improving performance under\nstandard fully-supervised conditions (up to 2%), achieving state-of-art\nperformance on VCR compared to other single models that are pretrained with\nimage-text data only. On SNLI-VE, CLIP-TD produces significant gains in\nlow-shot conditions (up to 6.6%) as well as fully supervised (up to 3%). On\nVQA, CLIP-TD provides improvement in low-shot (up to 9%), and in\nfully-supervised (up to 1.3%). Finally, CLIP-TD outperforms concurrent works\nutilizing CLIP for finetuning, as well as baseline naive distillation\napproaches. Code will be made available.\n",
                "链接": "https://arxiv.org/abs/2201.05729"
            },
            {
                "文章ID": "92757",
                "标题": "CLIP-KD: An Empirical Study of Distilling CLIP Models",
                "作者": " Chuanguang Yang,  Zhulin An,  Libo Huang,  Junyu Bi,  Xinqiang Yu,  Han Yang,  Yongjun Xu",
                "发布日期": "2023-07-25",
                "摘要": "  CLIP has become a promising language-supervised visual pre-training framework\nand achieves excellent performance over a wide range of tasks. This paper aims\nto distill small CLIP models supervised by a large teacher CLIP model. We\npropose several distillation strategies, including relation, feature, gradient\nand contrastive paradigm, to examine the impact on CLIP distillation. We show\nthat the simplest feature mimicry with MSE loss performs best. Moreover,\ninteractive contrastive learning and relation-based distillation are also\ncritical in performance improvement. We apply the unified method to distill\nseveral student networks trained on 15 million (image, text) pairs.\nDistillation improves the student CLIP models consistently over zero-shot\nImageNet classification and cross-modal retrieval benchmarks. We hope our\nempirical study will become an important baseline for future CLIP distillation\nresearch. The code is available at \\url{https://github.com/winycg/CLIP-KD}.\n",
                "链接": "https://arxiv.org/abs/2307.12732"
            },
            {
                "文章ID": "123828",
                "标题": "CLIP-DINOiser: Teaching CLIP a few DINO tricks",
                "作者": " Monika Wysoczańska,  Oriane Siméoni,  Michaël Ramamonjisoa,  Andrei Bursuc,  Tomasz Trzciński,  Patrick Pérez",
                "发布日期": "2023-12-20",
                "摘要": "  The popular CLIP model displays impressive zero-shot capabilities thanks to\nits seamless interaction with arbitrary text prompts. However, its lack of\nspatial awareness makes it unsuitable for dense computer vision tasks, e.g.,\nsemantic segmentation, without an additional fine-tuning step that often uses\nannotations and can potentially suppress its original open-vocabulary\nproperties. Meanwhile, self-supervised representation methods have demonstrated\ngood localization properties without human-made annotations nor explicit\nsupervision. In this work, we take the best of both worlds and propose a\nzero-shot open-vocabulary semantic segmentation method, which does not require\nany annotations. We propose to locally improve dense MaskCLIP features,\ncomputed with a simple modification of CLIP's last pooling layer, by\nintegrating localization priors extracted from self-supervised features. By\ndoing so, we greatly improve the performance of MaskCLIP and produce smooth\noutputs. Moreover, we show that the used self-supervised feature properties can\ndirectly be learnt from CLIP features therefore allowing us to obtain the best\nresults with a single pass through CLIP model. Our method CLIP-DINOiser needs\nonly a single forward pass of CLIP and two light convolutional layers at\ninference, no extra supervision nor extra memory and reaches state-of-the-art\nresults on challenging and fine-grained benchmarks such as COCO, Pascal\nContext, Cityscapes and ADE20k. The code to reproduce our results is available\nat https://github.com/wysoczanska/clip_dinoiser.\n",
                "链接": "https://arxiv.org/abs/2312.12359"
            },
            {
                "文章ID": "54009",
                "标题": "Attentive Mask CLIP",
                "作者": " Yifan Yang,  Weiquan Huang,  Yixuan Wei,  Houwen Peng,  Xinyang Jiang,  Huiqiang Jiang,  Fangyun Wei,  Yin Wang,  Han Hu,  Lili Qiu,  Yuqing Yang",
                "发布日期": "2023-10-10",
                "摘要": "  Image token removal is an efficient augmentation strategy for reducing the\ncost of computing image features. However, this efficient augmentation strategy\nhas been found to adversely affect the accuracy of CLIP-based training. We\nhypothesize that removing a large portion of image tokens may improperly\ndiscard the semantic content associated with a given text description, thus\nconstituting an incorrect pairing target in CLIP training. To address this\nissue, we propose an attentive token removal approach for CLIP training, which\nretains tokens with a high semantic correlation to the text description. The\ncorrelation scores are computed in an online fashion using the EMA version of\nthe visual encoder. Our experiments show that the proposed attentive masking\napproach performs better than the previous method of random token removal for\nCLIP training. The approach also makes it efficient to apply multiple\naugmentation views to the image, as well as introducing instance contrastive\nlearning tasks between these views into the CLIP framework. Compared to other\nCLIP improvements that combine different pre-training targets such as SLIP and\nMaskCLIP, our method is not only more effective, but also much more efficient.\nSpecifically, using ViT-B and YFCC-15M dataset, our approach achieves $43.9\\%$\ntop-1 accuracy on ImageNet-1K zero-shot classification, as well as $62.7/42.1$\nand $38.0/23.2$ I2T/T2I retrieval accuracy on Flickr30K and MS COCO, which are\n$+1.1\\%$, $+5.5/+0.9$, and $+4.4/+1.3$ higher than the SLIP method, while being\n$2.30\\times$ faster. An efficient version of our approach running $1.16\\times$\nfaster than the plain CLIP model achieves significant gains of $+5.3\\%$,\n$+11.3/+8.0$, and $+9.5/+4.9$ on these benchmarks.\n",
                "链接": "https://arxiv.org/abs/2212.08653"
            },
            {
                "文章ID": "120761",
                "标题": "Alpha-CLIP: A CLIP Model Focusing on Wherever You Want",
                "作者": " Zeyi Sun,  Ye Fang,  Tong Wu,  Pan Zhang,  Yuhang Zang,  Shu Kong,  Yuanjun Xiong,  Dahua Lin,  Jiaqi Wang",
                "发布日期": "2023-12-15",
                "摘要": "  Contrastive Language-Image Pre-training (CLIP) plays an essential role in\nextracting valuable content information from images across diverse tasks. It\naligns textual and visual modalities to comprehend the entire image, including\nall the details, even those irrelevant to specific tasks. However, for a finer\nunderstanding and controlled editing of images, it becomes crucial to focus on\nspecific regions of interest, which can be indicated as points, masks, or boxes\nby humans or perception models. To fulfill the requirements, we introduce\nAlpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to\nsuggest attentive regions and fine-tuned with constructed millions of RGBA\nregion-text pairs. Alpha-CLIP not only preserves the visual recognition ability\nof CLIP but also enables precise control over the emphasis of image contents.\nIt demonstrates effectiveness in various tasks, including but not limited to\nopen-world recognition, multimodal large language models, and conditional 2D /\n3D generation. It has a strong potential to serve as a versatile tool for\nimage-related tasks.\n",
                "链接": "https://arxiv.org/abs/2312.03818"
            },
            {
                "文章ID": "41976",
                "标题": "CLIP also Understands Text: Prompting CLIP for Phrase Understanding",
                "作者": " An Yan,  Jiacheng Li,  Wanrong Zhu,  Yujie Lu,  William Yang Wang,  Julian McAuley",
                "发布日期": "2022-10-13",
                "摘要": "  Contrastive Language-Image Pretraining (CLIP) efficiently learns visual\nconcepts by pre-training with natural language supervision. CLIP and its visual\nencoder have been explored on various vision and language tasks and achieve\nstrong zero-shot or transfer learning performance. However, the application of\nits text encoder solely for text understanding has been less explored. In this\npaper, we find that the text encoder of CLIP actually demonstrates strong\nability for phrase understanding, and can even significantly outperform popular\nlanguage models such as BERT with a properly designed prompt. Extensive\nexperiments validate the effectiveness of our method across different datasets\nand domains on entity clustering and entity set expansion tasks.\n",
                "链接": "https://arxiv.org/abs/2210.05836"
            },
            {
                "文章ID": "95815",
                "标题": "AD-CLIP: Adapting Domains in Prompt Space Using CLIP",
                "作者": " Mainak Singha,  Harsh Pal,  Ankit Jha,  Biplab Banerjee",
                "发布日期": "2023-08-11",
                "摘要": "  Although deep learning models have shown impressive performance on supervised\nlearning tasks, they often struggle to generalize well when the training\n(source) and test (target) domains differ. Unsupervised domain adaptation (DA)\nhas emerged as a popular solution to this problem. However, current DA\ntechniques rely on visual backbones, which may lack semantic richness. Despite\nthe potential of large-scale vision-language foundation models like CLIP, their\neffectiveness for DA has yet to be fully explored. To address this gap, we\nintroduce AD-CLIP, a domain-agnostic prompt learning strategy for CLIP that\naims to solve the DA problem in the prompt space. We leverage the frozen vision\nbackbone of CLIP to extract both image style (domain) and content information,\nwhich we apply to learn prompt tokens. Our prompts are designed to be\ndomain-invariant and class-generalizable, by conditioning prompt learning on\nimage style and content features simultaneously. We use standard supervised\ncontrastive learning in the source domain, while proposing an entropy\nminimization strategy to align domains in the embedding space given the target\ndomain data. We also consider a scenario where only target domain samples are\navailable during testing, without any source domain data, and propose a\ncross-domain style mapping network to hallucinate domain-agnostic tokens. Our\nextensive experiments on three benchmark DA datasets demonstrate the\neffectiveness of AD-CLIP compared to existing literature.\n",
                "链接": "https://arxiv.org/abs/2308.05659"
            },
            {
                "文章ID": "32031",
                "标题": "Per-Clip Video Object Segmentation",
                "作者": " Kwanyong Park,  Sanghyun Woo,  Seoung Wug Oh,  In So Kweon,  Joon-Young Lee",
                "发布日期": "2022-08-04",
                "摘要": "  Recently, memory-based approaches show promising results on semi-supervised\nvideo object segmentation. These methods predict object masks frame-by-frame\nwith the help of frequently updated memory of the previous mask. Different from\nthis per-frame inference, we investigate an alternative perspective by treating\nvideo object segmentation as clip-wise mask propagation. In this per-clip\ninference scheme, we update the memory with an interval and simultaneously\nprocess a set of consecutive frames (i.e. clip) between the memory updates. The\nscheme provides two potential benefits: accuracy gain by clip-level\noptimization and efficiency gain by parallel computation of multiple frames. To\nthis end, we propose a new method tailored for the per-clip inference.\nSpecifically, we first introduce a clip-wise operation to refine the features\nbased on intra-clip correlation. In addition, we employ a progressive matching\nmechanism for efficient information-passing within a clip. With the synergy of\ntwo modules and a newly proposed per-clip based training, our network achieves\nstate-of-the-art performance on Youtube-VOS 2018/2019 val (84.6% and 84.6%) and\nDAVIS 2016/2017 val (91.9% and 86.1%). Furthermore, our model shows a great\nspeed-accuracy trade-off with varying memory update intervals, which leads to\nhuge flexibility.\n",
                "链接": "https://arxiv.org/abs/2208.01924"
            },
            {
                "文章ID": "72809",
                "标题": "DisCo-CLIP: A Distributed Contrastive Loss for Memory Efficient CLIP\n  Training",
                "作者": " Yihao Chen,  Xianbiao Qi,  Jianan Wang,  Lei Zhang",
                "发布日期": "2023-04-18",
                "摘要": "  We propose DisCo-CLIP, a distributed memory-efficient CLIP training approach,\nto reduce the memory consumption of contrastive loss when training contrastive\nlearning models. Our approach decomposes the contrastive loss and its gradient\ncomputation into two parts, one to calculate the intra-GPU gradients and the\nother to compute the inter-GPU gradients. According to our decomposition, only\nthe intra-GPU gradients are computed on the current GPU, while the inter-GPU\ngradients are collected via all_reduce from other GPUs instead of being\nrepeatedly computed on every GPU. In this way, we can reduce the GPU memory\nconsumption of contrastive loss computation from $\\bigO(B^2)$ to\n$\\bigO(\\frac{B^2}{N})$, where $B$ and $N$ are the batch size and the number of\nGPUs used for training. Such a distributed solution is mathematically\nequivalent to the original non-distributed contrastive loss computation,\nwithout sacrificing any computation accuracy. It is particularly efficient for\nlarge-batch CLIP training. For instance, DisCo-CLIP can enable contrastive\ntraining of a ViT-B/32 model with a batch size of 32K or 196K using 8 or 64\nA100 40GB GPUs, compared with the original CLIP solution which requires 128\nA100 40GB GPUs to train a ViT-B/32 model with a batch size of 32K. The code\nwill be released at https://github.com/IDEA-Research/DisCo-CLIP\n",
                "链接": "https://arxiv.org/abs/2304.08480"
            },
            {
                "文章ID": "104220",
                "标题": "CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic\n  Segmentation For-Free",
                "作者": " Monika Wysoczańska,  Michaël Ramamonjisoa,  Tomasz Trzciński,  Oriane Siméoni",
                "发布日期": "2023-11-29",
                "摘要": "  The emergence of CLIP has opened the way for open-world image perception. The\nzero-shot classification capabilities of the model are impressive but are\nharder to use for dense tasks such as image segmentation. Several methods have\nproposed different modifications and learning schemes to produce dense output.\nInstead, we propose in this work an open-vocabulary semantic segmentation\nmethod, dubbed CLIP-DIY, which does not require any additional training or\nannotations, but instead leverages existing unsupervised object localization\napproaches. In particular, CLIP-DIY is a multi-scale approach that directly\nexploits CLIP classification abilities on patches of different sizes and\naggregates the decision in a single map. We further guide the segmentation\nusing foreground/background scores obtained using unsupervised object\nlocalization methods. With our method, we obtain state-of-the-art zero-shot\nsemantic segmentation results on PASCAL VOC and perform on par with the best\nmethods on COCO. The code is available at\nhttp://github.com/wysoczanska/clip-diy\n",
                "链接": "https://arxiv.org/abs/2309.14289"
            },
            {
                "文章ID": "9457",
                "标题": "One-stage Video Instance Segmentation: From Frame-in Frame-out to\n  Clip-in Clip-out",
                "作者": " Minghan Li,  Lei Zhang",
                "发布日期": "2022-03-15",
                "摘要": "  Many video instance segmentation (VIS) methods partition a video sequence\ninto individual frames to detect and segment objects frame by frame. However,\nsuch a frame-in frame-out (FiFo) pipeline is ineffective to exploit the\ntemporal information. Based on the fact that adjacent frames in a short clip\nare highly coherent in content, we propose to extend the one-stage FiFo\nframework to a clip-in clip-out (CiCo) one, which performs VIS clip by clip.\nSpecifically, we stack FPN features of all frames in a short video clip to\nbuild a spatio-temporal feature cube, and replace the 2D conv layers in the\nprediction heads and the mask branch with 3D conv layers, forming clip-level\nprediction heads (CPH) and clip-level mask heads (CMH). Then the clip-level\nmasks of an instance can be generated by feeding its box-level predictions from\nCPH and clip-level features from CMH into a small fully convolutional network.\nA clip-level segmentation loss is proposed to ensure that the generated\ninstance masks are temporally coherent in the clip. The proposed CiCo strategy\nis free of inter-frame alignment, and can be easily embedded into existing FiFo\nbased VIS approaches. To validate the generality and effectiveness of our CiCo\nstrategy, we apply it to two representative FiFo methods, Yolact\n\\cite{bolya2019yolact} and CondInst \\cite{tian2020conditional}, resulting in\ntwo new one-stage VIS models, namely CiCo-Yolact and CiCo-CondInst, which\nachieve 37.1/37.3\\%, 35.2/35.4\\% and 17.2/18.0\\% mask AP using the ResNet50\nbackbone, and 41.8/41.4\\%, 38.0/38.9\\% and 18.0/18.2\\% mask AP using the Swin\nTransformer tiny backbone on YouTube-VIS 2019, 2021 and OVIS valid sets,\nrespectively, recording new state-of-the-arts. Code and video demos of CiCo can\nbe found at \\url{https://github.com/MinghanLi/CiCo}.\n",
                "链接": "https://arxiv.org/abs/2203.06421"
            },
            {
                "文章ID": "122830",
                "标题": "TF-CLIP: Learning Text-free CLIP for Video-based Person\n  Re-Identification",
                "作者": " Chenyang Yu,  Xuehu Liu,  Yingquan Wang,  Pingping Zhang,  Huchuan Lu",
                "发布日期": "2023-12-18",
                "摘要": "  Large-scale language-image pre-trained models (e.g., CLIP) have shown\nsuperior performances on many cross-modal retrieval tasks. However, the problem\nof transferring the knowledge learned from such models to video-based person\nre-identification (ReID) has barely been explored. In addition, there is a lack\nof decent text descriptions in current ReID benchmarks. To address these\nissues, in this work, we propose a novel one-stage text-free CLIP-based\nlearning framework named TF-CLIP for video-based person ReID. More\nspecifically, we extract the identity-specific sequence feature as the\nCLIP-Memory to replace the text feature. Meanwhile, we design a\nSequence-Specific Prompt (SSP) module to update the CLIP-Memory online. To\ncapture temporal information, we further propose a Temporal Memory Diffusion\n(TMD) module, which consists of two key components: Temporal Memory\nConstruction (TMC) and Memory Diffusion (MD). Technically, TMC allows the\nframe-level memories in a sequence to communicate with each other, and to\nextract temporal information based on the relations within the sequence. MD\nfurther diffuses the temporal memories to each token in the original features\nto obtain more robust sequence features. Extensive experiments demonstrate that\nour proposed method shows much better results than other state-of-the-art\nmethods on MARS, LS-VID and iLIDS-VID. The code is available at\nhttps://github.com/AsuradaYuci/TF-CLIP.\n",
                "链接": "https://arxiv.org/abs/2312.09627"
            },
            {
                "文章ID": "97890",
                "标题": "LCCo: Lending CLIP to Co-Segmentation",
                "作者": " Xin Duan,  Yan Yang,  Liyuan Pan,  Xiabi Liu",
                "发布日期": "2023-08-23",
                "摘要": "  This paper studies co-segmenting the common semantic object in a set of\nimages. Existing works either rely on carefully engineered networks to mine the\nimplicit semantic information in visual features or require extra data (i.e.,\nclassification labels) for training. In this paper, we leverage the contrastive\nlanguage-image pre-training framework (CLIP) for the task. With a backbone\nsegmentation network that independently processes each image from the set, we\nintroduce semantics from CLIP into the backbone features, refining them in a\ncoarse-to-fine manner with three key modules: i) an image set feature\ncorrespondence module, encoding global consistent semantic information of the\nimage set; ii) a CLIP interaction module, using CLIP-mined common semantics of\nthe image set to refine the backbone feature; iii) a CLIP regularization\nmodule, drawing CLIP towards this co-segmentation task, identifying the best\nCLIP semantic and using it to regularize the backbone feature. Experiments on\nfour standard co-segmentation benchmark datasets show that the performance of\nour method outperforms state-of-the-art methods.\n",
                "链接": "https://arxiv.org/abs/2308.11506"
            },
            {
                "文章ID": "51291",
                "标题": "CLIP-Nav: Using CLIP for Zero-Shot Vision-and-Language Navigation",
                "作者": " Vishnu Sashank Dorbala,  Gunnar Sigurdsson,  Robinson Piramuthu,  Jesse Thomason,  Gaurav S. Sukhatme",
                "发布日期": "2022-12-01",
                "摘要": "  Household environments are visually diverse. Embodied agents performing\nVision-and-Language Navigation (VLN) in the wild must be able to handle this\ndiversity, while also following arbitrary language instructions. Recently,\nVision-Language models like CLIP have shown great performance on the task of\nzero-shot object recognition. In this work, we ask if these models are also\ncapable of zero-shot language grounding. In particular, we utilize CLIP to\ntackle the novel problem of zero-shot VLN using natural language referring\nexpressions that describe target objects, in contrast to past work that used\nsimple language templates describing object classes. We examine CLIP's\ncapability in making sequential navigational decisions without any\ndataset-specific finetuning, and study how it influences the path that an agent\ntakes. Our results on the coarse-grained instruction following task of REVERIE\ndemonstrate the navigational capability of CLIP, surpassing the supervised\nbaseline in terms of both success rate (SR) and success weighted by path length\n(SPL). More importantly, we quantitatively show that our CLIP-based zero-shot\napproach generalizes better to show consistent performance across environments\nwhen compared to SOTA, fully supervised learning approaches when evaluated via\nRelative Change in Success (RCS).\n",
                "链接": "https://arxiv.org/abs/2211.16649"
            },
            {
                "文章ID": "122010",
                "标题": "CLIP in Medical Imaging: A Comprehensive Survey",
                "作者": " Zihao Zhao,  Yuxiao Liu,  Han Wu,  Yonghao Li,  Sheng Wang,  Lin Teng,  Disheng Liu,  Zhiming Cui,  Qian Wang,  Dinggang Shen",
                "发布日期": "2023-12-27",
                "摘要": "  Contrastive Language-Image Pre-training (CLIP), a simple yet effective\npre-training paradigm, successfully introduces text supervision to vision\nmodels. It has shown promising results across various tasks, attributable to\nits generalizability and interpretability. The use of CLIP has recently gained\nincreasing interest in the medical imaging domain, serving both as a\npre-training paradigm for aligning medical vision and language, and as a\ncritical component in diverse clinical tasks. With the aim of facilitating a\ndeeper understanding of this promising direction, this survey offers an\nin-depth exploration of the CLIP paradigm within the domain of medical imaging,\nregarding both refined CLIP pre-training and CLIP-driven applications. In this\nstudy, We (1) start with a brief introduction to the fundamentals of CLIP\nmethodology. (2) Then, we investigate the adaptation of CLIP pre-training in\nthe medical domain, focusing on how to optimize CLIP given characteristics of\nmedical images and reports. (3) Furthermore, we explore the practical\nutilization of CLIP pre-trained models in various tasks, including\nclassification, dense prediction, and cross-modal tasks. (4) Finally, we\ndiscuss existing limitations of CLIP in the context of medical imaging and\npropose forward-looking directions to address the demands of medical imaging\ndomain. We expect that this comprehensive survey will provide researchers in\nthe field of medical image analysis with a holistic understanding of the CLIP\nparadigm and its potential implications. The project page can be found on\nhttps://github.com/zhaozh10/Awesome-CLIP-in-Medical-Imaging.\n",
                "链接": "https://arxiv.org/abs/2312.07353"
            },
            {
                "文章ID": "77603",
                "标题": "CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding",
                "作者": " Linhui Xiao,  Xiaoshan Yang,  Fang Peng,  Ming Yan,  Yaowei Wang,  Changsheng Xu",
                "发布日期": "2023-12-27",
                "摘要": "  Visual Grounding (VG) is a crucial topic in the field of vision and language,\nwhich involves locating a specific region described by expressions within an\nimage. To reduce the reliance on manually labeled data, unsupervised visual\ngrounding have been developed to locate regions using pseudo-labels. However,\nthe performance of existing unsupervised methods is highly dependent on the\nquality of pseudo-labels and these methods always encounter issues with limited\ndiversity. In order to utilize vision and language pre-trained models to\naddress the grounding problem, and reasonably take advantage of pseudo-labels,\nwe propose CLIP-VG, a novel method that can conduct self-paced curriculum\nadapting of CLIP with pseudo-language labels. We propose a simple yet efficient\nend-to-end network architecture to realize the transfer of CLIP to the visual\ngrounding. Based on the CLIP-based architecture, we further propose\nsingle-source and multi-source curriculum adapting algorithms, which can\nprogressively find more reliable pseudo-labels to learn an optimal model,\nthereby achieving a balance between reliability and diversity for the\npseudo-language labels. Our method outperforms the current state-of-the-art\nunsupervised method by a significant margin on RefCOCO/+/g datasets in both\nsingle-source and multi-source scenarios, with improvements ranging from\n6.78$\\%$ to 10.67$\\%$ and 11.39$\\%$ to 14.87$\\%$, respectively. The results\neven outperform existing weakly supervised visual grounding methods.\nFurthermore, our method is also competitive in fully supervised setting. The\ncode and models are available at https://github.com/linhuixiao/CLIP-VG.\n",
                "链接": "https://arxiv.org/abs/2305.08685"
            },
            {
                "文章ID": "94950",
                "标题": "E-CLIP: Towards Label-efficient Event-based Open-world Understanding by\n  CLIP",
                "作者": " Jiazhou Zhou,  Xu Zheng,  Yuanhuiyi Lyu,  Lin Wang",
                "发布日期": "2023-09-12",
                "摘要": "  Contrasting Language-image pertaining (CLIP) has recently shown promising\nopen-world and few-shot performance on 2D image-based recognition tasks.\nHowever, the transferred capability of CLIP to the novel event camera data\nstill remains under-explored. In particular, due to the modality gap with the\nimage-text data and the lack of large-scale datasets, achieving this goal is\nnon-trivial and thus requires significant research innovation. In this paper,\nwe propose E-CLIP, a novel and effective framework that unleashes the potential\nof CLIP for event-based recognition to compensate for the lack of large-scale\nevent-based datasets. Our work addresses two crucial challenges: 1) how to\ngeneralize CLIP's visual encoder to event data while fully leveraging events'\nunique properties, e.g., sparsity and high temporal resolution; 2) how to\neffectively align the multi-modal embeddings, i.e., image, text, and events. To\nthis end, we first introduce a novel event encoder that subtly models the\ntemporal information from events and meanwhile generates event prompts to\npromote the modality bridging. We then design a text encoder that generates\ncontent prompts and utilizes hybrid text prompts to enhance the E-CLIP's\ngeneralization ability across diverse datasets. With the proposed event\nencoder, text encoder, and original image encoder, a novel Hierarchical Triple\nContrastive Alignment (HTCA) module is introduced to jointly optimize the\ncorrelation and enable efficient knowledge transfer among the three modalities.\nWe conduct extensive experiments on two recognition benchmarks, and the results\ndemonstrate that our E-CLIP outperforms existing methods by a large margin of\n+3.94% and +4.62% on the N-Caltech dataset, respectively, in both fine-tuning\nand few-shot settings. Moreover, our E-CLIP can be flexibly extended to the\nevent retrieval task using both text or image queries, showing plausible\nperformance.\n",
                "链接": "https://arxiv.org/abs/2308.03135"
            },
            {
                "文章ID": "82542",
                "标题": "Improving CLIP Training with Language Rewrites",
                "作者": " Lijie Fan,  Dilip Krishnan,  Phillip Isola,  Dina Katabi,  Yonglong Tian",
                "发布日期": "2023-10-31",
                "摘要": "  Contrastive Language-Image Pre-training (CLIP) stands as one of the most\neffective and scalable methods for training transferable vision models using\npaired image and text data. CLIP models are trained using contrastive loss,\nwhich typically relies on data augmentations to prevent overfitting and\nshortcuts. However, in the CLIP training paradigm, data augmentations are\nexclusively applied to image inputs, while language inputs remain unchanged\nthroughout the entire training process, limiting the exposure of diverse texts\nto the same image. In this paper, we introduce Language augmented CLIP\n(LaCLIP), a simple yet highly effective approach to enhance CLIP training\nthrough language rewrites. Leveraging the in-context learning capability of\nlarge language models, we rewrite the text descriptions associated with each\nimage. These rewritten texts exhibit diversity in sentence structure and\nvocabulary while preserving the original key concepts and meanings. During\ntraining, LaCLIP randomly selects either the original texts or the rewritten\nversions as text augmentations for each image. Extensive experiments on CC3M,\nCC12M, RedCaps and LAION-400M datasets show that CLIP pre-training with\nlanguage rewrites significantly improves the transfer performance without\ncomputation or memory overhead during training. Specifically for ImageNet\nzero-shot accuracy, LaCLIP outperforms CLIP by 8.2% on CC12M and 2.4% on\nLAION-400M. Code is available at https://github.com/LijieFan/LaCLIP.\n",
                "链接": "https://arxiv.org/abs/2305.20088"
            },
            {
                "文章ID": "120751",
                "标题": "SYNC-CLIP: Synthetic Data Make CLIP Generalize Better in Data-Limited\n  Scenarios",
                "作者": " Mushui Liu,  Weijie He,  Ziqian Lu,  Yunlong Yu",
                "发布日期": "2023-12-08",
                "摘要": "  Prompt learning is a powerful technique for transferring Vision-Language\nModels (VLMs) such as CLIP to downstream tasks. However, the prompt-based\nmethods that are fine-tuned solely with base classes may struggle to generalize\nto novel classes in open-vocabulary scenarios, especially when data are\nlimited. To address this issue, we propose an innovative approach called\nSYNC-CLIP that leverages SYNthetiC data for enhancing the generalization\ncapability of CLIP. Based on the observation of the distribution shift between\nthe real and synthetic samples, we treat real and synthetic samples as distinct\ndomains and propose to optimize separate domain prompts to capture\ndomain-specific information, along with the shared visual prompts to preserve\nthe semantic consistency between two domains. By aligning the cross-domain\nfeatures, the synthetic data from novel classes can provide implicit guidance\nto rebalance the decision boundaries. Experimental results on three model\ngeneralization tasks demonstrate that our method performs very competitively\nacross various benchmarks. Notably, SYNC-CLIP outperforms the state-of-the-art\ncompetitor PromptSRC by an average improvement of 3.0% on novel classes across\n11 datasets in open-vocabulary scenarios.\n",
                "链接": "https://arxiv.org/abs/2312.03805"
            },
            {
                "文章ID": "124385",
                "标题": "Parrot Captions Teach CLIP to Spot Text",
                "作者": " Yiqi Lin,  Conghui He,  Alex Jinpeng Wang,  Bin Wang,  Weijia Li,  Mike Zheng Shou",
                "发布日期": "2023-12-29",
                "摘要": "  Despite CLIP being the foundation model in numerous vision-language\napplications, the CLIP suffers from a severe text spotting bias. Such bias\ncauses CLIP models to 'Parrot' the visual text embedded within images while\ndisregarding the authentic visual semantics. We uncover that in the most\npopular image-text dataset LAION-2B, the captions also densely parrot (spell)\nthe text embedded in images. Our analysis shows that around 50% of images are\nembedded with visual text content, and 90% of their captions more or less\nparrot the visual text. Based on such observation, we thoroughly inspect the\ndifferent released versions of CLIP models and verify that the visual text is\nthe dominant factor in measuring the LAION-style image-text similarity for\nthese models. To examine whether these parrot captions shape the text spotting\nbias, we train a series of CLIP models with LAION subsets curated by different\nparrot-caption-oriented criteria. We show that training with parrot captions\neasily shapes such bias but harms the expected visual-language representation\nlearning in CLIP models. This suggests that it is urgent to revisit either the\ndesign of CLIP-like models or the existing image-text dataset curation pipeline\nbuilt on CLIP score filtering.\n",
                "链接": "https://arxiv.org/abs/2312.14232"
            },
            {
                "文章ID": "83207",
                "标题": "Enhancing CLIP with CLIP: Exploring Pseudolabeling for Limited-Label\n  Prompt Tuning",
                "作者": " Cristina Menghini,  Andrew Delworth,  Stephen H. Bach",
                "发布日期": "2023-06-05",
                "摘要": "  Fine-tuning vision-language models (VLMs) like CLIP to downstream tasks is\noften necessary to optimize their performance. However, a major obstacle is the\nlimited availability of labeled data. We study the use of pseudolabels, i.e.,\nheuristic labels for unlabeled data, to enhance CLIP via prompt tuning.\nConventional pseudolabeling trains a model on labeled data and then generates\nlabels for unlabeled data. VLMs' zero-shot capabilities enable a ``second\ngeneration'' of pseudolabeling approaches that do not require task-specific\ntraining on labeled data. By using zero-shot pseudolabels as a source of\nsupervision, we observe that learning paradigms such as semi-supervised,\ntransductive zero-shot, and unsupervised learning can all be seen as optimizing\nthe same loss function. This unified view enables the development of versatile\ntraining strategies that are applicable across learning paradigms. We\ninvestigate them on image classification tasks where CLIP exhibits limitations,\nby varying prompt modalities, e.g., textual or visual prompts, and learning\nparadigms. We find that (1) unexplored prompt tuning strategies that\niteratively refine pseudolabels consistently improve CLIP accuracy, by 19.5\npoints in semi-supervised learning, by 28.4 points in transductive zero-shot\nlearning, and by 15.2 points in unsupervised learning, and (2) unlike\nconventional semi-supervised pseudolabeling, which exacerbates model biases\ntoward classes with higher-quality pseudolabels, prompt tuning leads to a more\nequitable distribution of per-class accuracy. The code to reproduce the\nexperiments is at github.com/BatsResearch/menghini-enhanceCLIPwithCLIP-code.\n",
                "链接": "https://arxiv.org/abs/2306.01669"
            },
            {
                "文章ID": "90539",
                "标题": "MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental\n  Learning",
                "作者": " Julien Nicolas,  Florent Chiaroni,  Imtiaz Ziko,  Ola Ahmad,  Christian Desrosiers,  Jose Dolz",
                "发布日期": "2023-07-13",
                "摘要": "  Despite the recent progress in incremental learning, addressing catastrophic\nforgetting under distributional drift is still an open and important problem.\nIndeed, while state-of-the-art domain incremental learning (DIL) methods\nperform satisfactorily within known domains, their performance largely degrades\nin the presence of novel domains. This limitation hampers their\ngeneralizability, and restricts their scalability to more realistic settings\nwhere train and test data are drawn from different distributions. To address\nthese limitations, we present a novel DIL approach based on a mixture of\nprompt-tuned CLIP models (MoP-CLIP), which generalizes the paradigm of\nS-Prompting to handle both in-distribution and out-of-distribution data at\ninference. In particular, at the training stage we model the features\ndistribution of every class in each domain, learning individual text and visual\nprompts to adapt to a given domain. At inference, the learned distributions\nallow us to identify whether a given test sample belongs to a known domain,\nselecting the correct prompt for the classification task, or from an unseen\ndomain, leveraging a mixture of the prompt-tuned CLIP models. Our empirical\nevaluation reveals the poor performance of existing DIL methods under domain\nshift, and suggests that the proposed MoP-CLIP performs competitively in the\nstandard DIL settings while outperforming state-of-the-art methods in OOD\nscenarios. These results demonstrate the superiority of MoP-CLIP, offering a\nrobust and general solution to the problem of domain incremental learning.\n",
                "链接": "https://arxiv.org/abs/2307.05707"
            },
            {
                "文章ID": "104979",
                "标题": "Demystifying CLIP Data",
                "作者": " Hu Xu,  Saining Xie,  Xiaoqing Ellen Tan,  Po-Yao Huang,  Russell Howes,  Vasu Sharma,  Shang-Wen Li,  Gargi Ghosh,  Luke Zettlemoyer,  Christoph Feichtenhofer",
                "发布日期": "2023-10-03",
                "摘要": "  Contrastive Language-Image Pre-training (CLIP) is an approach that has\nadvanced research and applications in computer vision, fueling modern\nrecognition systems and generative models. We believe that the main ingredient\nto the success of CLIP is its data and not the model architecture or\npre-training objective. However, CLIP only provides very limited information\nabout its data and how it has been collected, leading to works that aim to\nreproduce CLIP's data by filtering with its model parameters. In this work, we\nintend to reveal CLIP's data curation approach and in our pursuit of making it\nopen to the community introduce Metadata-Curated Language-Image Pre-training\n(MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's\nconcepts) and yields a balanced subset over the metadata distribution. Our\nexperimental study rigorously isolates the model and training settings,\nconcentrating solely on data. MetaCLIP applied to CommonCrawl with 400M\nimage-text data pairs outperforms CLIP's data on multiple standard benchmarks.\nIn zero-shot ImageNet classification, MetaCLIP achieves 70.8% accuracy,\nsurpassing CLIP's 68.3% on ViT-B models. Scaling to 1B data, while maintaining\nthe same training budget, attains 72.4%. Our observations hold across various\nmodel sizes, exemplified by ViT-H achieving 80.5%, without any\nbells-and-whistles. Curation code and training data distribution on metadata is\nmade available at https://github.com/facebookresearch/MetaCLIP.\n",
                "链接": "https://arxiv.org/abs/2309.16671"
            },
            {
                "文章ID": "21265",
                "标题": "Fine-grained Image Captioning with CLIP Reward",
                "作者": " Jaemin Cho,  Seunghyun Yoon,  Ajinkya Kale,  Franck Dernoncourt,  Trung Bui,  Mohit Bansal",
                "发布日期": "2023-03-31",
                "摘要": "  Modern image captioning models are usually trained with text similarity\nobjectives. However, since reference captions in public datasets often describe\nthe most salient common objects, models trained with text similarity objectives\ntend to ignore specific and detailed aspects of an image that distinguish it\nfrom others. Toward more descriptive and distinctive caption generation, we\npropose using CLIP, a multimodal encoder trained on huge image-text pairs from\nweb, to calculate multimodal similarity and use it as a reward function. We\nalso propose a simple finetuning strategy of the CLIP text encoder to improve\ngrammar that does not require extra text annotation. This completely eliminates\nthe need for reference captions during the reward computation. To\ncomprehensively evaluate descriptive captions, we introduce FineCapEval, a new\ndataset for caption evaluation with fine-grained criteria: overall, background,\nobject, relations. In our experiments on text-to-image retrieval and\nFineCapEval, the proposed CLIP-guided model generates more distinctive captions\nthan the CIDEr-optimized model. We also show that our unsupervised grammar\nfinetuning of the CLIP text encoder alleviates the degeneration problem of the\nnaive CLIP reward. Lastly, we show human analysis where the annotators strongly\nprefer the CLIP reward to the CIDEr and MLE objectives according to various\ncriteria. Code and Data: https://github.com/j-min/CLIP-Caption-Reward\n",
                "链接": "https://arxiv.org/abs/2205.13115"
            },
            {
                "文章ID": "22912",
                "标题": "Delving into the Openness of CLIP",
                "作者": " Shuhuai Ren,  Lei Li,  Xuancheng Ren,  Guangxiang Zhao,  Xu Sun",
                "发布日期": "2023-05-09",
                "摘要": "  Contrastive Language-Image Pre-training (CLIP) formulates image\nclassification as an image-to-text matching task, i.e., matching images to the\ncorresponding natural language descriptions instead of discrete category IDs.\nThis allows for open-vocabulary visual recognition, where the model can\nrecognize images from an open class set (also known as an open vocabulary) in a\nzero-shot manner. However, evaluating the openness of CLIP-like models is\nchallenging, as the models are open to arbitrary vocabulary in theory, but\ntheir accuracy varies in practice. To address this, we resort to an incremental\nperspective to assess the openness through vocabulary expansions, and define\nextensibility to measure a model's ability to handle novel classes. Our\nevaluation shows that CLIP-like models are not truly open, and their\nperformance deteriorates as the vocabulary expands. We further dissect the\nfeature space of CLIP from the perspectives of representation alignment and\nuniformity. Our investigation reveals that the overestimation of openness is\ndue to confusion among competing text features, rather than a failure to\ncapture the similarity between image features and text features of novel\nclasses. We hope that our investigation and analysis will facilitate future\nresearch on the CLIP openness issue.\n",
                "链接": "https://arxiv.org/abs/2206.01986"
            },
            {
                "文章ID": "122246",
                "标题": "EZ-CLIP: Efficient Zeroshot Video Action Recognition",
                "作者": " Shahzad Ahmad,  Sukalpa Chanda,  Yogesh S Rawat",
                "发布日期": "2023-12-14",
                "摘要": "  Recent advancements in large-scale pre-training of visual-language models on\npaired image-text data have demonstrated impressive generalization capabilities\nfor zero-shot tasks. Building on this success, efforts have been made to adapt\nthese image-based visual-language models, such as CLIP, for videos extending\ntheir zero-shot capabilities to the video domain. While these adaptations have\nshown promising results, they come at a significant computational cost and\nstruggle with effectively modeling the crucial temporal aspects inherent to the\nvideo domain. In this study, we present EZ-CLIP, a simple and efficient\nadaptation of CLIP that addresses these challenges. EZ-CLIP leverages temporal\nvisual prompting for seamless temporal adaptation, requiring no fundamental\nalterations to the core CLIP architecture while preserving its remarkable\ngeneralization abilities. Moreover, we introduce a novel learning objective\nthat guides the temporal visual prompts to focus on capturing motion, thereby\nenhancing its learning capabilities from video data. We conducted extensive\nexperiments on five different benchmark datasets, thoroughly evaluating EZ-CLIP\nfor zero-shot learning and base-to-novel video action recognition, and also\ndemonstrating its potential for few-shot generalization.Impressively, with a\nmere 5.2 million learnable parameters (as opposed to the 71.1 million in the\nprior best model), EZ-CLIP can be efficiently trained on a single GPU,\noutperforming existing approaches in several evaluations.\n",
                "链接": "https://arxiv.org/abs/2312.08010"
            },
            {
                "文章ID": "64995",
                "标题": "CLIP-guided Prototype Modulating for Few-shot Action Recognition",
                "作者": " Xiang Wang,  Shiwei Zhang,  Jun Cen,  Changxin Gao,  Yingya Zhang,  Deli Zhao,  Nong Sang",
                "发布日期": "2023-03-07",
                "摘要": "  Learning from large-scale contrastive language-image pre-training like CLIP\nhas shown remarkable success in a wide range of downstream tasks recently, but\nit is still under-explored on the challenging few-shot action recognition\n(FSAR) task. In this work, we aim to transfer the powerful multimodal knowledge\nof CLIP to alleviate the inaccurate prototype estimation issue due to data\nscarcity, which is a critical problem in low-shot regimes. To this end, we\npresent a CLIP-guided prototype modulating framework called CLIP-FSAR, which\nconsists of two key components: a video-text contrastive objective and a\nprototype modulation. Specifically, the former bridges the task discrepancy\nbetween CLIP and the few-shot video task by contrasting videos and\ncorresponding class text descriptions. The latter leverages the transferable\ntextual concepts from CLIP to adaptively refine visual prototypes with a\ntemporal Transformer. By this means, CLIP-FSAR can take full advantage of the\nrich semantic priors in CLIP to obtain reliable prototypes and achieve accurate\nfew-shot classification. Extensive experiments on five commonly used benchmarks\ndemonstrate the effectiveness of our proposed method, and CLIP-FSAR\nsignificantly outperforms existing state-of-the-art methods under various\nsettings. The source code and models will be publicly available at\nhttps://github.com/alibaba-mmai-research/CLIP-FSAR.\n",
                "链接": "https://arxiv.org/abs/2303.02982"
            },
            {
                "文章ID": "52593",
                "标题": "Fine-tuned CLIP Models are Efficient Video Learners",
                "作者": " Hanoona Rasheed,  Muhammad Uzair Khattak,  Muhammad Maaz,  Salman Khan,  Fahad Shahbaz Khan",
                "发布日期": "2023-03-28",
                "摘要": "  Large-scale multi-modal training with image-text pairs imparts strong\ngeneralization to CLIP model. Since training on a similar scale for videos is\ninfeasible, recent approaches focus on the effective transfer of image-based\nCLIP to the video domain. In this pursuit, new parametric modules are added to\nlearn temporal information and inter-frame relationships which require\nmeticulous design efforts. Furthermore, when the resulting models are learned\non videos, they tend to overfit on the given task distribution and lack in\ngeneralization aspect. This begs the following question: How to effectively\ntransfer image-level CLIP representations to videos? In this work, we show that\na simple Video Fine-tuned CLIP (ViFi-CLIP) baseline is generally sufficient to\nbridge the domain gap from images to videos. Our qualitative analysis\nillustrates that the frame-level processing from CLIP image-encoder followed by\nfeature pooling and similarity matching with corresponding text embeddings\nhelps in implicitly modeling the temporal cues within ViFi-CLIP. Such\nfine-tuning helps the model to focus on scene dynamics, moving objects and\ninter-object relationships. For low-data regimes where full fine-tuning is not\nviable, we propose a `bridge and prompt' approach that first uses fine-tuning\nto bridge the domain gap and then learns prompts on language and vision side to\nadapt CLIP representations. We extensively evaluate this simple yet strong\nbaseline on zero-shot, base-to-novel generalization, few-shot and fully\nsupervised settings across five video benchmarks. Our code is available at\nhttps://github.com/muzairkhattak/ViFi-CLIP.\n",
                "链接": "https://arxiv.org/abs/2212.03640"
            },
            {
                "文章ID": "91646",
                "标题": "Augmenting CLIP with Improved Visio-Linguistic Reasoning",
                "作者": " Samyadeep Basu,  Maziar Sanjabi,  Daniela Massiceti,  Shell Xu Hu,  Soheil Feizi",
                "发布日期": "2023-07-31",
                "摘要": "  Image-text contrastive models such as CLIP are useful for a variety of\ndownstream applications including zero-shot classification, image-text\nretrieval and transfer learning. However, these contrastively trained\nvision-language models often fail on compositional visio-linguistic tasks such\nas Winoground with performance equivalent to random chance. In our paper, we\naddress this issue and propose a sample-efficient light-weight method called\nSDS-CLIP to improve the compositional visio-linguistic reasoning capabilities\nof CLIP. The core idea of our method is to use differentiable image\nparameterizations to fine-tune CLIP with a distillation objective from large\ntext-to-image generative models such as Stable-Diffusion which are relatively\ngood at visio-linguistic reasoning tasks. On the challenging Winoground\ncompositional reasoning benchmark, our method improves the absolute\nvisio-linguistic performance of different CLIP models by up to 7%, while on the\nARO dataset, our method improves the visio-linguistic performance by upto 3%.\nAs a byproduct of inducing visio-linguistic reasoning into CLIP, we also find\nthat the zero-shot performance improves marginally on a variety of downstream\ndatasets. Our method reinforces that carefully designed distillation objectives\nfrom generative models can be leveraged to extend existing contrastive\nimage-text models with improved visio-linguistic reasoning capabilities.\n",
                "链接": "https://arxiv.org/abs/2307.09233"
            },
            {
                "文章ID": "53031",
                "标题": "CLIP-TSA: CLIP-Assisted Temporal Self-Attention for Weakly-Supervised\n  Video Anomaly Detection",
                "作者": " Hyekang Kevin Joo,  Khoa Vo,  Kashu Yamazaki,  Ngan Le",
                "发布日期": "2023-07-06",
                "摘要": "  Video anomaly detection (VAD) -- commonly formulated as a multiple-instance\nlearning problem in a weakly-supervised manner due to its labor-intensive\nnature -- is a challenging problem in video surveillance where the frames of\nanomaly need to be localized in an untrimmed video. In this paper, we first\npropose to utilize the ViT-encoded visual features from CLIP, in contrast with\nthe conventional C3D or I3D features in the domain, to efficiently extract\ndiscriminative representations in the novel technique. We then model temporal\ndependencies and nominate the snippets of interest by leveraging our proposed\nTemporal Self-Attention (TSA). The ablation study confirms the effectiveness of\nTSA and ViT feature. The extensive experiments show that our proposed CLIP-TSA\noutperforms the existing state-of-the-art (SOTA) methods by a large margin on\nthree commonly-used benchmark datasets in the VAD problem (UCF-Crime,\nShanghaiTech Campus, and XD-Violence). Our source code is available at\nhttps://github.com/joos2010kj/CLIP-TSA.\n",
                "链接": "https://arxiv.org/abs/2212.05136"
            },
            {
                "文章ID": "7665",
                "标题": "CLIP-GEN: Language-Free Training of a Text-to-Image Generator with CLIP",
                "作者": " Zihao Wang,  Wei Liu,  Qian He,  Xinglong Wu,  Zili Yi",
                "发布日期": "2022-03-02",
                "摘要": "  Training a text-to-image generator in the general domain (e.g., Dall.e,\nCogView) requires huge amounts of paired text-image data, which is too\nexpensive to collect. In this paper, we propose a self-supervised scheme named\nas CLIP-GEN for general text-to-image generation with the language-image priors\nextracted with a pre-trained CLIP model. In our approach, we only require a set\nof unlabeled images in the general domain to train a text-to-image generator.\nSpecifically, given an image without text labels, we first extract the\nembedding of the image in the united language-vision embedding space with the\nimage encoder of CLIP. Next, we convert the image into a sequence of discrete\ntokens in the VQGAN codebook space (the VQGAN model can be trained with the\nunlabeled image dataset in hand). Finally, we train an autoregressive\ntransformer that maps the image tokens from its unified language-vision\nrepresentation. Once trained, the transformer can generate coherent image\ntokens based on the text embedding extracted from the text encoder of CLIP upon\nan input text. Such a strategy enables us to train a strong and general\ntext-to-image generator with large text-free image dataset such as ImageNet.\nQualitative and quantitative evaluations verify that our method significantly\noutperforms optimization-based text-to-image methods in terms of image quality\nwhile not compromising the text-image matching. Our method can even achieve\ncomparable performance as flagship supervised models like CogView.\n",
                "链接": "https://arxiv.org/abs/2203.00386"
            },
            {
                "文章ID": "24850",
                "标题": "Disentangling visual and written concepts in CLIP",
                "作者": " Joanna Materzynska,  Antonio Torralba,  David Bau",
                "发布日期": "2022-06-17",
                "摘要": "  The CLIP network measures the similarity between natural text and images; in\nthis work, we investigate the entanglement of the representation of word images\nand natural images in its image encoder. First, we find that the image encoder\nhas an ability to match word images with natural images of scenes described by\nthose words. This is consistent with previous research that suggests that the\nmeaning and the spelling of a word might be entangled deep within the network.\nOn the other hand, we also find that CLIP has a strong ability to match\nnonsense words, suggesting that processing of letters is separated from\nprocessing of their meaning. To explicitly determine whether the spelling\ncapability of CLIP is separable, we devise a procedure for identifying\nrepresentation subspaces that selectively isolate or eliminate spelling\ncapabilities. We benchmark our methods against a range of retrieval tasks, and\nwe also test them by measuring the appearance of text in CLIP-guided generated\nimages. We find that our methods are able to cleanly separate spelling\ncapabilities of CLIP from the visual processing of natural images.\n",
                "链接": "https://arxiv.org/abs/2206.07835"
            },
            {
                "文章ID": "102895",
                "标题": "Improving CLIP Robustness with Knowledge Distillation and Self-Training",
                "作者": " Clement Laroudie,  Andrei Bursuc,  Mai Lan Ha,  Gianni Franchi",
                "发布日期": "2023-09-20",
                "摘要": "  This paper examines the robustness of a multi-modal computer vision model,\nCLIP (Contrastive Language-Image Pretraining), in the context of unsupervised\nlearning. The main objective is twofold: first, to evaluate the robustness of\nCLIP, and second, to explore strategies for augmenting its robustness. To\nachieve this, we introduce a novel approach named LP-CLIP. This technique\ninvolves the distillation of CLIP features through the incorporation of a\nlinear probing layer positioned atop its encoding structure. This newly added\nlayer is trained utilizing pseudo-labels produced by CLIP, coupled with a\nself-training strategy. The LP-CLIP technique offers a promising approach to\nenhance the robustness of CLIP without the need for annotations. By leveraging\na simple linear probing layer, we aim to improve the model's ability to\nwithstand various uncertainties and challenges commonly encountered in\nreal-world scenarios. Importantly, our approach does not rely on annotated\ndata, which makes it particularly valuable in situations where labeled data\nmight be scarce or costly to obtain. Our proposed approach increases the\nrobustness of CLIP with SOTA results compared to supervised technique on\nvarious datasets.\n",
                "链接": "https://arxiv.org/abs/2309.10361"
            },
            {
                "文章ID": "28054",
                "标题": "Towards Counterfactual Image Manipulation via CLIP",
                "作者": " Yingchen Yu,  Fangneng Zhan,  Rongliang Wu,  Jiahui Zhang,  Shijian Lu,  Miaomiao Cui,  Xuansong Xie,  Xian-Sheng Hua,  Chunyan Miao",
                "发布日期": "2022-07-13",
                "摘要": "  Leveraging StyleGAN's expressivity and its disentangled latent codes,\nexisting methods can achieve realistic editing of different visual attributes\nsuch as age and gender of facial images. An intriguing yet challenging problem\narises: Can generative models achieve counterfactual editing against their\nlearnt priors? Due to the lack of counterfactual samples in natural datasets,\nwe investigate this problem in a text-driven manner with\nContrastive-Language-Image-Pretraining (CLIP), which can offer rich semantic\nknowledge even for various counterfactual concepts. Different from in-domain\nmanipulation, counterfactual manipulation requires more comprehensive\nexploitation of semantic knowledge encapsulated in CLIP as well as more\ndelicate handling of editing directions for avoiding being stuck in local\nminimum or undesired editing. To this end, we design a novel contrastive loss\nthat exploits predefined CLIP-space directions to guide the editing toward\ndesired directions from different perspectives. In addition, we design a simple\nyet effective scheme that explicitly maps CLIP embeddings (of target text) to\nthe latent space and fuses them with latent codes for effective latent code\noptimization and accurate editing. Extensive experiments show that our design\nachieves accurate and realistic editing while driving by target texts with\nvarious counterfactual concepts.\n",
                "链接": "https://arxiv.org/abs/2207.02812"
            },
            {
                "文章ID": "107101",
                "标题": "Symmetrical Linguistic Feature Distillation with CLIP for Scene Text\n  Recognition",
                "作者": " Zixiao Wang,  Hongtao Xie,  Yuxin Wang,  Jianjun Xu,  Boqiang Zhang,  Yongdong Zhang",
                "发布日期": "2023-10-11",
                "摘要": "  In this paper, we explore the potential of the Contrastive Language-Image\nPretraining (CLIP) model in scene text recognition (STR), and establish a novel\nSymmetrical Linguistic Feature Distillation framework (named CLIP-OCR) to\nleverage both visual and linguistic knowledge in CLIP. Different from previous\nCLIP-based methods mainly considering feature generalization on visual\nencoding, we propose a symmetrical distillation strategy (SDS) that further\ncaptures the linguistic knowledge in the CLIP text encoder. By cascading the\nCLIP image encoder with the reversed CLIP text encoder, a symmetrical structure\nis built with an image-to-text feature flow that covers not only visual but\nalso linguistic information for distillation.Benefiting from the natural\nalignment in CLIP, such guidance flow provides a progressive optimization\nobjective from vision to language, which can supervise the STR feature\nforwarding process layer-by-layer.Besides, a new Linguistic Consistency Loss\n(LCL) is proposed to enhance the linguistic capability by considering\nsecond-order statistics during the optimization. Overall, CLIP-OCR is the first\nto design a smooth transition between image and text for the STR task.Extensive\nexperiments demonstrate the effectiveness of CLIP-OCR with 93.8% average\naccuracy on six popular STR benchmarks.Code will be available at\nhttps://github.com/wzx99/CLIPOCR.\n",
                "链接": "https://arxiv.org/abs/2310.04999"
            },
            {
                "文章ID": "40981",
                "标题": "CLIP model is an Efficient Continual Learner",
                "作者": " Vishal Thengane,  Salman Khan,  Munawar Hayat,  Fahad Khan",
                "发布日期": "2022-10-07",
                "摘要": "  The continual learning setting aims to learn new tasks over time without\nforgetting the previous ones. The literature reports several significant\nefforts to tackle this problem with limited or no access to previous task data.\nAmong such efforts, typical solutions offer sophisticated techniques involving\nmemory replay, knowledge distillation, model regularization, and dynamic\nnetwork expansion. The resulting methods have a retraining cost at each\nlearning task, dedicated memory requirements, and setting-specific design\nchoices. In this work, we show that a frozen CLIP (Contrastive Language-Image\nPretraining) model offers astounding continual learning performance without any\nfine-tuning (zero-shot evaluation). We evaluate CLIP under a variety of\nsettings including class-incremental, domain-incremental and task-agnostic\nincremental learning on five popular benchmarks (ImageNet-100 & 1K, CORe50,\nCIFAR-100, and TinyImageNet). Without any bells and whistles, the CLIP model\noutperforms the state-of-the-art continual learning approaches in the majority\nof the settings. We show the effect on the CLIP model's performance by varying\ntext inputs with simple prompt templates. To the best of our knowledge, this is\nthe first work to report the CLIP zero-shot performance in a continual setting.\nWe advocate the use of this strong yet embarrassingly simple baseline for\nfuture comparisons in the continual learning tasks.\n",
                "链接": "https://arxiv.org/abs/2210.03114"
            },
            {
                "文章ID": "77077",
                "标题": "An Inverse Scaling Law for CLIP Training",
                "作者": " Xianhang Li,  Zeyu Wang,  Cihang Xie",
                "发布日期": "2023-10-31",
                "摘要": "  CLIP, one of the pioneering foundation models that connect images and text,\nhas enabled many recent breakthroughs in computer vision. However, its\nassociated training cost is prohibitively high, imposing a significant barrier\nto its widespread exploration. In this paper, we present a surprising finding\nthat there exists an inverse scaling law for CLIP training, whereby the larger\nthe image/text encoders used, the shorter the sequence length of image/text\ntokens that can be applied in training. Moreover, we showcase that the strategy\nfor reducing image/text token length plays a crucial role in determining the\nquality of this scaling law.\n  As a result of this finding, we are able to successfully train CLIP even with\nlimited computational resources. For example, using 8 A100 GPUs, our CLIP\nmodels achieve zero-shot top-1 ImageNet-1k accuracies of 63.2% in ~2 days,\n67.8% in ~3 days, and 69.3% in ~4 days. Our method also works well when scaling\nup -- with G/14, we register a new record of 83.0% ImageNet-1k zero-shot\naccuracy, and meanwhile accelerate the training by ~33x compared to its\nOpenCLIP counterpart. By reducing the computation barrier associated with CLIP,\nwe hope to inspire more research in this field, particularly from academics.\nOur code is available at https://github.com/UCSC-VLAA/CLIPA.\n",
                "链接": "https://arxiv.org/abs/2305.07017"
            },
            {
                "文章ID": "13965",
                "标题": "Adapting CLIP For Phrase Localization Without Further Training",
                "作者": " Jiahao Li,  Greg Shakhnarovich,  Raymond A. Yeh",
                "发布日期": "2022-04-08",
                "摘要": "  Supervised or weakly supervised methods for phrase localization (textual\ngrounding) either rely on human annotations or some other supervised models,\ne.g., object detectors. Obtaining these annotations is labor-intensive and may\nbe difficult to scale in practice. We propose to leverage recent advances in\ncontrastive language-vision models, CLIP, pre-trained on image and caption\npairs collected from the internet. In its original form, CLIP only outputs an\nimage-level embedding without any spatial resolution. We adapt CLIP to generate\nhigh-resolution spatial feature maps. Importantly, we can extract feature maps\nfrom both ViT and ResNet CLIP model while maintaining the semantic properties\nof an image embedding. This provides a natural framework for phrase\nlocalization. Our method for phrase localization requires no human annotations\nor additional training. Extensive experiments show that our method outperforms\nexisting no-training methods in zero-shot phrase localization, and in some\ncases, it even outperforms supervised methods. Code is available at\nhttps://github.com/pals-ttic/adapting-CLIP .\n",
                "链接": "https://arxiv.org/abs/2204.03647"
            },
            {
                "文章ID": "117693",
                "标题": "Understanding the Vulnerability of CLIP to Image Compression",
                "作者": " Cangxiong Chen,  Vinay P. Namboodiri,  Julian Padget",
                "发布日期": "2023-11-27",
                "摘要": "  CLIP is a widely used foundational vision-language model that is used for\nzero-shot image recognition and other image-text alignment tasks. We\ndemonstrate that CLIP is vulnerable to change in image quality under\ncompression. This surprising result is further analysed using an attribution\nmethod-Integrated Gradients. Using this attribution method, we are able to\nbetter understand both quantitatively and qualitatively exactly the nature in\nwhich the compression affects the zero-shot recognition accuracy of this model.\nWe evaluate this extensively on CIFAR-10 and STL-10. Our work provides the\nbasis to understand this vulnerability of CLIP and can help us develop more\neffective methods to improve the robustness of CLIP and other vision-language\nmodels.\n",
                "链接": "https://arxiv.org/abs/2311.14029"
            },
            {
                "文章ID": "91557",
                "标题": "In Defense of Clip-based Video Relation Detection",
                "作者": " Meng Wei,  Long Chen,  Wei Ji,  Xiaoyu Yue,  Roger Zimmermann",
                "发布日期": "2023-07-19",
                "摘要": "  Video Visual Relation Detection (VidVRD) aims to detect visual relationship\ntriplets in videos using spatial bounding boxes and temporal boundaries.\nExisting VidVRD methods can be broadly categorized into bottom-up and top-down\nparadigms, depending on their approach to classifying relations. Bottom-up\nmethods follow a clip-based approach where they classify relations of short\nclip tubelet pairs and then merge them into long video relations. On the other\nhand, top-down methods directly classify long video tubelet pairs. While recent\nvideo-based methods utilizing video tubelets have shown promising results, we\nargue that the effective modeling of spatial and temporal context plays a more\nsignificant role than the choice between clip tubelets and video tubelets. This\nmotivates us to revisit the clip-based paradigm and explore the key success\nfactors in VidVRD. In this paper, we propose a Hierarchical Context Model (HCM)\nthat enriches the object-based spatial context and relation-based temporal\ncontext based on clips. We demonstrate that using clip tubelets can achieve\nsuperior performance compared to most video-based methods. Additionally, using\nclip tubelets offers more flexibility in model designs and helps alleviate the\nlimitations associated with video tubelets, such as the challenging long-term\nobject tracking problem and the loss of temporal information in long-term\ntubelet feature compression. Extensive experiments conducted on two challenging\nVidVRD benchmarks validate that our HCM achieves a new state-of-the-art\nperformance, highlighting the effectiveness of incorporating advanced spatial\nand temporal context modeling within the clip-based paradigm.\n",
                "链接": "https://arxiv.org/abs/2307.08984"
            },
            {
                "文章ID": "46534",
                "标题": "Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese",
                "作者": " An Yang,  Junshu Pan,  Junyang Lin,  Rui Men,  Yichang Zhang,  Jingren Zhou,  Chang Zhou",
                "发布日期": "2023-05-24",
                "摘要": "  The tremendous success of CLIP (Radford et al., 2021) has promoted the\nresearch and application of contrastive learning for vision-language\npretraining. In this work, we construct a large-scale dataset of image-text\npairs in Chinese, where most data are retrieved from publicly available\ndatasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5\nChinese CLIP models of multiple sizes, spanning from 77 to 958 million\nparameters. Furthermore, we propose a two-stage pretraining method, where the\nmodel is first trained with the image encoder frozen and then trained with all\nparameters being optimized, to achieve enhanced model performance. Our\ncomprehensive experiments demonstrate that Chinese CLIP can achieve the\nstate-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups\nof zero-shot learning and finetuning, and it is able to achieve competitive\nperformance in zero-shot image classification based on the evaluation on the\nELEVATER benchmark (Li et al., 2022). We have released our codes, models, and\ndemos in https://github.com/OFA-Sys/Chinese-CLIP\n",
                "链接": "https://arxiv.org/abs/2211.01335"
            },
            {
                "文章ID": "9926",
                "标题": "MotionCLIP: Exposing Human Motion Generation to CLIP Space",
                "作者": " Guy Tevet,  Brian Gordon,  Amir Hertz,  Amit H. Bermano,  Daniel Cohen-Or",
                "发布日期": "2022-03-16",
                "摘要": "  We introduce MotionCLIP, a 3D human motion auto-encoder featuring a latent\nembedding that is disentangled, well behaved, and supports highly semantic\ntextual descriptions. MotionCLIP gains its unique power by aligning its latent\nspace with that of the Contrastive Language-Image Pre-training (CLIP) model.\nAligning the human motion manifold to CLIP space implicitly infuses the\nextremely rich semantic knowledge of CLIP into the manifold. In particular, it\nhelps continuity by placing semantically similar motions close to one another,\nand disentanglement, which is inherited from the CLIP-space structure.\nMotionCLIP comprises a transformer-based motion auto-encoder, trained to\nreconstruct motion while being aligned to its text label's position in\nCLIP-space. We further leverage CLIP's unique visual understanding and inject\nan even stronger signal through aligning motion to rendered frames in a\nself-supervised manner. We show that although CLIP has never seen the motion\ndomain, MotionCLIP offers unprecedented text-to-motion abilities, allowing\nout-of-domain actions, disentangled editing, and abstract language\nspecification. For example, the text prompt \"couch\" is decoded into a sitting\ndown motion, due to lingual similarity, and the prompt \"Spiderman\" results in a\nweb-swinging-like solution that is far from seen during training. In addition,\nwe show how the introduced latent space can be leveraged for motion\ninterpolation, editing and recognition.\n",
                "链接": "https://arxiv.org/abs/2203.08063"
            },
            {
                "文章ID": "97336",
                "标题": "An Empirical Study of CLIP for Text-based Person Search",
                "作者": " Min Cao,  Yang Bai,  Ziyin Zeng,  Mang Ye,  Min Zhang",
                "发布日期": "2023-12-22",
                "摘要": "  Text-based Person Search (TBPS) aims to retrieve the person images using\nnatural language descriptions. Recently, Contrastive Language Image Pretraining\n(CLIP), a universal large cross-modal vision-language pre-training model, has\nremarkably performed over various cross-modal downstream tasks due to its\npowerful cross-modal semantic learning capacity. TPBS, as a fine-grained\ncross-modal retrieval task, is also facing the rise of research on the\nCLIP-based TBPS. In order to explore the potential of the visual-language\npre-training model for downstream TBPS tasks, this paper makes the first\nattempt to conduct a comprehensive empirical study of CLIP for TBPS and thus\ncontribute a straightforward, incremental, yet strong TBPS-CLIP baseline to the\nTBPS community. We revisit critical design considerations under CLIP, including\ndata augmentation and loss function. The model, with the aforementioned designs\nand practical training tricks, can attain satisfactory performance without any\nsophisticated modules. Also, we conduct the probing experiments of TBPS-CLIP in\nmodel generalization and model compression, demonstrating the effectiveness of\nTBPS-CLIP from various aspects. This work is expected to provide empirical\ninsights and highlight future CLIP-based TBPS research.\n",
                "链接": "https://arxiv.org/abs/2308.10045"
            },
            {
                "文章ID": "29930",
                "标题": "Don't Stop Learning: Towards Continual Learning for the CLIP Model",
                "作者": " Yuxuan Ding,  Lingqiao Liu,  Chunna Tian,  Jingyuan Yang,  Haoxuan Ding",
                "发布日期": "2022-07-21",
                "摘要": "  The Contrastive Language-Image Pre-training (CLIP) Model is a recently\nproposed large-scale pre-train model which attracts increasing attention in the\ncomputer vision community. Benefiting from its gigantic image-text training\nset, the CLIP model has learned outstanding capabilities in zero-shot learning\nand image-text matching. To boost the recognition performance of CLIP on some\ntarget visual concepts, it is often desirable to further update the CLIP model\nby fine-tuning some classes-of-interest on extra training data. This operation,\nhowever, raises an important concern: will the update hurt the zero-shot\nlearning or image-text matching capability of the CLIP, i.e., the catastrophic\nforgetting issue? If yes, could existing continual learning algorithms be\nadapted to alleviate the risk of catastrophic forgetting? To answer these\nquestions, this work conducts a systemic study on the continual learning issue\nof the CLIP model. We construct evaluation protocols to measure the impact of\nfine-tuning updates and explore different ways to upgrade existing continual\nlearning methods to mitigate the forgetting issue of the CLIP model. Our study\nreveals the particular challenges of CLIP continual learning problem and lays a\nfoundation for further researches. Moreover, we propose a new algorithm, dubbed\nLearning without Forgetting via Replayed Vocabulary (VR-LwF), which shows exact\neffectiveness for alleviating the forgetting issue of the CLIP model.\n",
                "链接": "https://arxiv.org/abs/2207.09248"
            },
            {
                "文章ID": "59592",
                "标题": "CLIPood: Generalizing CLIP to Out-of-Distributions",
                "作者": " Yang Shu,  Xingzhuo Guo,  Jialong Wu,  Ximei Wang,  Jianmin Wang,  Mingsheng Long",
                "发布日期": "2023-07-17",
                "摘要": "  Out-of-distribution (OOD) generalization, where the model needs to handle\ndistribution shifts from training, is a major challenge of machine learning.\nContrastive language-image pre-training (CLIP) models have shown impressive\nzero-shot ability, but the further adaptation of CLIP on downstream tasks\nundesirably degrades OOD performances. This paper aims at generalizing CLIP to\nout-of-distribution test data on downstream tasks. We propose CLIPood, a\nfine-tuning method that can adapt CLIP models to OOD situations where both\ndomain shifts and open classes may occur on the unseen test data. To exploit\nthe semantic relations between classes from the text modality, CLIPood\nintroduces a new training objective, margin metric softmax (MMS), with class\nadaptive margins for fine-tuning. To incorporate both pre-trained zero-shot\nmodel and fine-tuned task-adaptive model, CLIPood leverages a new optimization\nstrategy, Beta moving average (BMA), to maintain a temporal ensemble weighted\nby Beta distribution. Experiments on diverse datasets with different OOD\nscenarios show that CLIPood consistently outperforms existing generalization\ntechniques.\n",
                "链接": "https://arxiv.org/abs/2302.00864"
            },
            {
                "文章ID": "114514",
                "标题": "Training CLIP models on Data from Scientific Papers",
                "作者": " Calvin Metzger",
                "发布日期": "2023-11-09",
                "摘要": "  Contrastive Language-Image Pretraining (CLIP) models are able to capture the\nsemantic relationship of images and texts and have enabled a wide range of\napplications, from image retrieval to classification. These models are trained\nwith datasets extracted from web crawls, which are of large quantity but\nlimited quality. This paper explores whether limited amounts higher quality\ndata in a specific domain improve the general performance of CLIP models. To\nthis purpose, we extract text-image data from scientific papers hosted in the\narXiv and PubMed Central repositories. Experiments on small-scale CLIP models\n(ViT B/32) show that model performance increases on average, but only\nmoderately. This result indicates that using the data sources considered in the\npaper to train large-scale CLIP models is a worthwile research direction.\n",
                "链接": "https://arxiv.org/abs/2311.04711"
            },
            {
                "文章ID": "105355",
                "标题": "Learning Mask-aware CLIP Representations for Zero-Shot Segmentation",
                "作者": " Siyu Jiao,  Yunchao Wei,  Yaowei Wang,  Yao Zhao,  Humphrey Shi",
                "发布日期": "2023-10-03",
                "摘要": "  Recently, pre-trained vision-language models have been increasingly used to\ntackle the challenging zero-shot segmentation task. Typical solutions follow\nthe paradigm of first generating mask proposals and then adopting CLIP to\nclassify them. To maintain the CLIP's zero-shot transferability, previous\npractices favour to freeze CLIP during training. However, in the paper, we\nreveal that CLIP is insensitive to different mask proposals and tends to\nproduce similar predictions for various mask proposals of the same image. This\ninsensitivity results in numerous false positives when classifying mask\nproposals. This issue mainly relates to the fact that CLIP is trained with\nimage-level supervision. To alleviate this issue, we propose a simple yet\neffective method, named Mask-aware Fine-tuning (MAFT). Specifically,\nImage-Proposals CLIP Encoder (IP-CLIP Encoder) is proposed to handle arbitrary\nnumbers of image and mask proposals simultaneously. Then, mask-aware loss and\nself-distillation loss are designed to fine-tune IP-CLIP Encoder, ensuring CLIP\nis responsive to different mask proposals while not sacrificing\ntransferability. In this way, mask-aware representations can be easily learned\nto make the true positives stand out. Notably, our solution can seamlessly plug\ninto most existing methods without introducing any new parameters during the\nfine-tuning process. We conduct extensive experiments on the popular zero-shot\nbenchmarks. With MAFT, the performance of the state-of-the-art methods is\npromoted by a large margin: 50.4% (+ 8.2%) on COCO, 81.8% (+ 3.2%) on\nPascal-VOC, and 8.7% (+4.3%) on ADE20K in terms of mIoU for unseen classes. The\ncode is available at https://github.com/jiaosiyu1999/MAFT.git.\n",
                "链接": "https://arxiv.org/abs/2310.00240"
            },
            {
                "文章ID": "39762",
                "标题": "Understanding Pure CLIP Guidance for Voxel Grid NeRF Models",
                "作者": " Han-Hung Lee,  Angel X. Chang",
                "发布日期": "2022-10-03",
                "摘要": "  We explore the task of text to 3D object generation using CLIP. Specifically,\nwe use CLIP for guidance without access to any datasets, a setting we refer to\nas pure CLIP guidance. While prior work has adopted this setting, there is no\nsystematic study of mechanics for preventing adversarial generations within\nCLIP. We illustrate how different image-based augmentations prevent the\nadversarial generation problem, and how the generated results are impacted. We\ntest different CLIP model architectures and show that ensembling different\nmodels for guidance can prevent adversarial generations within bigger models\nand generate sharper results. Furthermore, we implement an implicit voxel grid\nmodel to show how neural networks provide an additional layer of\nregularization, resulting in better geometrical structure and coherency of\ngenerated objects. Compared to prior work, we achieve more coherent results\nwith higher memory efficiency and faster training speeds.\n",
                "链接": "https://arxiv.org/abs/2209.15172"
            },
            {
                "文章ID": "41910",
                "标题": "CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory",
                "作者": " Nur Muhammad Mahi Shafiullah,  Chris Paxton,  Lerrel Pinto,  Soumith Chintala,  Arthur Szlam",
                "发布日期": "2023-05-24",
                "摘要": "  We propose CLIP-Fields, an implicit scene model that can be used for a\nvariety of tasks, such as segmentation, instance identification, semantic\nsearch over space, and view localization. CLIP-Fields learns a mapping from\nspatial locations to semantic embedding vectors. Importantly, we show that this\nmapping can be trained with supervision coming only from web-image and web-text\ntrained models such as CLIP, Detic, and Sentence-BERT; and thus uses no direct\nhuman supervision. When compared to baselines like Mask-RCNN, our method\noutperforms on few-shot instance identification or semantic segmentation on the\nHM3D dataset with only a fraction of the examples. Finally, we show that using\nCLIP-Fields as a scene memory, robots can perform semantic navigation in\nreal-world environments. Our code and demonstration videos are available here:\nhttps://mahis.life/clip-fields\n",
                "链接": "https://arxiv.org/abs/2210.05663"
            },
            {
                "文章ID": "21689",
                "标题": "Multimodal Fake News Detection via CLIP-Guided Learning",
                "作者": " Yangming Zhou,  Qichao Ying,  Zhenxing Qian,  Sheng Li,  Xinpeng Zhang",
                "发布日期": "2022-05-31",
                "摘要": "  Multimodal fake news detection has attracted many research interests in\nsocial forensics. Many existing approaches introduce tailored attention\nmechanisms to guide the fusion of unimodal features. However, how the\nsimilarity of these features is calculated and how it will affect the\ndecision-making process in FND are still open questions. Besides, the potential\nof pretrained multi-modal feature learning models in fake news detection has\nnot been well exploited. This paper proposes a FND-CLIP framework, i.e., a\nmultimodal Fake News Detection network based on Contrastive Language-Image\nPretraining (CLIP). Given a targeted multimodal news, we extract the deep\nrepresentations from the image and text using a ResNet-based encoder, a\nBERT-based encoder and two pair-wise CLIP encoders. The multimodal feature is a\nconcatenation of the CLIP-generated features weighted by the standardized\ncross-modal similarity of the two modalities. The extracted features are\nfurther processed for redundancy reduction before feeding them into the final\nclassifier. We introduce a modality-wise attention module to adaptively\nreweight and aggregate the features. We have conducted extensive experiments on\ntypical fake news datasets. The results indicate that the proposed framework\nhas a better capability in mining crucial features for fake news detection. The\nproposed FND-CLIP can achieve better performances than previous works, i.e.,\n0.7\\%, 6.8\\% and 1.3\\% improvements in overall accuracy on Weibo, Politifact\nand Gossipcop, respectively. Besides, we justify that CLIP-based learning can\nallow better flexibility on multimodal feature selection.\n",
                "链接": "https://arxiv.org/abs/2205.14304"
            },
            {
                "文章ID": "123748",
                "标题": "PPO-Clip Attains Global Optimality: Towards Deeper Understandings of\n  Clipping",
                "作者": " Nai-Chieh Huang,  Ping-Chun Hsieh,  Kuo-Hao Ho,  I-Chen Wu",
                "发布日期": "2023-12-20",
                "摘要": "  Proximal Policy Optimization algorithm employing a clipped surrogate\nobjective (PPO-Clip) is a prominent exemplar of the policy optimization\nmethods. However, despite its remarkable empirical success, PPO-Clip lacks\ntheoretical substantiation to date. In this paper, we contribute to the field\nby establishing the first global convergence results of a PPO-Clip variant in\nboth tabular and neural function approximation settings. Our findings highlight\nthe $O(1/\\sqrt{T})$ min-iterate convergence rate specifically in the context of\nneural function approximation. We tackle the inherent challenges in analyzing\nPPO-Clip through three central concepts: (i) We introduce a generalized version\nof the PPO-Clip objective, illuminated by its connection with the hinge loss.\n(ii) Employing entropic mirror descent, we establish asymptotic convergence for\ntabular PPO-Clip with direct policy parameterization. (iii) Inspired by the\ntabular analysis, we streamline convergence analysis by introducing a two-step\npolicy improvement approach. This decouples policy search from complex neural\npolicy parameterization using a regression-based update scheme. Furthermore, we\ngain deeper insights into the efficacy of PPO-Clip by interpreting these\ngeneralized objectives. Our theoretical findings also mark the first\ncharacterization of the influence of the clipping mechanism on PPO-Clip\nconvergence. Importantly, the clipping range affects only the pre-constant of\nthe convergence rate.\n",
                "链接": "https://arxiv.org/abs/2312.12065"
            },
            {
                "文章ID": "60298",
                "标题": "U-Clip: On-Average Unbiased Stochastic Gradient Clipping",
                "作者": " Bryn Elesedy,  Marcus Hutter",
                "发布日期": "2023-02-07",
                "摘要": "  U-Clip is a simple amendment to gradient clipping that can be applied to any\niterative gradient optimization algorithm. Like regular clipping, U-Clip\ninvolves using gradients that are clipped to a prescribed size (e.g. with\ncomponent wise or norm based clipping) but instead of discarding the clipped\nportion of the gradient, U-Clip maintains a buffer of these values that is\nadded to the gradients on the next iteration (before clipping). We show that\nthe cumulative bias of the U-Clip updates is bounded by a constant. This\nimplies that the clipped updates are unbiased on average. Convergence follows\nvia a lemma that guarantees convergence with updates $u_i$ as long as\n$\\sum_{i=1}^t (u_i - g_i) = o(t)$ where $g_i$ are the gradients. Extensive\nexperimental exploration is performed on CIFAR10 with further validation given\non ImageNet.\n",
                "链接": "https://arxiv.org/abs/2302.02971"
            },
            {
                "文章ID": "27351",
                "标题": "Video + CLIP Baseline for Ego4D Long-term Action Anticipation",
                "作者": " Srijan Das,  Michael S. Ryoo",
                "发布日期": "2022-07-04",
                "摘要": "  In this report, we introduce our adaptation of image-text models for\nlong-term action anticipation. Our Video + CLIP framework makes use of a\nlarge-scale pre-trained paired image-text model: CLIP and a video encoder\nSlowfast network. The CLIP embedding provides fine-grained understanding of\nobjects relevant for an action whereas the slowfast network is responsible for\nmodeling temporal information within a video clip of few frames. We show that\nthe features obtained from both encoders are complementary to each other, thus\noutperforming the baseline on Ego4D for the task of long-term action\nanticipation. Our code is available at\ngithub.com/srijandas07/clip_baseline_LTA_Ego4d.\n",
                "链接": "https://arxiv.org/abs/2207.00579"
            },
            {
                "文章ID": "54979",
                "标题": "When are Lemons Purple? The Concept Association Bias of CLIP",
                "作者": " Yutaro Yamada,  Yingtian Tang,  Ilker Yildirim",
                "发布日期": "2022-12-26",
                "摘要": "  Large-scale vision-language models such as CLIP have shown impressive\nperformance on zero-shot image classification and image-to-text retrieval.\nHowever, such zero-shot performance of CLIP-based models does not realize in\ntasks that require a finer-grained correspondence between vision and language,\nsuch as Visual Question Answering (VQA). We investigate why this is the case,\nand report an interesting phenomenon of CLIP, which we call the Concept\nAssociation Bias (CAB), as a potential cause of the difficulty of applying CLIP\nto VQA and similar tasks. CAB is especially apparent when two concepts are\npresent in the given image while a text prompt only contains a single concept.\nIn such a case, we find that CLIP tends to treat input as a bag of concepts and\nattempts to fill in the other missing concept crossmodally, leading to an\nunexpected zero-shot prediction. For example, when asked for the color of a\nlemon in an image, CLIP predicts ``purple'' if the image contains a lemon and\nan eggplant. We demonstrate the Concept Association Bias of CLIP by showing\nthat CLIP's zero-shot classification performance greatly suffers when there is\na strong concept association between an object (e.g. lemon) and an attribute\n(e.g. its color). On the other hand, when the association between object and\nattribute is weak, we do not see this phenomenon. Furthermore, we show that CAB\nis significantly mitigated when we enable CLIP to learn deeper structure across\nimage and text embeddings by adding an additional Transformer on top of CLIP\nand fine-tuning it on VQA. We find that across such fine-tuned variants of\nCLIP, the strength of CAB in a model predicts how well it performs on VQA.\n",
                "链接": "https://arxiv.org/abs/2212.12043"
            },
            {
                "文章ID": "98111",
                "标题": "CgT-GAN: CLIP-guided Text GAN for Image Captioning",
                "作者": " Jiarui Yu,  Haoran Li,  Yanbin Hao,  Bin Zhu,  Tong Xu,  Xiangnan He",
                "发布日期": "2023-08-24",
                "摘要": "  The large-scale visual-language pre-trained model, Contrastive Language-Image\nPre-training (CLIP), has significantly improved image captioning for scenarios\nwithout human-annotated image-caption pairs. Recent advanced CLIP-based image\ncaptioning without human annotations follows a text-only training paradigm,\ni.e., reconstructing text from shared embedding space. Nevertheless, these\napproaches are limited by the training/inference gap or huge storage\nrequirements for text embeddings. Given that it is trivial to obtain images in\nthe real world, we propose CLIP-guided text GAN (CgT-GAN), which incorporates\nimages into the training process to enable the model to \"see\" real visual\nmodality. Particularly, we use adversarial training to teach CgT-GAN to mimic\nthe phrases of an external text corpus and CLIP-based reward to provide\nsemantic guidance. The caption generator is jointly rewarded based on the\ncaption naturalness to human language calculated from the GAN's discriminator\nand the semantic guidance reward computed by the CLIP-based reward module. In\naddition to the cosine similarity as the semantic guidance reward (i.e.,\nCLIP-cos), we further introduce a novel semantic guidance reward called\nCLIP-agg, which aligns the generated caption with a weighted text embedding by\nattentively aggregating the entire corpus. Experimental results on three\nsubtasks (ZS-IC, In-UIC and Cross-UIC) show that CgT-GAN outperforms\nstate-of-the-art methods significantly across all metrics. Code is available at\nhttps://github.com/Lihr747/CgtGAN.\n",
                "链接": "https://arxiv.org/abs/2308.12045"
            },
            {
                "文章ID": "57421",
                "标题": "Face Recognition in the age of CLIP & Billion image datasets",
                "作者": " Aaditya Bhat,  Shrey Jain",
                "发布日期": "2023-01-19",
                "摘要": "  CLIP (Contrastive Language-Image Pre-training) models developed by OpenAI\nhave achieved outstanding results on various image recognition and retrieval\ntasks, displaying strong zero-shot performance. This means that they are able\nto perform effectively on tasks for which they have not been explicitly\ntrained. Inspired by the success of OpenAI CLIP, a new publicly available\ndataset called LAION-5B was collected which resulted in the development of open\nViT-H/14, ViT-G/14 models that outperform the OpenAI L/14 model. The LAION-5B\ndataset also released an approximate nearest neighbor index, with a web\ninterface for search & subset creation.\n  In this paper, we evaluate the performance of various CLIP models as\nzero-shot face recognizers. Our findings show that CLIP models perform well on\nface recognition tasks, but increasing the size of the CLIP model does not\nnecessarily lead to improved accuracy. Additionally, we investigate the\nrobustness of CLIP models against data poisoning attacks by testing their\nperformance on poisoned data. Through this analysis, we aim to understand the\npotential consequences and misuse of search engines built using CLIP models,\nwhich could potentially function as unintentional face recognition engines.\n",
                "链接": "https://arxiv.org/abs/2301.07315"
            },
            {
                "文章ID": "110934",
                "标题": "SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial\n  Understanding",
                "作者": " Haoxiang Wang,  Pavan Kumar Anasosalu Vasu,  Fartash Faghri,  Raviteja Vemulapalli,  Mehrdad Farajtabar,  Sachin Mehta,  Mohammad Rastegari,  Oncel Tuzel,  Hadi Pouransari",
                "发布日期": "2023-11-21",
                "摘要": "  The landscape of publicly available vision foundation models (VFMs), such as\nCLIP and Segment Anything Model (SAM), is expanding rapidly. VFMs are endowed\nwith distinct capabilities stemming from their pre-training objectives. For\ninstance, CLIP excels in semantic understanding, while SAM specializes in\nspatial understanding for segmentation. In this work, we introduce a simple\nrecipe to efficiently merge VFMs into a unified model that absorbs their\nexpertise. Our method integrates techniques of multi-task learning, continual\nlearning, and distillation. Further, it demands significantly less\ncomputational cost compared to traditional multi-task training from scratch,\nand it only needs a small fraction of the pre-training datasets that were\ninitially used to train individual models. By applying our method to SAM and\nCLIP, we obtain SAM-CLIP: a unified model that combines the capabilities of SAM\nand CLIP into a single vision transformer. Compared with deploying SAM and CLIP\nindependently, our merged model, SAM-CLIP, reduces storage and compute costs\nfor inference, making it well-suited for edge device applications. We show that\nSAM-CLIP not only retains the foundational strengths of SAM and CLIP, but also\nintroduces synergistic functionalities, notably in zero-shot semantic\nsegmentation, where SAM-CLIP establishes new state-of-the-art results on 5\nbenchmarks. It outperforms previous models that are specifically designed for\nthis task by a large margin, including +6.8% and +5.9% mean IoU improvement on\nPascal-VOC and COCO-Stuff datasets, respectively.\n",
                "链接": "https://arxiv.org/abs/2310.15308"
            },
            {
                "文章ID": "22474",
                "标题": "CLIP4IDC: CLIP for Image Difference Captioning",
                "作者": " Zixin Guo,  Tzu-Jui Julius Wang,  Jorma Laaksonen",
                "发布日期": "2022-10-19",
                "摘要": "  Image Difference Captioning (IDC) aims at generating sentences to describe\ndifferences between two similar-looking images. Conventional approaches learn\nan IDC model with a pre-trained and usually frozen visual feature extractor.\nAccordingly, two major issues may arise: (1) a large domain gap usually exists\nbetween the pre-training datasets used for training such a visual encoder and\nthat of the downstream IDC task, and (2) the visual feature extractor, when\nseparately encoding two images, often does not effectively encode the visual\nchanges between two images. Due to the excellent zero-shot performance of the\nrecently proposed CLIP, we thus propose CLIP4IDC to transfer a CLIP model for\nthe IDC task to address those issues. Different from directly fine-tuning CLIP\nto generate sentences, we introduce an adaptation training process to adapt\nCLIP's visual encoder to capture and align differences in image pairs based on\nthe textual descriptions. Experiments on three IDC benchmark datasets,\nCLEVR-Change, Spot-the-Diff, and Image-Editing-Request, demonstrate the\neffectiveness of CLIP4IDC.\n",
                "链接": "https://arxiv.org/abs/2206.00629"
            },
            {
                "文章ID": "98001",
                "标题": "CLIP Multi-modal Hashing: A new baseline CLIPMH",
                "作者": " Jian Zhu,  Mingkai Sheng,  Mingda Ke,  Zhangmin Huang,  Jingfei Chang",
                "发布日期": "2023-08-24",
                "摘要": "  The multi-modal hashing method is widely used in multimedia retrieval. It can\nfuse multi-source data to generate binary hash code. However, the current\nmulti-modal methods have the problem of low retrieval accuracy. The reason is\nthat the individual backbone networks have limited feature expression\ncapabilities and are not jointly pre-trained on large-scale unsupervised\nmulti-modal data. To solve this problem, we propose a new baseline CLIP\nMulti-modal Hashing (CLIPMH) method. It uses CLIP model to extract text and\nimage features, and then fuse to generate hash code. CLIP improves the\nexpressiveness of each modal feature. In this way, it can greatly improve the\nretrieval performance of multi-modal hashing methods. In comparison to\nstate-of-the-art unsupervised and supervised multi-modal hashing methods,\nexperiments reveal that the proposed CLIPMH can significantly enhance\nperformance (Maximum increase of 8.38%). CLIP also has great advantages over\nthe text and visual backbone networks commonly used before.\n",
                "链接": "https://arxiv.org/abs/2308.11797"
            },
            {
                "文章ID": "68945",
                "标题": "GestureDiffuCLIP: Gesture Diffusion Model with CLIP Latents",
                "作者": " Tenglong Ao,  Zeyi Zhang,  Libin Liu",
                "发布日期": "2023-10-17",
                "摘要": "  The automatic generation of stylized co-speech gestures has recently received\nincreasing attention. Previous systems typically allow style control via\npredefined text labels or example motion clips, which are often not flexible\nenough to convey user intent accurately. In this work, we present\nGestureDiffuCLIP, a neural network framework for synthesizing realistic,\nstylized co-speech gestures with flexible style control. We leverage the power\nof the large-scale Contrastive-Language-Image-Pre-training (CLIP) model and\npresent a novel CLIP-guided mechanism that extracts efficient style\nrepresentations from multiple input modalities, such as a piece of text, an\nexample motion clip, or a video. Our system learns a latent diffusion model to\ngenerate high-quality gestures and infuses the CLIP representations of style\ninto the generator via an adaptive instance normalization (AdaIN) layer. We\nfurther devise a gesture-transcript alignment mechanism that ensures a\nsemantically correct gesture generation based on contrastive learning. Our\nsystem can also be extended to allow fine-grained style control of individual\nbody parts. We demonstrate an extensive set of examples showing the flexibility\nand generalizability of our model to a variety of style descriptions. In a user\nstudy, we show that our system outperforms the state-of-the-art approaches\nregarding human likeness, appropriateness, and style correctness.\n",
                "链接": "https://arxiv.org/abs/2303.14613"
            },
            {
                "文章ID": "79829",
                "标题": "S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist\n  Captions",
                "作者": " Sangwoo Mo,  Minkyu Kim,  Kyungmin Lee,  Jinwoo Shin",
                "发布日期": "2023-10-26",
                "摘要": "  Vision-language models, such as contrastive language-image pre-training\n(CLIP), have demonstrated impressive results in natural image domains. However,\nthese models often struggle when applied to specialized domains like remote\nsensing, and adapting to such domains is challenging due to the limited number\nof image-text pairs available for training. To address this, we propose S-CLIP,\na semi-supervised learning method for training CLIP that utilizes additional\nunpaired images. S-CLIP employs two pseudo-labeling strategies specifically\ndesigned for contrastive learning and the language modality. The caption-level\npseudo-label is given by a combination of captions of paired images, obtained\nby solving an optimal transport problem between unpaired and paired images. The\nkeyword-level pseudo-label is given by a keyword in the caption of the nearest\npaired image, trained through partial label learning that assumes a candidate\nset of labels for supervision instead of the exact one. By combining these\nobjectives, S-CLIP significantly enhances the training of CLIP using only a few\nimage-text pairs, as demonstrated in various specialist domains, including\nremote sensing, fashion, scientific figures, and comics. For instance, S-CLIP\nimproves CLIP by 10% for zero-shot classification and 4% for image-text\nretrieval on the remote sensing benchmark, matching the performance of\nsupervised CLIP while using three times fewer image-text pairs.\n",
                "链接": "https://arxiv.org/abs/2305.14095"
            },
            {
                "文章ID": "105622",
                "标题": "Understanding Transferable Representation Learning and Zero-shot\n  Transfer in CLIP",
                "作者": " Zixiang Chen,  Yihe Deng,  Yuanzhi Li,  Quanquan Gu",
                "发布日期": "2023-10-03",
                "摘要": "  Multi-modal learning has become increasingly popular due to its ability to\nleverage information from different data sources (e.g., text and images) to\nimprove the model performance. Recently, CLIP has emerged as an effective\napproach that employs vision-language contrastive pretraining to learn joint\nimage and text representations and exhibits remarkable performance in zero-shot\nlearning and text-guided natural image generation. Despite the huge practical\nsuccess of CLIP, its theoretical understanding remains elusive. In this paper,\nwe formally study transferrable representation learning underlying CLIP and\ndemonstrate how features from different modalities get aligned. We also analyze\nits zero-shot transfer performance on the downstream tasks. Inspired by our\nanalysis, we propose a new CLIP-type approach, which achieves better\nperformance than CLIP and other state-of-the-art methods on benchmark datasets.\n",
                "链接": "https://arxiv.org/abs/2310.00927"
            },
            {
                "文章ID": "10315",
                "标题": "One-Shot Adaptation of GAN in Just One CLIP",
                "作者": " Gihyun Kwon,  Jong Chul Ye",
                "发布日期": "2023-01-31",
                "摘要": "  There are many recent research efforts to fine-tune a pre-trained generator\nwith a few target images to generate images of a novel domain. Unfortunately,\nthese methods often suffer from overfitting or under-fitting when fine-tuned\nwith a single target image. To address this, here we present a novel\nsingle-shot GAN adaptation method through unified CLIP space manipulations.\nSpecifically, our model employs a two-step training strategy: reference image\nsearch in the source generator using a CLIP-guided latent optimization,\nfollowed by generator fine-tuning with a novel loss function that imposes CLIP\nspace consistency between the source and adapted generators. To further improve\nthe adapted model to produce spatially consistent samples with respect to the\nsource generator, we also propose contrastive regularization for patchwise\nrelationships in the CLIP space. Experimental results show that our model\ngenerates diverse outputs with the target texture and outperforms the baseline\nmodels both qualitatively and quantitatively. Furthermore, we show that our\nCLIP space manipulation strategy allows more effective attribute editing.\n",
                "链接": "https://arxiv.org/abs/2203.09301"
            },
            {
                "文章ID": "114262",
                "标题": "CLIP Guided Image-perceptive Prompt Learning for Image Enhancement",
                "作者": " Weiwen Chen,  Qiuhong Ke,  Zinuo Li",
                "发布日期": "2023-11-23",
                "摘要": "  Image enhancement is a significant research area in the fields of computer\nvision and image processing. In recent years, many learning-based methods for\nimage enhancement have been developed, where the Look-up-table (LUT) has proven\nto be an effective tool. In this paper, we delve into the potential of\nContrastive Language-Image Pre-Training (CLIP) Guided Prompt Learning,\nproposing a simple structure called CLIP-LUT for image enhancement. We found\nthat the prior knowledge of CLIP can effectively discern the quality of\ndegraded images, which can provide reliable guidance. To be specific, We\ninitially learn image-perceptive prompts to distinguish between original and\ntarget images using CLIP model, in the meanwhile, we introduce a very simple\nnetwork by incorporating a simple baseline to predict the weights of three\ndifferent LUT as enhancement network. The obtained prompts are used to steer\nthe enhancement network like a loss function and improve the performance of\nmodel. We demonstrate that by simply combining a straightforward method with\nCLIP, we can obtain satisfactory results.\n",
                "链接": "https://arxiv.org/abs/2311.03943"
            },
            {
                "文章ID": "18146",
                "标题": "CLIP-CLOP: CLIP-Guided Collage and Photomontage",
                "作者": " Piotr Mirowski,  Dylan Banarse,  Mateusz Malinowski,  Simon Osindero,  Chrisantha Fernando",
                "发布日期": "2022-07-26",
                "摘要": "  The unabated mystique of large-scale neural networks, such as the CLIP dual\nimage-and-text encoder, popularized automatically generated art. Increasingly\nmore sophisticated generators enhanced the artworks' realism and visual\nappearance, and creative prompt engineering enabled stylistic expression.\nGuided by an artist-in-the-loop ideal, we design a gradient-based generator to\nproduce collages. It requires the human artist to curate libraries of image\npatches and to describe (with prompts) the whole image composition, with the\noption to manually adjust the patches' positions during generation, thereby\nallowing humans to reclaim some control of the process and achieve greater\ncreative freedom. We explore the aesthetic potentials of high-resolution\ncollages, and provide an open-source Google Colab as an artistic tool.\n",
                "链接": "https://arxiv.org/abs/2205.03146"
            },
            {
                "文章ID": "111634",
                "标题": "Prototypical Contrastive Learning-based CLIP Fine-tuning for Object\n  Re-identification",
                "作者": " Jiachen Li,  Xiaojin Gong",
                "发布日期": "2023-10-27",
                "摘要": "  This work aims to adapt large-scale pre-trained vision-language models, such\nas contrastive language-image pretraining (CLIP), to enhance the performance of\nobject reidentification (Re-ID) across various supervision settings. Although\nprompt learning has enabled a recent work named CLIP-ReID to achieve promising\nperformance, the underlying mechanisms and the necessity of prompt learning\nremain unclear due to the absence of semantic labels in ReID tasks. In this\nwork, we first analyze the role prompt learning in CLIP-ReID and identify its\nlimitations. Based on our investigations, we propose a simple yet effective\napproach to adapt CLIP for supervised object Re-ID. Our approach directly\nfine-tunes the image encoder of CLIP using a prototypical contrastive learning\n(PCL) loss, eliminating the need for prompt learning. Experimental results on\nboth person and vehicle Re-ID datasets demonstrate the competitiveness of our\nmethod compared to CLIP-ReID. Furthermore, we extend our PCL-based CLIP\nfine-tuning approach to unsupervised scenarios, where we achieve state-of-the\nart performance.\n",
                "链接": "https://arxiv.org/abs/2310.17218"
            },
            {
                "文章ID": "24315",
                "标题": "Transductive CLIP with Class-Conditional Contrastive Learning",
                "作者": " Junchu Huang,  Weijie Chen,  Shicai Yang,  Di Xie,  Shiliang Pu,  Yueting Zhuang",
                "发布日期": "2022-06-14",
                "摘要": "  Inspired by the remarkable zero-shot generalization capacity of\nvision-language pre-trained model, we seek to leverage the supervision from\nCLIP model to alleviate the burden of data labeling. However, such supervision\ninevitably contains the label noise, which significantly degrades the\ndiscriminative power of the classification model. In this work, we propose\nTransductive CLIP, a novel framework for learning a classification network with\nnoisy labels from scratch. Firstly, a class-conditional contrastive learning\nmechanism is proposed to mitigate the reliance on pseudo labels and boost the\ntolerance to noisy labels. Secondly, ensemble labels is adopted as a pseudo\nlabel updating strategy to stabilize the training of deep neural networks with\nnoisy labels. This framework can reduce the impact of noisy labels from CLIP\nmodel effectively by combining both techniques. Experiments on multiple\nbenchmark datasets demonstrate the substantial improvements over other\nstate-of-the-art methods.\n",
                "链接": "https://arxiv.org/abs/2206.06177"
            },
            {
                "文章ID": "49022",
                "标题": "CAE v2: Context Autoencoder with CLIP Target",
                "作者": " Xinyu Zhang,  Jiahui Chen,  Junkun Yuan,  Qiang Chen,  Jian Wang,  Xiaodi Wang,  Shumin Han,  Xiaokang Chen,  Jimin Pi,  Kun Yao,  Junyu Han,  Errui Ding,  Jingdong Wang",
                "发布日期": "2022-11-18",
                "摘要": "  Masked image modeling (MIM) learns visual representation by masking and\nreconstructing image patches. Applying the reconstruction supervision on the\nCLIP representation has been proven effective for MIM. However, it is still\nunder-explored how CLIP supervision in MIM influences performance. To\ninvestigate strategies for refining the CLIP-targeted MIM, we study two\ncritical elements in MIM, i.e., the supervision position and the mask ratio,\nand reveal two interesting perspectives, relying on our developed simple\npipeline, context autodecoder with CLIP target (CAE v2). Firstly, we observe\nthat the supervision on visible patches achieves remarkable performance, even\nbetter than that on masked patches, where the latter is the standard format in\nthe existing MIM methods. Secondly, the optimal mask ratio positively\ncorrelates to the model size. That is to say, the smaller the model, the lower\nthe mask ratio needs to be. Driven by these two discoveries, our simple and\nconcise approach CAE v2 achieves superior performance on a series of downstream\ntasks. For example, a vanilla ViT-Large model achieves 81.7% and 86.7% top-1\naccuracy on linear probing and fine-tuning on ImageNet-1K, and 55.9% mIoU on\nsemantic segmentation on ADE20K with the pre-training for 300 epochs. We hope\nour findings can be helpful guidelines for the pre-training in the MIM area,\nespecially for the small-scale models.\n",
                "链接": "https://arxiv.org/abs/2211.09799"
            },
            {
                "文章ID": "41351",
                "标题": "Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP",
                "作者": " Feng Liang,  Bichen Wu,  Xiaoliang Dai,  Kunpeng Li,  Yinan Zhao,  Hang Zhang,  Peizhao Zhang,  Peter Vajda,  Diana Marculescu",
                "发布日期": "2023-04-04",
                "摘要": "  Open-vocabulary semantic segmentation aims to segment an image into semantic\nregions according to text descriptions, which may not have been seen during\ntraining. Recent two-stage methods first generate class-agnostic mask proposals\nand then leverage pre-trained vision-language models, e.g., CLIP, to classify\nmasked regions. We identify the performance bottleneck of this paradigm to be\nthe pre-trained CLIP model, since it does not perform well on masked images. To\naddress this, we propose to finetune CLIP on a collection of masked image\nregions and their corresponding text descriptions. We collect training data by\nmining an existing image-caption dataset (e.g., COCO Captions), using CLIP to\nmatch masked image regions to nouns in the image captions. Compared with the\nmore precise and manually annotated segmentation labels with fixed classes\n(e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain\nCLIP's generalization ability. Along with finetuning the entire model, we\nutilize the \"blank\" areas in masked images using a method we dub mask prompt\ntuning. Experiments demonstrate mask prompt tuning brings significant\nimprovement without modifying any weights of CLIP, and it can further improve a\nfully finetuned model. In particular, when trained on COCO and evaluated on\nADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the\nprevious state-of-the-art. For the first time, open-vocabulary generalist\nmodels match the performance of supervised specialist models in 2017 without\ndataset-specific adaptations.\n",
                "链接": "https://arxiv.org/abs/2210.04150"
            },
            {
                "文章ID": "94707",
                "标题": "Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen\n  Convolutional CLIP",
                "作者": " Qihang Yu,  Ju He,  Xueqing Deng,  Xiaohui Shen,  Liang-Chieh Chen",
                "发布日期": "2023-11-16",
                "摘要": "  Open-vocabulary segmentation is a challenging task requiring segmenting and\nrecognizing objects from an open set of categories. One way to address this\nchallenge is to leverage multi-modal models, such as CLIP, to provide image and\ntext features in a shared embedding space, which bridges the gap between\nclosed-vocabulary and open-vocabulary recognition. Hence, existing methods\noften adopt a two-stage framework to tackle the problem, where the inputs first\ngo through a mask generator and then through the CLIP model along with the\npredicted masks. This process involves extracting features from images multiple\ntimes, which can be ineffective and inefficient. By contrast, we propose to\nbuild everything into a single-stage framework using a shared Frozen\nConvolutional CLIP backbone, which not only significantly simplifies the\ncurrent two-stage pipeline, but also remarkably yields a better accuracy-cost\ntrade-off. The proposed FC-CLIP, benefits from the following observations: the\nfrozen CLIP backbone maintains the ability of open-vocabulary classification\nand can also serve as a strong mask generator, and the convolutional CLIP\ngeneralizes well to a larger input resolution than the one used during\ncontrastive image-text pretraining. When training on COCO panoptic data only\nand testing in a zero-shot manner, FC-CLIP achieve 26.8 PQ, 16.8 AP, and 34.1\nmIoU on ADE20K, 18.2 PQ, 27.9 mIoU on Mapillary Vistas, 44.0 PQ, 26.8 AP, 56.2\nmIoU on Cityscapes, outperforming the prior art by +4.2 PQ, +2.4 AP, +4.2 mIoU\non ADE20K, +4.0 PQ on Mapillary Vistas and +20.1 PQ on Cityscapes,\nrespectively. Additionally, the training and testing time of FC-CLIP is 7.5x\nand 6.6x significantly faster than the same prior art, while using 5.9x fewer\nparameters. FC-CLIP also sets a new state-of-the-art performance across various\nopen-vocabulary semantic segmentation datasets. Code at\nhttps://github.com/bytedance/fc-clip\n",
                "链接": "https://arxiv.org/abs/2308.02487"
            },
            {
                "文章ID": "71145",
                "标题": "Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting",
                "作者": " Syed Talal Wasim,  Muzammal Naseer,  Salman Khan,  Fahad Shahbaz Khan,  Mubarak Shah",
                "发布日期": "2023-04-10",
                "摘要": "  Adopting contrastive image-text pretrained models like CLIP towards video\nclassification has gained attention due to its cost-effectiveness and\ncompetitive performance. However, recent works in this area face a trade-off.\nFinetuning the pretrained model to achieve strong supervised performance\nresults in low zero-shot generalization. Similarly, freezing the backbone to\nretain zero-shot capability causes significant drop in supervised accuracy.\nBecause of this, recent works in literature typically train separate models for\nsupervised and zero-shot action recognition. In this work, we propose a\nmultimodal prompt learning scheme that works to balance the supervised and\nzero-shot performance under a single unified training. Our prompting approach\non the vision side caters for three aspects: 1) Global video-level prompts to\nmodel the data distribution; 2) Local frame-level prompts to provide per-frame\ndiscriminative conditioning; and 3) a summary prompt to extract a condensed\nvideo representation. Additionally, we define a prompting scheme on the text\nside to augment the textual context. Through this prompting scheme, we can\nachieve state-of-the-art zero-shot performance on Kinetics-600, HMDB51 and\nUCF101 while remaining competitive in the supervised setting. By keeping the\npretrained backbone frozen, we optimize a much lower number of parameters and\nretain the existing general representation which helps achieve the strong\nzero-shot performance. Our codes/models are released at\nhttps://github.com/TalalWasim/Vita-CLIP.\n",
                "链接": "https://arxiv.org/abs/2304.03307"
            },
            {
                "文章ID": "85801",
                "标题": "RISCLIP: Referring Image Segmentation Framework using CLIP",
                "作者": " Seoyeon Kim,  Minguk Kang,  Jaesik Park",
                "发布日期": "2023-06-16",
                "摘要": "  Recent advances in computer vision and natural language processing have\nnaturally led to active research in multi-modal tasks, including Referring\nImage Segmentation (RIS). Recent approaches have advanced the frontier of RIS\nby impressive margins, but they require an additional pretraining stage on\nexternal visual grounding datasets to achieve the state-of-the-art\nperformances. We attempt to break free from this requirement by effectively\nadapting Contrastive Language-Image Pretraining (CLIP) to RIS. We propose a\nnovel framework that residually adapts frozen CLIP features to RIS with Fusion\nAdapters and Backbone Adapters. Freezing CLIP preserves the backbone's rich,\ngeneral image-text alignment knowledge, whilst Fusion Adapters introduce\nmulti-modal communication and Backbone Adapters inject new knowledge useful in\nsolving RIS. Our method reaches a new state of the art on three major RIS\nbenchmarks. We attain such performance without additional pretraining and\nthereby absolve the necessity of extra training and data preparation. Source\ncode and model weights will be available upon publication.\n",
                "链接": "https://arxiv.org/abs/2306.08498"
            },
            {
                "文章ID": "122443",
                "标题": "CLIP-guided Federated Learning on Heterogeneous and Long-Tailed Data",
                "作者": " Jiangming Shi,  Shanshan Zheng,  Xiangbo Yin,  Yang Lu,  Yuan Xie,  Yanyun Qu",
                "发布日期": "2023-12-15",
                "摘要": "  Federated learning (FL) provides a decentralized machine learning paradigm\nwhere a server collaborates with a group of clients to learn a global model\nwithout accessing the clients' data. User heterogeneity is a significant\nchallenge for FL, which together with the class-distribution imbalance further\nenhances the difficulty of FL. Great progress has been made in large\nvision-language models, such as Contrastive Language-Image Pre-training (CLIP),\nwhich paves a new way for image classification and object recognition. Inspired\nby the success of CLIP on few-shot and zero-shot learning, we use CLIP to\noptimize the federated learning between server and client models under its\nvision-language supervision. It is promising to mitigate the user heterogeneity\nand class-distribution balance due to the powerful cross-modality\nrepresentation and rich open-vocabulary prior knowledge. In this paper, we\npropose the CLIP-guided FL (CLIP2FL) method on heterogeneous and long-tailed\ndata. In CLIP2FL, the knowledge of the off-the-shelf CLIP model is transferred\nto the client-server models, and a bridge is built between the client and\nserver. Specifically, for client-side learning, knowledge distillation is\nconducted between client models and CLIP to improve the ability of client-side\nfeature representation. For server-side learning, in order to mitigate the\nheterogeneity and class-distribution imbalance, we generate federated features\nto retrain the server model. A prototype contrastive learning with the\nsupervision of the text encoder of CLIP is introduced to generate federated\nfeatures depending on the client-side gradients, and they are used to retrain a\nbalanced server classifier.\n",
                "链接": "https://arxiv.org/abs/2312.08648"
            },
            {
                "文章ID": "119417",
                "标题": "CLIP-QDA: An Explainable Concept Bottleneck Model",
                "作者": " Rémi Kazmierczak,  Eloïse Berthier,  Goran Frehse,  Gianni Franchi",
                "发布日期": "2023-12-04",
                "摘要": "  In this paper, we introduce an explainable algorithm designed from a\nmulti-modal foundation model, that performs fast and explainable image\nclassification. Drawing inspiration from CLIP-based Concept Bottleneck Models\n(CBMs), our method creates a latent space where each neuron is linked to a\nspecific word. Observing that this latent space can be modeled with simple\ndistributions, we use a Mixture of Gaussians (MoG) formalism to enhance the\ninterpretability of this latent space. Then, we introduce CLIP-QDA, a\nclassifier that only uses statistical values to infer labels from the concepts.\nIn addition, this formalism allows for both local and global explanations.\nThese explanations come from the inner design of our architecture, our work is\npart of a new family of greybox models, combining performances of opaque\nfoundation models and the interpretability of transparent models. Our empirical\nfindings show that in instances where the MoG assumption holds, CLIP-QDA\nachieves similar accuracy with state-of-the-art methods CBMs. Our explanations\ncompete with existing XAI methods while being faster to compute.\n",
                "链接": "https://arxiv.org/abs/2312.00110"
            },
            {
                "文章ID": "46300",
                "标题": "Text-Only Training for Image Captioning using Noise-Injected CLIP",
                "作者": " David Nukrai,  Ron Mokady,  Amir Globerson",
                "发布日期": "2023-10-13",
                "摘要": "  We consider the task of image-captioning using only the CLIP model and\nadditional text data at training time, and no additional captioned images. Our\napproach relies on the fact that CLIP is trained to make visual and textual\nembeddings similar. Therefore, we only need to learn how to translate CLIP\ntextual embeddings back into text, and we can learn how to do this by learning\na decoder for the frozen CLIP text encoder using only text. We argue that this\nintuition is \"almost correct\" because of a gap between the embedding spaces,\nand propose to rectify this via noise injection during training. We demonstrate\nthe effectiveness of our approach by showing SOTA zero-shot image captioning\nacross four benchmarks, including style transfer. Code, data, and models are\navailable on GitHub.\n",
                "链接": "https://arxiv.org/abs/2211.00575"
            },
            {
                "文章ID": "89736",
                "标题": "Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning",
                "作者": " Jishnu Jaykumar P,  Kamalesh Palanisamy,  Yu-Wei Chao,  Xinya Du,  Yu Xiang",
                "发布日期": "2023-07-11",
                "摘要": "  We propose a novel framework for few-shot learning by leveraging large-scale\nvision-language models such as CLIP. Motivated by the unimodal prototypical\nnetworks for few-shot learning, we introduce PROTO-CLIP that utilizes image\nprototypes and text prototypes for few-shot learning. Specifically, PROTO-CLIP\nadapts the image encoder and text encoder in CLIP in a joint fashion using\nfew-shot examples. The two encoders are used to compute prototypes of image\nclasses for classification. During adaptation, we propose aligning the image\nand text prototypes of corresponding classes. Such a proposed alignment is\nbeneficial for few-shot classification due to the contributions from both types\nof prototypes. We demonstrate the effectiveness of our method by conducting\nexperiments on benchmark datasets for few-shot learning as well as in the real\nworld for robot perception.\n",
                "链接": "https://arxiv.org/abs/2307.03073"
            },
            {
                "文章ID": "110458",
                "标题": "CLIP meets Model Zoo Experts: Pseudo-Supervision for Visual Enhancement",
                "作者": " Mohammadreza Salehi,  Mehrdad Farajtabar,  Maxwell Horton,  Fartash Faghri,  Hadi Pouransari,  Raviteja Vemulapalli,  Oncel Tuzel,  Ali Farhadi,  Mohammad Rastegari,  Sachin Mehta",
                "发布日期": "2023-10-24",
                "摘要": "  Contrastive language image pretraining (CLIP) is a standard method for\ntraining vision-language models. While CLIP is scalable, promptable, and robust\nto distribution shifts on image classification tasks, it lacks object\nlocalization capabilities. This paper studies the following question: Can we\naugment CLIP training with task-specific vision models from model zoos to\nimprove its visual representations? Towards this end, we leverage open-source\ntask-specific vision models to generate pseudo-labels for an uncurated and\nnoisy image-text dataset. Subsequently, we train CLIP models on these\npseudo-labels in addition to the contrastive training on image and text pairs.\nThis simple setup shows substantial improvements of up to 16.3% across\ndifferent vision tasks, including segmentation, detection, depth estimation,\nand surface normal estimation. Importantly, these enhancements are achieved\nwithout compromising CLIP's existing capabilities, including its proficiency in\npromptable zero-shot classification.\n",
                "链接": "https://arxiv.org/abs/2310.14108"
            },
            {
                "文章ID": "82987",
                "标题": "Exploring the Versatility of Zero-Shot CLIP for Interstitial Lung\n  Disease Classification",
                "作者": " Cara Van Uden,  Christian Bluethgen,  Maayane Attias,  Malgorzata Polacin,  Haiwei Henry Guo,  Neha Simha,  Rishi Raj,  Curtis Langlotz",
                "发布日期": "2023-09-14",
                "摘要": "  Interstitial lung diseases (ILD) present diagnostic challenges due to their\nvaried manifestations and overlapping imaging features. To address this, we\npropose a machine learning approach that utilizes CLIP, a multimodal (image and\ntext) self-supervised model, for ILD classification. We extensively integrate\nzero-shot CLIP throughout our workflow, starting from the initial extraction of\nimage patches from volumetric CT scans and proceeding to ILD classification\nusing \"patch montages\". Furthermore, we investigate how domain adaptive\npretraining (DAPT) CLIP with task-specific images (CT \"patch montages\"\nextracted with ILD-specific prompts for CLIP) and/or text (lung-specific\nsections of radiology reports) affects downstream ILD classification\nperformance. By leveraging CLIP-extracted \"patch montages\" and DAPT, we achieve\nstrong zero-shot ILD classification results, including an AUROC of 0.893,\nwithout the need for any labeled training data. This work highlights the\nversatility and potential of multimodal models like CLIP for medical image\nclassification tasks where labeled data is scarce.\n",
                "链接": "https://arxiv.org/abs/2306.01111"
            },
            {
                "文章ID": "52585",
                "标题": "ZegCLIP: Towards Adapting CLIP for Zero-shot Semantic Segmentation",
                "作者": " Ziqin Zhou,  Bowen Zhang,  Yinjie Lei,  Lingqiao Liu,  Yifan Liu",
                "发布日期": "2023-06-21",
                "摘要": "  Recently, CLIP has been applied to pixel-level zero-shot learning tasks via a\ntwo-stage scheme. The general idea is to first generate class-agnostic region\nproposals and then feed the cropped proposal regions to CLIP to utilize its\nimage-level zero-shot classification capability. While effective, such a scheme\nrequires two image encoders, one for proposal generation and one for CLIP,\nleading to a complicated pipeline and high computational cost. In this work, we\npursue a simpler-and-efficient one-stage solution that directly extends CLIP's\nzero-shot prediction capability from image to pixel level. Our investigation\nstarts with a straightforward extension as our baseline that generates semantic\nmasks by comparing the similarity between text and patch embeddings extracted\nfrom CLIP. However, such a paradigm could heavily overfit the seen classes and\nfail to generalize to unseen classes. To handle this issue, we propose three\nsimple-but-effective designs and figure out that they can significantly retain\nthe inherent zero-shot capacity of CLIP and improve pixel-level generalization\nability. Incorporating those modifications leads to an efficient zero-shot\nsemantic segmentation system called ZegCLIP. Through extensive experiments on\nthree public benchmarks, ZegCLIP demonstrates superior performance,\noutperforming the state-of-the-art methods by a large margin under both\n\"inductive\" and \"transductive\" zero-shot settings. In addition, compared with\nthe two-stage method, our one-stage ZegCLIP achieves a speedup of about 5 times\nfaster during inference. We release the code at\nhttps://github.com/ZiqinZhou66/ZegCLIP.git.\n",
                "链接": "https://arxiv.org/abs/2212.03588"
            },
            {
                "文章ID": "94396",
                "标题": "Multimodal Adaptation of CLIP for Few-Shot Action Recognition",
                "作者": " Jiazheng Xing,  Mengmeng Wang,  Xiaojun Hou,  Guang Dai,  Jingdong Wang,  Yong Liu",
                "发布日期": "2023-08-04",
                "摘要": "  Applying large-scale pre-trained visual models like CLIP to few-shot action\nrecognition tasks can benefit performance and efficiency. Utilizing the\n\"pre-training, fine-tuning\" paradigm makes it possible to avoid training a\nnetwork from scratch, which can be time-consuming and resource-intensive.\nHowever, this method has two drawbacks. First, limited labeled samples for\nfew-shot action recognition necessitate minimizing the number of tunable\nparameters to mitigate over-fitting, also leading to inadequate fine-tuning\nthat increases resource consumption and may disrupt the generalized\nrepresentation of models. Second, the video's extra-temporal dimension\nchallenges few-shot recognition's effective temporal modeling, while\npre-trained visual models are usually image models. This paper proposes a novel\nmethod called Multimodal Adaptation of CLIP (MA-CLIP) to address these issues.\nIt adapts CLIP for few-shot action recognition by adding lightweight adapters,\nwhich can minimize the number of learnable parameters and enable the model to\ntransfer across different tasks quickly. The adapters we design can combine\ninformation from video-text multimodal sources for task-oriented spatiotemporal\nmodeling, which is fast, efficient, and has low training costs. Additionally,\nbased on the attention mechanism, we design a text-guided prototype\nconstruction module that can fully utilize video-text information to enhance\nthe representation of video prototypes. Our MA-CLIP is plug-and-play, which can\nbe used in any different few-shot action recognition temporal alignment metric.\n",
                "链接": "https://arxiv.org/abs/2308.01532"
            },
            {
                "文章ID": "67925",
                "标题": "CLIP-ReIdent: Contrastive Training for Player Re-Identification",
                "作者": " Konrad Habel,  Fabian Deuser,  Norbert Oswald",
                "发布日期": "2023-03-22",
                "摘要": "  Sports analytics benefits from recent advances in machine learning providing\na competitive advantage for teams or individuals. One important task in this\ncontext is the performance measurement of individual players to provide reports\nand log files for subsequent analysis. During sport events like basketball,\nthis involves the re-identification of players during a match either from\nmultiple camera viewpoints or from a single camera viewpoint at different\ntimes. In this work, we investigate whether it is possible to transfer the\nout-standing zero-shot performance of pre-trained CLIP models to the domain of\nplayer re-identification. For this purpose we reformulate the contrastive\nlanguage-to-image pre-training approach from CLIP to a contrastive\nimage-to-image training approach using the InfoNCE loss as training objective.\nUnlike previous work, our approach is entirely class-agnostic and benefits from\nlarge-scale pre-training. With a fine-tuned CLIP ViT-L/14 model we achieve\n98.44 % mAP on the MMSports 2022 Player Re-Identification challenge.\nFurthermore we show that the CLIP Vision Transformers have already strong OCR\ncapabilities to identify useful player features like shirt numbers in a\nzero-shot manner without any fine-tuning on the dataset. By applying the\nScore-CAM algorithm we visualise the most important image regions that our\nfine-tuned model identifies when calculating the similarity score between two\nimages of a player.\n",
                "链接": "https://arxiv.org/abs/2303.11855"
            },
            {
                "文章ID": "98399",
                "标题": "Towards Realistic Unsupervised Fine-tuning with CLIP",
                "作者": " Jian Liang,  Lijun Sheng,  Zhengbo Wang,  Ran He,  Tieniu Tan",
                "发布日期": "2023-08-25",
                "摘要": "  The emergence of vision-language models (VLMs), such as CLIP, has spurred a\nsignificant research effort towards their application for downstream supervised\nlearning tasks. Although some previous studies have explored the unsupervised\nfine-tuning of CLIP, they often rely on prior knowledge in the form of class\nnames associated with ground truth labels. In this paper, we delve into a\nrealistic unsupervised fine-tuning scenario by assuming that the unlabeled data\nmight contain out-of-distribution samples from unknown classes. Furthermore, we\nemphasize the importance of simultaneously enhancing out-of-distribution\ndetection capabilities alongside the recognition of instances associated with\npredefined class labels.\n  To tackle this problem, we present a simple, efficient, and effective\nfine-tuning approach called Universal Entropy Optimization (UEO). UEO leverages\nsample-level confidence to approximately minimize the conditional entropy of\nconfident instances and maximize the marginal entropy of less confident\ninstances. Apart from optimizing the textual prompts, UEO also incorporates\noptimization of channel-wise affine transformations within the visual branch of\nCLIP. Through extensive experiments conducted across 15 domains and 4 different\ntypes of prior knowledge, we demonstrate that UEO surpasses baseline methods in\nterms of both generalization and out-of-distribution detection.\n",
                "链接": "https://arxiv.org/abs/2308.12919"
            },
            {
                "文章ID": "67021",
                "标题": "GridCLIP: One-Stage Object Detection by Grid-Level CLIP Representation\n  Learning",
                "作者": " Jiayi Lin,  Shaogang Gong",
                "发布日期": "2023-03-17",
                "摘要": "  A vision-language foundation model pretrained on very large-scale image-text\npaired data has the potential to provide generalizable knowledge representation\nfor downstream visual recognition and detection tasks, especially on\nsupplementing the undersampled categories in downstream model training. Recent\nstudies utilizing CLIP for object detection have shown that a two-stage\ndetector design typically outperforms a one-stage detector, while requiring\nmore expensive training resources and longer inference time. In this work, we\npropose a one-stage detector GridCLIP that narrows its performance gap to those\nof two-stage detectors, with approximately 43 and 5 times faster than its\ntwo-stage counterpart (ViLD) in the training and test process respectively.\nGridCLIP learns grid-level representations to adapt to the intrinsic principle\nof one-stage detection learning by expanding the conventional CLIP image-text\nholistic mapping to a more fine-grained, grid-text alignment. This differs from\nthe region-text mapping in two-stage detectors that apply CLIP directly by\ntreating regions as images. Specifically, GridCLIP performs Grid-level\nAlignment to adapt the CLIP image-level representations to grid-level\nrepresentations by aligning to CLIP category representations to learn the\nannotated (especially frequent) categories. To learn generalizable visual\nrepresentations of broader categories, especially undersampled ones, we perform\nImage-level Alignment during training to propagate broad pre-learned categories\nin the CLIP image encoder from the image-level to the grid-level\nrepresentations. Experiments show that the learned CLIP-based grid-level\nrepresentations boost the performance of undersampled (infrequent and novel)\ncategories, reaching comparable detection performance on the LVIS benchmark.\n",
                "链接": "https://arxiv.org/abs/2303.09252"
            },
            {
                "文章ID": "14710",
                "标题": "Hierarchical Text-Conditional Image Generation with CLIP Latents",
                "作者": " Aditya Ramesh,  Prafulla Dhariwal,  Alex Nichol,  Casey Chu,  Mark Chen",
                "发布日期": "2022-04-14",
                "摘要": "  Contrastive models like CLIP have been shown to learn robust representations\nof images that capture both semantics and style. To leverage these\nrepresentations for image generation, we propose a two-stage model: a prior\nthat generates a CLIP image embedding given a text caption, and a decoder that\ngenerates an image conditioned on the image embedding. We show that explicitly\ngenerating image representations improves image diversity with minimal loss in\nphotorealism and caption similarity. Our decoders conditioned on image\nrepresentations can also produce variations of an image that preserve both its\nsemantics and style, while varying the non-essential details absent from the\nimage representation. Moreover, the joint embedding space of CLIP enables\nlanguage-guided image manipulations in a zero-shot fashion. We use diffusion\nmodels for the decoder and experiment with both autoregressive and diffusion\nmodels for the prior, finding that the latter are computationally more\nefficient and produce higher-quality samples.\n",
                "链接": "https://arxiv.org/abs/2204.06125"
            },
            {
                "文章ID": "63882",
                "标题": "Turning a CLIP Model into a Scene Text Detector",
                "作者": " Wenwen Yu,  Yuliang Liu,  Wei Hua,  Deqiang Jiang,  Bo Ren,  Xiang Bai",
                "发布日期": "2023-03-28",
                "摘要": "  The recent large-scale Contrastive Language-Image Pretraining (CLIP) model\nhas shown great potential in various downstream tasks via leveraging the\npretrained vision and language knowledge. Scene text, which contains rich\ntextual and visual information, has an inherent connection with a model like\nCLIP. Recently, pretraining approaches based on vision language models have\nmade effective progresses in the field of text detection. In contrast to these\nworks, this paper proposes a new method, termed TCM, focusing on Turning the\nCLIP Model directly for text detection without pretraining process. We\ndemonstrate the advantages of the proposed TCM as follows: (1) The underlying\nprinciple of our framework can be applied to improve existing scene text\ndetector. (2) It facilitates the few-shot training capability of existing\nmethods, e.g., by using 10% of labeled data, we significantly improve the\nperformance of the baseline method with an average of 22% in terms of the\nF-measure on 4 benchmarks. (3) By turning the CLIP model into existing scene\ntext detection methods, we further achieve promising domain adaptation ability.\nThe code will be publicly released at https://github.com/wenwenyu/TCM.\n",
                "链接": "https://arxiv.org/abs/2302.14338"
            },
            {
                "文章ID": "72669",
                "标题": "CLIP-Lung: Textual Knowledge-Guided Lung Nodule Malignancy Prediction",
                "作者": " Yiming Lei,  Zilong Li,  Yan Shen,  Junping Zhang,  Hongming Shan",
                "发布日期": "2023-10-03",
                "摘要": "  Lung nodule malignancy prediction has been enhanced by advanced deep-learning\ntechniques and effective tricks. Nevertheless, current methods are mainly\ntrained with cross-entropy loss using one-hot categorical labels, which results\nin difficulty in distinguishing those nodules with closer progression labels.\nInterestingly, we observe that clinical text information annotated by\nradiologists provides us with discriminative knowledge to identify challenging\nsamples. Drawing on the capability of the contrastive language-image\npre-training (CLIP) model to learn generalized visual representations from text\nannotations, in this paper, we propose CLIP-Lung, a textual knowledge-guided\nframework for lung nodule malignancy prediction. First, CLIP-Lung introduces\nboth class and attribute annotations into the training of the lung nodule\nclassifier without any additional overheads in inference. Second, we designed a\nchannel-wise conditional prompt (CCP) module to establish consistent\nrelationships between learnable context prompts and specific feature maps.\nThird, we align image features with both class and attribute features via\ncontrastive learning, rectifying false positives and false negatives in latent\nspace. The experimental results on the benchmark LIDC-IDRI dataset have\ndemonstrated the superiority of CLIP-Lung, both in classification performance\nand interpretability of attention maps.\n",
                "链接": "https://arxiv.org/abs/2304.08013"
            },
            {
                "文章ID": "56853",
                "标题": "CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP",
                "作者": " Runnan Chen,  Youquan Liu,  Lingdong Kong,  Xinge Zhu,  Yuexin Ma,  Yikang Li,  Yuenan Hou,  Yu Qiao,  Wenping Wang",
                "发布日期": "2023-04-07",
                "摘要": "  Contrastive Language-Image Pre-training (CLIP) achieves promising results in\n2D zero-shot and few-shot learning. Despite the impressive performance in 2D,\napplying CLIP to help the learning in 3D scene understanding has yet to be\nexplored. In this paper, we make the first attempt to investigate how CLIP\nknowledge benefits 3D scene understanding. We propose CLIP2Scene, a simple yet\neffective framework that transfers CLIP knowledge from 2D image-text\npre-trained models to a 3D point cloud network. We show that the pre-trained 3D\nnetwork yields impressive performance on various downstream tasks, i.e.,\nannotation-free and fine-tuning with labelled data for semantic segmentation.\nSpecifically, built upon CLIP, we design a Semantic-driven Cross-modal\nContrastive Learning framework that pre-trains a 3D network via semantic and\nspatial-temporal consistency regularization. For the former, we first leverage\nCLIP's text semantics to select the positive and negative point samples and\nthen employ the contrastive loss to train the 3D network. In terms of the\nlatter, we force the consistency between the temporally coherent point cloud\nfeatures and their corresponding image features. We conduct experiments on\nSemanticKITTI, nuScenes, and ScanNet. For the first time, our pre-trained\nnetwork achieves annotation-free 3D semantic segmentation with 20.8% and 25.08%\nmIoU on nuScenes and ScanNet, respectively. When fine-tuned with 1% or 100%\nlabelled data, our method significantly outperforms other self-supervised\nmethods, with improvements of 8% and 1% mIoU, respectively. Furthermore, we\ndemonstrate the generalizability for handling cross-domain datasets. Code is\npublicly available https://github.com/runnanchen/CLIP2Scene.\n",
                "链接": "https://arxiv.org/abs/2301.04926"
            },
            {
                "文章ID": "37191",
                "标题": "CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language\n  Representation Alignment",
                "作者": " Hongwei Xue,  Yuchong Sun,  Bei Liu,  Jianlong Fu,  Ruihua Song,  Houqiang Li,  Jiebo Luo",
                "发布日期": "2023-03-03",
                "摘要": "  The pre-trained image-text models, like CLIP, have demonstrated the strong\npower of vision-language representation learned from a large scale of\nweb-collected image-text data. In light of the well-learned visual features,\nsome existing works transfer image representation to video domain and achieve\ngood results. However, how to utilize image-language pre-trained model (e.g.,\nCLIP) for video-language pre-training (post-pretraining) is still under\nexplored. In this paper, we investigate two questions: 1) what are the factors\nhindering post-pretraining CLIP to further improve the performance on\nvideo-language tasks? and 2) how to mitigate the impact of these factors?\nThrough a series of comparative experiments and analyses, we find that the data\nscale and domain gap between language sources have great impacts. Motivated by\nthese, we propose a Omnisource Cross-modal Learning method equipped with a\nVideo Proxy mechanism on the basis of CLIP, namely CLIP-ViP. Extensive results\nshow that our approach improves the performance of CLIP on video-text retrieval\nby a large margin. Our model also achieves SOTA results on a variety of\ndatasets, including MSR-VTT, DiDeMo, LSMDC, and ActivityNet. We will release\nour code and pre-trained CLIP-ViP models at\nhttps://github.com/microsoft/XPretrain/tree/main/CLIP-ViP.\n",
                "链接": "https://arxiv.org/abs/2209.06430"
            },
            {
                "文章ID": "16228",
                "标题": "CLIP-Dissect: Automatic Description of Neuron Representations in Deep\n  Vision Networks",
                "作者": " Tuomas Oikarinen,  Tsui-Wei Weng",
                "发布日期": "2023-06-06",
                "摘要": "  In this paper, we propose CLIP-Dissect, a new technique to automatically\ndescribe the function of individual hidden neurons inside vision networks.\nCLIP-Dissect leverages recent advances in multimodal vision/language models to\nlabel internal neurons with open-ended concepts without the need for any\nlabeled data or human examples. We show that CLIP-Dissect provides more\naccurate descriptions than existing methods for last layer neurons where the\nground-truth is available as well as qualitatively good descriptions for hidden\nlayer neurons. In addition, our method is very flexible: it is model agnostic,\ncan easily handle new concepts and can be extended to take advantage of better\nmultimodal models in the future. Finally CLIP-Dissect is computationally\nefficient and can label all neurons from five layers of ResNet-50 in just 4\nminutes, which is more than 10 times faster than existing methods. Our code is\navailable at https://github.com/Trustworthy-ML-Lab/CLIP-dissect. Finally,\ncrowdsourced user study results are available at Appendix B to further support\nthe effectiveness of our method.\n",
                "链接": "https://arxiv.org/abs/2204.10965"
            },
            {
                "文章ID": "99386",
                "标题": "On the Potential of CLIP for Compositional Logical Reasoning",
                "作者": "Franklin and Marshall College  Justin Brody",
                "发布日期": "2023-08-31",
                "摘要": "  In this paper we explore the possibility of using OpenAI's CLIP to perform\nlogically coherent grounded visual reasoning. To that end, we formalize our\nterms and give a geometric analysis of how embeddings in CLIP's latent space\nwould need to be configured in order for the system to be logically coherent.\nOur main conclusion is that, as usually configured, CLIP cannot perform such\nreasoning.\n",
                "链接": "https://arxiv.org/abs/2308.15887"
            },
            {
                "文章ID": "67721",
                "标题": "CLIP goes 3D: Leveraging Prompt Tuning for Language Grounded 3D\n  Recognition",
                "作者": " Deepti Hegde,  Jeya Maria Jose Valanarasu,  Vishal M. Patel",
                "发布日期": "2023-04-20",
                "摘要": "  Vision-Language models like CLIP have been widely adopted for various tasks\ndue to their impressive zero-shot capabilities. However, CLIP is not suitable\nfor extracting 3D geometric features as it was trained on only images and text\nby natural language supervision. We work on addressing this limitation and\npropose a new framework termed CG3D (CLIP Goes 3D) where a 3D encoder is\nlearned to exhibit zero-shot capabilities. CG3D is trained using triplets of\npointclouds, corresponding rendered 2D images, and texts using natural language\nsupervision. To align the features in a multimodal embedding space, we utilize\ncontrastive loss on 3D features obtained from the 3D encoder, as well as visual\nand text features extracted from CLIP. We note that the natural images used to\ntrain CLIP and the rendered 2D images in CG3D have a distribution shift.\nAttempting to train the visual and text encoder to account for this shift\nresults in catastrophic forgetting and a notable decrease in performance. To\nsolve this, we employ prompt tuning and introduce trainable parameters in the\ninput space to shift CLIP towards the 3D pre-training dataset utilized in CG3D.\nWe extensively test our pre-trained CG3D framework and demonstrate its\nimpressive capabilities in zero-shot, open scene understanding, and retrieval\ntasks. Further, it also serves as strong starting weights for fine-tuning in\ndownstream 3D recognition tasks.\n",
                "链接": "https://arxiv.org/abs/2303.11313"
            },
            {
                "文章ID": "65858",
                "标题": "Adapting Contrastive Language-Image Pretrained (CLIP) Models for\n  Out-of-Distribution Detection",
                "作者": " Nikolas Adaloglou,  Felix Michels,  Tim Kaiser,  Markus Kollmann",
                "发布日期": "2023-11-10",
                "摘要": "  We present a comprehensive experimental study on pretrained feature\nextractors for visual out-of-distribution (OOD) detection, focusing on adapting\ncontrastive language-image pretrained (CLIP) models. Without fine-tuning on the\ntraining data, we are able to establish a positive correlation ($R^2\\geq0.92$)\nbetween in-distribution classification and unsupervised OOD detection for CLIP\nmodels in $4$ benchmarks. We further propose a new simple and scalable method\ncalled \\textit{pseudo-label probing} (PLP) that adapts vision-language models\nfor OOD detection. Given a set of label names of the training set, PLP trains a\nlinear layer using the pseudo-labels derived from the text encoder of CLIP. To\ntest the OOD detection robustness of pretrained models, we develop a novel\nfeature-based adversarial OOD data manipulation approach to create adversarial\nsamples. Intriguingly, we show that (i) PLP outperforms the previous\nstate-of-the-art \\citep{ming2022mcm} on all $5$ large-scale benchmarks based on\nImageNet, specifically by an average AUROC gain of 3.4\\% using the largest CLIP\nmodel (ViT-G), (ii) we show that linear probing outperforms fine-tuning by\nlarge margins for CLIP architectures (i.e. CLIP ViT-H achieves a mean gain of\n7.3\\% AUROC on average on all ImageNet-based benchmarks), and (iii)\nbillion-parameter CLIP models still fail at detecting adversarially manipulated\nOOD images. The code and adversarially created datasets will be made publicly\navailable.\n",
                "链接": "https://arxiv.org/abs/2303.05828"
            },
            {
                "文章ID": "115956",
                "标题": "Domain Aligned CLIP for Few-shot Classification",
                "作者": " Muhammad Waleed Gondal,  Jochen Gast,  Inigo Alonso Ruiz,  Richard Droste,  Tommaso Macri,  Suren Kumar,  Luitpold Staudigl",
                "发布日期": "2023-11-16",
                "摘要": "  Large vision-language representation learning models like CLIP have\ndemonstrated impressive performance for zero-shot transfer to downstream tasks\nwhile largely benefiting from inter-modal (image-text) alignment via\ncontrastive objectives. This downstream performance can further be enhanced by\nfull-scale fine-tuning which is often compute intensive, requires large\nlabelled data, and can reduce out-of-distribution (OOD) robustness.\nFurthermore, sole reliance on inter-modal alignment might overlook the rich\ninformation embedded within each individual modality. In this work, we\nintroduce a sample-efficient domain adaptation strategy for CLIP, termed Domain\nAligned CLIP (DAC), which improves both intra-modal (image-image) and\ninter-modal alignment on target distributions without fine-tuning the main\nmodel. For intra-modal alignment, we introduce a lightweight adapter that is\nspecifically trained with an intra-modal contrastive objective. To improve\ninter-modal alignment, we introduce a simple framework to modulate the\nprecomputed class text embeddings. The proposed few-shot fine-tuning framework\nis computationally efficient, robust to distribution shifts, and does not alter\nCLIP's parameters. We study the effectiveness of DAC by benchmarking on 11\nwidely used image classification tasks with consistent improvements in 16-shot\nclassification upon strong baselines by about 2.3% and demonstrate competitive\nperformance on 4 OOD robustness benchmarks.\n",
                "链接": "https://arxiv.org/abs/2311.09191"
            },
            {
                "文章ID": "84202",
                "标题": "Self-supervised Audio Teacher-Student Transformer for Both Clip-level\n  and Frame-level Tasks",
                "作者": " Xian Li,  Nian Shao,  Xiaofei Li",
                "发布日期": "2023-11-08",
                "摘要": "  Self-supervised learning (SSL) has emerged as a popular approach for learning\naudio representations. One goal of audio self-supervised pre-training is to\ntransfer knowledge to downstream audio tasks, generally including clip-level\nand frame-level tasks. While frame-level tasks are important for fine-grained\nacoustic scene/event understanding, prior studies primarily evaluate on\nclip-level downstream tasks. In order to tackle both clip-level and frame-level\ntasks, this paper proposes Audio Teacher-Student Transformer (ATST), with a\nclip-level version (named ATST-Clip) and a frame-level version (named\nATST-Frame), responsible for learning clip-level and frame-level\nrepresentations, respectively. Both methods use a Transformer encoder and a\nteacher-student training scheme. We have carefully designed the view creation\nstrategy for ATST-Clip and ATST-Frame. Specifically, ATST-Clip uses\nsegment-wise data augmentations, and ATST-Frame integrates frame-wise data\naugmentations and masking. Experimental results show that our ATST-Frame model\nobtains state-of-the-art (SOTA) performances on most of the clip-level and\nframe-level downstream tasks. Especially, it outperforms other models by a\nlarge margin on the frame-level sound event detection task. In addition, the\nperformance can be further improved by combining the two models through\nknowledge distillation. Our code is available online.\n",
                "链接": "https://arxiv.org/abs/2306.04186"
            },
            {
                "文章ID": "111255",
                "标题": "TiC-CLIP: Continual Training of CLIP Models",
                "作者": " Saurabh Garg,  Mehrdad Farajtabar,  Hadi Pouransari,  Raviteja Vemulapalli,  Sachin Mehta,  Oncel Tuzel,  Vaishaal Shankar,  Fartash Faghri",
                "发布日期": "2023-10-26",
                "摘要": "  Keeping large foundation models up to date on latest data is inherently\nexpensive. To avoid the prohibitive costs of constantly retraining, it is\nimperative to continually train these models. This problem is exacerbated by\nthe lack of any large scale continual learning benchmarks or baselines. We\nintroduce the first set of web-scale Time-Continual (TiC) benchmarks for\ntraining vision-language models: TiC-DataCompt, TiC-YFCC, and TiC-RedCaps with\nover 12.7B timestamped image-text pairs spanning 9 years (2014--2022). We first\nuse our benchmarks to curate various dynamic evaluations to measure temporal\nrobustness of existing models. We show OpenAI's CLIP (trained on data up to\n2020) loses $\\approx 8\\%$ zero-shot accuracy on our curated retrieval task from\n2021--2022 compared with more recently trained models in OpenCLIP repository.\nWe then study how to efficiently train models on time-continuous data. We\ndemonstrate that a simple rehearsal-based approach that continues training from\nthe last checkpoint and replays old data reduces compute by $2.5\\times$ when\ncompared to the standard practice of retraining from scratch.\n",
                "链接": "https://arxiv.org/abs/2310.16226"
            },
            {
                "文章ID": "37431",
                "标题": "Does CLIP Know My Face?",
                "作者": " Dominik Hintersdorf,  Lukas Struppek,  Manuel Brack,  Felix Friedrich,  Patrick Schramowski,  Kristian Kersting",
                "发布日期": "2023-05-31",
                "摘要": "  With the rise of deep learning in various applications, privacy concerns\naround the protection of training data has become a critical area of research.\nWhereas prior studies have focused on privacy risks in single-modal models, we\nintroduce a novel method to assess privacy for multi-modal models, specifically\nvision-language models like CLIP. The proposed Identity Inference Attack (IDIA)\nreveals whether an individual was included in the training data by querying the\nmodel with images of the same person. Letting the model choose from a wide\nvariety of possible text labels, the model reveals whether it recognizes the\nperson and, therefore, was used for training. Our large-scale experiments on\nCLIP demonstrate that individuals used for training can be identified with very\nhigh accuracy. We confirm that the model has learned to associate names with\ndepicted individuals, implying the existence of sensitive information that can\nbe extracted by adversaries. Our results highlight the need for stronger\nprivacy protection in large-scale models and suggest that IDIAs can be used to\nprove the unauthorized use of data for training and to enforce privacy laws.\n",
                "链接": "https://arxiv.org/abs/2209.07341"
            },
            {
                "文章ID": "108541",
                "标题": "Incremental Object Detection with CLIP",
                "作者": " Yupeng He,  Ziyue Huang,  Qingjie Liu,  Yunhong Wang",
                "发布日期": "2023-10-16",
                "摘要": "  In the incremental detection task, unlike the incremental classification\ntask, data ambiguity exists due to the possibility of an image having different\nlabeled bounding boxes in multiple continuous learning stages. This phenomenon\noften impairs the model's ability to learn new classes. However, the forward\ncompatibility of the model is less considered in existing work, which hinders\nthe model's suitability for incremental learning. To overcome this obstacle, we\npropose to use a language-visual model such as CLIP to generate text feature\nembeddings for different class sets, which enhances the feature space globally.\nWe then employ the broad classes to replace the unavailable novel classes in\nthe early learning stage to simulate the actual incremental scenario. Finally,\nwe use the CLIP image encoder to identify potential objects in the proposals,\nwhich are classified into the background by the model. We modify the background\nlabels of those proposals to known classes and add the boxes to the training\nset to alleviate the problem of data ambiguity. We evaluate our approach on\nvarious incremental learning settings on the PASCAL VOC 2007 dataset, and our\napproach outperforms state-of-the-art methods, particularly for the new\nclasses.\n",
                "链接": "https://arxiv.org/abs/2310.08815"
            },
            {
                "文章ID": "75208",
                "标题": "CLIP-S$^4$: Language-Guided Self-Supervised Semantic Segmentation",
                "作者": " Wenbin He,  Suphanut Jamonnak,  Liang Gou,  Liu Ren",
                "发布日期": "2023-05-03",
                "摘要": "  Existing semantic segmentation approaches are often limited by costly\npixel-wise annotations and predefined classes. In this work, we present\nCLIP-S$^4$ that leverages self-supervised pixel representation learning and\nvision-language models to enable various semantic segmentation tasks (e.g.,\nunsupervised, transfer learning, language-driven segmentation) without any\nhuman annotations and unknown class information. We first learn pixel\nembeddings with pixel-segment contrastive learning from different augmented\nviews of images. To further improve the pixel embeddings and enable\nlanguage-driven semantic segmentation, we design two types of consistency\nguided by vision-language models: 1) embedding consistency, aligning our pixel\nembeddings to the joint feature space of a pre-trained vision-language model,\nCLIP; and 2) semantic consistency, forcing our model to make the same\npredictions as CLIP over a set of carefully designed target classes with both\nknown and unknown prototypes. Thus, CLIP-S$^4$ enables a new task of class-free\nsemantic segmentation where no unknown class information is needed during\ntraining. As a result, our approach shows consistent and substantial\nperformance improvement over four popular benchmarks compared with the\nstate-of-the-art unsupervised and language-driven semantic segmentation\nmethods. More importantly, our method outperforms these methods on unknown\nclass recognition by a large margin.\n",
                "链接": "https://arxiv.org/abs/2305.01040"
            },
            {
                "文章ID": "90258",
                "标题": "CREPE: Learnable Prompting With CLIP Improves Visual Relationship\n  Prediction",
                "作者": " Rakshith Subramanyam,  T. S. Jayram,  Rushil Anirudh,  Jayaraman J. Thiagarajan",
                "发布日期": "2023-07-20",
                "摘要": "  In this paper, we explore the potential of Vision-Language Models (VLMs),\nspecifically CLIP, in predicting visual object relationships, which involves\ninterpreting visual features from images into language-based relations. Current\nstate-of-the-art methods use complex graphical models that utilize language\ncues and visual features to address this challenge. We hypothesize that the\nstrong language priors in CLIP embeddings can simplify these graphical models\npaving for a simpler approach. We adopt the UVTransE relation prediction\nframework, which learns the relation as a translational embedding with subject,\nobject, and union box embeddings from a scene. We systematically explore the\ndesign of CLIP-based subject, object, and union-box representations within the\nUVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate\nEstimation). CREPE utilizes text-based representations for all three bounding\nboxes and introduces a novel contrastive training strategy to automatically\ninfer the text prompt for union-box. Our approach achieves state-of-the-art\nperformance in predicate estimation, mR@5 27.79, and mR@20 31.95 on the Visual\nGenome benchmark, achieving a 15.3\\% gain in performance over recent\nstate-of-the-art at mR@20. This work demonstrates CLIP's effectiveness in\nobject relation prediction and encourages further research on VLMs in this\nchallenging domain.\n",
                "链接": "https://arxiv.org/abs/2307.04838"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下使用2020年以后CONLL 2004数据集进行NER评测的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "46217",
                "标题": "Recognizing Nested Entities from Flat Supervision: A New NER Subtask,\n  Feasibility and Challenges",
                "作者": " Enwei Zhu,  Yiyang Liu,  Ming Jin,  Jinpeng Li",
                "发布日期": "2022-11-02",
                "摘要": "  Many recent named entity recognition (NER) studies criticize flat NER for its\nnon-overlapping assumption, and switch to investigating nested NER. However,\nexisting nested NER models heavily rely on training data annotated with nested\nentities, while labeling such data is costly. This study proposes a new\nsubtask, nested-from-flat NER, which corresponds to a realistic application\nscenario: given data annotated with flat entities only, one may still desire\nthe trained model capable of recognizing nested entities. To address this task,\nwe train span-based models and deliberately ignore the spans nested inside\nlabeled entities, since these spans are possibly unlabeled entities. With\nnested entities removed from the training data, our model achieves 54.8%, 54.2%\nand 41.1% F1 scores on the subset of spans within entities on ACE 2004, ACE\n2005 and GENIA, respectively. This suggests the effectiveness of our approach\nand the feasibility of the task. In addition, the model's performance on flat\nentities is entirely unaffected. We further manually annotate the nested\nentities in the test set of CoNLL 2003, creating a nested-from-flat NER\nbenchmark. Analysis results show that the main challenges stem from the data\nand annotation inconsistencies between the flat and nested entities.\n",
                "链接": "https://arxiv.org/abs/2211.00301"
            },
            {
                "文章ID": "54218",
                "标题": "E-NER -- An Annotated Named Entity Recognition Corpus of Legal Text",
                "作者": " Ting Wai Terence Au,  Ingemar J. Cox,  Vasileios Lampos",
                "发布日期": "2022-12-20",
                "摘要": "  Identifying named entities such as a person, location or organization, in\ndocuments can highlight key information to readers. Training Named Entity\nRecognition (NER) models requires an annotated data set, which can be a\ntime-consuming labour-intensive task. Nevertheless, there are publicly\navailable NER data sets for general English. Recently there has been interest\nin developing NER for legal text. However, prior work and experimental results\nreported here indicate that there is a significant degradation in performance\nwhen NER methods trained on a general English data set are applied to legal\ntext. We describe a publicly available legal NER data set, called E-NER, based\non legal company filings available from the US Securities and Exchange\nCommission's EDGAR data set. Training a number of different NER algorithms on\nthe general English CoNLL-2003 corpus but testing on our test collection\nconfirmed significant degradations in accuracy, as measured by the F1-score, of\nbetween 29.4\\% and 60.4\\%, compared to training and testing on the E-NER\ncollection.\n",
                "链接": "https://arxiv.org/abs/2212.09306"
            },
            {
                "文章ID": "84278",
                "标题": "Multilingual Clinical NER: Translation or Cross-lingual Transfer?",
                "作者": " Xavier Fontaine,  Félix Gaschi,  Parisa Rastin,  Yannick Toussaint",
                "发布日期": "2023-06-08",
                "摘要": "  Natural language tasks like Named Entity Recognition (NER) in the clinical\ndomain on non-English texts can be very time-consuming and expensive due to the\nlack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent\nthis issue thanks to the ability of multilingual large language models to be\nfine-tuned on a specific task in one language and to provide high accuracy for\nthe same task in another language. However, other methods leveraging\ntranslation models can be used to perform NER without annotated data in the\ntarget language, by either translating the training set or test set. This paper\ncompares cross-lingual transfer with these two alternative methods, to perform\nclinical NER in French and in German without any training data in those\nlanguages. To this end, we release MedNERF a medical NER test set extracted\nfrom French drug prescriptions and annotated with the same guidelines as an\nEnglish dataset. Through extensive experiments on this dataset and on a German\nmedical dataset (Frei and Kramer, 2021), we show that translation-based methods\ncan achieve similar performance to CLT but require more care in their design.\nAnd while they can take advantage of monolingual clinical language models,\nthose do not guarantee better results than large general-purpose multilingual\nmodels, whether with cross-lingual transfer or translation.\n",
                "链接": "https://arxiv.org/abs/2306.04384"
            },
            {
                "文章ID": "17226",
                "标题": "What do we Really Know about State of the Art NER?",
                "作者": " Sowmya Vajjala,  Ramya Balasubramaniam",
                "发布日期": "2022-05-05",
                "摘要": "  Named Entity Recognition (NER) is a well researched NLP task and is widely\nused in real world NLP scenarios. NER research typically focuses on the\ncreation of new ways of training NER, with relatively less emphasis on\nresources and evaluation. Further, state of the art (SOTA) NER models, trained\non standard datasets, typically report only a single performance measure\n(F-score) and we don't really know how well they do for different entity types\nand genres of text, or how robust are they to new, unseen entities. In this\npaper, we perform a broad evaluation of NER using a popular dataset, that takes\ninto consideration various text genres and sources constituting the dataset at\nhand. Additionally, we generate six new adversarial test sets through small\nperturbations in the original test set, replacing select entities while\nretaining the context. We also train and test our models on randomly generated\ntrain/dev/test splits followed by an experiment where the models are trained on\na select set of genres but tested genres not seen in training. These\ncomprehensive evaluation strategies were performed using three SOTA NER models.\nBased on our results, we recommend some useful reporting practices for NER\nresearchers, that could help in providing a better understanding of a SOTA\nmodel's performance in future.\n",
                "链接": "https://arxiv.org/abs/2205.00034"
            },
            {
                "文章ID": "82089",
                "标题": "A Multilingual Evaluation of NER Robustness to Adversarial Inputs",
                "作者": " Akshay Srinivasan,  Sowmya Vajjala",
                "发布日期": "2023-05-31",
                "摘要": "  Adversarial evaluations of language models typically focus on English alone.\nIn this paper, we performed a multilingual evaluation of Named Entity\nRecognition (NER) in terms of its robustness to small perturbations in the\ninput. Our results showed the NER models we explored across three languages\n(English, German and Hindi) are not very robust to such changes, as indicated\nby the fluctuations in the overall F1 score as well as in a more fine-grained\nevaluation. With that knowledge, we further explored whether it is possible to\nimprove the existing NER models using a part of the generated adversarial data\nsets as augmented training data to train a new NER model or as fine-tuning data\nto adapt an existing NER model. Our results showed that both these approaches\nimprove performance on the original as well as adversarial test sets. While\nthere is no significant difference between the two approaches for English,\nre-training is significantly better than fine-tuning for German and Hindi.\n",
                "链接": "https://arxiv.org/abs/2305.18933"
            },
            {
                "文章ID": "78785",
                "标题": "Enhancing Few-shot NER with Prompt Ordering based Data Augmentation",
                "作者": " Huiming Wang,  Liying Cheng,  Wenxuan Zhang,  De Wen Soh,  Lidong Bing",
                "发布日期": "2023-05-22",
                "摘要": "  Recently, data augmentation (DA) methods have been proven to be effective for\npre-trained language models (PLMs) in low-resource settings, including few-shot\nnamed entity recognition (NER). However, conventional NER DA methods are mostly\naimed at sequence labeling models, i.e., token-level classification, and few\nare compatible with unified autoregressive generation frameworks, which can\nhandle a wider range of NER tasks, such as nested NER. Furthermore, these\ngeneration frameworks have a strong assumption that the entities will appear in\nthe target sequence with the same left-to-right order as the source sequence.\nIn this paper, we claim that there is no need to keep this strict order, and\nmore diversified but reasonable target entity sequences can be provided during\nthe training stage as a novel DA method. Nevertheless, a naive mixture of\naugmented data can confuse the model since one source sequence will then be\npaired with different target sequences. Therefore, we propose a simple but\neffective Prompt Ordering based Data Augmentation (PODA) method to improve the\ntraining of unified autoregressive generation frameworks under few-shot NER\nscenarios. Experimental results on three public NER datasets and further\nanalyses demonstrate the effectiveness of our approach.\n",
                "链接": "https://arxiv.org/abs/2305.11791"
            },
            {
                "文章ID": "39331",
                "标题": "mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark",
                "作者": " Vitor Jeronymo,  Mauricio Nascimento,  Roberto Lotufo,  Rodrigo Nogueira",
                "发布日期": "2022-09-29",
                "摘要": "  Robust 2004 is an information retrieval benchmark whose large number of\njudgments per query make it a reliable evaluation dataset. In this paper, we\npresent mRobust04, a multilingual version of Robust04 that was translated to 8\nlanguages using Google Translate. We also provide results of three different\nmultilingual retrievers on this dataset. The dataset is available at\nhttps://huggingface.co/datasets/unicamp-dl/mrobust\n",
                "链接": "https://arxiv.org/abs/2209.13738"
            },
            {
                "文章ID": "12836",
                "标题": "$k$NN-NER: Named Entity Recognition with Nearest Neighbor Search",
                "作者": " Shuhe Wang,  Xiaoya Li,  Yuxian Meng,  Tianwei Zhang,  Rongbin Ouyang,  Jiwei Li,  Guoyin Wang",
                "发布日期": "2022-04-01",
                "摘要": "  Inspired by recent advances in retrieval augmented methods in\nNLP~\\citep{khandelwal2019generalization,khandelwal2020nearest,meng2021gnn}, in\nthis paper, we introduce a $k$ nearest neighbor NER ($k$NN-NER) framework,\nwhich augments the distribution of entity labels by assigning $k$ nearest\nneighbors retrieved from the training set. This strategy makes the model more\ncapable of handling long-tail cases, along with better few-shot learning\nabilities. $k$NN-NER requires no additional operation during the training\nphase, and by interpolating $k$ nearest neighbors search into the vanilla NER\nmodel, $k$NN-NER consistently outperforms its vanilla counterparts: we achieve\na new state-of-the-art F1-score of 72.03 (+1.25) on the Chinese Weibo dataset\nand improved results on a variety of widely used NER benchmarks. Additionally,\nwe show that $k$NN-NER can achieve comparable results to the vanilla NER model\nwith 40\\% less amount of training data. Code available at\n\\url{https://github.com/ShannonAI/KNN-NER}.\n",
                "链接": "https://arxiv.org/abs/2203.17103"
            },
            {
                "文章ID": "111774",
                "标题": "Proving Test Set Contamination in Black Box Language Models",
                "作者": " Yonatan Oren,  Nicole Meister,  Niladri Chatterji,  Faisal Ladhak,  Tatsunori B. Hashimoto",
                "发布日期": "2023-11-27",
                "摘要": "  Large language models are trained on vast amounts of internet data, prompting\nconcerns and speculation that they have memorized public benchmarks. Going from\nspeculation to proof of contamination is challenging, as the pretraining data\nused by proprietary models are often not publicly accessible. We show that it\nis possible to provide provable guarantees of test set contamination in\nlanguage models without access to pretraining data or model weights. Our\napproach leverages the fact that when there is no data contamination, all\norderings of an exchangeable benchmark should be equally likely. In contrast,\nthe tendency for language models to memorize example order means that a\ncontaminated language model will find certain canonical orderings to be much\nmore likely than others. Our test flags potential contamination whenever the\nlikelihood of a canonically ordered benchmark dataset is significantly higher\nthan the likelihood after shuffling the examples. We demonstrate that our\nprocedure is sensitive enough to reliably prove test set contamination in\nchallenging situations, including models as small as 1.4 billion parameters, on\nsmall test sets of only 1000 examples, and datasets that appear only a few\ntimes in the pretraining corpus. Using our test, we audit five popular publicly\naccessible language models for test set contamination and find little evidence\nfor pervasive contamination.\n",
                "链接": "https://arxiv.org/abs/2310.17623"
            },
            {
                "文章ID": "81674",
                "标题": "Extrinsic Factors Affecting the Accuracy of Biomedical NER",
                "作者": " Zhiyi Li,  Shengjie Zhang,  Yujie Song,  Jungyeul Park",
                "发布日期": "2023-05-30",
                "摘要": "  Biomedical named entity recognition (NER) is a critial task that aims to\nidentify structured information in clinical text, which is often replete with\ncomplex, technical terms and a high degree of variability. Accurate and\nreliable NER can facilitate the extraction and analysis of important biomedical\ninformation, which can be used to improve downstream applications including the\nhealthcare system. However, NER in the biomedical domain is challenging due to\nlimited data availability, as the high expertise, time, and expenses are\nrequired to annotate its data. In this paper, by using the limited data, we\nexplore various extrinsic factors including the corpus annotation scheme, data\naugmentation techniques, semi-supervised learning and Brill transformation, to\nimprove the performance of a NER model on a clinical text dataset (i2b2 2012,\n\\citet{sun-rumshisky-uzuner:2013}). Our experiments demonstrate that these\napproaches can significantly improve the model's F1 score from original 73.74\nto 77.55. Our findings suggest that considering different extrinsic factors and\ncombining these techniques is a promising approach for improving NER\nperformance in the biomedical domain where the size of data is limited.\n",
                "链接": "https://arxiv.org/abs/2305.18152"
            }
        ]
    },
    {
        "question": {
            "question": "查找基于优化实现模型越狱的文献",
            "type": "5"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找一下近三个月有关语言模型rlhf的arxiv上的全部文章。",
            "type": "5"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找一下近两年关于语言模型奖励建模评估的文章。",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "请帮我找到从位置编码角度改善模型长序列建模能力的相关论文。",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找关于深度学习在医学影像分析中的最新研究，特别关注使用自动标注和迁移学习方法的论文，时间跨度覆盖2019年至今。",
            "type": "5"
        },
        "results": []
    }
]