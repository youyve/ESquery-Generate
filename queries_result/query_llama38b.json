[
    {
        "question": {
            "question": "与大模型工具学习相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "16395",
                "标题": "Mining and searching association relation of scientific papers based on\n  deep learning",
                "作者": " Jie Song,  Meiyu Liang,  Zhe Xue,  Feifei Kou,  Ang Li",
                "发布日期": "2022-04-26",
                "摘要": "  There is a complex correlation among the data of scientific papers. The\nphenomenon reveals the data characteristics, laws, and correlations contained\nin the data of scientific and technological papers in specific fields, which\ncan realize the analysis of scientific and technological big data and help to\ndesign applications to serve scientific researchers. Therefore, the research on\nmining and searching the association relationship of scientific papers based on\ndeep learning has far-reaching practical significance.\n",
                "链接": "https://arxiv.org/abs/2204.11488"
            },
            {
                "文章ID": "124027",
                "标题": "Big Tech influence over AI research revisited: memetic analysis of\n  attribution of ideas to affiliation",
                "作者": " Stanisław Giziński,  Paulina Kaczyńska,  Hubert Ruczyński,  Emilia Wiśnios,  Bartosz Pieliński,  Przemysław Biecek,  Julian Sienkiewicz",
                "发布日期": "2023-12-21",
                "摘要": "  There exists a growing discourse around the domination of Big Tech on the\nlandscape of artificial intelligence (AI) research, yet our comprehension of\nthis phenomenon remains cursory. This paper aims to broaden and deepen our\nunderstanding of Big Tech's reach and power within AI research. It highlights\nthe dominance not merely in terms of sheer publication volume but rather in the\npropagation of new ideas or \\textit{memes}. Current studies often oversimplify\nthe concept of influence to the share of affiliations in academic papers,\ntypically sourced from limited databases such as arXiv or specific academic\nconferences.\n  The main goal of this paper is to unravel the specific nuances of such\ninfluence, determining which AI ideas are predominantly driven by Big Tech\nentities. By employing network and memetic analysis on AI-oriented paper\nabstracts and their citation network, we are able to grasp a deeper insight\ninto this phenomenon. By utilizing two databases: OpenAlex and S2ORC, we are\nable to perform such analysis on a much bigger scale than previous attempts.\n  Our findings suggest, that while Big Tech-affiliated papers are\ndisproportionately more cited in some areas, the most cited papers are those\naffiliated with both Big Tech and Academia. Focusing on the most contagious\nmemes, their attribution to specific affiliation groups (Big Tech, Academia,\nmixed affiliation) seems to be equally distributed between those three groups.\nThis suggests that the notion of Big Tech domination over AI research is\noversimplified in the discourse.\n  Ultimately, this more nuanced understanding of Big Tech's and Academia's\ninfluence could inform a more symbiotic alliance between these stakeholders\nwhich would better serve the dual goals of societal welfare and the scientific\nintegrity of AI research.\n",
                "链接": "https://arxiv.org/abs/2312.12881"
            },
            {
                "文章ID": "26216",
                "标题": "How Does Automation Shape the Process of Narrative Visualization: A\n  Survey of Tools",
                "作者": " Qing Chen,  Shixiong Cao,  Jiazhe Wang,  Nan Cao",
                "发布日期": "2023-03-23",
                "摘要": "  In recent years, narrative visualization has gained much attention.\nResearchers have proposed different design spaces for various narrative\nvisualization genres and scenarios to facilitate the creation process. As\nusers' needs grow and automation technologies advance, increasingly more tools\nhave been designed and developed. In this study, we summarized six genres of\nnarrative visualization (annotated charts, infographics, timelines &\nstorylines, data comics, scrollytelling & slideshow, and data videos) based on\nprevious research and four types of tools (design spaces, authoring tools,\nML/AI-supported tools and ML/AI-generator tools) based on the intelligence and\nautomation level of the tools. We surveyed 105 papers and tools to study how\nautomation can progressively engage in visualization design and narrative\nprocesses to help users easily create narrative visualizations. This research\naims to provide an overview of current research and development in the\nautomation involvement of narrative visualization tools. We discuss key\nresearch problems in each category and suggest new opportunities to encourage\nfurther research in the related domain.\n",
                "链接": "https://arxiv.org/abs/2206.12118"
            },
            {
                "文章ID": "104308",
                "标题": "People's Perceptions Toward Bias and Related Concepts in Large Language\n  Models: A Systematic Review",
                "作者": " Lu Wang,  Max Song,  Rezvaneh Rezapour,  Bum Chul Kwon,  Jina Huh-Yoo",
                "发布日期": "2023-09-27",
                "摘要": "  Large language models (LLMs) have brought breakthroughs in tasks including\ntranslation, summarization, information retrieval, and language generation,\ngaining growing interest in the CHI community. Meanwhile, the literature shows\nresearchers' controversial perceptions about the efficacy, ethics, and\nintellectual abilities of LLMs. However, we do not know how lay people perceive\nLLMs that are pervasive in everyday tools, specifically regarding their\nexperience with LLMs around bias, stereotypes, social norms, or safety. In this\nstudy, we conducted a systematic review to understand what empirical insights\npapers have gathered about people's perceptions toward LLMs. From a total of\n231 retrieved papers, we full-text reviewed 15 papers that recruited human\nevaluators to assess their experiences with LLMs. We report different biases\nand related concepts investigated by these studies, four broader LLM\napplication areas, the evaluators' perceptions toward LLMs' performances\nincluding advantages, biases, and conflicting perceptions, factors influencing\nthese perceptions, and concerns about LLM applications.\n",
                "链接": "https://arxiv.org/abs/2309.14504"
            },
            {
                "文章ID": "12139",
                "标题": "AUC Maximization in the Era of Big Data and AI: A Survey",
                "作者": " Tianbao Yang,  Yiming Ying",
                "发布日期": "2022-08-04",
                "摘要": "  Area under the ROC curve, a.k.a. AUC, is a measure of choice for assessing\nthe performance of a classifier for imbalanced data. AUC maximization refers to\na learning paradigm that learns a predictive model by directly maximizing its\nAUC score. It has been studied for more than two decades dating back to late\n90s and a huge amount of work has been devoted to AUC maximization since then.\nRecently, stochastic AUC maximization for big data and deep AUC maximization\nfor deep learning have received increasing attention and yielded dramatic\nimpact for solving real-world problems. However, to the best our knowledge\nthere is no comprehensive survey of related works for AUC maximization. This\npaper aims to address the gap by reviewing the literature in the past two\ndecades. We not only give a holistic view of the literature but also present\ndetailed explanations and comparisons of different papers from formulations to\nalgorithms and theoretical guarantees. We also identify and discuss remaining\nand emerging issues for deep AUC maximization, and provide suggestions on\ntopics for future work.\n",
                "链接": "https://arxiv.org/abs/2203.15046"
            },
            {
                "文章ID": "30341",
                "标题": "Big Data and Education: using big data analytics in language learning",
                "作者": " Vahid Ashrafimoghari",
                "发布日期": "2022-07-22",
                "摘要": "  Working with big data using data mining tools is rapidly becoming a trend in\neducation industry. The combination of the current capacity to collect, store,\nmanage and process data in a timely manner, and data from online educational\nplatforms represents an unprecedented opportunity for educational institutes,\nlearners, educators, and researchers. In this position paper, we consider some\nbasic concepts as well as most popular tools, methods and techniques regarding\nEducational Data Mining and Learning Analytics, and discuss big data\napplications in language learning, in particular.\n",
                "链接": "https://arxiv.org/abs/2207.10572"
            },
            {
                "文章ID": "95587",
                "标题": "NLLG Quarterly arXiv Report 06/23: What are the most influential current\n  AI Papers?",
                "作者": " Steffen Eger,  Christoph Leiter,  Jonas Belouadi,  Ran Zhang,  Aida Kostikova,  Daniil Larionov,  Yanran Chen,  Vivian Fresen",
                "发布日期": "2023-08-15",
                "摘要": "  The rapid growth of information in the field of Generative Artificial\nIntelligence (AI), particularly in the subfields of Natural Language Processing\n(NLP) and Machine Learning (ML), presents a significant challenge for\nresearchers and practitioners to keep pace with the latest developments. To\naddress the problem of information overload, this report by the Natural\nLanguage Learning Group at Bielefeld University focuses on identifying the most\npopular papers on arXiv, with a specific emphasis on NLP and ML. The objective\nis to offer a quick guide to the most relevant and widely discussed research,\naiding both newcomers and established researchers in staying abreast of current\ntrends. In particular, we compile a list of the 40 most popular papers based on\nnormalized citation counts from the first half of 2023. We observe the\ndominance of papers related to Large Language Models (LLMs) and specifically\nChatGPT during the first half of 2023, with the latter showing signs of\ndeclining popularity more recently, however. Further, NLP related papers are\nthe most influential (around 60\\% of top papers) even though there are twice as\nmany ML related papers in our data. Core issues investigated in the most\nheavily cited papers are: LLM efficiency, evaluation techniques, ethical\nconsiderations, embodied agents, and problem-solving with LLMs. Additionally,\nwe examine the characteristics of top papers in comparison to others outside\nthe top-40 list (noticing the top paper's focus on LLM related issues and\nhigher number of co-authors) and analyze the citation distributions in our\ndataset, among others.\n",
                "链接": "https://arxiv.org/abs/2308.04889"
            },
            {
                "文章ID": "42172",
                "标题": "Predicting the clinical citation count of biomedical papers using\n  multilayer perceptron neural network",
                "作者": " Xin Li,  Xuli Tang,  Qikai Cheng",
                "发布日期": "2022-10-24",
                "摘要": "  The number of clinical citations received from clinical guidelines or\nclinical trials has been considered as one of the most appropriate indicators\nfor quantifying the clinical impact of biomedical papers. Therefore, the early\nprediction of the clinical citation count of biomedical papers is critical to\nscientific activities in biomedicine, such as research evaluation, resource\nallocation, and clinical translation. In this study, we designed a four-layer\nmultilayer perceptron neural network (MPNN) model to predict the clinical\ncitation count of biomedical papers in the future by using 9,822,620 biomedical\npapers published from 1985 to 2005. We extracted ninety-one paper features from\nthree dimensions as the input of the model, including twenty-one features in\nthe paper dimension, thirty-five in the reference dimension, and thirty-five in\nthe citing paper dimension. In each dimension, the features can be classified\ninto three categories, i.e., the citation-related features, the clinical\ntranslation-related features, and the topic-related features. Besides, in the\npaper dimension, we also considered the features that have previously been\ndemonstrated to be related to the citation counts of research papers. The\nresults showed that the proposed MPNN model outperformed the other five\nbaseline models, and the features in the reference dimension were the most\nimportant.\n",
                "链接": "https://arxiv.org/abs/2210.06346"
            },
            {
                "文章ID": "75809",
                "标题": "SuperNOVA: Design Strategies and Opportunities for Interactive\n  Visualization in Computational Notebooks",
                "作者": " Zijie J. Wang,  David Munechika,  Seongmin Lee,  Duen Horng Chau",
                "发布日期": "2023-05-05",
                "摘要": "  Computational notebooks such as Jupyter Notebook have become data scientists'\nde facto programming environments. Many visualization researchers and\npractitioners have developed interactive visualization tools that support\nnotebooks. However, little is known about the appropriate design of visual\nanalytics (VA) tools in notebooks. To bridge this critical research gap, we\ninvestigate the design strategies in this space by analyzing 159 notebook VA\ntools and their users' feedback. Our analysis encompasses 62 systems from\nacademic papers and 103 systems sourced from a pool of 55k notebooks containing\ninteractive visualizations that we obtain via scraping 8.6 million notebooks on\nGitHub. We also examine findings from 15 user studies and user feedback in 379\nGitHub issues. Through this work, we identify unique design opportunities and\nconsiderations for future notebook VA tools, such as using and manipulating\nmultimodal data in notebooks as well as balancing the degree of\nvisualization-notebook integration. Finally, we develop SuperNOVA, an\nopen-source interactive tool to help researchers explore existing notebook VA\ntools and search for related work.\n",
                "链接": "https://arxiv.org/abs/2305.03039"
            },
            {
                "文章ID": "32433",
                "标题": "Threddy: An Interactive System for Personalized Thread-based Exploration\n  and Organization of Scientific Literature",
                "作者": " Hyeonsu B. Kang,  Joseph Chee Chang,  Yongsung Kim,  Aniket Kittur",
                "发布日期": "2022-08-17",
                "摘要": "  Reviewing the literature to understand relevant threads of past work is a\ncritical part of research and vehicle for learning. However, as the scientific\nliterature grows the challenges for users to find and make sense of the many\ndifferent threads of research grow as well. Previous work has helped scholars\nto find and group papers with citation information or textual similarity using\nstandalone tools or overview visualizations. Instead, in this work we explore a\ntool integrated into users' reading process that helps them with leveraging\nauthors' existing summarization of threads, typically in introduction or\nrelated work sections, in order to situate their own work's contributions. To\nexplore this we developed a prototype that supports efficient extraction and\norganization of threads along with supporting evidence as scientists read\nresearch articles. The system then recommends further relevant articles based\non user-created threads. We evaluate the system in a lab study and find that it\nhelps scientists to follow and curate research threads without breaking out of\ntheir flow of reading, collect relevant papers and clips, and discover\ninteresting new articles to further grow threads.\n",
                "链接": "https://arxiv.org/abs/2208.03455"
            }
        ]
    },
    {
        "question": {
            "question": "和大模型可解释相关的最新论文",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "查询近一年模型推理加速相关的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "52105",
                "标题": "INGeo: Accelerating Instant Neural Scene Reconstruction with Noisy\n  Geometry Priors",
                "作者": " Chaojian Li,  Bichen Wu,  Albert Pumarola,  Peizhao Zhang,  Yingyan Lin,  Peter Vajda",
                "发布日期": "2022-12-06",
                "摘要": "  We present a method that accelerates reconstruction of 3D scenes and objects,\naiming to enable instant reconstruction on edge devices such as mobile phones\nand AR/VR headsets. While recent works have accelerated scene reconstruction\ntraining to minute/second-level on high-end GPUs, there is still a large gap to\nthe goal of instant training on edge devices which is yet highly desired in\nmany emerging applications such as immersive AR/VR. To this end, this work aims\nto further accelerate training by leveraging geometry priors of the target\nscene. Our method proposes strategies to alleviate the noise of the imperfect\ngeometry priors to accelerate the training speed on top of the highly optimized\nInstant-NGP. On the NeRF Synthetic dataset, our work uses half of the training\niterations to reach an average test PSNR of >30.\n",
                "链接": "https://arxiv.org/abs/2212.01959"
            },
            {
                "文章ID": "88910",
                "标题": "Accelerated primal-dual methods with enlarged step sizes and operator\n  learning for nonsmooth optimal control problems",
                "作者": " Yongcun Song,  Xiaoming Yuan,  Hangrui Yue",
                "发布日期": "2023-07-26",
                "摘要": "  We consider a general class of nonsmooth optimal control problems with\npartial differential equation (PDE) constraints, which are very challenging due\nto its nonsmooth objective functionals and the resulting high-dimensional and\nill-conditioned systems after discretization. We focus on the application of a\nprimal-dual method, with which different types of variables can be treated\nindividually and thus its main computation at each iteration only requires\nsolving two PDEs. Our target is to accelerate the primal-dual method with\neither larger step sizes or operator learning techniques. For the accelerated\nprimal-dual method with larger step sizes, its convergence can be still proved\nrigorously while it numerically accelerates the original primal-dual method in\na simple and universal way. For the operator learning acceleration, we\nconstruct deep neural network surrogate models for the involved PDEs. Once a\nneural operator is learned, solving a PDE requires only a forward pass of the\nneural network, and the computational cost is thus substantially reduced. The\naccelerated primal-dual method with operator learning is mesh-free, numerically\nefficient, and scalable to different types of PDEs. The acceleration\neffectiveness of these two techniques is promisingly validated by some\npreliminary numerical results.\n",
                "链接": "https://arxiv.org/abs/2307.00296"
            },
            {
                "文章ID": "4263",
                "标题": "Wave-Encoded Model-based Deep Learning for Highly Accelerated Imaging\n  with Joint Reconstruction",
                "作者": " Jaejin Cho,  Borjan Gagoski,  Taehyung Kim,  Qiyuan Tian,  Stephen Robert Frost,  Itthi Chatnuntawech,  Berkin Bilgic",
                "发布日期": "2022-02-08",
                "摘要": "  Purpose: To propose a wave-encoded model-based deep learning (wave-MoDL)\nstrategy for highly accelerated 3D imaging and joint multi-contrast image\nreconstruction, and further extend this to enable rapid quantitative imaging\nusing an interleaved look-locker acquisition sequence with T2 preparation pulse\n(3D-QALAS).\n  Method: Recently introduced MoDL technique successfully incorporates\nconvolutional neural network (CNN)-based regularizers into physics-based\nparallel imaging reconstruction using a small number of network parameters.\nWave-CAIPI is an emerging parallel imaging method that accelerates the imaging\nspeed by employing sinusoidal gradients in the phase- and slice-encoding\ndirections during the readout to take better advantage of 3D coil sensitivity\nprofiles. In wave-MoDL, we propose to combine the wave-encoding strategy with\nunrolled network constraints to accelerate the acquisition speed while\nenforcing wave-encoded data consistency. We further extend wave-MoDL to\nreconstruct multi-contrast data with controlled aliasing in parallel imaging\n(CAIPI) sampling patterns to leverage similarity between multiple images to\nimprove the reconstruction quality.\n  Result: Wave-MoDL enables a 47-second MPRAGE acquisition at 1 mm resolution\nat 16-fold acceleration. For quantitative imaging, wave-MoDL permits a 2-minute\nacquisition for T1, T2, and proton density mapping at 1 mm resolution at\n12-fold acceleration, from which contrast weighted images can be synthesized as\nwell.\n  Conclusion: Wave-MoDL allows rapid MR acquisition and high-fidelity image\nreconstruction and may facilitate clinical and neuroscientific applications by\nincorporating unrolled neural networks into wave-CAIPI reconstruction.\n",
                "链接": "https://arxiv.org/abs/2202.02814"
            },
            {
                "文章ID": "27949",
                "标题": "Learning Apparent Diffusion Coefficient Maps from Accelerated Radial\n  k-Space Diffusion-Weighted MRI in Mice using a Deep CNN-Transformer Model",
                "作者": " Yuemeng Li,  Miguel Romanello Joaquim,  Stephen Pickup,  Hee Kwon Song,  Rong Zhou,  Yong Fan",
                "发布日期": "2023-08-15",
                "摘要": "  Purpose: To accelerate radially sampled diffusion weighted spin-echo\n(Rad-DW-SE) acquisition method for generating high quality apparent diffusion\ncoefficient (ADC) maps. Methods: A deep learning method was developed to\ngenerate accurate ADC maps from accelerated DWI data acquired with the\nRad-DW-SE method. The deep learning method integrates convolutional neural\nnetworks (CNNs) with vision transformers to generate high quality ADC maps from\naccelerated DWI data, regularized by a monoexponential ADC model fitting term.\nA model was trained on DWI data of 147 mice and evaluated on DWI data of 36\nmice, with acceleration factors of 4x and 8x compared to the original\nacquisition parameters. We have made our code publicly available at GitHub:\nhttps://github.com/ymli39/DeepADC-Net-Learning-Apparent-Diffusion-Coefficient-Maps,\nand our dataset can be downloaded at\nhttps://pennpancreaticcancerimagingresource.github.io/data.html. Results:\nAblation studies and experimental results have demonstrated that the proposed\ndeep learning model generates higher quality ADC maps from accelerated DWI data\nthan alternative deep learning methods under comparison when their performance\nis quantified in whole images as well as in regions of interest, including\ntumors, kidneys, and muscles. Conclusions: The deep learning method with\nintegrated CNNs and transformers provides an effective means to accurately\ncompute ADC maps from accelerated DWI data acquired with the Rad-DW-SE method.\n",
                "链接": "https://arxiv.org/abs/2207.02399"
            },
            {
                "文章ID": "96754",
                "标题": "Improving CTC-AED model with integrated-CTC and auxiliary loss\n  regularization",
                "作者": " Daobin Zhu,  Xiangdong Su,  Hongbin Zhang",
                "发布日期": "2023-08-17",
                "摘要": "  Connectionist temporal classification (CTC) and attention-based encoder\ndecoder (AED) joint training has been widely applied in automatic speech\nrecognition (ASR). Unlike most hybrid models that separately calculate the CTC\nand AED losses, our proposed integrated-CTC utilizes the attention mechanism of\nAED to guide the output of CTC. In this paper, we employ two fusion methods,\nnamely direct addition of logits (DAL) and preserving the maximum probability\n(PMP). We achieve dimensional consistency by adaptively affine transforming the\nattention results to match the dimensions of CTC. To accelerate model\nconvergence and improve accuracy, we introduce auxiliary loss regularization\nfor accelerated convergence. Experimental results demonstrate that the DAL\nmethod performs better in attention rescoring, while the PMP method excels in\nCTC prefix beam search and greedy search.\n",
                "链接": "https://arxiv.org/abs/2308.08449"
            },
            {
                "文章ID": "47842",
                "标题": "Fast model averaging via buffered states and first-order accelerated\n  optimization algorithms",
                "作者": " Amir-Salar Esteki,  Hossein Moradian,  Solmaz S. Kia",
                "发布日期": "2022-11-14",
                "摘要": "  In this letter, we study the problem of accelerating reaching average\nconsensus over connected graphs in a discrete-time communication setting.\nLiterature has shown that consensus algorithms can be accelerated by increasing\nthe graph connectivity or optimizing the weights agents place on the\ninformation received from their neighbors. Here, instead of altering the\ncommunication graph, we investigate two methods that use buffered states to\naccelerate reaching average consensus over a given graph. In the first method,\nwe study how convergence rate of the well-known first-order Laplacian average\nconsensus algorithm changes when agreement feedback is generated from buffered\nstates. For this study, we obtain a sufficient condition on the ranges of\nbuffered state that leads to faster convergence. In the second proposed method,\nwe show how the average consensus problem can be cast as a convex optimization\nproblem and solved by first-order accelerated optimization algorithms for\nstrongly-convex cost functions. We construct an accelerated average consensus\nalgorithm using the so-called Triple Momentum optimization algorithm. The first\napproach requires less global knowledge for choosing the step size, whereas the\nsecond one converges faster in our numerical results by using extra information\nfrom the graph topology. We demonstrate our results by implementing the\nproposed algorithms in a Gaussian Mixture Model (GMM) estimation problem used\nin sensor networks.\n",
                "链接": "https://arxiv.org/abs/2211.05959"
            },
            {
                "文章ID": "81780",
                "标题": "RT-kNNS Unbound: Using RT Cores to Accelerate Unrestricted Neighbor\n  Search",
                "作者": " Vani Nagarajan,  Durga Mandarapu,  Milind Kulkarni",
                "发布日期": "2023-05-31",
                "摘要": "  The problem of identifying the k-Nearest Neighbors (kNNS) of a point has\nproven to be very useful both as a standalone application and as a subroutine\nin larger applications. Given its far-reaching applicability in areas such as\nmachine learning and point clouds, extensive research has gone into leveraging\nGPU acceleration to solve this problem. Recent work has shown that using Ray\nTracing cores in recent GPUs to accelerate kNNS is much more efficient compared\nto traditional acceleration using shader cores. However, the existing\ntranslation of kNNS to a ray tracing problem imposes a constraint on the search\nspace for neighbors. Due to this, we can only use RT cores to accelerate\nfixed-radius kNNS, which requires the user to set a search radius a priori and\nhence can miss neighbors. In this work, we propose TrueKNN, the first unbounded\nRT-accelerated neighbor search. TrueKNN adopts an iterative approach where we\nincrementally grow the search space until all points have found their k\nneighbors. We show that our approach is orders of magnitude faster than\nexisting approaches and can even be used to accelerate fixed-radius neighbor\nsearches.\n",
                "链接": "https://arxiv.org/abs/2305.18356"
            },
            {
                "文章ID": "117009",
                "标题": "Fast Controllable Diffusion Models for Undersampled MRI Reconstruction",
                "作者": " Wei Jiang,  Zhuang Xiong,  Feng Liu,  Nan Ye,  Hongfu Sun",
                "发布日期": "2023-12-04",
                "摘要": "  Supervised deep learning methods have shown promise in undersampled Magnetic\nResonance Imaging (MRI) reconstruction, but their requirement for paired data\nlimits their generalizability to the diverse MRI acquisition parameters.\nRecently, unsupervised controllable generative diffusion models have been\napplied to undersampled MRI reconstruction, without paired data or model\nretraining for different MRI acquisitions. However, diffusion models are\ngenerally slow in sampling and state-of-the-art acceleration techniques can\nlead to sub-optimal results when directly applied to the controllable\ngeneration process. This study introduces a new algorithm called\nPredictor-Projector-Noisor (PPN), which enhances and accelerates controllable\ngeneration of diffusion models for undersampled MRI reconstruction. Our results\ndemonstrate that PPN produces high-fidelity MR images that conform to\nundersampled k-space measurements with significantly shorter reconstruction\ntime than other controllable sampling methods. In addition, the unsupervised\nPPN accelerated diffusion models are adaptable to different MRI acquisition\nparameters, making them more practical for clinical use than supervised\nlearning techniques.\n",
                "链接": "https://arxiv.org/abs/2311.12078"
            },
            {
                "文章ID": "103518",
                "标题": "Parallelizing non-linear sequential models over the sequence length",
                "作者": " Yi Heng Lim,  Qi Zhu,  Joshua Selfridge,  Muhammad Firmansyah Kasim",
                "发布日期": "2023-10-04",
                "摘要": "  Sequential models, such as Recurrent Neural Networks and Neural Ordinary\nDifferential Equations, have long suffered from slow training due to their\ninherent sequential nature. For many years this bottleneck has persisted, as\nmany thought sequential models could not be parallelized. We challenge this\nlong-held belief with our parallel algorithm that accelerates GPU evaluation of\nsequential models by up to 3 orders of magnitude faster without compromising\noutput accuracy. The algorithm does not need any special structure in the\nsequential models' architecture, making it applicable to a wide range of\narchitectures. Using our method, training sequential models can be more than 10\ntimes faster than the common sequential method without any meaningful\ndifference in the training results. Leveraging this accelerated training, we\ndiscovered the efficacy of the Gated Recurrent Unit in a long time series\nclassification problem with 17k time samples. By overcoming the training\nbottleneck, our work serves as the first step to unlock the potential of\nnon-linear sequential models for long sequence problems.\n",
                "链接": "https://arxiv.org/abs/2309.12252"
            },
            {
                "文章ID": "33333",
                "标题": "Multinomial Logistic Regression Algorithms via Quadratic Gradient",
                "作者": " John Chiang",
                "发布日期": "2023-03-30",
                "摘要": "  Multinomial logistic regression, also known by other names such as multiclass\nlogistic regression and softmax regression, is a fundamental classification\nmethod that generalizes binary logistic regression to multiclass problems. A\nrecently work proposed a faster gradient called $\\texttt{quadratic gradient}$\nthat can accelerate the binary logistic regression training, and presented an\nenhanced Nesterov's accelerated gradient (NAG) method for binary logistic\nregression.\n  In this paper, we extend this work to multiclass logistic regression and\npropose an enhanced Adaptive Gradient Algorithm (Adagrad) that can accelerate\nthe original Adagrad method. We test the enhanced NAG method and the enhanced\nAdagrad method on some multiclass-problem datasets. Experimental results show\nthat both enhanced methods converge faster than their original ones\nrespectively.\n",
                "链接": "https://arxiv.org/abs/2208.06828"
            }
        ]
    },
    {
        "question": {
            "question": "查找论文中包含指令微调细节描述的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "23495",
                "标题": "Network Report: A Structured Description for Network Datasets",
                "作者": " Xinyi Zheng,  Ryan A. Rossi,  Nesreen Ahmed,  Dominik Moritz",
                "发布日期": "2022-06-09",
                "摘要": "  The rapid development of network science and technologies depends on\nshareable datasets. Currently, there is no standard practice for reporting and\nsharing network datasets. Some network dataset providers only share links,\nwhile others provide some contexts or basic statistics. As a result, critical\ninformation may be unintentionally dropped, and network dataset consumers may\nmisunderstand or overlook critical aspects. Inappropriately using a network\ndataset can lead to severe consequences (e.g., discrimination) especially when\nmachine learning models on networks are deployed in high-stake domains.\nChallenges arise as networks are often used across different domains (e.g.,\nnetwork science, physics, etc) and have complex structures. To facilitate the\ncommunication between network dataset providers and consumers, we propose\nnetwork report. A network report is a structured description that summarizes\nand contextualizes a network dataset. Network report extends the idea of\ndataset reports (e.g., Datasheets for Datasets) from prior work with\nnetwork-specific descriptions of the non-i.i.d. nature, demographic\ninformation, network characteristics, etc. We hope network reports encourage\ntransparency and accountability in network research and development across\ndifferent fields.\n",
                "链接": "https://arxiv.org/abs/2206.03635"
            },
            {
                "文章ID": "41625",
                "标题": "Deep Insights of Learning based Micro Expression Recognition: A\n  Perspective on Promises, Challenges and Research Needs",
                "作者": " Monu Verma,  Santosh Kumar Vipparthi,  Girdhari Singh",
                "发布日期": "2022-10-12",
                "摘要": "  Micro expression recognition (MER) is a very challenging area of research due\nto its intrinsic nature and fine-grained changes. In the literature, the\nproblem of MER has been solved through handcrafted/descriptor-based techniques.\nHowever, in recent times, deep learning (DL) based techniques have been adopted\nto gain higher performance for MER. Also, rich survey articles on MER are\navailable by summarizing the datasets, experimental settings, conventional and\ndeep learning methods. In contrast, these studies lack the ability to convey\nthe impact of network design paradigms and experimental setting strategies for\nDL-based MER. Therefore, this paper aims to provide a deep insight into the\nDL-based MER frameworks with a perspective on promises in network model\ndesigning, experimental strategies, challenges, and research needs. Also, the\ndetailed categorization of available MER frameworks is presented in various\naspects of model design and technical characteristics. Moreover, an empirical\nanalysis of the experimental and validation protocols adopted by MER methods is\npresented. The challenges mentioned earlier and network design strategies may\nassist the affective computing research community in forging ahead in MER\nresearch. Finally, we point out the future directions, research needs, and draw\nour conclusions.\n",
                "链接": "https://arxiv.org/abs/2210.04935"
            },
            {
                "文章ID": "82480",
                "标题": "Image Registration of In Vivo Micro-Ultrasound and Ex Vivo Pseudo-Whole\n  Mount Histopathology Images of the Prostate: A Proof-of-Concept Study",
                "作者": " Muhammad Imran,  Brianna Nguyen,  Jake Pensa,  Sara M. Falzarano,  Anthony E. Sisk,  Muxuan Liang,  John Michael DiBianco,  Li-Ming Su,  Yuyin Zhou,  Wayne G. Brisbane,  Wei Shao",
                "发布日期": "2023-06-21",
                "摘要": "  Early diagnosis of prostate cancer significantly improves a patient's 5-year\nsurvival rate. Biopsy of small prostate cancers is improved with image-guided\nbiopsy. MRI-ultrasound fusion-guided biopsy is sensitive to smaller tumors but\nis underutilized due to the high cost of MRI and fusion equipment.\nMicro-ultrasound (micro-US), a novel high-resolution ultrasound technology,\nprovides a cost-effective alternative to MRI while delivering comparable\ndiagnostic accuracy. However, the interpretation of micro-US is challenging due\nto subtle gray scale changes indicating cancer vs normal tissue. This challenge\ncan be addressed by training urologists with a large dataset of micro-US images\ncontaining the ground truth cancer outlines. Such a dataset can be mapped from\nsurgical specimens (histopathology) onto micro-US images via image\nregistration. In this paper, we present a semi-automated pipeline for\nregistering in vivo micro-US images with ex vivo whole-mount histopathology\nimages. Our pipeline begins with the reconstruction of pseudo-whole-mount\nhistopathology images and a 3-dimensional (3D) micro-US volume. Each\npseudo-whole-mount histopathology image is then registered with the\ncorresponding axial micro-US slice using a two-stage approach that estimates an\naffine transformation followed by a deformable transformation. We evaluated our\nregistration pipeline using micro-US and histopathology images from 18 patients\nwho underwent radical prostatectomy. The results showed a Dice coefficient of\n0.94 and a landmark error of 2.7 mm, indicating the accuracy of our\nregistration pipeline. This proof-of-concept study demonstrates the feasibility\nof accurately aligning micro-US and histopathology images. To promote\ntransparency and collaboration in research, we will make our code and dataset\npublicly available.\n",
                "链接": "https://arxiv.org/abs/2305.19939"
            },
            {
                "文章ID": "108972",
                "标题": "An Empirical Study of Super-resolution on Low-resolution\n  Micro-expression Recognition",
                "作者": " Ling Zhou,  Mingpei Wang,  Xiaohua Huang,  Wenming Zheng,  Qirong Mao,  Guoying Zhao",
                "发布日期": "2023-10-17",
                "摘要": "  Micro-expression recognition (MER) in low-resolution (LR) scenarios presents\nan important and complex challenge, particularly for practical applications\nsuch as group MER in crowded environments. Despite considerable advancements in\nsuper-resolution techniques for enhancing the quality of LR images and videos,\nfew study has focused on investigate super-resolution for improving LR MER. The\nscarcity of investigation can be attributed to the inherent difficulty in\ncapturing the subtle motions of micro-expressions, even in original-resolution\nMER samples, which becomes even more challenging in LR samples due to the loss\nof distinctive features. Furthermore, a lack of systematic benchmarking and\nthorough analysis of super-resolution-assisted MER methods has been noted. This\npaper tackles these issues by conducting a series of benchmark experiments that\nintegrate both super-resolution (SR) and MER methods, guided by an in-depth\nliterature survey. Specifically, we employ seven cutting-edge state-of-the-art\n(SOTA) MER techniques and evaluate their performance on samples generated from\n13 SOTA SR techniques, thereby addressing the problem of super-resolution in\nMER. Through our empirical study, we uncover the primary challenges associated\nwith SR-assisted MER and identify avenues to tackle these challenges by\nleveraging recent advancements in both SR and MER methodologies. Our analysis\nprovides insights for progressing toward more efficient SR-assisted MER.\n",
                "链接": "https://arxiv.org/abs/2310.10022"
            },
            {
                "文章ID": "59549",
                "标题": "ImageNomer: description of a functional connectivity and omics analysis\n  tool and case study identifying a race confound",
                "作者": " Anton Orlichenko,  Grant Daly,  Ziyu Zhou,  Anqi Liu,  Hui Shen,  Hong-Wen Deng,  Yu-Ping Wang",
                "发布日期": "2023-10-13",
                "摘要": "  Most packages for the analysis of fMRI-based functional connectivity (FC) and\ngenomic data are used with a programming language interface, lacking an\neasy-to-navigate GUI frontend. This exacerbates two problems found in these\ntypes of data: demographic confounds and quality control in the face of high\ndimensionality of features. The reason is that it is too slow and cumbersome to\nuse a programming interface to create all the necessary visualizations required\nto identify all correlations, confounding effects, or quality control problems\nin a dataset. To remedy this situation, we have developed ImageNomer, a data\nvisualization and analysis tool that allows inspection of both subject-level\nand cohort-level demographic, genomic, and imaging features. The software is\nPython-based, runs in a self-contained Docker image, and contains a\nbrowser-based GUI frontend. We demonstrate the usefulness of ImageNomer by\nidentifying an unexpected race confound when predicting achievement scores in\nthe Philadelphia Neurodevelopmental Cohort (PNC) dataset. In the past, many\nstudies have attempted to use FC to identify achievement-related features in\nfMRI. Using ImageNomer, we find a clear potential for confounding effects of\nrace. Using correlation analysis in the ImageNomer software, we show that FCs\ncorrelated with Wide Range Achievement Test (WRAT) score are in fact more\nhighly correlated with race. Investigating further, we find that whereas both\nFC and SNP (genomic) features can account for 10-15\\% of WRAT score variation,\nthis predictive ability disappears when controlling for race. In this work, we\ndemonstrate the advantage of our ImageNomer GUI tool in data exploration and\nconfound detection. Additionally, this work identifies race as a strong\nconfound in FC data and casts doubt on the possibility of finding unbiased\nachievement-related features in fMRI and SNP data of healthy adolescents.\n",
                "链接": "https://arxiv.org/abs/2302.00767"
            }
        ]
    },
    {
        "question": {
            "question": "查找大模型推理的论文",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "近几个月自然语言处理相关的文章。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "26137",
                "标题": "Urdu News Article Recommendation Model using Natural Language Processing\n  Techniques",
                "作者": " Syed Zain Abbas,  Dr. Arif ur Rahman,  Abdul Basit Mughal,  Syed Mujtaba Haider",
                "发布日期": "2022-06-24",
                "摘要": "  There are several online newspapers in urdu but for the users it is difficult\nto find the content they are looking for because these most of them contain\nirrelevant data and most users did not get what they want to retrieve. Our\nproposed framework will help to predict Urdu news in the interests of users and\nreduce the users searching time for news. For this purpose, NLP techniques are\nused for pre-processing, and then TF-IDF with cosine similarity is used for\ngaining the highest similarity and recommended news on user preferences.\nMoreover, the BERT language model is also used for similarity, and by using the\nBERT model similarity increases as compared to TF-IDF so the approach works\nbetter with the BERT language model and recommends news to the user on their\ninterest. The news is recommended when the similarity of the articles is above\n60 percent.\n",
                "链接": "https://arxiv.org/abs/2206.11862"
            },
            {
                "文章ID": "197",
                "标题": "Robust Natural Language Processing: Recent Advances, Challenges, and\n  Future Directions",
                "作者": " Marwan Omar,  Soohyeon Choi,  DaeHun Nyang,  David Mohaisen",
                "发布日期": "2022-10-24",
                "摘要": "  Recent natural language processing (NLP) techniques have accomplished high\nperformance on benchmark datasets, primarily due to the significant improvement\nin the performance of deep learning. The advances in the research community\nhave led to great enhancements in state-of-the-art production systems for NLP\ntasks, such as virtual assistants, speech recognition, and sentiment analysis.\nHowever, such NLP systems still often fail when tested with adversarial\nattacks. The initial lack of robustness exposed troubling gaps in current\nmodels' language understanding capabilities, creating problems when NLP systems\nare deployed in real life. In this paper, we present a structured overview of\nNLP robustness research by summarizing the literature in a systemic way across\nvarious dimensions. We then take a deep-dive into the various dimensions of\nrobustness, across techniques, metrics, embeddings, and benchmarks. Finally, we\nargue that robustness should be multi-dimensional, provide insights into\ncurrent research, identify gaps in the literature to suggest directions worth\npursuing to address these gaps.\n",
                "链接": "https://arxiv.org/abs/2201.00768"
            },
            {
                "文章ID": "62254",
                "标题": "Transformadores: Fundamentos teoricos y Aplicaciones",
                "作者": " Jordi de la Torre",
                "发布日期": "2023-02-21",
                "摘要": "  Transformers are a neural network architecture originally designed for\nnatural language processing that it is now a mainstream tool for solving a wide\nvariety of problems, including natural language processing, sound, image,\nreinforcement learning, and other problems with heterogeneous input data. Its\ndistinctive feature is its self-attention system, based on attention to one's\nown sequence, which derives from the previously introduced attention system.\nThis article provides the reader with the necessary context to understand the\nmost recent research articles and presents the mathematical and algorithmic\nfoundations of the elements that make up this type of network. The different\ncomponents that make up this architecture and the variations that may exist are\nalso studied, as well as some applications of the transformer models. This\narticle is in Spanish to bring this scientific knowledge to the\nSpanish-speaking community.\n",
                "链接": "https://arxiv.org/abs/2302.09327"
            },
            {
                "文章ID": "12668",
                "标题": "Generating Scientific Articles with Machine Learning",
                "作者": " Eliot H. Ayache,  Conor M. B. Omand",
                "发布日期": "2022-04-01",
                "摘要": "  In recent years, the field of machine learning has seen rapid growth, with\napplications in a variety of domains, including image recognition, natural\nlanguage processing, and predictive modeling. In this paper, we explore the\napplication of machine learning to the generation of scientific articles. We\npresent a method for using machine learning to generate scientific articles\nbased on a data set of scientific papers. The method uses a machine-learning\nalgorithm to learn the structure of a scientific article and a set of training\ndata consisting of scientific papers. The machine-learning algorithm is used to\ngenerate a scientific article based on the data set of scientific papers. We\nevaluate the performance of the method by comparing the generated article to a\nset of manually written articles. The results show that the machine-generated\narticle is of similar quality to the manually written articles.\n",
                "链接": "https://arxiv.org/abs/2203.16569"
            },
            {
                "文章ID": "26135",
                "标题": "Exploiting Transliterated Words for Finding Similarity in Inter-Language\n  News Articles using Machine Learning",
                "作者": " Sameea Naeem,  Dr. Arif ur Rahman,  Syed Mujtaba Haider,  Abdul Basit Mughal",
                "发布日期": "2022-06-24",
                "摘要": "  Finding similarities between two inter-language news articles is a\nchallenging problem of Natural Language Processing (NLP). It is difficult to\nfind similar news articles in a different language other than the native\nlanguage of user, there is a need for a Machine Learning based automatic system\nto find the similarity between two inter-language news articles. In this\narticle, we propose a Machine Learning model with the combination of English\nUrdu word transliteration which will show whether the English news article is\nsimilar to the Urdu news article or not. The existing approaches to find\nsimilarities has a major drawback when the archives contain articles of\nlow-resourced languages like Urdu along with English news article. The existing\napproaches to find similarities has drawback when the archives contain\nlow-resourced languages like Urdu along with English news articles. We used\nlexicon to link Urdu and English news articles. As Urdu language processing\napplications like machine translation, text to speech, etc are unable to handle\nEnglish text at the same time so this research proposed technique to find\nsimilarities in English and Urdu news articles based on transliteration.\n",
                "链接": "https://arxiv.org/abs/2206.11860"
            },
            {
                "文章ID": "22756",
                "标题": "TCE at Qur'an QA 2022: Arabic Language Question Answering Over Holy\n  Qur'an Using a Post-Processed Ensemble of BERT-based Models",
                "作者": " Mohammed ElKomy,  Amany M. Sarhan",
                "发布日期": "2022-06-06",
                "摘要": "  In recent years, we witnessed great progress in different tasks of natural\nlanguage understanding using machine learning. Question answering is one of\nthese tasks which is used by search engines and social media platforms for\nimproved user experience. Arabic is the language of the Holy Qur'an; the sacred\ntext for 1.8 billion people across the world. Arabic is a challenging language\nfor Natural Language Processing (NLP) due to its complex structures. In this\narticle, we describe our attempts at OSACT5 Qur'an QA 2022 Shared Task, which\nis a question answering challenge on the Holy Qur'an in Arabic. We propose an\nensemble learning model based on Arabic variants of BERT models. In addition,\nwe perform post-processing to enhance the model predictions. Our system\nachieves a Partial Reciprocal Rank (pRR) score of 56.6% on the official test\nset.\n",
                "链接": "https://arxiv.org/abs/2206.01550"
            },
            {
                "文章ID": "117134",
                "标题": "Extracting Definienda in Mathematical Scholarly Articles with\n  Transformers",
                "作者": "VALDA  Shufan Jiang, DI-ENS, VALDA  Pierre Senellart",
                "发布日期": "2023-11-22",
                "摘要": "  We consider automatically identifying the defined term within a mathematical\ndefinition from the text of an academic article. Inspired by the development of\ntransformer-based natural language processing applications, we pose the problem\nas (a) a token-level classification task using fine-tuned pre-trained\ntransformers; and (b) a question-answering task using a generalist large\nlanguage model (GPT). We also propose a rule-based approach to build a labeled\ndataset from the LATEX source of papers. Experimental results show that it is\npossible to reach high levels of precision and recall using either recent (and\nexpensive) GPT 4 or simpler pre-trained models fine-tuned on our task.\n",
                "链接": "https://arxiv.org/abs/2311.12448"
            },
            {
                "文章ID": "115729",
                "标题": "Natural Language Processing for Financial Regulation",
                "作者": " Ixandra Achitouv,  Dragos Gorduza,  Antoine Jacquier",
                "发布日期": "2023-11-16",
                "摘要": "  This article provides an understanding of Natural Language Processing\ntechniques in the framework of financial regulation, more specifically in order\nto perform semantic matching search between rules and policy when no dataset is\navailable for supervised learning. We outline how to outperform simple\npre-trained sentences-transformer models using freely available resources and\nexplain the mathematical concepts behind the key building blocks of Natural\nLanguage Processing.\n",
                "链接": "https://arxiv.org/abs/2311.08533"
            },
            {
                "文章ID": "113097",
                "标题": "The Development of LLMs for Embodied Navigation",
                "作者": " Jinzhou Lin,  Han Gao,  Xuxiang Feng,  Rongtao Xu,  Changwei Wang,  Man Zhang,  Li Guo,  Shibiao Xu",
                "发布日期": "2023-11-21",
                "摘要": "  In recent years, the rapid advancement of Large Language Models (LLMs) such\nas the Generative Pre-trained Transformer (GPT) has attracted increasing\nattention due to their potential in a variety of practical applications. The\napplication of LLMs with Embodied Intelligence has emerged as a significant\narea of focus. Among the myriad applications of LLMs, navigation tasks are\nparticularly noteworthy because they demand a deep understanding of the\nenvironment and quick, accurate decision-making. LLMs can augment embodied\nintelligence systems with sophisticated environmental perception and\ndecision-making support, leveraging their robust language and image-processing\ncapabilities. This article offers an exhaustive summary of the symbiosis\nbetween LLMs and embodied intelligence with a focus on navigation. It reviews\nstate-of-the-art models, research methodologies, and assesses the advantages\nand disadvantages of existing embodied navigation models and datasets. Finally,\nthe article elucidates the role of LLMs in embodied intelligence, based on\ncurrent research, and forecasts future directions in the field. A comprehensive\nlist of studies in this survey is available at\nhttps://github.com/Rongtao-Xu/Awesome-LLM-EN\n",
                "链接": "https://arxiv.org/abs/2311.00530"
            },
            {
                "文章ID": "101369",
                "标题": "NeCo@ALQAC 2023: Legal Domain Knowledge Acquisition for Low-Resource\n  Languages through Data Enrichment",
                "作者": " Hai-Long Nguyen,  Dieu-Quynh Nguyen,  Hoang-Trung Nguyen,  Thu-Trang Pham,  Huu-Dong Nguyen,  Thach-Anh Nguyen,  Thi-Hai-Yen Vuong,  Ha-Thanh Nguyen",
                "发布日期": "2023-09-12",
                "摘要": "  In recent years, natural language processing has gained significant\npopularity in various sectors, including the legal domain. This paper presents\nNeCo Team's solutions to the Vietnamese text processing tasks provided in the\nAutomated Legal Question Answering Competition 2023 (ALQAC 2023), focusing on\nlegal domain knowledge acquisition for low-resource languages through data\nenrichment. Our methods for the legal document retrieval task employ a\ncombination of similarity ranking and deep learning models, while for the\nsecond task, which requires extracting an answer from a relevant legal article\nin response to a question, we propose a range of adaptive techniques to handle\ndifferent question types. Our approaches achieve outstanding results on both\ntasks of the competition, demonstrating the potential benefits and\neffectiveness of question answering systems in the legal field, particularly\nfor low-resource languages.\n",
                "链接": "https://arxiv.org/abs/2309.05500"
            }
        ]
    },
    {
        "question": {
            "question": "查找多语言情感分析的最新论文。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "112858",
                "标题": "Linked Papers With Code: The Latest in Machine Learning as an RDF\n  Knowledge Graph",
                "作者": " Michael Färber,  David Lamprecht",
                "发布日期": "2023-11-01",
                "摘要": "  In this paper, we introduce Linked Papers With Code (LPWC), an RDF knowledge\ngraph that provides comprehensive, current information about almost 400,000\nmachine learning publications. This includes the tasks addressed, the datasets\nutilized, the methods implemented, and the evaluations conducted, along with\ntheir results. Compared to its non-RDF-based counterpart Papers With Code, LPWC\nnot only translates the latest advancements in machine learning into RDF\nformat, but also enables novel ways for scientific impact quantification and\nscholarly key content recommendation. LPWC is openly accessible at\nhttps://linkedpaperswithcode.com and is licensed under CC-BY-SA 4.0. As a\nknowledge graph in the Linked Open Data cloud, we offer LPWC in multiple\nformats, from RDF dump files to a SPARQL endpoint for direct web queries, as\nwell as a data source with resolvable URIs and links to the data sources\nSemOpenAlex, Wikidata, and DBLP. Additionally, we supply knowledge graph\nembeddings, enabling LPWC to be readily applied in machine learning\napplications.\n",
                "链接": "https://arxiv.org/abs/2310.20475"
            },
            {
                "文章ID": "20209",
                "标题": "M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database",
                "作者": " Jinming Zhao,  Tenggan Zhang,  Jingwen Hu,  Yuchen Liu,  Qin Jin,  Xinchao Wang,  Haizhou Li",
                "发布日期": "2022-05-23",
                "摘要": "  The emotional state of a speaker can be influenced by many different factors\nin dialogues, such as dialogue scene, dialogue topic, and interlocutor\nstimulus. The currently available data resources to support such multimodal\naffective analysis in dialogues are however limited in scale and diversity. In\nthis work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue\ndataset, M3ED, which contains 990 dyadic emotional dialogues from 56 different\nTV series, a total of 9,082 turns and 24,449 utterances. M3 ED is annotated\nwith 7 emotion categories (happy, surprise, sad, disgust, anger, fear, and\nneutral) at utterance level, and encompasses acoustic, visual, and textual\nmodalities. To the best of our knowledge, M3ED is the first multimodal\nemotional dialogue dataset in Chinese. It is valuable for cross-culture emotion\nanalysis and recognition. We apply several state-of-the-art methods on the M3ED\ndataset to verify the validity and quality of the dataset. We also propose a\ngeneral Multimodal Dialogue-aware Interaction framework, MDI, to model the\ndialogue context for emotion recognition, which achieves comparable performance\nto the state-of-the-art methods on the M3ED. The full dataset and codes are\navailable.\n",
                "链接": "https://arxiv.org/abs/2205.10237"
            },
            {
                "文章ID": "71155",
                "标题": "Towards Interpretable Mental Health Analysis with Large Language Models",
                "作者": " Kailai Yang,  Shaoxiong Ji,  Tianlin Zhang,  Qianqian Xie,  Ziyan Kuang,  Sophia Ananiadou",
                "发布日期": "2023-10-12",
                "摘要": "  The latest large language models (LLMs) such as ChatGPT, exhibit strong\ncapabilities in automated mental health analysis. However, existing relevant\nstudies bear several limitations, including inadequate evaluations, lack of\nprompting strategies, and ignorance of exploring LLMs for explainability. To\nbridge these gaps, we comprehensively evaluate the mental health analysis and\nemotional reasoning ability of LLMs on 11 datasets across 5 tasks. We explore\nthe effects of different prompting strategies with unsupervised and distantly\nsupervised emotional information. Based on these prompts, we explore LLMs for\ninterpretable mental health analysis by instructing them to generate\nexplanations for each of their decisions. We convey strict human evaluations to\nassess the quality of the generated explanations, leading to a novel dataset\nwith 163 human-assessed explanations. We benchmark existing automatic\nevaluation metrics on this dataset to guide future related works. According to\nthe results, ChatGPT shows strong in-context learning ability but still has a\nsignificant gap with advanced task-specific methods. Careful prompt engineering\nwith emotional cues and expert-written few-shot examples can also effectively\nimprove performance on mental health analysis. In addition, ChatGPT generates\nexplanations that approach human performance, showing its great potential in\nexplainable mental health analysis.\n",
                "链接": "https://arxiv.org/abs/2304.03347"
            },
            {
                "文章ID": "121628",
                "标题": "EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models",
                "作者": " Samuel J. Paech",
                "发布日期": "2023-12-12",
                "摘要": "  We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of\nemotional intelligence in Large Language Models (LLMs). We assess the ability\nof LLMs to understand complex emotions and social interactions by asking them\nto predict the intensity of emotional states of characters in a dialogue. The\nbenchmark is able to discriminate effectively between a wide range of models.\nWe find that EQ-Bench correlates strongly with comprehensive multi-domain\nbenchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may\nbe capturing similar aspects of broad intelligence. Our benchmark produces\nhighly repeatable results using a set of 60 English-language questions. We also\nprovide open-source code for an automated benchmarking pipeline at\nhttps://github.com/EQ-bench/EQ-Bench and a leaderboard at\nhttps://www.eqbench.com\n",
                "链接": "https://arxiv.org/abs/2312.06281"
            },
            {
                "文章ID": "39463",
                "标题": "Automatic Analysis of Available Source Code of Top Artificial\n  Intelligence Conference Papers",
                "作者": " Jialiang Lin,  Yingmin Wang,  Yao Yu,  Yu Zhou,  Yidong Chen,  Xiaodong Shi",
                "发布日期": "2022-09-29",
                "摘要": "  Source code is essential for researchers to reproduce the methods and\nreplicate the results of artificial intelligence (AI) papers. Some\norganizations and researchers manually collect AI papers with available source\ncode to contribute to the AI community. However, manual collection is a\nlabor-intensive and time-consuming task. To address this issue, we propose a\nmethod to automatically identify papers with available source code and extract\ntheir source code repository URLs. With this method, we find that 20.5% of\nregular papers of 10 top AI conferences published from 2010 to 2019 are\nidentified as papers with available source code and that 8.1% of these source\ncode repositories are no longer accessible. We also create the XMU NLP Lab\nREADME Dataset, the largest dataset of labeled README files for source code\ndocument research. Through this dataset, we have discovered that quite a few\nREADME files have no installation instructions or usage tutorials provided.\nFurther, a large-scale comprehensive statistical analysis is made for a general\npicture of the source code of AI conference papers. The proposed solution can\nalso go beyond AI conference papers to analyze other scientific papers from\nboth journals and conferences to shed light on more domains.\n",
                "链接": "https://arxiv.org/abs/2209.14155"
            },
            {
                "文章ID": "91586",
                "标题": "Emotional Intelligence of Large Language Models",
                "作者": " Xuena Wang,  Xueting Li,  Zi Yin,  Yue Wu,  Liu Jia",
                "发布日期": "2023-07-31",
                "摘要": "  Large Language Models (LLMs) have demonstrated remarkable abilities across\nnumerous disciplines, primarily assessed through tasks in language generation,\nknowledge utilization, and complex reasoning. However, their alignment with\nhuman emotions and values, which is critical for real-world applications, has\nnot been systematically evaluated. Here, we assessed LLMs' Emotional\nIntelligence (EI), encompassing emotion recognition, interpretation, and\nunderstanding, which is necessary for effective communication and social\ninteractions. Specifically, we first developed a novel psychometric assessment\nfocusing on Emotion Understanding (EU), a core component of EI, suitable for\nboth humans and LLMs. This test requires evaluating complex emotions (e.g.,\nsurprised, joyful, puzzled, proud) in realistic scenarios (e.g., despite\nfeeling underperformed, John surprisingly achieved a top score). With a\nreference frame constructed from over 500 adults, we tested a variety of\nmainstream LLMs. Most achieved above-average EQ scores, with GPT-4 exceeding\n89% of human participants with an EQ of 117. Interestingly, a multivariate\npattern analysis revealed that some LLMs apparently did not reply on the\nhuman-like mechanism to achieve human-level performance, as their\nrepresentational patterns were qualitatively distinct from humans. In addition,\nwe discussed the impact of factors such as model size, training method, and\narchitecture on LLMs' EQ. In summary, our study presents one of the first\npsychometric evaluations of the human-like characteristics of LLMs, which may\nshed light on the future development of LLMs aiming for both high intellectual\nand emotional intelligence. Project website:\nhttps://emotional-intelligence.github.io/\n",
                "链接": "https://arxiv.org/abs/2307.09042"
            },
            {
                "文章ID": "15904",
                "标题": "Multi-task recommendation system for scientific papers with high-way\n  networks",
                "作者": " Aram Karimi,  Simon Dobnik",
                "发布日期": "2022-04-22",
                "摘要": "  Finding and selecting the most relevant scientific papers from a large number\nof papers written in a research community is one of the key challenges for\nresearchers these days. As we know, much information around research interest\nfor scholars and academicians belongs to papers they read. Analysis and\nextracting contextual features from these papers could help us to suggest the\nmost related paper to them. In this paper, we present a multi-task\nrecommendation system (RS) that predicts a paper recommendation and generates\nits meta-data such as keywords. The system is implemented as a three-stage deep\nneural network encoder that tries to maps longer sequences of text to an\nembedding vector and learns simultaneously to predict the recommendation rate\nfor a particular user and the paper's keywords. The motivation behind this\napproach is that the paper's topics expressed as keywords are a useful\npredictor of preferences of researchers. To achieve this goal, we use a system\ncombination of RNNs, Highway and Convolutional Neural Networks to train\nend-to-end a context-aware collaborative matrix. Our application uses Highway\nnetworks to train the system very deep, combine the benefits of RNN and CNN to\nfind the most important factor and make latent representation. Highway Networks\nallow us to enhance the traditional RNN and CNN pipeline by learning more\nsophisticated semantic structural representations. Using this method we can\nalso overcome the cold start problem and learn latent features over large\nsequences of text.\n",
                "链接": "https://arxiv.org/abs/2204.09930"
            },
            {
                "文章ID": "105928",
                "标题": "Can large language models provide useful feedback on research papers? A\n  large-scale empirical analysis",
                "作者": " Weixin Liang,  Yuhui Zhang,  Hancheng Cao,  Binglu Wang,  Daisy Ding,  Xinyu Yang,  Kailas Vodrahalli,  Siyu He,  Daniel Smith,  Yian Yin,  Daniel McFarland,  James Zou",
                "发布日期": "2023-10-04",
                "摘要": "  Expert feedback lays the foundation of rigorous research. However, the rapid\ngrowth of scholarly production and intricate knowledge specialization challenge\nthe conventional scientific feedback mechanisms. High-quality peer reviews are\nincreasingly difficult to obtain. Researchers who are more junior or from\nunder-resourced settings have especially hard times getting timely feedback.\nWith the breakthrough of large language models (LLM) such as GPT-4, there is\ngrowing interest in using LLMs to generate scientific feedback on research\nmanuscripts. However, the utility of LLM-generated feedback has not been\nsystematically studied. To address this gap, we created an automated pipeline\nusing GPT-4 to provide comments on the full PDFs of scientific papers. We\nevaluated the quality of GPT-4's feedback through two large-scale studies. We\nfirst quantitatively compared GPT-4's generated feedback with human peer\nreviewer feedback in 15 Nature family journals (3,096 papers in total) and the\nICLR machine learning conference (1,709 papers). The overlap in the points\nraised by GPT-4 and by human reviewers (average overlap 30.85% for Nature\njournals, 39.23% for ICLR) is comparable to the overlap between two human\nreviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The\noverlap between GPT-4 and human reviewers is larger for the weaker papers. We\nthen conducted a prospective user study with 308 researchers from 110 US\ninstitutions in the field of AI and computational biology to understand how\nresearchers perceive feedback generated by our GPT-4 system on their own\npapers. Overall, more than half (57.4%) of the users found GPT-4 generated\nfeedback helpful/very helpful and 82.4% found it more beneficial than feedback\nfrom at least some human reviewers. While our findings show that LLM-generated\nfeedback can help researchers, we also identify several limitations.\n",
                "链接": "https://arxiv.org/abs/2310.01783"
            },
            {
                "文章ID": "12450",
                "标题": "An Overview & Analysis of Sequence-to-Sequence Emotional Voice\n  Conversion",
                "作者": " Zijiang Yang,  Xin Jing,  Andreas Triantafyllopoulos,  Meishu Song,  Ilhan Aslan,  Björn W. Schuller",
                "发布日期": "2022-03-31",
                "摘要": "  Emotional voice conversion (EVC) focuses on converting a speech utterance\nfrom a source to a target emotion; it can thus be a key enabling technology for\nhuman-computer interaction applications and beyond. However, EVC remains an\nunsolved research problem with several challenges. In particular, as speech\nrate and rhythm are two key factors of emotional conversion, models have to\ngenerate output sequences of differing length. Sequence-to-sequence modelling\nis recently emerging as a competitive paradigm for models that can overcome\nthose challenges. In an attempt to stimulate further research in this promising\nnew direction, recent sequence-to-sequence EVC papers were systematically\ninvestigated and reviewed from six perspectives: their motivation, training\nstrategies, model architectures, datasets, model inputs, and evaluation\nmethods. This information is organised to provide the research community with\nan easily digestible overview of the current state-of-the-art. Finally, we\ndiscuss existing challenges of sequence-to-sequence EVC.\n",
                "链接": "https://arxiv.org/abs/2203.15873"
            },
            {
                "文章ID": "109092",
                "标题": "Legal NLP Meets MiCAR: Advancing the Analysis of Crypto White Papers",
                "作者": " Carolina Camassa",
                "发布日期": "2023-10-26",
                "摘要": "  In the rapidly evolving field of crypto assets, white papers are essential\ndocuments for investor guidance, and are now subject to unprecedented content\nrequirements under the European Union's Markets in Crypto-Assets Regulation\n(MiCAR). Natural Language Processing (NLP) can serve as a powerful tool for\nboth analyzing these documents and assisting in regulatory compliance. This\npaper delivers two contributions to the topic. First, we survey existing\napplications of textual analysis to unregulated crypto asset white papers,\nuncovering a research gap that could be bridged with interdisciplinary\ncollaboration. We then conduct an analysis of the changes introduced by MiCAR,\nhighlighting the opportunities and challenges of integrating NLP within the new\nregulatory framework. The findings set the stage for further research, with the\npotential to benefit regulators, crypto asset issuers, and investors.\n",
                "链接": "https://arxiv.org/abs/2310.10333"
            }
        ]
    },
    {
        "question": {
            "question": "近一个月与多模态大模型相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "91534",
                "标题": "Federated Large Language Model: A Position Paper",
                "作者": " Chaochao Chen,  Xiaohua Feng,  Jun Zhou,  Jianwei Yin,  Xiaolin Zheng",
                "发布日期": "2023-07-19",
                "摘要": "  Large scale language models (LLM) have received significant attention and\nfound diverse applications across various domains, but their development\nencounters challenges in real-world scenarios. These challenges arise due to\nthe scarcity of public domain data availability and the need to maintain\nprivacy with respect to private domain data. To address these issues, federated\nlearning (FL) has emerged as a promising technology that enables collaborative\ntraining of shared models while preserving decentralized data. We propose the\nconcept of federated LLM, which comprises three key components, i.e., federated\nLLM pre-training, federated LLM fine-tuning, and federated LLM prompt\nengineering. For each component, we discuss its advantage over traditional LLM\ntraining methods and propose specific engineering strategies for\nimplementation. Furthermore, we explore the novel challenges introduced by the\nintegration of FL and LLM. We analyze existing solutions and identify potential\nobstacles faced by these solutions within the context of federated LLM.\n",
                "链接": "https://arxiv.org/abs/2307.08925"
            },
            {
                "文章ID": "124998",
                "标题": "On the non-slippery slope: Some observations on a recent paper on\n  rolling bodies published in Nature",
                "作者": " Olaf Müller",
                "发布日期": "2023-12-27",
                "摘要": "  An interesting recent paper in Nature explores a new method to construct\nsolid bodies rolling along given curves on an inclined plane, based on the\nGauss Theorem. The present article complements those examinations with rigorous\nexistence theorems and connections to seemingly unrelated questions like\nmaritime rescue operations.\n",
                "链接": "https://arxiv.org/abs/2312.16128"
            },
            {
                "文章ID": "62829",
                "标题": "Human-Centric Multimodal Machine Learning: Recent Advances and Testbed\n  on AI-based Recruitment",
                "作者": " Alejandro Peña,  Ignacio Serna,  Aythami Morales,  Julian Fierrez,  Alfonso Ortega,  Ainhoa Herrarte,  Manuel Alcantara,  Javier Ortega-Garcia",
                "发布日期": "2023-06-13",
                "摘要": "  The presence of decision-making algorithms in society is rapidly increasing\nnowadays, while concerns about their transparency and the possibility of these\nalgorithms becoming new sources of discrimination are arising. There is a\ncertain consensus about the need to develop AI applications with a\nHuman-Centric approach. Human-Centric Machine Learning needs to be developed\nbased on four main requirements: (i) utility and social good; (ii) privacy and\ndata ownership; (iii) transparency and accountability; and (iv) fairness in\nAI-driven decision-making processes. All these four Human-Centric requirements\nare closely related to each other. With the aim of studying how current\nmultimodal algorithms based on heterogeneous sources of information are\naffected by sensitive elements and inner biases in the data, we propose a\nfictitious case study focused on automated recruitment: FairCVtest. We train\nautomatic recruitment algorithms using a set of multimodal synthetic profiles\nincluding image, text, and structured data, which are consciously scored with\ngender and racial biases. FairCVtest shows the capacity of the Artificial\nIntelligence (AI) behind automatic recruitment tools built this way (a common\npractice in many other application scenarios beyond recruitment) to extract\nsensitive information from unstructured data and exploit it in combination to\ndata biases in undesirable (unfair) ways. We present an overview of recent\nworks developing techniques capable of removing sensitive information and\nbiases from the decision-making process of deep learning architectures, as well\nas commonly used databases for fairness research in AI. We demonstrate how\nlearning approaches developed to guarantee privacy in latent spaces can lead to\nunbiased and fair automatic decision-making process.\n",
                "链接": "https://arxiv.org/abs/2302.10908"
            },
            {
                "文章ID": "10750",
                "标题": "Model-based Multi-agent Reinforcement Learning: Recent Progress and\n  Prospects",
                "作者": " Xihuai Wang,  Zhicheng Zhang,  Weinan Zhang",
                "发布日期": "2022-03-22",
                "摘要": "  Significant advances have recently been achieved in Multi-Agent Reinforcement\nLearning (MARL) which tackles sequential decision-making problems involving\nmultiple participants. However, MARL requires a tremendous number of samples\nfor effective training. On the other hand, model-based methods have been shown\nto achieve provable advantages of sample efficiency. However, the attempts of\nmodel-based methods to MARL have just started very recently. This paper\npresents a review of the existing research on model-based MARL, including\ntheoretical analyses, algorithms, and applications, and analyzes the advantages\nand potential of model-based MARL. Specifically, we provide a detailed taxonomy\nof the algorithms and point out the pros and cons for each algorithm according\nto the challenges inherent to multi-agent scenarios. We also outline promising\ndirections for future development of this field.\n",
                "链接": "https://arxiv.org/abs/2203.10603"
            },
            {
                "文章ID": "96003",
                "标题": "Foundation Model is Efficient Multimodal Multitask Model Selector",
                "作者": " Fanqing Meng,  Wenqi Shao,  Zhanglin Peng,  Chonghe Jiang,  Kaipeng Zhang,  Yu Qiao,  Ping Luo",
                "发布日期": "2023-08-14",
                "摘要": "  This paper investigates an under-explored but important problem: given a\ncollection of pre-trained neural networks, predicting their performance on each\nmulti-modal task without fine-tuning them, such as image recognition,\nreferring, captioning, visual question answering, and text question answering.\nA brute-force approach is to finetune all models on all target datasets,\nbringing high computational costs. Although recent-advanced approaches employed\nlightweight metrics to measure models' transferability,they often depend\nheavily on the prior knowledge of a single task, making them inapplicable in a\nmulti-modal multi-task scenario. To tackle this issue, we propose an efficient\nmulti-task model selector (EMMS), which employs large-scale foundation models\nto transform diverse label formats such as categories, texts, and bounding\nboxes of different downstream tasks into a unified noisy label embedding. EMMS\ncan estimate a model's transferability through a simple weighted linear\nregression, which can be efficiently solved by an alternating minimization\nalgorithm with a convergence guarantee. Extensive experiments on 5 downstream\ntasks with 24 datasets show that EMMS is fast, effective, and generic enough to\nassess the transferability of pre-trained models, making it the first model\nselection method in the multi-task scenario. For instance, compared with the\nstate-of-the-art method LogME enhanced by our label embeddings, EMMS achieves\n9.0\\%, 26.3\\%, 20.1\\%, 54.8\\%, 12.2\\% performance gain on image recognition,\nreferring, captioning, visual question answering, and text question answering,\nwhile bringing 5.13x, 6.29x, 3.59x, 6.19x, and 5.66x speedup in wall-clock\ntime, respectively. The code is available at\nhttps://github.com/OpenGVLab/Multitask-Model-Selector.\n",
                "链接": "https://arxiv.org/abs/2308.06262"
            },
            {
                "文章ID": "46032",
                "标题": "Latent Multimodal Functional Graphical Model Estimation",
                "作者": " Katherine Tsai,  Boxin Zhao,  Sanmi Koyejo,  Mladen Kolar",
                "发布日期": "2023-10-03",
                "摘要": "  Joint multimodal functional data acquisition, where functional data from\nmultiple modes are measured simultaneously from the same subject, has emerged\nas an exciting modern approach enabled by recent engineering breakthroughs in\nthe neurological and biological sciences. One prominent motivation to acquire\nsuch data is to enable new discoveries of the underlying connectivity by\ncombining multimodal signals. Despite the scientific interest, there remains a\ngap in principled statistical methods for estimating the graph underlying\nmultimodal functional data. To this end, we propose a new integrative framework\nthat models the data generation process and identifies operators mapping from\nthe observation space to the latent space. We then develop an estimator that\nsimultaneously estimates the transformation operators and the latent graph.\nThis estimator is based on the partial correlation operator, which we\nrigorously extend from the multivariate to the functional setting. Our\nprocedure is provably efficient, with the estimator converging to a stationary\npoint with quantifiable statistical error. Furthermore, we show recovery of the\nlatent graph under mild conditions. Our work is applied to analyze\nsimultaneously acquired multimodal brain imaging data where the graph indicates\nfunctional connectivity of the brain. We present simulation and empirical\nresults that support the benefits of joint estimation.\n",
                "链接": "https://arxiv.org/abs/2210.17237"
            },
            {
                "文章ID": "91726",
                "标题": "MolFM: A Multimodal Molecular Foundation Model",
                "作者": " Yizhen Luo,  Kai Yang,  Massimo Hong,  Xing Yi Liu,  Zaiqing Nie",
                "发布日期": "2023-07-24",
                "摘要": "  Molecular knowledge resides within three different modalities of information\nsources: molecular structures, biomedical documents, and knowledge bases.\nEffective incorporation of molecular knowledge from these modalities holds\nparamount significance in facilitating biomedical research. However, existing\nmultimodal molecular foundation models exhibit limitations in capturing\nintricate connections between molecular structures and texts, and more\nimportantly, none of them attempt to leverage a wealth of molecular expertise\nderived from knowledge graphs. In this study, we introduce MolFM, a multimodal\nmolecular foundation model designed to facilitate joint representation learning\nfrom molecular structures, biomedical texts, and knowledge graphs. We propose\ncross-modal attention between atoms of molecular structures, neighbors of\nmolecule entities and semantically related texts to facilitate cross-modal\ncomprehension. We provide theoretical analysis that our cross-modal\npre-training captures local and global molecular knowledge by minimizing the\ndistance in the feature space between different modalities of the same\nmolecule, as well as molecules sharing similar structures or functions. MolFM\nachieves state-of-the-art performance on various downstream tasks. On\ncross-modal retrieval, MolFM outperforms existing models with 12.13% and 5.04%\nabsolute gains under the zero-shot and fine-tuning settings, respectively.\nFurthermore, qualitative analysis showcases MolFM's implicit ability to provide\ngrounding from molecular substructures and knowledge graphs. Code and models\nare available on https://github.com/BioFM/OpenBioMed.\n",
                "链接": "https://arxiv.org/abs/2307.09484"
            },
            {
                "文章ID": "103254",
                "标题": "Kosmos-2.5: A Multimodal Literate Model",
                "作者": " Tengchao Lv,  Yupan Huang,  Jingye Chen,  Lei Cui,  Shuming Ma,  Yaoyao Chang,  Shaohan Huang,  Wenhui Wang,  Li Dong,  Weiyao Luo,  Shaoxiang Wu,  Guoxin Wang,  Cha Zhang,  Furu Wei",
                "发布日期": "2023-09-21",
                "摘要": "  We present Kosmos-2.5, a multimodal literate model for machine reading of\ntext-intensive images. Pre-trained on large-scale text-intensive images,\nKosmos-2.5 excels in two distinct yet cooperative transcription tasks: (1)\ngenerating spatially-aware text blocks, where each block of text is assigned\nits spatial coordinates within the image, and (2) producing structured text\noutput that captures styles and structures into the markdown format. This\nunified multimodal literate capability is achieved through a shared Transformer\narchitecture, task-specific prompts, and flexible text representations. We\nevaluate Kosmos-2.5 on end-to-end document-level text recognition and\nimage-to-markdown text generation. Furthermore, the model can be readily\nadapted for any text-intensive image understanding task with different prompts\nthrough supervised fine-tuning, making it a general-purpose tool for real-world\napplications involving text-rich images. This work also paves the way for the\nfuture scaling of multimodal large language models.\n",
                "链接": "https://arxiv.org/abs/2309.11419"
            },
            {
                "文章ID": "114068",
                "标题": "GLaMM: Pixel Grounding Large Multimodal Model",
                "作者": " Hanoona Rasheed,  Muhammad Maaz,  Sahal Shaji,  Abdelrahman Shaker,  Salman Khan,  Hisham Cholakkal,  Rao M. Anwer,  Erix Xing,  Ming-Hsuan Yang,  Fahad S. Khan",
                "发布日期": "2023-11-07",
                "摘要": "  Large Multimodal Models (LMMs) extend Large Language Models to the vision\ndomain. Initial efforts towards LMMs used holistic images and text prompts to\ngenerate ungrounded textual responses. Very recently, region-level LMMs have\nbeen used to generate visually grounded responses. However, they are limited to\nonly referring a single object category at a time, require users to specify the\nregions in inputs, or cannot offer dense pixel-wise object grounding. In this\nwork, we present Grounding LMM (GLaMM), the first model that can generate\nnatural language responses seamlessly intertwined with corresponding object\nsegmentation masks. GLaMM not only grounds objects appearing in the\nconversations but is flexible enough to accept both textual and optional visual\nprompts (region of interest) as input. This empowers users to interact with the\nmodel at various levels of granularity, both in textual and visual domains. Due\nto the lack of standard benchmarks for the novel setting of generating visually\ngrounded detailed conversations, we introduce a comprehensive evaluation\nprotocol with our curated grounded conversations. Our proposed Grounded\nConversation Generation (GCG) task requires densely grounded concepts in\nnatural scenes at a large-scale. To this end, we propose a densely annotated\nGrounding-anything Dataset (GranD) using our proposed automated annotation\npipeline that encompasses 7.5M unique concepts grounded in a total of 810M\nregions available with segmentation masks. Besides GCG, GLaMM also performs\neffectively on several downstream tasks e.g., referring expression\nsegmentation, image and region-level captioning and vision-language\nconversations. Project Page: https://mbzuai-oryx.github.io/groundingLMM.\n",
                "链接": "https://arxiv.org/abs/2311.03356"
            },
            {
                "文章ID": "119541",
                "标题": "Dolphins: Multimodal Language Model for Driving",
                "作者": " Yingzi Ma,  Yulong Cao,  Jiachen Sun,  Marco Pavone,  Chaowei Xiao",
                "发布日期": "2023-12-04",
                "摘要": "  The quest for fully autonomous vehicles (AVs) capable of navigating complex\nreal-world scenarios with human-like understanding and responsiveness. In this\npaper, we introduce Dolphins, a novel vision-language model architected to\nimbibe human-like abilities as a conversational driving assistant. Dolphins is\nadept at processing multimodal inputs comprising video (or image) data, text\ninstructions, and historical control signals to generate informed outputs\ncorresponding to the provided instructions. Building upon the open-sourced\npretrained Vision-Language Model, OpenFlamingo, we first enhance Dolphins's\nreasoning capabilities through an innovative Grounded Chain of Thought (GCoT)\nprocess. Then we tailored Dolphins to the driving domain by constructing\ndriving-specific instruction data and conducting instruction tuning. Through\nthe utilization of the BDD-X dataset, we designed and consolidated four\ndistinct AV tasks into Dolphins to foster a holistic understanding of intricate\ndriving scenarios. As a result, the distinctive features of Dolphins are\ncharacterized into two dimensions: (1) the ability to provide a comprehensive\nunderstanding of complex and long-tailed open-world driving scenarios and solve\na spectrum of AV tasks, and (2) the emergence of human-like capabilities\nincluding gradient-free instant adaptation via in-context learning and error\nrecovery via reflection.\n",
                "链接": "https://arxiv.org/abs/2312.00438"
            }
        ]
    },
    {
        "question": {
            "question": "查找近六个月工具学习评测数据集的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "63180",
                "标题": "Natural Language Processing in the Legal Domain",
                "作者": " Daniel Martin Katz,  Dirk Hartung,  Lauritz Gerlach,  Abhik Jana, II Michael J. Bommarito",
                "发布日期": "2023-02-24",
                "摘要": "  In this paper, we summarize the current state of the field of NLP & Law with\na specific focus on recent technical and substantive developments. To support\nour analysis, we construct and analyze a nearly complete corpus of more than\nsix hundred NLP & Law related papers published over the past decade. Our\nanalysis highlights several major trends. Namely, we document an increasing\nnumber of papers written, tasks undertaken, and languages covered over the\ncourse of the past decade. We observe an increase in the sophistication of the\nmethods which researchers deployed in this applied context. Slowly but surely,\nLegal NLP is beginning to match not only the methodological sophistication of\ngeneral NLP but also the professional standards of data availability and code\nreproducibility observed within the broader scientific community. We believe\nall of these trends bode well for the future of the field, but many questions\nin both the academic and commercial sphere still remain open.\n",
                "链接": "https://arxiv.org/abs/2302.12039"
            },
            {
                "文章ID": "55193",
                "标题": "A Survey of Face Recognition",
                "作者": " Xinyi Wang,  Jianteng Peng,  Sufang Zhang,  Bihui Chen,  Yi Wang,  Yandong Guo",
                "发布日期": "2022-12-27",
                "摘要": "  Recent years witnessed the breakthrough of face recognition with deep\nconvolutional neural networks. Dozens of papers in the field of FR are\npublished every year. Some of them were applied in the industrial community and\nplayed an important role in human life such as device unlock, mobile payment,\nand so on. This paper provides an introduction to face recognition, including\nits history, pipeline, algorithms based on conventional manually designed\nfeatures or deep learning, mainstream training, evaluation datasets, and\nrelated applications. We have analyzed and compared state-of-the-art works as\nmany as possible, and also carefully designed a set of experiments to find the\neffect of backbone size and data distribution. This survey is a material of the\ntutorial named The Practical Face Recognition Technology in the Industrial\nWorld in the FG2023.\n",
                "链接": "https://arxiv.org/abs/2212.13038"
            },
            {
                "文章ID": "25848",
                "标题": "DaisyRec 2.0: Benchmarking Recommendation for Rigorous Evaluation",
                "作者": " Zhu Sun,  Hui Fang,  Jie Yang,  Xinghua Qu,  Hongyang Liu,  Di Yu,  Yew-Soon Ong,  Jie Zhang",
                "发布日期": "2022-06-23",
                "摘要": "  Recently, one critical issue looms large in the field of recommender systems\n-- there are no effective benchmarks for rigorous evaluation -- which\nconsequently leads to unreproducible evaluation and unfair comparison. We,\ntherefore, conduct studies from the perspectives of practical theory and\nexperiments, aiming at benchmarking recommendation for rigorous evaluation.\nRegarding the theoretical study, a series of hyper-factors affecting\nrecommendation performance throughout the whole evaluation chain are\nsystematically summarized and analyzed via an exhaustive review on 141 papers\npublished at eight top-tier conferences within 2017-2020. We then classify them\ninto model-independent and model-dependent hyper-factors, and different modes\nof rigorous evaluation are defined and discussed in-depth accordingly. For the\nexperimental study, we release DaisyRec 2.0 library by integrating these\nhyper-factors to perform rigorous evaluation, whereby a holistic empirical\nstudy is conducted to unveil the impacts of different hyper-factors on\nrecommendation performance. Supported by the theoretical and experimental\nstudies, we finally create benchmarks for rigorous evaluation by proposing\nstandardized procedures and providing performance of ten state-of-the-arts\nacross six evaluation metrics on six datasets as a reference for later study.\nOverall, our work sheds light on the issues in recommendation evaluation,\nprovides potential solutions for rigorous evaluation, and lays foundation for\nfurther investigation.\n",
                "链接": "https://arxiv.org/abs/2206.10848"
            },
            {
                "文章ID": "100544",
                "标题": "Large Language Models for Automated Open-domain Scientific Hypotheses\n  Discovery",
                "作者": " Zonglin Yang,  Xinya Du,  Junxian Li,  Jie Zheng,  Soujanya Poria,  Erik Cambria",
                "发布日期": "2023-09-07",
                "摘要": "  Hypothetical induction is recognized as the main reasoning type when\nscientists make observations about the world and try to propose hypotheses to\nexplain those observations. Past research on hypothetical induction has a\nlimited setting that (1) the observation annotations of the dataset are not raw\nweb corpus but are manually selected sentences (resulting in a close-domain\nsetting); and (2) the ground truth hypotheses annotations are mostly\ncommonsense knowledge, making the task less challenging. In this work, we\npropose the first NLP dataset for social science academic hypotheses discovery,\nconsisting of 50 recent papers published in top social science journals. Raw\nweb corpora that are necessary for developing hypotheses in the published\npapers are also collected in the dataset, with the final goal of creating a\nsystem that automatically generates valid, novel, and helpful (to human\nresearchers) hypotheses, given only a pile of raw web corpora. The new dataset\ncan tackle the previous problems because it requires to (1) use raw web corpora\nas observations; and (2) propose hypotheses even new to humanity. A\nmulti-module framework is developed for the task, as well as three different\nfeedback mechanisms that empirically show performance gain over the base\nframework. Finally, our framework exhibits high performance in terms of both\nGPT-4 based evaluation and social science expert evaluation.\n",
                "链接": "https://arxiv.org/abs/2309.02726"
            },
            {
                "文章ID": "78885",
                "标题": "Deep Learning Approaches to Lexical Simplification: A Survey",
                "作者": " Kai North,  Tharindu Ranasinghe,  Matthew Shardlow,  Marcos Zampieri",
                "发布日期": "2023-05-23",
                "摘要": "  Lexical Simplification (LS) is the task of replacing complex for simpler\nwords in a sentence whilst preserving the sentence's original meaning. LS is\nthe lexical component of Text Simplification (TS) with the aim of making texts\nmore accessible to various target populations. A past survey (Paetzold and\nSpecia, 2017) has provided a detailed overview of LS. Since this survey,\nhowever, the AI/NLP community has been taken by storm by recent advances in\ndeep learning, particularly with the introduction of large language models\n(LLM) and prompt learning. The high performance of these models sparked renewed\ninterest in LS. To reflect these recent advances, we present a comprehensive\nsurvey of papers published between 2017 and 2023 on LS and its sub-tasks with a\nspecial focus on deep learning. We also present benchmark datasets for the\nfuture development of LS systems.\n",
                "链接": "https://arxiv.org/abs/2305.12000"
            },
            {
                "文章ID": "32433",
                "标题": "Threddy: An Interactive System for Personalized Thread-based Exploration\n  and Organization of Scientific Literature",
                "作者": " Hyeonsu B. Kang,  Joseph Chee Chang,  Yongsung Kim,  Aniket Kittur",
                "发布日期": "2022-08-17",
                "摘要": "  Reviewing the literature to understand relevant threads of past work is a\ncritical part of research and vehicle for learning. However, as the scientific\nliterature grows the challenges for users to find and make sense of the many\ndifferent threads of research grow as well. Previous work has helped scholars\nto find and group papers with citation information or textual similarity using\nstandalone tools or overview visualizations. Instead, in this work we explore a\ntool integrated into users' reading process that helps them with leveraging\nauthors' existing summarization of threads, typically in introduction or\nrelated work sections, in order to situate their own work's contributions. To\nexplore this we developed a prototype that supports efficient extraction and\norganization of threads along with supporting evidence as scientists read\nresearch articles. The system then recommends further relevant articles based\non user-created threads. We evaluate the system in a lab study and find that it\nhelps scientists to follow and curate research threads without breaking out of\ntheir flow of reading, collect relevant papers and clips, and discover\ninteresting new articles to further grow threads.\n",
                "链接": "https://arxiv.org/abs/2208.03455"
            },
            {
                "文章ID": "63198",
                "标题": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution\n  Perspective",
                "作者": " Jindong Wang,  Xixu Hu,  Wenxin Hou,  Hao Chen,  Runkai Zheng,  Yidong Wang,  Linyi Yang,  Haojun Huang,  Wei Ye,  Xiubo Geng,  Binxin Jiao,  Yue Zhang,  Xing Xie",
                "发布日期": "2023-08-30",
                "摘要": "  ChatGPT is a recent chatbot service released by OpenAI and is receiving\nincreasing attention over the past few months. While evaluations of various\naspects of ChatGPT have been done, its robustness, i.e., the performance to\nunexpected inputs, is still unclear to the public. Robustness is of particular\nconcern in responsible AI, especially for safety-critical applications. In this\npaper, we conduct a thorough evaluation of the robustness of ChatGPT from the\nadversarial and out-of-distribution (OOD) perspective. To do so, we employ the\nAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart\nreview and DDXPlus medical diagnosis datasets for OOD evaluation. We select\nseveral popular foundation models as baselines. Results show that ChatGPT shows\nconsistent advantages on most adversarial and OOD classification and\ntranslation tasks. However, the absolute performance is far from perfection,\nwhich suggests that adversarial and OOD robustness remains a significant threat\nto foundation models. Moreover, ChatGPT shows astounding performance in\nunderstanding dialogue-related texts and we find that it tends to provide\ninformal suggestions for medical tasks instead of definitive answers. Finally,\nwe present in-depth discussions of possible research directions.\n",
                "链接": "https://arxiv.org/abs/2302.12095"
            },
            {
                "文章ID": "51780",
                "标题": "Navigating causal deep learning",
                "作者": " Jeroen Berrevoets,  Krzysztof Kacprzyk,  Zhaozhi Qian,  Mihaela van der Schaar",
                "发布日期": "2022-12-05",
                "摘要": "  Causal deep learning (CDL) is a new and important research area in the larger\nfield of machine learning. With CDL, researchers aim to structure and encode\ncausal knowledge in the extremely flexible representation space of deep\nlearning models. Doing so will lead to more informed, robust, and general\npredictions and inference -- which is important! However, CDL is still in its\ninfancy. For example, it is not clear how we ought to compare different methods\nas they are so different in their output, the way they encode causal knowledge,\nor even how they represent this knowledge. This is a living paper that\ncategorises methods in causal deep learning beyond Pearl's ladder of causation.\nWe refine the rungs in Pearl's ladder, while also adding a separate dimension\nthat categorises the parametric assumptions of both input and representation,\narriving at the map of causal deep learning. Our map covers machine learning\ndisciplines such as supervised learning, reinforcement learning, generative\nmodelling and beyond. Our paradigm is a tool which helps researchers to: find\nbenchmarks, compare methods, and most importantly: identify research gaps. With\nthis work we aim to structure the avalanche of papers being published on causal\ndeep learning. While papers on the topic are being published daily, our map\nremains fixed. We open-source our map for others to use as they see fit:\nperhaps to offer guidance in a related works section, or to better highlight\nthe contribution of their paper.\n",
                "链接": "https://arxiv.org/abs/2212.00911"
            },
            {
                "文章ID": "2000",
                "标题": "From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic\n  Review on Evaluating Explainable AI",
                "作者": " Meike Nauta,  Jan Trienes,  Shreyasi Pathak,  Elisa Nguyen,  Michelle Peters,  Yasmin Schmitt,  Jörg Schlötterer,  Maurice van Keulen,  Christin Seifert",
                "发布日期": "2023-02-27",
                "摘要": "  The rising popularity of explainable artificial intelligence (XAI) to\nunderstand high-performing black boxes raised the question of how to evaluate\nexplanations of machine learning (ML) models. While interpretability and\nexplainability are often presented as a subjectively validated binary property,\nwe consider it a multi-faceted concept. We identify 12 conceptual properties,\nsuch as Compactness and Correctness, that should be evaluated for\ncomprehensively assessing the quality of an explanation. Our so-called Co-12\nproperties serve as categorization scheme for systematically reviewing the\nevaluation practices of more than 300 papers published in the last 7 years at\nmajor AI and ML conferences that introduce an XAI method. We find that 1 in 3\npapers evaluate exclusively with anecdotal evidence, and 1 in 5 papers evaluate\nwith users. This survey also contributes to the call for objective,\nquantifiable evaluation methods by presenting an extensive overview of\nquantitative XAI evaluation methods. Our systematic collection of evaluation\nmethods provides researchers and practitioners with concrete tools to\nthoroughly validate, benchmark and compare new and existing XAI methods. The\nCo-12 categorization scheme and our identified evaluation methods open up\nopportunities to include quantitative metrics as optimization criteria during\nmodel training in order to optimize for accuracy and interpretability\nsimultaneously.\n",
                "链接": "https://arxiv.org/abs/2201.08164"
            },
            {
                "文章ID": "124998",
                "标题": "On the non-slippery slope: Some observations on a recent paper on\n  rolling bodies published in Nature",
                "作者": " Olaf Müller",
                "发布日期": "2023-12-27",
                "摘要": "  An interesting recent paper in Nature explores a new method to construct\nsolid bodies rolling along given curves on an inclined plane, based on the\nGauss Theorem. The present article complements those examinations with rigorous\nexistence theorems and connections to seemingly unrelated questions like\nmaritime rescue operations.\n",
                "链接": "https://arxiv.org/abs/2312.16128"
            },
            {
                "文章ID": "85316",
                "标题": "On building machine learning pipelines for Android malware detection: a\n  procedural survey of practices, challenges and opportunities",
                "作者": " Masoud Mehrabi Koushki,  Ibrahim AbuAlhaol,  Anandharaju Durai Raju,  Yang Zhou,  Ronnie Salvador Giagone,  Huang Shengqiang",
                "发布日期": "2023-06-13",
                "摘要": "  As the smartphone market leader, Android has been a prominent target for\nmalware attacks. The number of malicious applications (apps) identified for it\nhas increased continually over the past decade, creating an immense challenge\nfor all parties involved. For market holders and researchers, in particular,\nthe large number of samples has made manual malware detection unfeasible,\nleading to an influx of research that investigate Machine Learning (ML)\napproaches to automate this process. However, while some of the proposed\napproaches achieve high performance, rapidly evolving Android malware has made\nthem unable to maintain their accuracy over time. This has created a need in\nthe community to conduct further research, and build more flexible ML\npipelines. Doing so, however, is currently hindered by a lack of systematic\noverview of the existing literature, to learn from and improve upon the\nexisting solutions. Existing survey papers often focus only on parts of the ML\nprocess (e.g., data collection or model deployment), while omitting other\nimportant stages, such as model evaluation and explanation. In this paper, we\naddress this problem with a review of 42 highly-cited papers, spanning a decade\nof research (from 2011 to 2021). We introduce a novel procedural taxonomy of\nthe published literature, covering how they have used ML algorithms, what\nfeatures they have engineered, which dimensionality reduction techniques they\nhave employed, what datasets they have employed for training, and what their\nevaluation and explanation strategies are. Drawing from this taxonomy, we also\nidentify gaps in knowledge and provide ideas for improvement and future work.\n",
                "链接": "https://arxiv.org/abs/2306.07118"
            },
            {
                "文章ID": "97907",
                "标题": "BELB: a Biomedical Entity Linking Benchmark",
                "作者": " Samuele Garda,  Leon Weber-Genzel,  Robert Martin,  Ulf Leser",
                "发布日期": "2023-08-23",
                "摘要": "  Biomedical entity linking (BEL) is the task of grounding entity mentions to a\nknowledge base. It plays a vital role in information extraction pipelines for\nthe life sciences literature. We review recent work in the field and find that,\nas the task is absent from existing benchmarks for biomedical text mining,\ndifferent studies adopt different experimental setups making comparisons based\non published numbers problematic. Furthermore, neural systems are tested\nprimarily on instances linked to the broad coverage knowledge base UMLS,\nleaving their performance to more specialized ones, e.g. genes or variants,\nunderstudied. We therefore developed BELB, a Biomedical Entity Linking\nBenchmark, providing access in a unified format to 11 corpora linked to 7\nknowledge bases and spanning six entity types: gene, disease, chemical,\nspecies, cell line and variant. BELB greatly reduces preprocessing overhead in\ntesting BEL systems on multiple corpora offering a standardized testbed for\nreproducible experiments. Using BELB we perform an extensive evaluation of six\nrule-based entity-specific systems and three recent neural approaches\nleveraging pre-trained language models. Our results reveal a mixed picture\nshowing that neural approaches fail to perform consistently across entity\ntypes, highlighting the need of further studies towards entity-agnostic models.\n",
                "链接": "https://arxiv.org/abs/2308.11537"
            },
            {
                "文章ID": "111553",
                "标题": "An Integrative Survey on Mental Health Conversational Agents to Bridge\n  Computer Science and Medical Perspectives",
                "作者": " Young Min Cho,  Sunny Rai,  Lyle Ungar,  João Sedoc,  Sharath Chandra Guntuku",
                "发布日期": "2023-12-01",
                "摘要": "  Mental health conversational agents (a.k.a. chatbots) are widely studied for\ntheir potential to offer accessible support to those experiencing mental health\nchallenges. Previous surveys on the topic primarily consider papers published\nin either computer science or medicine, leading to a divide in understanding\nand hindering the sharing of beneficial knowledge between both domains. To\nbridge this gap, we conduct a comprehensive literature review using the PRISMA\nframework, reviewing 534 papers published in both computer science and\nmedicine. Our systematic review reveals 136 key papers on building mental\nhealth-related conversational agents with diverse characteristics of modeling\nand experimental design techniques. We find that computer science papers focus\non LLM techniques and evaluating response quality using automated metrics with\nlittle attention to the application while medical papers use rule-based\nconversational agents and outcome metrics to measure the health outcomes of\nparticipants. Based on our findings on transparency, ethics, and cultural\nheterogeneity in this review, we provide a few recommendations to help bridge\nthe disciplinary divide and enable the cross-disciplinary development of mental\nhealth conversational agents.\n",
                "链接": "https://arxiv.org/abs/2310.17017"
            },
            {
                "文章ID": "55062",
                "标题": "Image Classification with Small Datasets: Overview and Benchmark",
                "作者": " L. Brigato,  B. Barz,  L. Iocchi,  J. Denzler",
                "发布日期": "2022-12-26",
                "摘要": "  Image classification with small datasets has been an active research area in\nthe recent past. However, as research in this scope is still in its infancy,\ntwo key ingredients are missing for ensuring reliable and truthful progress: a\nsystematic and extensive overview of the state of the art, and a common\nbenchmark to allow for objective comparisons between published methods. This\narticle addresses both issues. First, we systematically organize and connect\npast studies to consolidate a community that is currently fragmented and\nscattered. Second, we propose a common benchmark that allows for an objective\ncomparison of approaches. It consists of five datasets spanning various domains\n(e.g., natural images, medical imagery, satellite data) and data types (RGB,\ngrayscale, multispectral). We use this benchmark to re-evaluate the standard\ncross-entropy baseline and ten existing methods published between 2017 and 2021\nat renowned venues. Surprisingly, we find that thorough hyper-parameter tuning\non held-out validation data results in a highly competitive baseline and\nhighlights a stunted growth of performance over the years. Indeed, only a\nsingle specialized method dating back to 2019 clearly wins our benchmark and\noutperforms the baseline classifier.\n",
                "链接": "https://arxiv.org/abs/2212.12478"
            },
            {
                "文章ID": "46571",
                "标题": "Phoneme Segmentation Using Self-Supervised Speech Models",
                "作者": " Luke Strgar,  David Harwath",
                "发布日期": "2022-11-04",
                "摘要": "  We apply transfer learning to the task of phoneme segmentation and\ndemonstrate the utility of representations learned in self-supervised\npre-training for the task. Our model extends transformer-style encoders with\nstrategically placed convolutions that manipulate features learned in\npre-training. Using the TIMIT and Buckeye corpora we train and test the model\nin the supervised and unsupervised settings. The latter case is accomplished by\nfurnishing a noisy label-set with the predictions of a separate model, it\nhaving been trained in an unsupervised fashion. Results indicate our model\neclipses previous state-of-the-art performance in both settings and on both\ndatasets. Finally, following observations during published code review and\nattempts to reproduce past segmentation results, we find a need to disambiguate\nthe definition and implementation of widely-used evaluation metrics. We resolve\nthis ambiguity by delineating two distinct evaluation schemes and describing\ntheir nuances.\n",
                "链接": "https://arxiv.org/abs/2211.01461"
            },
            {
                "文章ID": "102125",
                "标题": "Traveling Waves Encode the Recent Past and Enhance Sequence Learning",
                "作者": " T. Anderson Keller,  Lyle Muller,  Terrence Sejnowski,  Max Welling",
                "发布日期": "2023-09-18",
                "摘要": "  Traveling waves of neural activity have been observed throughout the brain at\na diversity of regions and scales; however, their precise computational role is\nstill debated. One physically grounded hypothesis suggests that the cortical\nsheet may act like a wave-field capable of storing a short-term memory of\nsequential stimuli through induced waves traveling across the cortical surface.\nTo date, however, the computational implications of this idea have remained\nhypothetical due to the lack of a simple recurrent neural network architecture\ncapable of exhibiting such waves. In this work, we introduce a model to fill\nthis gap, which we denote the Wave-RNN (wRNN), and demonstrate how both\nconnectivity constraints and initialization play a crucial role in the\nemergence of wave-like dynamics. We then empirically show how such an\narchitecture indeed efficiently encodes the recent past through a suite of\nsynthetic memory tasks where wRNNs learn faster and perform significantly\nbetter than wave-free counterparts. Finally, we explore the implications of\nthis memory storage system on more complex sequence modeling tasks such as\nsequential image classification and find that wave-based models not only again\noutperform comparable wave-free RNNs while using significantly fewer\nparameters, but additionally perform comparably to more complex gated\narchitectures such as LSTMs and GRUs. We conclude with a discussion of the\nimplications of these results for both neuroscience and machine learning.\n",
                "链接": "https://arxiv.org/abs/2309.08045"
            },
            {
                "文章ID": "2252",
                "标题": "What and How Are We Reporting in HRI? A Review and Recommendations for\n  Reporting Recruitment, Compensation, and Gender",
                "作者": " Julia R. Cordero,  Thomas R. Groechel,  Maja J. Matarić",
                "发布日期": "2022-03-07",
                "摘要": "  Study reproducibility and generalizability of results to broadly inclusive\npopulations is crucial in any research. Previous meta-analyses in HRI have\nfocused on the consistency of reported information from papers in various\ncategories. However, members of the HRI community have noted that much of the\ninformation needed for reproducible and generalizable studies is not found in\npublished papers. We address this issue by surveying the reported study\nmetadata over the past three years (2019 through 2021) of the main proceedings\nof the International Conference on Human-Robot Interaction (HRI) as well as\nalt.HRI. Based on the analysis results, we propose a set of recommendations for\nthe HRI community that follow the longer-standing reporting guidelines from\nhuman-computer interaction (HCI), psychology, and other fields most related to\nHRI. Finally, we examine three key areas for user study reproducibility:\nrecruitment details, participant compensation, and participant gender. We find\na lack of reporting within each of these study metadata categories: of the 236\nstudies, 139 studies failed to report recruitment method, 118 studies failed to\nreport compensation, and 62 studies failed to report gender data. This analysis\ntherefore provides guidance about specific types of needed reporting\nimprovements for HRI.\n",
                "链接": "https://arxiv.org/abs/2201.09114"
            },
            {
                "文章ID": "43550",
                "标题": "Detecting and analyzing missing citations to published scientific\n  entities",
                "作者": " Jialiang Lin,  Yao Yu,  Jiaxin Song,  Xiaodong Shi",
                "发布日期": "2022-10-20",
                "摘要": "  Proper citation is of great importance in academic writing for it enables\nknowledge accumulation and maintains academic integrity. However, citing\nproperly is not an easy task. For published scientific entities, the\never-growing academic publications and over-familiarity of terms easily lead to\nmissing citations. To deal with this situation, we design a special method\nCitation Recommendation for Published Scientific Entity (CRPSE) based on the\ncooccurrences between published scientific entities and in-text citations in\nthe same sentences from previous researchers. Experimental outcomes show the\neffectiveness of our method in recommending the source papers for published\nscientific entities. We further conduct a statistical analysis on missing\ncitations among papers published in prestigious computer science conferences in\n2020. In the 12,278 papers collected, 475 published scientific entities of\ncomputer science and mathematics are found to have missing citations. Many\nentities mentioned without citations are found to be well-accepted research\nresults. On a median basis, the papers proposing these published scientific\nentities with missing citations were published 8 years ago, which can be\nconsidered the time frame for a published scientific entity to develop into a\nwell-accepted concept. For published scientific entities, we appeal for\naccurate and full citation of their source papers as required by academic\nstandards.\n",
                "链接": "https://arxiv.org/abs/2210.10073"
            },
            {
                "文章ID": "8979",
                "标题": "The Cross-evaluation of Machine Learning-based Network Intrusion\n  Detection Systems",
                "作者": " Giovanni Apruzzese,  Luca Pajola,  Mauro Conti",
                "发布日期": "2022-03-10",
                "摘要": "  Enhancing Network Intrusion Detection Systems (NIDS) with supervised Machine\nLearning (ML) is tough. ML-NIDS must be trained and evaluated, operations\nrequiring data where benign and malicious samples are clearly labelled. Such\nlabels demand costly expert knowledge, resulting in a lack of real deployments,\nas well as on papers always relying on the same outdated data. The situation\nimproved recently, as some efforts disclosed their labelled datasets. However,\nmost past works used such datasets just as a 'yet another' testbed, overlooking\nthe added potential provided by such availability.\n  In contrast, we promote using such existing labelled data to cross-evaluate\nML-NIDS. Such approach received only limited attention and, due to its\ncomplexity, requires a dedicated treatment. We hence propose the first\ncross-evaluation model. Our model highlights the broader range of realistic\nuse-cases that can be assessed via cross-evaluations, allowing the discovery of\nstill unknown qualities of state-of-the-art ML-NIDS. For instance, their\ndetection surface can be extended--at no additional labelling cost. However,\nconducting such cross-evaluations is challenging. Hence, we propose the first\nframework, XeNIDS, for reliable cross-evaluations based on Network Flows. By\nusing XeNIDS on six well-known datasets, we demonstrate the concealed\npotential, but also the risks, of cross-evaluations of ML-NIDS.\n",
                "链接": "https://arxiv.org/abs/2203.04686"
            },
            {
                "文章ID": "28482",
                "标题": "A novel evaluation methodology for supervised Feature Ranking algorithms",
                "作者": " Jeroen G. S. Overschie",
                "发布日期": "2022-07-12",
                "摘要": "  Both in the domains of Feature Selection and Interpretable AI, there exists a\ndesire to `rank' features based on their importance. Such feature importance\nrankings can then be used to either: (1) reduce the dataset size or (2)\ninterpret the Machine Learning model. In the literature, however, such Feature\nRankers are not evaluated in a systematic, consistent way. Many papers have a\ndifferent way of arguing which feature importance ranker works best. This paper\nfills this gap, by proposing a new evaluation methodology. By making use of\nsynthetic datasets, feature importance scores can be known beforehand, allowing\nmore systematic evaluation. To facilitate large-scale experimentation using the\nnew methodology, a benchmarking framework was built in Python, called fseval.\nThe framework allows running experiments in parallel and distributed over\nmachines on HPC systems. By integrating with an online platform called Weights\nand Biases, charts can be interactively explored on a live dashboard. The\nsoftware was released as open-source software, and is published as a package on\nthe PyPi platform. The research concludes by exploring one such large-scale\nexperiment, to find the strengths and weaknesses of the participating\nalgorithms, on many fronts.\n",
                "链接": "https://arxiv.org/abs/2207.04258"
            },
            {
                "文章ID": "123228",
                "标题": "A Survey on Query-based API Recommendation",
                "作者": "Jack  Moshi Wei, Jack  Nima Shiri Harzevili, Jack  Alvine Boaye Belle, Jack  Junjie Wang, Jack  Lin Shi, Jack  Jinqiu Yang, Jack  Song Wang, Jack  Ming Zhen,   Jiang",
                "发布日期": "2023-12-22",
                "摘要": "  Application Programming Interfaces (APIs) are designed to help developers\nbuild software more effectively. Recommending the right APIs for specific tasks\nhas gained increasing attention among researchers and developers in recent\nyears. To comprehensively understand this research domain, we have surveyed to\nanalyze API recommendation studies published in the last 10 years. Our study\nbegins with an overview of the structure of API recommendation tools.\nSubsequently, we systematically analyze prior research and pose four key\nresearch questions. For RQ1, we examine the volume of published papers and the\nvenues in which these papers appear within the API recommendation field. In\nRQ2, we categorize and summarize the prevalent data sources and collection\nmethods employed in API recommendation research. In RQ3, we explore the types\nof data and common data representations utilized by API recommendation\napproaches. We also investigate the typical data extraction procedures and\ncollection approaches employed by the existing approaches. RQ4 delves into the\nmodeling techniques employed by API recommendation approaches, encompassing\nboth statistical and deep learning models. Additionally, we compile an overview\nof the prevalent ranking strategies and evaluation metrics used for assessing\nAPI recommendation tools. Drawing from our survey findings, we identify current\nchallenges in API recommendation research that warrant further exploration,\nalong with potential avenues for future research.\n",
                "链接": "https://arxiv.org/abs/2312.10623"
            },
            {
                "文章ID": "60733",
                "标题": "Continuous Learning for Android Malware Detection",
                "作者": " Yizheng Chen,  Zhoujie Ding,  David Wagner",
                "发布日期": "2023-06-16",
                "摘要": "  Machine learning methods can detect Android malware with very high accuracy.\nHowever, these classifiers have an Achilles heel, concept drift: they rapidly\nbecome out of date and ineffective, due to the evolution of malware apps and\nbenign apps. Our research finds that, after training an Android malware\nclassifier on one year's worth of data, the F1 score quickly dropped from 0.99\nto 0.76 after 6 months of deployment on new test samples.\n  In this paper, we propose new methods to combat the concept drift problem of\nAndroid malware classifiers. Since machine learning technique needs to be\ncontinuously deployed, we use active learning: we select new samples for\nanalysts to label, and then add the labeled samples to the training set to\nretrain the classifier. Our key idea is, similarity-based uncertainty is more\nrobust against concept drift. Therefore, we combine contrastive learning with\nactive learning. We propose a new hierarchical contrastive learning scheme, and\na new sample selection technique to continuously train the Android malware\nclassifier. Our evaluation shows that this leads to significant improvements,\ncompared to previously published methods for active learning. Our approach\nreduces the false negative rate from 14% (for the best baseline) to 9%, while\nalso reducing the false positive rate (from 0.86% to 0.48%). Also, our approach\nmaintains more consistent performance across a seven-year time period than past\nmethods.\n",
                "链接": "https://arxiv.org/abs/2302.04332"
            },
            {
                "文章ID": "106592",
                "标题": "Comparing Time-Series Analysis Approaches Utilized in Research Papers to\n  Forecast COVID-19 Cases in Africa: A Literature Review",
                "作者": " Ali Ebadi,  Ebrahim Sahafizadeh",
                "发布日期": "2023-10-06",
                "摘要": "  This literature review aimed to compare various time-series analysis\napproaches utilized in forecasting COVID-19 cases in Africa. The study involved\na methodical search for English-language research papers published between\nJanuary 2020 and July 2023, focusing specifically on papers that utilized\ntime-series analysis approaches on COVID-19 datasets in Africa. A variety of\ndatabases including PubMed, Google Scholar, Scopus, and Web of Science were\nutilized for this process. The research papers underwent an evaluation process\nto extract relevant information regarding the implementation and performance of\nthe time-series analysis models. The study highlighted the different\nmethodologies employed, evaluating their effectiveness and limitations in\nforecasting the spread of the virus. The result of this review could contribute\ndeeper insights into the field, and future research should consider these\ninsights to improve time series analysis models and explore the integration of\ndifferent approaches for enhanced public health decision-making.\n",
                "链接": "https://arxiv.org/abs/2310.03606"
            },
            {
                "文章ID": "19105",
                "标题": "A Comprehensive Survey of Few-shot Learning: Evolution, Applications,\n  Challenges, and Opportunities",
                "作者": " Yisheng Song,  Ting Wang,  Subrota K Mondal,  Jyoti Prakash Sahoo",
                "发布日期": "2022-05-25",
                "摘要": "  Few-shot learning (FSL) has emerged as an effective learning method and shows\ngreat potential. Despite the recent creative works in tackling FSL tasks,\nlearning valid information rapidly from just a few or even zero samples still\nremains a serious challenge. In this context, we extensively investigated 200+\nlatest papers on FSL published in the past three years, aiming to present a\ntimely and comprehensive overview of the most recent advances in FSL along with\nimpartial comparisons of the strengths and weaknesses of the existing works.\nFor the sake of avoiding conceptual confusion, we first elaborate and compare a\nset of similar concepts including few-shot learning, transfer learning, and\nmeta-learning. Furthermore, we propose a novel taxonomy to classify the\nexisting work according to the level of abstraction of knowledge in accordance\nwith the challenges of FSL. To enrich this survey, in each subsection we\nprovide in-depth analysis and insightful discussion about recent advances on\nthese topics. Moreover, taking computer vision as an example, we highlight the\nimportant application of FSL, covering various research hotspots. Finally, we\nconclude the survey with unique insights into the technology evolution trends\ntogether with potential future research opportunities in the hope of providing\nguidance to follow-up research.\n",
                "链接": "https://arxiv.org/abs/2205.06743"
            },
            {
                "文章ID": "62764",
                "标题": "Real-World Deployment and Evaluation of Kwame for Science, An AI\n  Teaching Assistant for Science Education in West Africa",
                "作者": " George Boateng,  Samuel John,  Samuel Boateng,  Philemon Badu,  Patrick Agyeman-Budu,  Victor Kumbol",
                "发布日期": "2023-02-22",
                "摘要": "  Africa has a high student-to-teacher ratio which limits students' access to\nteachers for learning support such as educational question answering. In this\nwork, we extended Kwame, our previous AI teaching assistant for coding\neducation, adapted it for science education, and deployed it as a web app.\nKwame for Science provides passages from well-curated knowledge sources and\nrelated past national exam questions as answers to questions from students\nbased on the Integrated Science subject of the West African Senior Secondary\nCertificate Examination (WASSCE). Furthermore, students can view past national\nexam questions along with their answers and filter by year, question type\n(objectives, theory, and practicals), and topics that were automatically\ncategorized by a topic detection model which we developed (91% unweighted\naverage recall). We deployed Kwame for Science in the real world over 8 months\nand had 750 users across 32 countries (15 in Africa) and 1.5K questions asked.\nOur evaluation showed an 87.2% top 3 accuracy (n=109 questions) implying that\nKwame for Science has a high chance of giving at least one useful answer among\nthe 3 displayed. We categorized the reasons the model incorrectly answered\nquestions to provide insights for future improvements. We also share challenges\nand lessons with the development, deployment, and human-computer interaction\ncomponent of such a tool to enable other researchers to deploy similar tools.\nWith a first-of-its-kind tool within the African context, Kwame for Science has\nthe potential to enable the delivery of scalable, cost-effective, and quality\nremote education to millions of people across Africa.\n",
                "链接": "https://arxiv.org/abs/2302.10786"
            },
            {
                "文章ID": "5483",
                "标题": "Repairing the Cracked Foundation: A Survey of Obstacles in Evaluation\n  Practices for Generated Text",
                "作者": " Sebastian Gehrmann,  Elizabeth Clark,  Thibault Sellam",
                "发布日期": "2022-02-15",
                "摘要": "  Evaluation practices in natural language generation (NLG) have many known\nflaws, but improved evaluation approaches are rarely widely adopted. This issue\nhas become more urgent, since neural NLG models have improved to the point\nwhere they can often no longer be distinguished based on the surface-level\nfeatures that older metrics rely on. This paper surveys the issues with human\nand automatic model evaluations and with commonly used datasets in NLG that\nhave been pointed out over the past 20 years. We summarize, categorize, and\ndiscuss how researchers have been addressing these issues and what their\nfindings mean for the current state of model evaluations. Building on those\ninsights, we lay out a long-term vision for NLG evaluation and propose concrete\nsteps for researchers to improve their evaluation processes. Finally, we\nanalyze 66 NLG papers from recent NLP conferences in how well they already\nfollow these suggestions and identify which areas require more drastic changes\nto the status quo.\n",
                "链接": "https://arxiv.org/abs/2202.06935"
            },
            {
                "文章ID": "75352",
                "标题": "H2CGL: Modeling Dynamics of Citation Network for Impact Prediction",
                "作者": " Guoxiu He,  Zhikai Xue,  Zhuoren Jiang,  Yangyang Kang,  Star Zhao,  Wei Lu",
                "发布日期": "2023-10-17",
                "摘要": "  The potential impact of a paper is often quantified by how many citations it\nwill receive. However, most commonly used models may underestimate the\ninfluence of newly published papers over time, and fail to encapsulate this\ndynamics of citation network into the graph. In this study, we construct\nhierarchical and heterogeneous graphs for target papers with an annual\nperspective. The constructed graphs can record the annual dynamics of target\npapers' scientific context information. Then, a novel graph neural network,\nHierarchical and Heterogeneous Contrastive Graph Learning Model (H2CGL), is\nproposed to incorporate heterogeneity and dynamics of the citation network.\nH2CGL separately aggregates the heterogeneous information for each year and\nprioritizes the highly-cited papers and relationships among references,\ncitations, and the target paper. It then employs a weighted GIN to capture\ndynamics between heterogeneous subgraphs over years. Moreover, it leverages\ncontrastive learning to make the graph representations more sensitive to\npotential citations. Particularly, co-cited or co-citing papers of the target\npaper with large citation gap are taken as hard negative samples, while\nrandomly dropping low-cited papers could generate positive samples. Extensive\nexperimental results on two scholarly datasets demonstrate that the proposed\nH2CGL significantly outperforms a series of baseline approaches for both\npreviously and freshly published papers. Additional analyses highlight the\nsignificance of the proposed modules. Our codes and settings have been released\non Github (https://github.com/ECNU-Text-Computing/H2CGL)\n",
                "链接": "https://arxiv.org/abs/2305.01572"
            },
            {
                "文章ID": "24746",
                "标题": "A Meta-Analysis of Distributionally-Robust Models",
                "作者": " Benjamin Feuer,  Ameya Joshi,  Chinmay Hegde",
                "发布日期": "2022-06-16",
                "摘要": "  State-of-the-art image classifiers trained on massive datasets (such as\nImageNet) have been shown to be vulnerable to a range of both intentional and\nincidental distribution shifts. On the other hand, several recent classifiers\nwith favorable out-of-distribution (OOD) robustness properties have emerged,\nachieving high accuracy on their target tasks while maintaining their\nin-distribution accuracy on challenging benchmarks. We present a meta-analysis\non a wide range of publicly released models, most of which have been published\nover the last twelve months. Through this meta-analysis, we empirically\nidentify four main commonalities for all the best-performing OOD-robust models,\nall of which illuminate the considerable promise of vision-language\npre-training.\n",
                "链接": "https://arxiv.org/abs/2206.07565"
            },
            {
                "文章ID": "119852",
                "标题": "Deep learning and traditional-based CAD schemes for the pulmonary\n  embolism diagnosis: A survey",
                "作者": " Seyed Hesamoddin Hosseini,  Amir Hossein Taherinia,  Mahdi Saadatmand",
                "发布日期": "2023-12-05",
                "摘要": "  Nowadays, pulmonary Computed Tomography Angiography (CTA) is the main tool\nfor detecting Pulmonary Embolism (PE). However, manual interpretation of CTA\nvolume requires a radiologist, which is time-consuming and error-prone due to\nthe specific conditions of lung tissue, large volume of data, lack of\nexperience, and eye fatigue. Therefore, Computer-Aided Design (CAD) systems are\nused as a second opinion for the diagnosis of PE. The purpose of this article\nis to review, evaluate, and compare the performance of deep learning and\ntraditional-based CAD system for diagnosis PE and to help physicians and\nresearchers in this field. In this study, all articles available in databases\nsuch as IEEE, ScienceDirect, Wiley, Springer, Nature, and Wolters Kluwer in the\nfield of PE diagnosis were examined using traditional and deep learning\nmethods. From 2002 to 2023, 23 papers were studied to extract the articles with\nthe considered limitations. Each paper presents an automatic PE detection\nsystem that we evaluate using criteria such as sensitivity, False Positives\n(FP), and the number of datasets. This research work includes recent studies,\nstate-of-the-art research works, and a more comprehensive overview compared to\npreviously published review articles in this research area.\n",
                "链接": "https://arxiv.org/abs/2312.01351"
            },
            {
                "文章ID": "108599",
                "标题": "Textual Analysis of ICALEPCS and IPAC Conference Proceedings: Revealing\n  Research Trends, Topics, and Collaborations for Future Insights and Advanced\n  Search",
                "作者": " Antonin Sulc,  Annika Eichler,  Tim Wilksen",
                "发布日期": "2023-10-16",
                "摘要": "  In this paper, we show a textual analysis of past ICALEPCS and IPAC\nconference proceedings to gain insights into the research trends and topics\ndiscussed in the field. We use natural language processing techniques to\nextract meaningful information from the abstracts and papers of past conference\nproceedings. We extract topics to visualize and identify trends, analyze their\nevolution to identify emerging research directions, and highlight interesting\npublications based solely on their content with an analysis of their network.\nAdditionally, we will provide an advanced search tool to better search the\nexisting papers to prevent duplication and easier reference findings. Our\nanalysis provides a comprehensive overview of the research landscape in the\nfield and helps researchers and practitioners to better understand the\nstate-of-the-art and identify areas for future research.\n",
                "链接": "https://arxiv.org/abs/2310.08954"
            },
            {
                "文章ID": "116662",
                "标题": "Responsible AI Considerations in Text Summarization Research: A Review\n  of Current Practices",
                "作者": " Yu Lu Liu,  Meng Cao,  Su Lin Blodgett,  Jackie Chi Kit Cheung,  Alexandra Olteanu,  Adam Trischler",
                "发布日期": "2023-11-21",
                "摘要": "  AI and NLP publication venues have increasingly encouraged researchers to\nreflect on possible ethical considerations, adverse impacts, and other\nresponsible AI issues their work might engender. However, for specific NLP\ntasks our understanding of how prevalent such issues are, or when and why these\nissues are likely to arise, remains limited. Focusing on text summarization --\na common NLP task largely overlooked by the responsible AI community -- we\nexamine research and reporting practices in the current literature. We conduct\na multi-round qualitative analysis of 333 summarization papers from the ACL\nAnthology published between 2020-2022. We focus on how, which, and when\nresponsible AI issues are covered, which relevant stakeholders are considered,\nand mismatches between stated and realized research goals. We also discuss\ncurrent evaluation practices and consider how authors discuss the limitations\nof both prior work and their own work. Overall, we find that relatively few\npapers engage with possible stakeholders or contexts of use, which limits their\nconsideration of potential downstream adverse impacts or other responsible AI\nissues. Based on our findings, we make recommendations on concrete practices\nand research directions.\n",
                "链接": "https://arxiv.org/abs/2311.11103"
            },
            {
                "文章ID": "4354",
                "标题": "Machine Translation from Signed to Spoken Languages: State of the Art\n  and Challenges",
                "作者": " Mathieu De Coster,  Dimitar Shterionov,  Mieke Van Herreweghe,  Joni Dambre",
                "发布日期": "2023-04-06",
                "摘要": "  Automatic translation from signed to spoken languages is an interdisciplinary\nresearch domain, lying on the intersection of computer vision, machine\ntranslation and linguistics. Nevertheless, research in this domain is performed\nmostly by computer scientists in isolation. As the domain is becoming\nincreasingly popular - the majority of scientific papers on the topic of sign\nlanguage translation have been published in the past three years - we provide\nan overview of the state of the art as well as some required background in the\ndifferent related disciplines. We give a high-level introduction to sign\nlanguage linguistics and machine translation to illustrate the requirements of\nautomatic sign language translation. We present a systematic literature review\nto illustrate the state of the art in the domain and then, harking back to the\nrequirements, lay out several challenges for future research. We find that\nsignificant advances have been made on the shoulders of spoken language machine\ntranslation research. However, current approaches are often not linguistically\nmotivated or are not adapted to the different input modality of sign languages.\nWe explore challenges related to the representation of sign language data, the\ncollection of datasets, the need for interdisciplinary research and\nrequirements for moving beyond research, towards applications. Based on our\nfindings, we advocate for interdisciplinary research and to base future\nresearch on linguistic analysis of sign languages. Furthermore, the inclusion\nof deaf and hearing end users of sign language translation applications in use\ncase identification, data collection and evaluation is of the utmost importance\nin the creation of useful sign language translation models. We recommend\niterative, human-in-the-loop, design and development of sign language\ntranslation models.\n",
                "链接": "https://arxiv.org/abs/2202.03086"
            },
            {
                "文章ID": "113873",
                "标题": "Pyclipse, a library for deidentification of free-text clinical notes",
                "作者": " Callandra Moore,  Jonathan Ranisau,  Walter Nelson,  Jeremy Petch,  Alistair Johnson",
                "发布日期": "2023-11-07",
                "摘要": "  Automated deidentification of clinical text data is crucial due to the high\ncost of manual deidentification, which has been a barrier to sharing clinical\ntext and the advancement of clinical natural language processing. However,\ncreating effective automated deidentification tools faces several challenges,\nincluding issues in reproducibility due to differences in text processing,\nevaluation methods, and a lack of consistency across clinical domains and\ninstitutions. To address these challenges, we propose the pyclipse framework, a\nunified and configurable evaluation procedure to streamline the comparison of\ndeidentification algorithms. Pyclipse serves as a single interface for running\nopen-source deidentification algorithms on local clinical data, allowing for\ncontext-specific evaluation. To demonstrate the utility of pyclipse, we compare\nsix deidentification algorithms across four public and two private clinical\ntext datasets. We find that algorithm performance consistently falls short of\nthe results reported in the original papers, even when evaluated on the same\nbenchmark dataset. These discrepancies highlight the complexity of accurately\nassessing and comparing deidentification algorithms, emphasizing the need for a\nreproducible, adjustable, and extensible framework like pyclipse. Our framework\nlays the foundation for a unified approach to evaluate and improve\ndeidentification tools, ultimately enhancing patient protection in clinical\nnatural language processing.\n",
                "链接": "https://arxiv.org/abs/2311.02748"
            },
            {
                "文章ID": "36572",
                "标题": "Lost in Translation: Reimagining the Machine Learning Life Cycle in\n  Education",
                "作者": " Lydia T. Liu,  Serena Wang,  Tolani Britton,  Rediet Abebe",
                "发布日期": "2022-09-09",
                "摘要": "  Machine learning (ML) techniques are increasingly prevalent in education,\nfrom their use in predicting student dropout, to assisting in university\nadmissions, and facilitating the rise of MOOCs. Given the rapid growth of these\nnovel uses, there is a pressing need to investigate how ML techniques support\nlong-standing education principles and goals. In this work, we shed light on\nthis complex landscape drawing on qualitative insights from interviews with\neducation experts. These interviews comprise in-depth evaluations of ML for\neducation (ML4Ed) papers published in preeminent applied ML conferences over\nthe past decade. Our central research goal is to critically examine how the\nstated or implied education and societal objectives of these papers are aligned\nwith the ML problems they tackle. That is, to what extent does the technical\nproblem formulation, objectives, approach, and interpretation of results align\nwith the education problem at hand. We find that a cross-disciplinary gap\nexists and is particularly salient in two parts of the ML life cycle: the\nformulation of an ML problem from education goals and the translation of\npredictions to interventions. We use these insights to propose an extended ML\nlife cycle, which may also apply to the use of ML in other domains. Our work\njoins a growing number of meta-analytical studies across education and ML\nresearch, as well as critical analyses of the societal impact of ML.\nSpecifically, it fills a gap between the prevailing technical understanding of\nmachine learning and the perspective of education researchers working with\nstudents and in policy.\n",
                "链接": "https://arxiv.org/abs/2209.03929"
            },
            {
                "文章ID": "81423",
                "标题": "Modeling Dynamic Heterogeneous Graph and Node Importance for Future\n  Citation Prediction",
                "作者": " Hao Geng,  Deqing Wang,  Fuzhen Zhuang,  Xuehua Ming,  Chenguang Du,  Ting Jiang,  Haolong Guo,  Rui Liu",
                "发布日期": "2023-05-30",
                "摘要": "  Accurate citation count prediction of newly published papers could help\neditors and readers rapidly figure out the influential papers in the future.\nThough many approaches are proposed to predict a paper's future citation, most\nignore the dynamic heterogeneous graph structure or node importance in academic\nnetworks. To cope with this problem, we propose a Dynamic heterogeneous Graph\nand Node Importance network (DGNI) learning framework, which fully leverages\nthe dynamic heterogeneous graph and node importance information to predict\nfuture citation trends of newly published papers. First, a dynamic\nheterogeneous network embedding module is provided to capture the dynamic\nevolutionary trends of the whole academic network. Then, a node importance\nembedding module is proposed to capture the global consistency relationship to\nfigure out each paper's node importance. Finally, the dynamic evolutionary\ntrend embeddings and node importance embeddings calculated above are combined\nto jointly predict the future citation counts of each paper, by a log-normal\ndistribution model according to multi-faced paper node representations.\nExtensive experiments on two large-scale datasets demonstrate that our model\nsignificantly improves all indicators compared to the SOTA models.\n",
                "链接": "https://arxiv.org/abs/2305.17417"
            },
            {
                "文章ID": "73410",
                "标题": "A Study on Reproducibility and Replicability of Table Structure\n  Recognition Methods",
                "作者": " Kehinde Ajayi,  Muntabir Hasan Choudhury,  Sarah Rajtmajer,  Jian Wu",
                "发布日期": "2023-04-21",
                "摘要": "  Concerns about reproducibility in artificial intelligence (AI) have emerged,\nas researchers have reported unsuccessful attempts to directly reproduce\npublished findings in the field. Replicability, the ability to affirm a finding\nusing the same procedures on new data, has not been well studied. In this\npaper, we examine both reproducibility and replicability of a corpus of 16\npapers on table structure recognition (TSR), an AI task aimed at identifying\ncell locations of tables in digital documents. We attempt to reproduce\npublished results using codes and datasets provided by the original authors. We\nthen examine replicability using a dataset similar to the original as well as a\nnew dataset, GenTSR, consisting of 386 annotated tables extracted from\nscientific papers. Out of 16 papers studied, we reproduce results consistent\nwith the original in only four. Two of the four papers are identified as\nreplicable using the similar dataset under certain IoU values. No paper is\nidentified as replicable using the new dataset. We offer observations on the\ncauses of irreproducibility and irreplicability. All code and data are\navailable on Codeocean at https://codeocean.com/capsule/6680116/tree.\n",
                "链接": "https://arxiv.org/abs/2304.10439"
            },
            {
                "文章ID": "3094",
                "标题": "A Review on Deep-Learning Algorithms for Fetal Ultrasound-Image Analysis",
                "作者": " Maria Chiara Fiorentino,  Francesca Pia Villani,  Mariachiara Di Cosmo,  Emanuele Frontoni,  Sara Moccia",
                "发布日期": "2022-11-08",
                "摘要": "  Deep-learning (DL) algorithms are becoming the standard for processing\nultrasound (US) fetal images. Despite a large number of survey papers already\npresent in this field, most of them are focusing on a broader area of\nmedical-image analysis or not covering all fetal US DL applications. This paper\nsurveys the most recent work in the field, with a total of 145 research papers\npublished after 2017. Each paper is analyzed and commented on from both the\nmethodology and application perspective. We categorized the papers in (i) fetal\nstandard-plane detection, (ii) anatomical-structure analysis, and (iii)\nbiometry parameter estimation. For each category, main limitations and open\nissues are presented. Summary tables are included to facilitate the comparison\namong the different approaches. Publicly-available datasets and performance\nmetrics commonly used to assess algorithm performance are summarized, too. This\npaper ends with a critical summary of the current state of the art on DL\nalgorithms for fetal US image analysis and a discussion on current challenges\nthat have to be tackled by researchers working in the field to translate the\nresearch methodology into the actual clinical practice.\n",
                "链接": "https://arxiv.org/abs/2201.12260"
            },
            {
                "文章ID": "116946",
                "标题": "Continual Learning: Applications and the Road Forward",
                "作者": " Eli Verwimp,  Rahaf Aljundi,  Shai Ben-David,  Matthias Bethge,  Andrea Cossu,  Alexander Gepperth,  Tyler L. Hayes,  Eyke Hüllermeier,  Christopher Kanan,  Dhireesha Kudithipudi,  Christoph H. Lampert,  Martin Mundt,  Razvan Pascanu,  Adrian Popescu,  Andreas S. Tolias,  Joost van de Weijer,  Bing Liu,  Vincenzo Lomonaco,  Tinne Tuytelaars,  Gido M. van de Ven",
                "发布日期": "2023-11-22",
                "摘要": "  Continual learning is a sub-field of machine learning, which aims to allow\nmachine learning models to continuously learn on new data, by accumulating\nknowledge without forgetting what was learned in the past. In this work, we\ntake a step back, and ask: \"Why should one care about continual learning in the\nfirst place?\". We set the stage by surveying recent continual learning papers\npublished at three major machine learning conferences, and show that\nmemory-constrained settings dominate the field. Then, we discuss five open\nproblems in machine learning, and even though they seem unrelated to continual\nlearning at first sight, we show that continual learning will inevitably be\npart of their solution. These problems are model-editing, personalization,\non-device learning, faster (re-)training and reinforcement learning. Finally,\nby comparing the desiderata from these unsolved problems and the current\nassumptions in continual learning, we highlight and discuss four future\ndirections for continual learning research. We hope that this work offers an\ninteresting perspective on the future of continual learning, while displaying\nits potential value and the paths we have to pursue in order to make it\nsuccessful. This work is the result of the many discussions the authors had at\nthe Dagstuhl seminar on Deep Continual Learning, in March 2023.\n",
                "链接": "https://arxiv.org/abs/2311.11908"
            },
            {
                "文章ID": "39463",
                "标题": "Automatic Analysis of Available Source Code of Top Artificial\n  Intelligence Conference Papers",
                "作者": " Jialiang Lin,  Yingmin Wang,  Yao Yu,  Yu Zhou,  Yidong Chen,  Xiaodong Shi",
                "发布日期": "2022-09-29",
                "摘要": "  Source code is essential for researchers to reproduce the methods and\nreplicate the results of artificial intelligence (AI) papers. Some\norganizations and researchers manually collect AI papers with available source\ncode to contribute to the AI community. However, manual collection is a\nlabor-intensive and time-consuming task. To address this issue, we propose a\nmethod to automatically identify papers with available source code and extract\ntheir source code repository URLs. With this method, we find that 20.5% of\nregular papers of 10 top AI conferences published from 2010 to 2019 are\nidentified as papers with available source code and that 8.1% of these source\ncode repositories are no longer accessible. We also create the XMU NLP Lab\nREADME Dataset, the largest dataset of labeled README files for source code\ndocument research. Through this dataset, we have discovered that quite a few\nREADME files have no installation instructions or usage tutorials provided.\nFurther, a large-scale comprehensive statistical analysis is made for a general\npicture of the source code of AI conference papers. The proposed solution can\nalso go beyond AI conference papers to analyze other scientific papers from\nboth journals and conferences to shed light on more domains.\n",
                "链接": "https://arxiv.org/abs/2209.14155"
            },
            {
                "文章ID": "115040",
                "标题": "A design of Convolutional Neural Network model for the Diagnosis of the\n  COVID-19",
                "作者": " Xinyuan Song",
                "发布日期": "2023-11-14",
                "摘要": "  With the spread of COVID-19 around the globe over the past year, the usage of\nartificial intelligence (AI) algorithms and image processing methods to analyze\nthe X-ray images of patients' chest with COVID-19 has become essential. The\nCOVID-19 virus recognition in the lung area of a patient is one of the basic\nand essential needs of clicical centers and hospitals. Most research in this\nfield has been devoted to papers on the basis of deep learning methods\nutilizing CNNs (Convolutional Neural Network), which mainly deal with the\nscreening of sick and healthy people.In this study, a new structure of a\n19-layer CNN has been recommended for accurately recognition of the COVID-19\nfrom the X-ray pictures of chest. The offered CNN is developed to serve as a\nprecise diagnosis system for a three class (viral pneumonia, Normal, COVID) and\na four classclassification (Lung opacity, Normal, COVID-19, and pneumonia). A\ncomparison is conducted among the outcomes of the offered procedure and some\npopular pretrained networks, including Inception, Alexnet, ResNet50,\nSqueezenet, and VGG19 and based on Specificity, Accuracy, Precision,\nSensitivity, Confusion Matrix, and F1-score. The experimental results of the\noffered CNN method specify its dominance over the existing published\nprocedures. This method can be a useful tool for clinicians in deciding\nproperly about COVID-19.\n",
                "链接": "https://arxiv.org/abs/2311.06394"
            },
            {
                "文章ID": "69268",
                "标题": "Beyond Accuracy: A Critical Review of Fairness in Machine Learning for\n  Mobile and Wearable Computing",
                "作者": " Sofia Yfantidou,  Marios Constantinides,  Dimitris Spathis,  Athena Vakali,  Daniele Quercia,  Fahim Kawsar",
                "发布日期": "2023-09-25",
                "摘要": "  The field of mobile and wearable computing is undergoing a revolutionary\nintegration of machine learning. Devices can now diagnose diseases, predict\nheart irregularities, and unlock the full potential of human cognition.\nHowever, the underlying algorithms powering these predictions are not immune to\nbiases with respect to sensitive attributes (e.g., gender, race), leading to\ndiscriminatory outcomes. The goal of this work is to explore the extent to\nwhich the mobile and wearable computing community has adopted ways of reporting\ninformation about datasets and models to surface and, eventually, counter\nbiases. Our systematic review of papers published in the Proceedings of the ACM\nInteractive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) journal from\n2018-2022 indicates that, while there has been progress made on algorithmic\nfairness, there is still ample room for growth. Our findings show that only a\nsmall portion (5%) of published papers adheres to modern fairness reporting,\nwhile the overwhelming majority thereof focuses on accuracy or error metrics.\nTo generalize these results across venues of similar scope, we analyzed recent\nproceedings of ACM MobiCom, MobiSys, and SenSys, IEEE Pervasive, and IEEE\nTransactions on Mobile Computing Computing, and found no deviation from our\nprimary result. In light of these findings, our work provides practical\nguidelines for the design and development of mobile and wearable technologies\nthat not only strive for accuracy but also fairness.\n",
                "链接": "https://arxiv.org/abs/2303.15585"
            },
            {
                "文章ID": "57930",
                "标题": "A Comprehensive Survey on Heart Sound Analysis in the Deep Learning Era",
                "作者": " Zhao Ren,  Yi Chang,  Thanh Tam Nguyen,  Yang Tan,  Kun Qian,  Björn W. Schuller",
                "发布日期": "2023-01-24",
                "摘要": "  Heart sound auscultation has been demonstrated to be beneficial in clinical\nusage for early screening of cardiovascular diseases. Due to the high\nrequirement of well-trained professionals for auscultation, automatic\nauscultation benefiting from signal processing and machine learning can help\nauxiliary diagnosis and reduce the burdens of training professional clinicians.\nNevertheless, classic machine learning is limited to performance improvement in\nthe era of big data. Deep learning has achieved better performance than classic\nmachine learning in many research fields, as it employs more complex model\narchitectures with stronger capability of extracting effective representations.\nDeep learning has been successfully applied to heart sound analysis in the past\nyears. As most review works about heart sound analysis were given before 2017,\nthe present survey is the first to work on a comprehensive overview to\nsummarise papers on heart sound analysis with deep learning in the past six\nyears 2017--2022. We introduce both classic machine learning and deep learning\nfor comparison, and further offer insights about the advances and future\nresearch directions in deep learning for heart sound analysis.\n",
                "链接": "https://arxiv.org/abs/2301.09362"
            },
            {
                "文章ID": "116000",
                "标题": "Disentangling the Potential Impacts of Papers into Diffusion,\n  Conformity, and Contribution Values",
                "作者": " Zhikai Xue,  Guoxiu He,  Zhuoren Jiang,  Yangyang Kang,  Star Zhao,  Wei Lu",
                "发布日期": "2023-11-17",
                "摘要": "  The potential impact of an academic paper is determined by various factors,\nincluding its popularity and contribution. Existing models usually estimate\noriginal citation counts based on static graphs and fail to differentiate\nvalues from nuanced perspectives. In this study, we propose a novel graph\nneural network to Disentangle the Potential impacts of Papers into Diffusion,\nConformity, and Contribution values (called DPPDCC). Given a target paper,\nDPPDCC encodes temporal and structural features within the constructed dynamic\nheterogeneous graph. Particularly, to capture the knowledge flow, we emphasize\nthe importance of comparative and co-cited/citing information between papers\nand aggregate snapshots evolutionarily. To unravel popularity, we contrast\naugmented graphs to extract the essence of diffusion and predict the\naccumulated citation binning to model conformity. We further apply orthogonal\nconstraints to encourage distinct modeling of each perspective and preserve the\ninherent value of contribution. To evaluate models' generalization for papers\npublished at various times, we reformulate the problem by partitioning data\nbased on specific time points to mirror real-world conditions. Extensive\nexperimental results on three datasets demonstrate that DPPDCC significantly\noutperforms baselines for previously, freshly, and immediately published\npapers. Further analyses confirm its robust capabilities. We will make our\ndatasets and codes publicly available.\n",
                "链接": "https://arxiv.org/abs/2311.09262"
            },
            {
                "文章ID": "26857",
                "标题": "Reinforcement Learning in Medical Image Analysis: Concepts,\n  Applications, Challenges, and Future Directions",
                "作者": " Mingzhe Hu,  Jiahan Zhang,  Luke Matkovic,  Tian Liu,  Xiaofeng Yang",
                "发布日期": "2022-06-30",
                "摘要": "  Motivation: Medical image analysis involves tasks to assist physicians in\nqualitative and quantitative analysis of lesions or anatomical structures,\nsignificantly improving the accuracy and reliability of diagnosis and\nprognosis. Traditionally, these tasks are finished by physicians or medical\nphysicists and lead to two major problems: (i) low efficiency; (ii) biased by\npersonal experience. In the past decade, many machine learning methods have\nbeen applied to accelerate and automate the image analysis process. Compared to\nthe enormous deployments of supervised and unsupervised learning models,\nattempts to use reinforcement learning in medical image analysis are scarce.\nThis review article could serve as the stepping-stone for related research.\nSignificance: From our observation, though reinforcement learning has gradually\ngained momentum in recent years, many researchers in the medical analysis field\nfind it hard to understand and deploy in clinics. One cause is lacking\nwell-organized review articles targeting readers lacking professional computer\nscience backgrounds. Rather than providing a comprehensive list of all\nreinforcement learning models in medical image analysis, this paper may help\nthe readers to learn how to formulate and solve their medical image analysis\nresearch as reinforcement learning problems. Approach & Results: We selected\npublished articles from Google Scholar and PubMed. Considering the scarcity of\nrelated articles, we also included some outstanding newest preprints. The\npapers are carefully reviewed and categorized according to the type of image\nanalysis task. We first review the basic concepts and popular models of\nreinforcement learning. Then we explore the applications of reinforcement\nlearning models in landmark detection. Finally, we conclude the article by\ndiscussing the reviewed reinforcement learning approaches' limitations and\npossible improvements.\n",
                "链接": "https://arxiv.org/abs/2206.14302"
            },
            {
                "文章ID": "76175",
                "标题": "MIReAD: Simple Method for Learning High-quality Representations from\n  Scientific Documents",
                "作者": " Anastasia Razdaibiedina,  Alexander Brechalov",
                "发布日期": "2023-05-09",
                "摘要": "  Learning semantically meaningful representations from scientific documents\ncan facilitate academic literature search and improve performance of\nrecommendation systems. Pre-trained language models have been shown to learn\nrich textual representations, yet they cannot provide powerful document-level\nrepresentations for scientific articles. We propose MIReAD, a simple method\nthat learns high-quality representations of scientific papers by fine-tuning\ntransformer model to predict the target journal class based on the abstract. We\ntrain MIReAD on more than 500,000 PubMed and arXiv abstracts across over 2,000\njournal classes. We show that MIReAD produces representations that can be used\nfor similar papers retrieval, topic categorization and literature search. Our\nproposed approach outperforms six existing models for representation learning\non scientific documents across four evaluation standards.\n",
                "链接": "https://arxiv.org/abs/2305.04177"
            },
            {
                "文章ID": "117259",
                "标题": "MatGD: Materials Graph Digitizer",
                "作者": " Jaewoong Lee,  Wonseok Lee,  Jihan Kim",
                "发布日期": "2023-11-23",
                "摘要": "  We have developed MatGD (Material Graph Digitizer), which is a tool for\ndigitizing a data line from scientific graphs. The algorithm behind the tool\nconsists of four steps: (1) identifying graphs within subfigures, (2)\nseparating axes and data sections, (3) discerning the data lines by eliminating\nirrelevant graph objects and matching with the legend, and (4) data extraction\nand saving. From the 62,534 papers in the areas of batteries, catalysis, and\nMOFs, 501,045 figures were mined. Remarkably, our tool showcased performance\nwith over 99% accuracy in legend marker and text detection. Moreover, its\ncapability for data line separation stood at 66%, which is much higher compared\nto other existing figure mining tools. We believe that this tool will be\nintegral to collecting both past and future data from publications, and these\ndata can be used to train various machine learning models that can enhance\nmaterial predictions and new materials discovery.\n",
                "链接": "https://arxiv.org/abs/2311.12806"
            },
            {
                "文章ID": "45040",
                "标题": "A Survey on Deep Generative 3D-aware Image Synthesis",
                "作者": " Weihao Xia,  Jing-Hao Xue",
                "发布日期": "2023-10-04",
                "摘要": "  Recent years have seen remarkable progress in deep learning powered visual\ncontent creation. This includes deep generative 3D-aware image synthesis, which\nproduces high-idelity images in a 3D-consistent manner while simultaneously\ncapturing compact surfaces of objects from pure image collections without the\nneed for any 3D supervision, thus bridging the gap between 2D imagery and 3D\nreality. The ield of computer vision has been recently captivated by the task\nof deep generative 3D-aware image synthesis, with hundreds of papers appearing\nin top-tier journals and conferences over the past few years (mainly the past\ntwo years), but there lacks a comprehensive survey of this remarkable and swift\nprogress. Our survey aims to introduce new researchers to this topic, provide a\nuseful reference for related works, and stimulate future research directions\nthrough our discussion section. Apart from the presented papers, we aim to\nconstantly update the latest relevant papers along with corresponding\nimplementations at https://weihaox.github.io/3D-aware-Gen.\n",
                "链接": "https://arxiv.org/abs/2210.14267"
            },
            {
                "文章ID": "119189",
                "标题": "A Survey on Deep Learning for Polyp Segmentation: Techniques, Challenges\n  and Future Trends",
                "作者": " Jiaxin Mei,  Tao Zhou,  Kaiwen Huang,  Yizhe Zhang,  Yi Zhou,  Ye Wu,  Huazhu Fu",
                "发布日期": "2023-12-01",
                "摘要": "  Early detection and assessment of polyps play a crucial role in the\nprevention and treatment of colorectal cancer (CRC). Polyp segmentation\nprovides an effective solution to assist clinicians in accurately locating and\nsegmenting polyp regions. In the past, people often relied on manually\nextracted lower-level features such as color, texture, and shape, which often\nhad issues capturing global context and lacked robustness to complex scenarios.\nWith the advent of deep learning, more and more outstanding medical image\nsegmentation algorithms based on deep learning networks have emerged, making\nsignificant progress in this field. This paper provides a comprehensive review\nof polyp segmentation algorithms. We first review some traditional algorithms\nbased on manually extracted features and deep segmentation algorithms, then\ndetail benchmark datasets related to the topic. Specifically, we carry out a\ncomprehensive evaluation of recent deep learning models and results based on\npolyp sizes, considering the pain points of research topics and differences in\nnetwork structures. Finally, we discuss the challenges of polyp segmentation\nand future trends in this field. The models, benchmark datasets, and source\ncode links we collected are all published at\nhttps://github.com/taozh2017/Awesome-Polyp-Segmentation.\n",
                "链接": "https://arxiv.org/abs/2311.18373"
            },
            {
                "文章ID": "81377",
                "标题": "Counterfactual Evaluation of Peer-Review Assignment Policies",
                "作者": " Martin Saveski,  Steven Jecmen,  Nihar B. Shah,  Johan Ugander",
                "发布日期": "2023-05-30",
                "摘要": "  Peer review assignment algorithms aim to match research papers to suitable\nexpert reviewers, working to maximize the quality of the resulting reviews. A\nkey challenge in designing effective assignment policies is evaluating how\nchanges to the assignment algorithm map to changes in review quality. In this\nwork, we leverage recently proposed policies that introduce randomness in\npeer-review assignment--in order to mitigate fraud--as a valuable opportunity\nto evaluate counterfactual assignment policies. Specifically, we exploit how\nsuch randomized assignments provide a positive probability of observing the\nreviews of many assignment policies of interest. To address challenges in\napplying standard off-policy evaluation methods, such as violations of\npositivity, we introduce novel methods for partial identification based on\nmonotonicity and Lipschitz smoothness assumptions for the mapping between\nreviewer-paper covariates and outcomes. We apply our methods to peer-review\ndata from two computer science venues: the TPDP'21 workshop (95 papers and 35\nreviewers) and the AAAI'22 conference (8,450 papers and 3,145 reviewers). We\nconsider estimates of (i) the effect on review quality when changing weights in\nthe assignment algorithm, e.g., weighting reviewers' bids vs. textual\nsimilarity (between the review's past papers and the submission), and (ii) the\n\"cost of randomization\", capturing the difference in expected quality between\nthe perturbed and unperturbed optimal match. We find that placing higher weight\non text similarity results in higher review quality and that introducing\nrandomization in the reviewer-paper assignment only marginally reduces the\nreview quality. Our methods for partial identification may be of independent\ninterest, while our off-policy approach can likely find use evaluating a broad\nclass of algorithmic matching systems.\n",
                "链接": "https://arxiv.org/abs/2305.17339"
            },
            {
                "文章ID": "99093",
                "标题": "Gender bias and stereotypes in Large Language Models",
                "作者": " Hadas Kotek,  Rikker Dockum,  David Q. Sun",
                "发布日期": "2023-08-30",
                "摘要": "  Large Language Models (LLMs) have made substantial progress in the past\nseveral months, shattering state-of-the-art benchmarks in many domains. This\npaper investigates LLMs' behavior with respect to gender stereotypes, a known\nissue for prior models. We use a simple paradigm to test the presence of gender\nbias, building on but differing from WinoBias, a commonly used gender bias\ndataset, which is likely to be included in the training data of current LLMs.\nWe test four recently published LLMs and demonstrate that they express biased\nassumptions about men and women's occupations. Our contributions in this paper\nare as follows: (a) LLMs are 3-6 times more likely to choose an occupation that\nstereotypically aligns with a person's gender; (b) these choices align with\npeople's perceptions better than with the ground truth as reflected in official\njob statistics; (c) LLMs in fact amplify the bias beyond what is reflected in\nperceptions or the ground truth; (d) LLMs ignore crucial ambiguities in\nsentence structure 95% of the time in our study items, but when explicitly\nprompted, they recognize the ambiguity; (e) LLMs provide explanations for their\nchoices that are factually inaccurate and likely obscure the true reason behind\ntheir predictions. That is, they provide rationalizations of their biased\nbehavior. This highlights a key property of these models: LLMs are trained on\nimbalanced datasets; as such, even with the recent successes of reinforcement\nlearning with human feedback, they tend to reflect those imbalances back at us.\nAs with other types of societal biases, we suggest that LLMs must be carefully\ntested to ensure that they treat minoritized individuals and communities\nequitably.\n",
                "链接": "https://arxiv.org/abs/2308.14921"
            },
            {
                "文章ID": "103635",
                "标题": "Before Blue Birds Became X-tinct: Understanding the Effect of Regime\n  Change on Twitter's Advertising and Compliance of Advertising Policies",
                "作者": " Yash Vekaria,  Zubair Shafiq,  Savvas Zannettou",
                "发布日期": "2023-09-25",
                "摘要": "  Social media platforms, including Twitter (now X), have policies in place to\nmaintain a safe and trustworthy advertising environment. However, the extent to\nwhich these policies are adhered to and enforced remains a subject of interest\nand concern. We present the first large-scale audit of advertising on Twitter\nfocusing on compliance with the platform's advertising policies, particularly\nthose related to political and adult content. We investigate the compliance of\nadvertisements on Twitter with the platform's stated policies and the impact of\nrecent acquisition on the advertising activity of the platform. By analyzing\n34K advertisements from ~6M tweets, collected over six months, we find evidence\nof widespread noncompliance with Twitter's political and adult content\nadvertising policies suggesting a lack of effective ad content moderation. We\nalso find that Elon Musk's acquisition of Twitter had a noticeable impact on\nthe advertising landscape, with most existing advertisers either completely\nstopping their advertising activity or reducing it. Major brands decreased\ntheir advertising on Twitter, suggesting a negative immediate effect on the\nplatform's advertising revenue. Our findings underscore the importance of\nexternal audits to monitor compliance and improve transparency in online\nadvertising.\n",
                "链接": "https://arxiv.org/abs/2309.12591"
            },
            {
                "文章ID": "1837",
                "标题": "Automation of Citation Screening for Systematic Literature Reviews using\n  Neural Networks: A Replicability Study",
                "作者": " Wojciech Kusa,  Allan Hanbury,  Petr Knoth",
                "发布日期": "2022-01-20",
                "摘要": "  In the process of Systematic Literature Review, citation screening is\nestimated to be one of the most time-consuming steps. Multiple approaches to\nautomate it using various machine learning techniques have been proposed. The\nfirst research papers that apply deep neural networks to this problem were\npublished in the last two years. In this work, we conduct a replicability study\nof the first two deep learning papers for citation screening and evaluate their\nperformance on 23 publicly available datasets. While we succeeded in\nreplicating the results of one of the papers, we were unable to replicate the\nresults of the other. We summarise the challenges involved in the replication,\nincluding difficulties in obtaining the datasets to match the experimental\nsetup of the original papers and problems with executing the original source\ncode. Motivated by this experience, we subsequently present a simpler model\nbased on averaging word embeddings that outperforms one of the models on 18 out\nof 23 datasets and is, on average, 72 times faster than the second replicated\napproach. Finally, we measure the training time and the invariance of the\nmodels when exposed to a variety of input features and random initialisations,\ndemonstrating differences in the robustness of these approaches.\n",
                "链接": "https://arxiv.org/abs/2201.07534"
            },
            {
                "文章ID": "112370",
                "标题": "Perspectives from India: Challenges and Opportunities for Computational\n  Tools to Enhance Confidence in Published Research",
                "作者": " Tatiana Chakravorti,  Chuhao Wu,  Sai Koneru,  Sarah Rajtmajer",
                "发布日期": "2023-10-31",
                "摘要": "  Over the past decade, a crisis of confidence in published scientific findings\nhas catalyzed widespread response from the research community, particularly in\nthe West. These responses have included policy discussions and changes to\nexisting practice as well as computational infrastructure to support and\nevaluate research. Our work studies Indian researchers' awareness, perceptions,\nand challenges around research integrity. We explore opportunities for\nArtificial Intelligence (AI)-powered tools to evaluate reproducibility and\nreplicability, centering cultural perspectives. We discuss requirements for\nsuch tools, including signals within papers and metadata to be included, and\nsystem hybridity (fully-AI vs. collaborative human-AI). We draw upon 19\nsemi-structured interviews and 72 follow-up surveys with researchers at\nuniversities throughout India. Our findings highlight the need for\ncomputational tools to contextualize confidence in published research. In\nparticular, researchers prefer approaches that enable human-AI collaboration.\nAdditionally, our findings emphasize the shortcomings of current incentive\nstructures for publication, funding, and promotion.\n",
                "链接": "https://arxiv.org/abs/2310.19158"
            },
            {
                "文章ID": "105850",
                "标题": "ImagenHub: Standardizing the evaluation of conditional image generation\n  models",
                "作者": " Max Ku,  Tianle Li,  Kai Zhang,  Yujie Lu,  Xingyu Fu,  Wenwen Zhuang,  Wenhu Chen",
                "发布日期": "2023-12-05",
                "摘要": "  Recently, a myriad of conditional image generation and editing models have\nbeen developed to serve different downstream tasks, including text-to-image\ngeneration, text-guided image editing, subject-driven image generation,\ncontrol-guided image generation, etc. However, we observe huge inconsistencies\nin experimental conditions: datasets, inference, and evaluation metrics -\nrender fair comparisons difficult. This paper proposes ImagenHub, which is a\none-stop library to standardize the inference and evaluation of all the\nconditional image generation models. Firstly, we define seven prominent tasks\nand curate high-quality evaluation datasets for them. Secondly, we built a\nunified inference pipeline to ensure fair comparison. Thirdly, we design two\nhuman evaluation scores, i.e. Semantic Consistency and Perceptual Quality,\nalong with comprehensive guidelines to evaluate generated images. We train\nexpert raters to evaluate the model outputs based on the proposed metrics. Our\nhuman evaluation achieves a high inter-worker agreement of Krippendorff's alpha\non 76% models with a value higher than 0.4. We comprehensively evaluated a\ntotal of around 30 models and observed three key takeaways: (1) the existing\nmodels' performance is generally unsatisfying except for Text-guided Image\nGeneration and Subject-driven Image Generation, with 74% models achieving an\noverall score lower than 0.5. (2) we examined the claims from published papers\nand found 83% of them hold with a few exceptions. (3) None of the existing\nautomatic metrics has a Spearman's correlation higher than 0.2 except\nsubject-driven image generation. Moving forward, we will continue our efforts\nto evaluate newly published models and update our leaderboard to keep track of\nthe progress in conditional image generation.\n",
                "链接": "https://arxiv.org/abs/2310.01596"
            },
            {
                "文章ID": "33718",
                "标题": "A Survey of User Perspectives on Security and Privacy in a Home\n  Networking Environment",
                "作者": " Nandita Pattnaik,  Shujun Li,  Jason R. C. Nurse",
                "发布日期": "2022-12-13",
                "摘要": "  The security and privacy of smart home systems, particularly from a home\nuser's perspective, have been a very active research area in recent years.\nHowever, via a meta-review of 52 review papers covering related topics\n(published between 2000 and 2021), this paper shows a lack of a more recent\nliterature review on user perspectives of smart home security and privacy since\nthe 2010s. This identified gap motivated us to conduct a systematic literature\nreview (SLR) covering 126 relevant research papers published from 2010 to 2021.\nOur SLR led to the discovery of a number of important areas where further\nresearch is needed; these include holistic methods that consider a more diverse\nand heterogeneous range of home devices, interactions between multiple home\nusers, complicated data flow between multiple home devices and home users, some\nless-studied demographic factors, and advanced conceptual frameworks. Based on\nthese findings, we recommended key future research directions, e.g., research\nfor a better understanding of security and privacy aspects in different\nmulti-device and multi-user contexts, and a more comprehensive ontology on the\nsecurity and privacy of the smart home covering varying types of home devices\nand behaviors of different types of home users.\n",
                "链接": "https://arxiv.org/abs/2208.08193"
            },
            {
                "文章ID": "12450",
                "标题": "An Overview & Analysis of Sequence-to-Sequence Emotional Voice\n  Conversion",
                "作者": " Zijiang Yang,  Xin Jing,  Andreas Triantafyllopoulos,  Meishu Song,  Ilhan Aslan,  Björn W. Schuller",
                "发布日期": "2022-03-31",
                "摘要": "  Emotional voice conversion (EVC) focuses on converting a speech utterance\nfrom a source to a target emotion; it can thus be a key enabling technology for\nhuman-computer interaction applications and beyond. However, EVC remains an\nunsolved research problem with several challenges. In particular, as speech\nrate and rhythm are two key factors of emotional conversion, models have to\ngenerate output sequences of differing length. Sequence-to-sequence modelling\nis recently emerging as a competitive paradigm for models that can overcome\nthose challenges. In an attempt to stimulate further research in this promising\nnew direction, recent sequence-to-sequence EVC papers were systematically\ninvestigated and reviewed from six perspectives: their motivation, training\nstrategies, model architectures, datasets, model inputs, and evaluation\nmethods. This information is organised to provide the research community with\nan easily digestible overview of the current state-of-the-art. Finally, we\ndiscuss existing challenges of sequence-to-sequence EVC.\n",
                "链接": "https://arxiv.org/abs/2203.15873"
            },
            {
                "文章ID": "98167",
                "标题": "The Challenges of Machine Learning for Trust and Safety: A Case Study on\n  Misinformation Detection",
                "作者": " Madelyne Xiao,  Jonathan Mayer",
                "发布日期": "2023-08-24",
                "摘要": "  We examine the disconnect between scholarship and practice in applying\nmachine learning to trust and safety problems, using misinformation detection\nas a case study. We systematize literature on automated detection of\nmisinformation across a corpus of 270 well-cited papers in the field. We then\nexamine subsets of papers for data and code availability, design missteps,\nreproducibility, and generalizability. We find significant shortcomings in the\nliterature that call into question claimed performance and practicality.\nDetection tasks are often meaningfully distinct from the challenges that online\nservices actually face. Datasets and model evaluation are often\nnon-representative of real-world contexts, and evaluation frequently is not\nindependent of model training. Data and code availability is poor. Models do\nnot generalize well to out-of-domain data. Based on these results, we offer\nrecommendations for evaluating machine learning applications to trust and\nsafety problems. Our aim is for future work to avoid the pitfalls that we\nidentify.\n",
                "链接": "https://arxiv.org/abs/2308.12215"
            },
            {
                "文章ID": "49585",
                "标题": "Applications of statistical causal inference in software engineering",
                "作者": " Julien Siebert",
                "发布日期": "2023-03-24",
                "摘要": "  This paper reviews existing work in software engineering that applies\nstatistical causal inference methods. These methods aim at estimating causal\neffects from observational data. The review covers 32 papers published between\n2010 and 2022. Our results show that the application of statistical causal\ninference methods is relatively recent and that the corresponding research\ncommunity remains relatively fragmented.\n",
                "链接": "https://arxiv.org/abs/2211.11482"
            },
            {
                "文章ID": "103034",
                "标题": "Interactive Distillation of Large Single-Topic Corpora of Scientific\n  Papers",
                "作者": " Nicholas Solovyev,  Ryan Barron,  Manish Bhattarai,  Maksim E. Eren,  Kim O. Rasmussen,  Boian S. Alexandrov",
                "发布日期": "2023-09-20",
                "摘要": "  Highly specific datasets of scientific literature are important for both\nresearch and education. However, it is difficult to build such datasets at\nscale. A common approach is to build these datasets reductively by applying\ntopic modeling on an established corpus and selecting specific topics. A more\nrobust but time-consuming approach is to build the dataset constructively in\nwhich a subject matter expert (SME) handpicks documents. This method does not\nscale and is prone to error as the dataset grows. Here we showcase a new tool,\nbased on machine learning, for constructively generating targeted datasets of\nscientific literature. Given a small initial \"core\" corpus of papers, we build\na citation network of documents. At each step of the citation network, we\ngenerate text embeddings and visualize the embeddings through dimensionality\nreduction. Papers are kept in the dataset if they are \"similar\" to the core or\nare otherwise pruned through human-in-the-loop selection. Additional insight\ninto the papers is gained through sub-topic modeling using SeNMFk. We\ndemonstrate our new tool for literature review by applying it to two different\nfields in machine learning.\n",
                "链接": "https://arxiv.org/abs/2309.10772"
            },
            {
                "文章ID": "63716",
                "标题": "ChatGPT: A Meta-Analysis after 2.5 Months",
                "作者": " Christoph Leiter,  Ran Zhang,  Yanran Chen,  Jonas Belouadi,  Daniil Larionov,  Vivian Fresen,  Steffen Eger",
                "发布日期": "2023-02-28",
                "摘要": "  ChatGPT, a chatbot developed by OpenAI, has gained widespread popularity and\nmedia attention since its release in November 2022. However, little hard\nevidence is available regarding its perception in various sources. In this\npaper, we analyze over 300,000 tweets and more than 150 scientific papers to\ninvestigate how ChatGPT is perceived and discussed. Our findings show that\nChatGPT is generally viewed as of high quality, with positive sentiment and\nemotions of joy dominating in social media. Its perception has slightly\ndecreased since its debut, however, with joy decreasing and (negative) surprise\non the rise, and it is perceived more negatively in languages other than\nEnglish. In recent scientific papers, ChatGPT is characterized as a great\nopportunity across various fields including the medical domain, but also as a\nthreat concerning ethics and receives mixed assessments for education. Our\ncomprehensive meta-analysis of ChatGPT's current perception after 2.5 months\nsince its release can contribute to shaping the public debate and informing its\nfuture development. We make our data available.\n",
                "链接": "https://arxiv.org/abs/2302.13795"
            },
            {
                "文章ID": "55508",
                "标题": "\"Real Attackers Don't Compute Gradients\": Bridging the Gap Between\n  Adversarial ML Research and Practice",
                "作者": " Giovanni Apruzzese,  Hyrum S. Anderson,  Savino Dambra,  David Freeman,  Fabio Pierazzi,  Kevin A. Roundy",
                "发布日期": "2023-01-02",
                "摘要": "  Recent years have seen a proliferation of research on adversarial machine\nlearning. Numerous papers demonstrate powerful algorithmic attacks against a\nwide variety of machine learning (ML) models, and numerous other papers propose\ndefenses that can withstand most attacks. However, abundant real-world evidence\nsuggests that actual attackers use simple tactics to subvert ML-driven systems,\nand as a result security practitioners have not prioritized adversarial ML\ndefenses.\n  Motivated by the apparent gap between researchers and practitioners, this\nposition paper aims to bridge the two domains. We first present three\nreal-world case studies from which we can glean practical insights unknown or\nneglected in research. Next we analyze all adversarial ML papers recently\npublished in top security conferences, highlighting positive trends and blind\nspots. Finally, we state positions on precise and cost-driven threat modeling,\ncollaboration between industry and academia, and reproducible research. We\nbelieve that our positions, if adopted, will increase the real-world impact of\nfuture endeavours in adversarial ML, bringing both researchers and\npractitioners closer to their shared goal of improving the security of ML\nsystems.\n",
                "链接": "https://arxiv.org/abs/2212.14315"
            },
            {
                "文章ID": "32588",
                "标题": "Eight Years of Face Recognition Research: Reproducibility, Achievements\n  and Open Issues",
                "作者": " Tiago de Freitas Pereira,  Dominic Schmidli,  Yu Linghu,  Xinyi Zhang,  Sébastien Marcel,  Manuel Günther",
                "发布日期": "2022-08-10",
                "摘要": "  Automatic face recognition is a research area with high popularity. Many\ndifferent face recognition algorithms have been proposed in the last thirty\nyears of intensive research in the field. With the popularity of deep learning\nand its capability to solve a huge variety of different problems, face\nrecognition researchers have concentrated effort on creating better models\nunder this paradigm. From the year 2015, state-of-the-art face recognition has\nbeen rooted in deep learning models. Despite the availability of large-scale\nand diverse datasets for evaluating the performance of face recognition\nalgorithms, many of the modern datasets just combine different factors that\ninfluence face recognition, such as face pose, occlusion, illumination, facial\nexpression and image quality. When algorithms produce errors on these datasets,\nit is not clear which of the factors has caused this error and, hence, there is\nno guidance in which direction more research is required. This work is a\nfollowup from our previous works developed in 2014 and eventually published in\n2016, showing the impact of various facial aspects on face recognition\nalgorithms. By comparing the current state-of-the-art with the best systems\nfrom the past, we demonstrate that faces under strong occlusions, some types of\nillumination, and strong expressions are problems mastered by deep learning\nalgorithms, whereas recognition with low-resolution images, extreme pose\nvariations, and open-set recognition is still an open problem. To show this, we\nrun a sequence of experiments using six different datasets and five different\nface recognition algorithms in an open-source and reproducible manner. We\nprovide the source code to run all of our experiments, which is easily\nextensible so that utilizing your own deep network in our evaluation is just a\nfew minutes away.\n",
                "链接": "https://arxiv.org/abs/2208.04040"
            },
            {
                "文章ID": "7190",
                "标题": "Automated Extraction of Energy Systems Information from Remotely Sensed\n  Data: A Review and Analysis",
                "作者": " Simiao Ren,  Wei Hu,  Kyle Bradbury,  Dylan Harrison-Atlas,  Laura Malaguzzi Valeri,  Brian Murray,  Jordan M. Malof",
                "发布日期": "2022-10-04",
                "摘要": "  High quality energy systems information is a crucial input to energy systems\nresearch, modeling, and decision-making. Unfortunately, actionable information\nabout energy systems is often of limited availability, incomplete, or only\naccessible for a substantial fee or through a non-disclosure agreement.\nRecently, remotely sensed data (e.g., satellite imagery, aerial photography)\nhave emerged as a potentially rich source of energy systems information.\nHowever, the use of these data is frequently challenged by its sheer volume and\ncomplexity, precluding manual analysis. Recent breakthroughs in machine\nlearning have enabled automated and rapid extraction of useful information from\nremotely sensed data, facilitating large-scale acquisition of critical energy\nsystem variables. Here we present a systematic review of the literature on this\nemerging topic, providing an in-depth survey and review of papers published\nwithin the past two decades. We first taxonomize the existing literature into\nten major areas, spanning the energy value chain. Within each research area, we\ndistill and critically discuss major features that are relevant to energy\nresearchers, including, for example, key challenges regarding the accessibility\nand reliability of the methods. We then synthesize our findings to identify\nlimitations and trends in the literature as a whole, and discuss opportunities\nfor innovation. These include the opportunity to extend the methods beyond\nelectricity to broader energy systems and wider geographic areas; and the\nability to expand the use of these methods in research and decision making as\nsatellite data become cheaper and easier to access. We also find that there are\npersistent challenges: limited standardization and rigor of performance\nassessments; limited sharing of code, which would improve replicability; and a\nlimited consideration of the ethics and privacy of data.\n",
                "链接": "https://arxiv.org/abs/2202.12939"
            },
            {
                "文章ID": "77431",
                "标题": "Privacy-Preserving Taxi-Demand Prediction Using Federated Learning",
                "作者": " Yumeki Goto,  Tomoya Matsumoto,  Hamada Rizk,  Naoto Yanai,  Hirozumi Yamaguchi",
                "发布日期": "2023-05-23",
                "摘要": "  Taxi-demand prediction is an important application of machine learning that\nenables taxi-providing facilities to optimize their operations and city\nplanners to improve transportation infrastructure and services. However, the\nuse of sensitive data in these systems raises concerns about privacy and\nsecurity. In this paper, we propose the use of federated learning for\ntaxi-demand prediction that allows multiple parties to train a machine learning\nmodel on their own data while keeping the data private and secure. This can\nenable organizations to build models on data they otherwise would not be able\nto access. Evaluation with real-world data collected from 16 taxi service\nproviders in Japan over a period of six months showed that the proposed system\ncan predict the demand level accurately within 1\\% error compared to a single\nmodel trained with integrated data.\n",
                "链接": "https://arxiv.org/abs/2305.08107"
            },
            {
                "文章ID": "32341",
                "标题": "Out of the BLEU: how should we assess quality of the Code Generation\n  models?",
                "作者": " Mikhail Evtikhiev,  Egor Bogomolov,  Yaroslav Sokolov,  Timofey Bryksin",
                "发布日期": "2023-05-11",
                "摘要": "  In recent years, researchers have created and introduced a significant number\nof various code generation models. As human evaluation of every new model\nversion is unfeasible, the community adopted automatic evaluation metrics such\nas BLEU to approximate the results of human judgement. These metrics originate\nfrom the machine translation domain and it is unclear whether they are\napplicable for the code generation tasks and how well they agree with the human\nevaluation on this task. There are also other metrics, CodeBLEU and RUBY,\ndeveloped to estimate the similarity of code, that take into account the\nproperties of source code. However, for these metrics there are hardly any\nstudies on their agreement with the human evaluation. Despite all that, minimal\ndifferences in the metric scores have been used in recent papers to claim\nsuperiority of some code generation models over the others.\n  In this paper, we present a study on the applicability of six metrics --\nBLEU, ROUGE-L, METEOR, ChrF, CodeBLEU, and RUBY -- for evaluation of code\ngeneration models. We conduct a study on two different code generation datasets\nand use human annotators to assess the quality of all models run on these\ndatasets. The results indicate that for the CoNaLa dataset of Python\none-liners, none of the metrics can correctly emulate human judgement on which\nmodel is better with >95% certainty if the difference in model scores is less\nthan 5 points. For the HearthStone dataset, which consists of classes of a\nparticular structure, a difference in model scores of at least 2 points is\nenough to claim the superiority of one model over the other. Our findings\nsuggest that the ChrF metric is a better fit for the evaluation of code\ngeneration models than the commonly used BLEU and CodeBLEU. Yet, finding a\nmetric for code generation that closely agrees with humans requires additional\nwork.\n",
                "链接": "https://arxiv.org/abs/2208.03133"
            },
            {
                "文章ID": "88831",
                "标题": "Dataset balancing can hurt model performance",
                "作者": " R. Channing Moore,  Daniel P. W. Ellis,  Eduardo Fonseca,  Shawn Hershey,  Aren Jansen,  Manoj Plakal",
                "发布日期": "2023-07-04",
                "摘要": "  Machine learning from training data with a skewed distribution of examples\nper class can lead to models that favor performance on common classes at the\nexpense of performance on rare ones. AudioSet has a very wide range of priors\nover its 527 sound event classes. Classification performance on AudioSet is\nusually evaluated by a simple average over per-class metrics, meaning that\nperformance on rare classes is equal in importance to the performance on common\nones. Several recent papers have used dataset balancing techniques to improve\nperformance on AudioSet. We find, however, that while balancing improves\nperformance on the public AudioSet evaluation data it simultaneously hurts\nperformance on an unpublished evaluation set collected under the same\nconditions. By varying the degree of balancing, we show that its benefits are\nfragile and depend on the evaluation set. We also do not find evidence\nindicating that balancing improves rare class performance relative to common\nclasses. We therefore caution against blind application of balancing, as well\nas against paying too much attention to small improvements on a public\nevaluation set.\n",
                "链接": "https://arxiv.org/abs/2307.00079"
            },
            {
                "文章ID": "91118",
                "标题": "Visual Analytics For Machine Learning: A Data Perspective Survey",
                "作者": " Junpeng Wang,  Shixia Liu,  Wei Zhang",
                "发布日期": "2023-07-18",
                "摘要": "  The past decade has witnessed a plethora of works that leverage the power of\nvisualization (VIS) to interpret machine learning (ML) models. The\ncorresponding research topic, VIS4ML, keeps growing at a fast pace. To better\norganize the enormous works and shed light on the developing trend of VIS4ML,\nwe provide a systematic review of these works through this survey. Since data\nquality greatly impacts the performance of ML models, our survey focuses\nspecifically on summarizing VIS4ML works from the data perspective. First, we\ncategorize the common data handled by ML models into five types, explain the\nunique features of each type, and highlight the corresponding ML models that\nare good at learning from them. Second, from the large number of VIS4ML works,\nwe tease out six tasks that operate on these types of data (i.e., data-centric\ntasks) at different stages of the ML pipeline to understand, diagnose, and\nrefine ML models. Lastly, by studying the distribution of 143 surveyed papers\nacross the five data types, six data-centric tasks, and their intersections, we\nanalyze the prospective research directions and envision future research\ntrends.\n",
                "链接": "https://arxiv.org/abs/2307.07712"
            },
            {
                "文章ID": "68322",
                "标题": "Is ChatGPT A Good Keyphrase Generator? A Preliminary Study",
                "作者": " Mingyang Song,  Haiyun Jiang,  Shuming Shi,  Songfang Yao,  Shilong Lu,  Yi Feng,  Huafeng Liu,  Liping Jing",
                "发布日期": "2023-12-25",
                "摘要": "  The emergence of ChatGPT has recently garnered significant attention from the\ncomputational linguistics community. To demonstrate its capabilities as a\nkeyphrase generator, we conduct a preliminary evaluation of ChatGPT for the\nkeyphrase generation task. We evaluate its performance in various aspects,\nincluding keyphrase generation prompts, keyphrase generation diversity, and\nlong document understanding. Our evaluation is based on six benchmark datasets,\nand we adopt the prompt suggested by OpenAI while extending it to six candidate\nprompts. We find that ChatGPT performs exceptionally well on all six candidate\nprompts, with minor performance differences observed across the datasets. Based\non our findings, we conclude that ChatGPT has great potential for keyphrase\ngeneration. Moreover, we discover that ChatGPT still faces challenges when it\ncomes to generating absent keyphrases. Meanwhile, in the final section, we also\npresent some limitations and future expansions of this report.\n",
                "链接": "https://arxiv.org/abs/2303.13001"
            },
            {
                "文章ID": "5343",
                "标题": "Measuring \"Why\" in Recommender Systems: a Comprehensive Survey on the\n  Evaluation of Explainable Recommendation",
                "作者": " Xu Chen,  Yongfeng Zhang,  Ji-Rong Wen",
                "发布日期": "2022-02-15",
                "摘要": "  Explainable recommendation has shown its great advantages for improving\nrecommendation persuasiveness, user satisfaction, system transparency, among\nothers. A fundamental problem of explainable recommendation is how to evaluate\nthe explanations. In the past few years, various evaluation strategies have\nbeen proposed. However, they are scattered in different papers, and there lacks\na systematic and detailed comparison between them. To bridge this gap, in this\npaper, we comprehensively review the previous work, and provide different\ntaxonomies for them according to the evaluation perspectives and evaluation\nmethods. Beyond summarizing the previous work, we also analyze the\n(dis)advantages of existing evaluation methods and provide a series of\nguidelines on how to select them. The contents of this survey are based on more\nthan 100 papers from top-tier conferences like IJCAI, AAAI, TheWebConf, Recsys,\nUMAP, and IUI, and their complete summarization are presented at\nhttps://shimo.im/sheets/VKrpYTcwVH6KXgdy/MODOC/. With this survey, we finally\naim to provide a clear and comprehensive review on the evaluation of\nexplainable recommendation.\n",
                "链接": "https://arxiv.org/abs/2202.06466"
            },
            {
                "文章ID": "17532",
                "标题": "Cost-Aware Evaluation and Model Scaling for LiDAR-Based 3D Object\n  Detection",
                "作者": " Xiaofang Wang,  Kris M. Kitani",
                "发布日期": "2023-03-13",
                "摘要": "  Considerable research effort has been devoted to LiDAR-based 3D object\ndetection and empirical performance has been significantly improved. While\nprogress has been encouraging, we observe an overlooked issue: it is not yet\ncommon practice to compare different 3D detectors under the same cost, e.g.,\ninference latency. This makes it difficult to quantify the true performance\ngain brought by recently proposed architecture designs. The goal of this work\nis to conduct a cost-aware evaluation of LiDAR-based 3D object detectors.\nSpecifically, we focus on SECOND, a simple grid-based one-stage detector, and\nanalyze its performance under different costs by scaling its original\narchitecture. Then we compare the family of scaled SECOND with recent 3D\ndetection methods, such as Voxel R-CNN and PV-RCNN++. The results are\nsurprising. We find that, if allowed to use the same latency, SECOND can match\nthe performance of PV-RCNN++, the current state-of-the-art method on the Waymo\nOpen Dataset. Scaled SECOND also easily outperforms many recent 3D detection\nmethods published during the past year. We recommend future research control\nthe inference cost in their empirical comparison and include the family of\nscaled SECOND as a strong baseline when presenting novel 3D detection methods.\n",
                "链接": "https://arxiv.org/abs/2205.01142"
            },
            {
                "文章ID": "110763",
                "标题": "We are Who We Cite: Bridges of Influence Between Natural Language\n  Processing and Other Academic Fields",
                "作者": " Jan Philip Wahle,  Terry Ruas,  Mohamed Abdalla,  Bela Gipp,  Saif M. Mohammad",
                "发布日期": "2023-10-24",
                "摘要": "  Natural Language Processing (NLP) is poised to substantially influence the\nworld. However, significant progress comes hand-in-hand with substantial risks.\nAddressing them requires broad engagement with various fields of study. Yet,\nlittle empirical work examines the state of such engagement (past or current).\nIn this paper, we quantify the degree of influence between 23 fields of study\nand NLP (on each other). We analyzed ~77k NLP papers, ~3.1m citations from NLP\npapers to other papers, and ~1.8m citations from other papers to NLP papers. We\nshow that, unlike most fields, the cross-field engagement of NLP, measured by\nour proposed Citation Field Diversity Index (CFDI), has declined from 0.58 in\n1980 to 0.31 in 2022 (an all-time low). In addition, we find that NLP has grown\nmore insular -- citing increasingly more NLP papers and having fewer papers\nthat act as bridges between fields. NLP citations are dominated by computer\nscience; Less than 8% of NLP citations are to linguistics, and less than 3% are\nto math and psychology. These findings underscore NLP's urgent need to reflect\non its engagement with various fields.\n",
                "链接": "https://arxiv.org/abs/2310.14870"
            },
            {
                "文章ID": "3487",
                "标题": "Calibration of P-values for calibration and for deviation of a\n  subpopulation from the full population",
                "作者": " Mark Tygert",
                "发布日期": "2023-04-11",
                "摘要": "  The author's recent research papers, \"Cumulative deviation of a subpopulation\nfrom the full population\" and \"A graphical method of cumulative differences\nbetween two subpopulations\" (both published in volume 8 of Springer's\nopen-access \"Journal of Big Data\" during 2021), propose graphical methods and\nsummary statistics, without extensively calibrating formal significance tests.\nThe summary metrics and methods can measure the calibration of probabilistic\npredictions and can assess differences in responses between a subpopulation and\nthe full population while controlling for a covariate or score via conditioning\non it. These recently published papers construct significance tests based on\nthe scalar summary statistics, but only sketch how to calibrate the attained\nsignificance levels (also known as \"P-values\") for the tests. The present\narticle reviews and synthesizes work spanning many decades in order to detail\nhow to calibrate the P-values. The present paper presents computationally\nefficient, easily implemented numerical methods for evaluating properly\ncalibrated P-values, together with rigorous mathematical proofs guaranteeing\ntheir accuracy, and illustrates and validates the methods with open-source\nsoftware and numerical examples.\n",
                "链接": "https://arxiv.org/abs/2202.00100"
            },
            {
                "文章ID": "106317",
                "标题": "Recent Methodological Advances in Federated Learning for Healthcare",
                "作者": " Fan Zhang,  Daniel Kreuter,  Yichen Chen,  Sören Dittmer,  Samuel Tull,  Tolou Shadbahr,  BloodCounts! Collaboration,  Jacobus Preller,  James H. F. Rudd,  John A. D. Aston,  Carola-Bibiane Schönlieb,  Nicholas Gleadall,  Michael Roberts",
                "发布日期": "2023-10-05",
                "摘要": "  For healthcare datasets, it is often not possible to combine data samples\nfrom multiple sites due to ethical, privacy or logistical concerns. Federated\nlearning allows for the utilisation of powerful machine learning algorithms\nwithout requiring the pooling of data. Healthcare data has many simultaneous\nchallenges which require new methodologies to address, such as highly-siloed\ndata, class imbalance, missing data, distribution shifts and non-standardised\nvariables. Federated learning adds significant methodological complexity to\nconventional centralised machine learning, requiring distributed optimisation,\ncommunication between nodes, aggregation of models and redistribution of\nmodels. In this systematic review, we consider all papers on Scopus that were\npublished between January 2015 and February 2023 and which describe new\nfederated learning methodologies for addressing challenges with healthcare\ndata. We performed a detailed review of the 89 papers which fulfilled these\ncriteria. Significant systemic issues were identified throughout the literature\nwhich compromise the methodologies in many of the papers reviewed. We give\ndetailed recommendations to help improve the quality of the methodology\ndevelopment for federated learning in healthcare.\n",
                "链接": "https://arxiv.org/abs/2310.02874"
            },
            {
                "文章ID": "50847",
                "标题": "Bayesian Network Models of Causal Interventions in Healthcare Decision\n  Making: Literature Review and Software Evaluation",
                "作者": " Artem Velikzhanin,  Benjie Wang,  Marta Kwiatkowska",
                "发布日期": "2022-11-29",
                "摘要": "  This report summarises the outcomes of a systematic literature search to\nidentify Bayesian network models used to support decision making in healthcare.\nAfter describing the search methodology, the selected research papers are\nbriefly reviewed, with the view to identify publicly available models and\ndatasets that are well suited to analysis using the causal interventional\nanalysis software tool developed in Wang B, Lyle C, Kwiatkowska M (2021).\nFinally, an experimental evaluation of applying the software on a selection of\nmodels is carried out and preliminary results are reported.\n",
                "链接": "https://arxiv.org/abs/2211.15258"
            },
            {
                "文章ID": "125176",
                "标题": "Performance Comparison of Session-based Recommendation Algorithms based\n  on GNNs",
                "作者": " Faisal Shehzad,  Dietmar Jannach",
                "发布日期": "2023-12-29",
                "摘要": "  In session-based recommendation settings, a recommender system has to base\nits suggestions on the user interactions that are ob served in an ongoing\nsession. Since such sessions can consist of only a small set of interactions,\nvarious approaches based on Graph Neural Networks (GNN) were recently proposed,\nas they allow us to integrate various types of side information about the items\nin a natural way. Unfortunately, a variety of evaluation settings are used in\nthe literature, e.g., in terms of protocols, metrics and baselines, making it\ndifficult to assess what represents the state of the art. In this work, we\npresent the results of an evaluation of eight recent GNN-based approaches that\nwere published in high-quality outlets. For a fair comparison, all models are\nsystematically tuned and tested under identical conditions using three common\ndatasets. We furthermore include k-nearest-neighbor and sequential rules-based\nmodels as baselines, as such models have previously exhibited competitive\nperformance results for similar settings. To our surprise, the evaluation\nshowed that the simple models outperform all recent GNN models in terms of the\nMean Reciprocal Rank, which we used as an optimization criterion, and were only\noutperformed in three cases in terms of the Hit Rate. Additional analyses\nfurthermore reveal that several other factors that are often not deeply\ndiscussed in papers, e.g., random seeds, can markedly impact the performance of\nGNN-based models. Our results therefore (a) point to continuing issues in the\ncommunity in terms of research methodology and (b) indicate that there is ample\nroom for improvement in session-based recommendation.\n",
                "链接": "https://arxiv.org/abs/2312.16695"
            },
            {
                "文章ID": "23539",
                "标题": "A Survey on the Fairness of Recommender Systems",
                "作者": " Yifan Wang,  Weizhi Ma,  Min Zhang,  Yiqun Liu,  Shaoping Ma",
                "发布日期": "2022-07-12",
                "摘要": "  Recommender systems are an essential tool to relieve the information overload\nchallenge and play an important role in people's daily lives. Since\nrecommendations involve allocations of social resources (e.g., job\nrecommendation), an important issue is whether recommendations are fair. Unfair\nrecommendations are not only unethical but also harm the long-term interests of\nthe recommender system itself. As a result, fairness issues in recommender\nsystems have recently attracted increasing attention. However, due to multiple\ncomplex resource allocation processes and various fairness definitions, the\nresearch on fairness in recommendation is scattered. To fill this gap, we\nreview over 60 papers published in top conferences/journals, including TOIS,\nSIGIR, and WWW. First, we summarize fairness definitions in the recommendation\nand provide several views to classify fairness issues. Then, we review\nrecommendation datasets and measurements in fairness studies and provide an\nelaborate taxonomy of fairness methods in the recommendation. Finally, we\nconclude this survey by outlining some promising future directions.\n",
                "链接": "https://arxiv.org/abs/2206.03761"
            },
            {
                "文章ID": "96547",
                "标题": "Impression-Aware Recommender Systems",
                "作者": " Fernando B. Pérez Maurera,  Maurizio Ferrari Dacrema,  Pablo Castells,  Paolo Cremonesi",
                "发布日期": "2023-08-16",
                "摘要": "  Novel data sources bring new opportunities to improve the quality of\nrecommender systems. Impressions are a novel data source containing past\nrecommendations (shown items) and traditional interactions. Researchers may use\nimpressions to refine user preferences and overcome the current limitations in\nrecommender systems research. The relevance and interest of impressions have\nincreased over the years; hence, the need for a review of relevant work on this\ntype of recommenders. We present a systematic literature review on recommender\nsystems using impressions, focusing on three fundamental angles in research:\nrecommenders, datasets, and evaluation methodologies. We provide three\ncategorizations of papers describing recommenders using impressions, present\neach reviewed paper in detail, describe datasets with impressions, and analyze\nthe existing evaluation methodologies. Lastly, we present open questions and\nfuture directions of interest, highlighting aspects missing in the literature\nthat can be addressed in future works.\n",
                "链接": "https://arxiv.org/abs/2308.07857"
            },
            {
                "文章ID": "7420",
                "标题": "Did AI get more negative recently?",
                "作者": " Dominik Beese,  Begüm Altunbaş,  Görkem Güzeler,  Steffen Eger",
                "发布日期": "2023-06-30",
                "摘要": "  In this paper, we classify scientific articles in the domain of natural\nlanguage processing (NLP) and machine learning (ML), as core subfields of\nartificial intelligence (AI), into whether (i) they extend the current\nstate-of-the-art by the introduction of novel techniques which beat existing\nmodels or whether (ii) they mainly criticize the existing state-of-the-art,\ni.e. that it is deficient with respect to some property (e.g. wrong evaluation,\nwrong datasets, misleading task specification). We refer to contributions under\n(i) as having a 'positive stance' and contributions under (ii) as having a\n'negative stance' (to related work). We annotate over 1.5 k papers from NLP and\nML to train a SciBERT-based model to automatically predict the stance of a\npaper based on its title and abstract. We then analyse large-scale trends on\nover 41 k papers from the last approximately 35 years in NLP and ML, finding\nthat papers have become substantially more positive over time, but negative\npapers also got more negative and we observe considerably more negative papers\nin recent years. Negative papers are also more influential in terms of\ncitations they receive.\n",
                "链接": "https://arxiv.org/abs/2202.13610"
            },
            {
                "文章ID": "121615",
                "标题": "Team-related Features in Code Review Prediction Models",
                "作者": " Eduardo Witter,  Ingrid Nunes,  Dietmar Jannach",
                "发布日期": "2023-12-12",
                "摘要": "  Modern Code Review (MCR) is an informal tool-assisted quality assurance\npractice. It relies on the asynchronous communication among the authors of code\nchanges and reviewers, who are developers that provide feedback. However, from\ncandidate developers, some are able to provide better feedback than others\ngiven a particular context. The selection of reviewers is thus an important\ntask, which can benefit from automated support. Many approaches have been\nproposed in this direction, using for example data from code review\nrepositories to recommend reviewers. In this paper, we propose the use of\nteam-related features to improve the performance of predictions that are\nhelpful to build code reviewer recommenders, with our target predictions being\nthe identification of reviewers that would participate in a review and the\nprovided amount of feedback. We evaluate the prediction power of these\nfeatures, which are related to code ownership, workload, and team relationship.\nThis evaluation was done by carefully addressing challenges imposed by the MCR\ndomain, such as temporal aspects of the dataset and unbalanced classes.\nMoreover, given that it is currently unknown how much past data is needed for\nbuilding MCR prediction models with acceptable performance, we explore the\namount of past data used to build prediction models. Our results show that,\nindividually, features related to code ownership have the best prediction\npower. However, based on feature selection, we conclude that all proposed\nfeatures together with lines of code can make the best predictions for both\nreviewer participation and amount of feedback. Regarding the amount of past\ndata, the timeframes of 3, 6, 9, and 12 months of data produce similar results.\nTherefore, models can be trained considering short timeframes, thus reducing\nthe computational costs with negligible impact in the prediction performance\n...\n",
                "链接": "https://arxiv.org/abs/2312.06244"
            },
            {
                "文章ID": "32995",
                "标题": "Finding Reusable Machine Learning Components to Build Programming\n  Language Processing Pipelines",
                "作者": " Patrick Flynn,  Tristan Vanderbruggen,  Chunhua Liao,  Pei-Hung Lin,  Murali Emani,  Xipeng Shen",
                "发布日期": "2023-06-19",
                "摘要": "  Programming Language Processing (PLP) using machine learning has made vast\nimprovements in the past few years. Increasingly more people are interested in\nexploring this promising field. However, it is challenging for new researchers\nand developers to find the right components to construct their own machine\nlearning pipelines, given the diverse PLP tasks to be solved, the large number\nof datasets and models being released, and the set of complex compilers or\ntools involved. To improve the findability, accessibility, interoperability and\nreusability (FAIRness) of machine learning components, we collect and analyze a\nset of representative papers in the domain of machine learning-based PLP. We\nthen identify and characterize key concepts including PLP tasks, model\narchitectures and supportive tools. Finally, we show some example use cases of\nleveraging the reusable components to construct machine learning pipelines to\nsolve a set of PLP tasks.\n",
                "链接": "https://arxiv.org/abs/2208.05596"
            },
            {
                "文章ID": "86386",
                "标题": "TOBY: A Tool for Exploring Data in Academic Survey Papers",
                "作者": " Tathagata Chakraborti,  Jungkoo Kang,  Christian Muise,  Sarath Sreedharan,  Michael Walker,  Daniel Szafir,  Tom Williams",
                "发布日期": "2023-06-21",
                "摘要": "  This paper describes TOBY, a visualization tool that helps a user explore the\ncontents of an academic survey paper. The visualization consists of four\ncomponents: a hierarchical view of taxonomic data in the survey, a document\nsimilarity view in the space of taxonomic classes, a network view of citations,\nand a new paper recommendation tool. In this paper, we will discuss these\nfeatures in the context of three separate deployments of the tool.\n",
                "链接": "https://arxiv.org/abs/2306.10051"
            },
            {
                "文章ID": "42172",
                "标题": "Predicting the clinical citation count of biomedical papers using\n  multilayer perceptron neural network",
                "作者": " Xin Li,  Xuli Tang,  Qikai Cheng",
                "发布日期": "2022-10-24",
                "摘要": "  The number of clinical citations received from clinical guidelines or\nclinical trials has been considered as one of the most appropriate indicators\nfor quantifying the clinical impact of biomedical papers. Therefore, the early\nprediction of the clinical citation count of biomedical papers is critical to\nscientific activities in biomedicine, such as research evaluation, resource\nallocation, and clinical translation. In this study, we designed a four-layer\nmultilayer perceptron neural network (MPNN) model to predict the clinical\ncitation count of biomedical papers in the future by using 9,822,620 biomedical\npapers published from 1985 to 2005. We extracted ninety-one paper features from\nthree dimensions as the input of the model, including twenty-one features in\nthe paper dimension, thirty-five in the reference dimension, and thirty-five in\nthe citing paper dimension. In each dimension, the features can be classified\ninto three categories, i.e., the citation-related features, the clinical\ntranslation-related features, and the topic-related features. Besides, in the\npaper dimension, we also considered the features that have previously been\ndemonstrated to be related to the citation counts of research papers. The\nresults showed that the proposed MPNN model outperformed the other five\nbaseline models, and the features in the reference dimension were the most\nimportant.\n",
                "链接": "https://arxiv.org/abs/2210.06346"
            },
            {
                "文章ID": "51717",
                "标题": "Improving astroBERT using Semantic Textual Similarity",
                "作者": " Felix Grezes,  Thomas Allen,  Sergi Blanco-Cuaresma,  Alberto Accomazzi,  Michael J. Kurtz,  Golnaz Shapurian,  Edwin Henneken,  Carolyn S. Grant,  Donna M. Thompson,  Timothy W. Hostetler,  Matthew R. Templeton,  Kelly E. Lockhart,  Shinyi Chen,  Jennifer Koch,  Taylor Jacovich,  Pavlos Protopapas",
                "发布日期": "2022-12-02",
                "摘要": "  The NASA Astrophysics Data System (ADS) is an essential tool for researchers\nthat allows them to explore the astronomy and astrophysics scientific\nliterature, but it has yet to exploit recent advances in natural language\nprocessing. At ADASS 2021, we introduced astroBERT, a machine learning language\nmodel tailored to the text used in astronomy papers in ADS. In this work we:\n  - announce the first public release of the astroBERT language model;\n  - show how astroBERT improves over existing public language models on\nastrophysics specific tasks;\n  - and detail how ADS plans to harness the unique structure of scientific\npapers, the citation graph and citation context, to further improve astroBERT.\n",
                "链接": "https://arxiv.org/abs/2212.00744"
            },
            {
                "文章ID": "18332",
                "标题": "Results of the NeurIPS'21 Challenge on Billion-Scale Approximate Nearest\n  Neighbor Search",
                "作者": " Harsha Vardhan Simhadri,  George Williams,  Martin Aumüller,  Matthijs Douze,  Artem Babenko,  Dmitry Baranchuk,  Qi Chen,  Lucas Hosseini,  Ravishankar Krishnaswamy,  Gopal Srinivasa,  Suhas Jayaram Subramanya,  Jingdong Wang",
                "发布日期": "2022-05-10",
                "摘要": "  Despite the broad range of algorithms for Approximate Nearest Neighbor\nSearch, most empirical evaluations of algorithms have focused on smaller\ndatasets, typically of 1 million points~\\citep{Benchmark}. However, deploying\nrecent advances in embedding based techniques for search, recommendation and\nranking at scale require ANNS indices at billion, trillion or larger scale.\nBarring a few recent papers, there is limited consensus on which algorithms are\neffective at this scale vis-\\`a-vis their hardware cost.\n  This competition compares ANNS algorithms at billion-scale by hardware cost,\naccuracy and performance. We set up an open source evaluation framework and\nleaderboards for both standardized and specialized hardware. The competition\ninvolves three tracks. The standard hardware track T1 evaluates algorithms on\nan Azure VM with limited DRAM, often the bottleneck in serving billion-scale\nindices, where the embedding data can be hundreds of GigaBytes in size. It uses\nFAISS~\\citep{Faiss17} as the baseline. The standard hardware track T2\nadditional allows inexpensive SSDs in addition to the limited DRAM and uses\nDiskANN~\\citep{DiskANN19} as the baseline. The specialized hardware track T3\nallows any hardware configuration, and again uses FAISS as the baseline.\n  We compiled six diverse billion-scale datasets, four newly released for this\ncompetition, that span a variety of modalities, data types, dimensions, deep\nlearning models, distance functions and sources. The outcome of the competition\nwas ranked leaderboards of algorithms in each track based on recall at a query\nthroughput threshold. Additionally, for track T3, separate leaderboards were\ncreated based on recall as well as cost-normalized and power-normalized query\nthroughput.\n",
                "链接": "https://arxiv.org/abs/2205.03763"
            },
            {
                "文章ID": "66477",
                "标题": "Recent Advances and Applications of Machine Learning in Experimental\n  Solid Mechanics: A Review",
                "作者": " Hanxun Jin,  Enrui Zhang,  Horacio D. Espinosa",
                "发布日期": "2023-09-07",
                "摘要": "  For many decades, experimental solid mechanics has played a crucial role in\ncharacterizing and understanding the mechanical properties of natural and novel\nmaterials. Recent advances in machine learning (ML) provide new opportunities\nfor the field, including experimental design, data analysis, uncertainty\nquantification, and inverse problems. As the number of papers published in\nrecent years in this emerging field is exploding, it is timely to conduct a\ncomprehensive and up-to-date review of recent ML applications in experimental\nsolid mechanics. Here, we first provide an overview of common ML algorithms and\nterminologies that are pertinent to this review, with emphasis placed on\nphysics-informed and physics-based ML methods. Then, we provide thorough\ncoverage of recent ML applications in traditional and emerging areas of\nexperimental mechanics, including fracture mechanics, biomechanics, nano- and\nmicro-mechanics, architected materials, and 2D material. Finally, we highlight\nsome current challenges of applying ML to multi-modality and multi-fidelity\nexperimental datasets and propose several future research directions. This\nreview aims to provide valuable insights into the use of ML methods as well as\na variety of examples for researchers in solid mechanics to integrate into\ntheir experiments.\n",
                "链接": "https://arxiv.org/abs/2303.07647"
            },
            {
                "文章ID": "96219",
                "标题": "Bridging Offline-Online Evaluation with a Time-dependent and Popularity\n  Bias-free Offline Metric for Recommenders",
                "作者": " Petr Kasalický,  Rodrigo Alves,  Pavel Kordík",
                "发布日期": "2023-08-15",
                "摘要": "  The evaluation of recommendation systems is a complex task. The offline and\nonline evaluation metrics for recommender systems are ambiguous in their true\nobjectives. The majority of recently published papers benchmark their methods\nusing ill-posed offline evaluation methodology that often fails to predict true\nonline performance. Because of this, the impact that academic research has on\nthe industry is reduced. The aim of our research is to investigate and compare\nthe online performance of offline evaluation metrics. We show that penalizing\npopular items and considering the time of transactions during the evaluation\nsignificantly improves our ability to choose the best recommendation model for\na live recommender system. Our results, averaged over five large-size\nreal-world live data procured from recommenders, aim to help the academic\ncommunity to understand better offline evaluation and optimization criteria\nthat are more relevant for real applications of recommender systems.\n",
                "链接": "https://arxiv.org/abs/2308.06885"
            },
            {
                "文章ID": "74641",
                "标题": "The Intended Uses of Automated Fact-Checking Artefacts: Why, How and Who",
                "作者": " Michael Schlichtkrull,  Nedjma Ousidhoum,  Andreas Vlachos",
                "发布日期": "2023-11-09",
                "摘要": "  Automated fact-checking is often presented as an epistemic tool that\nfact-checkers, social media consumers, and other stakeholders can use to fight\nmisinformation. Nevertheless, few papers thoroughly discuss how. We document\nthis by analysing 100 highly-cited papers, and annotating epistemic elements\nrelated to intended use, i.e., means, ends, and stakeholders. We find that\nnarratives leaving out some of these aspects are common, that many papers\npropose inconsistent means and ends, and that the feasibility of suggested\nstrategies rarely has empirical backing. We argue that this vagueness actively\nhinders the technology from reaching its goals, as it encourages overclaiming,\nlimits criticism, and prevents stakeholder feedback. Accordingly, we provide\nseveral recommendations for thinking and writing about the use of fact-checking\nartefacts.\n",
                "链接": "https://arxiv.org/abs/2304.14238"
            },
            {
                "文章ID": "83520",
                "标题": "Evaluating and Improving Tool-Augmented Computation-Intensive Math\n  Reasoning",
                "作者": " Beichen Zhang,  Kun Zhou,  Xilin Wei,  Wayne Xin Zhao,  Jing Sha,  Shijin Wang,  Ji-Rong Wen",
                "发布日期": "2023-06-06",
                "摘要": "  Chain-of-thought prompting~(CoT) and tool augmentation have been validated in\nrecent work as effective practices for improving large language models~(LLMs)\nto perform step-by-step reasoning on complex math-related tasks. However, most\nexisting math reasoning datasets may be not able to fully evaluate and analyze\nthe ability of LLMs in manipulating tools and performing reasoning, as they may\nonly require very few invocations of tools or miss annotations for evaluating\nintermediate reasoning steps. To address the issue, we construct \\textbf{CARP},\na new Chinese dataset consisting of 4,886 computation-intensive algebra\nproblems with formulated annotations on intermediate steps. In CARP, we test\nfour LLMs with CoT prompting, and find that they are all prone to make mistakes\nat the early steps of the solution, leading to wrong answers. Based on this\nfinding, we propose a new approach that can deliberate the reasoning steps with\ntool interfaces, namely \\textbf{DELI}. In DELI, we first initialize a\nstep-by-step solution based on retrieved exemplars, then iterate two\ndeliberation procedures that check and refine the intermediate steps of the\ngenerated solution, from the perspectives of tool manipulation and natural\nlanguage reasoning, until obtaining converged solutions or reaching the maximum\nturn. Experimental results on CARP and six other datasets show that the\nproposed DELI mostly outperforms competitive baselines, and can further boost\nthe performance of existing CoT methods. Our data and code are available in\n\\url{https://github.com/RUCAIBox/CARP}.\n",
                "链接": "https://arxiv.org/abs/2306.02408"
            },
            {
                "文章ID": "60474",
                "标题": "A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and\n  Future Trends",
                "作者": " Xiaoye Qu,  Yingjie Gu,  Qingrong Xia,  Zechang Li,  Zhefeng Wang,  Baoxing Huai",
                "发布日期": "2023-08-09",
                "摘要": "  As more and more Arabic texts emerged on the Internet, extracting important\ninformation from these Arabic texts is especially useful. As a fundamental\ntechnology, Named entity recognition (NER) serves as the core component in\ninformation extraction technology, while also playing a critical role in many\nother Natural Language Processing (NLP) systems, such as question answering and\nknowledge graph building. In this paper, we provide a comprehensive review of\nthe development of Arabic NER, especially the recent advances in deep learning\nand pre-trained language model. Specifically, we first introduce the background\nof Arabic NER, including the characteristics of Arabic and existing resources\nfor Arabic NER. Then, we systematically review the development of Arabic NER\nmethods. Traditional Arabic NER systems focus on feature engineering and\ndesigning domain-specific rules. In recent years, deep learning methods achieve\nsignificant progress by representing texts via continuous vector\nrepresentations. With the growth of pre-trained language model, Arabic NER\nyields better performance. Finally, we conclude the method gap between Arabic\nNER and NER methods from other languages, which helps outline future directions\nfor Arabic NER.\n",
                "链接": "https://arxiv.org/abs/2302.03512"
            },
            {
                "文章ID": "119810",
                "标题": "A Comprehensive Study of Vision Transformers in Image Classification\n  Tasks",
                "作者": " Mahmoud Khalil,  Ahmad Khalil,  Alioune Ngom",
                "发布日期": "2023-12-06",
                "摘要": "  Image Classification is a fundamental task in the field of computer vision\nthat frequently serves as a benchmark for gauging advancements in Computer\nVision. Over the past few years, significant progress has been made in image\nclassification due to the emergence of deep learning. However, challenges still\nexist, such as modeling fine-grained visual information, high computation\ncosts, the parallelism of the model, and inconsistent evaluation protocols\nacross datasets. In this paper, we conduct a comprehensive survey of existing\npapers on Vision Transformers for image classification. We first introduce the\npopular image classification datasets that influenced the design of models.\nThen, we present Vision Transformers models in chronological order, starting\nwith early attempts at adapting attention mechanism to vision tasks followed by\nthe adoption of vision transformers, as they have demonstrated success in\ncapturing intricate patterns and long-range dependencies within images.\nFinally, we discuss open problems and shed light on opportunities for image\nclassification to facilitate new research ideas.\n",
                "链接": "https://arxiv.org/abs/2312.01232"
            },
            {
                "文章ID": "34534",
                "标题": "Deepfake: Definitions, Performance Metrics and Standards, Datasets and\n  Benchmarks, and a Meta-Review",
                "作者": " Enes Altuncu,  Virginia N. L. Franqueira,  Shujun Li",
                "发布日期": "2022-08-24",
                "摘要": "  Recent advancements in AI, especially deep learning, have contributed to a\nsignificant increase in the creation of new realistic-looking synthetic media\n(video, image, and audio) and manipulation of existing media, which has led to\nthe creation of the new term ``deepfake''. Based on both the research\nliterature and resources in English and in Chinese, this paper gives a\ncomprehensive overview of deepfake, covering multiple important aspects of this\nemerging concept, including 1) different definitions, 2) commonly used\nperformance metrics and standards, and 3) deepfake-related datasets,\nchallenges, competitions and benchmarks. In addition, the paper also reports a\nmeta-review of 12 selected deepfake-related survey papers published in 2020 and\n2021, focusing not only on the mentioned aspects, but also on the analysis of\nkey challenges and recommendations. We believe that this paper is the most\ncomprehensive review of deepfake in terms of aspects covered, and the first one\ncovering both the English and Chinese literature and sources.\n",
                "链接": "https://arxiv.org/abs/2208.10913"
            },
            {
                "文章ID": "110348",
                "标题": "The Human Behind the Data: Reflections from an Ongoing Co-Design and\n  Deployment of a Data-Navigation Interface for Front-Line Emergency Housing\n  Shelter Staff",
                "作者": " Teale W Masrani,  Helen Ai He,  Geoffrey Messier",
                "发布日期": "2023-10-24",
                "摘要": "  On any night in Canada, at least 35,000 individuals experience homelessness.\nThese individuals use emergency shelters to transition out of homelessness and\ninto permanent housing. We designed and deployed a technology to support\nfront-line staff at the largest emergency housing shelter in Calgary, Canada.\nOver a period of five months in 2022, we worked closely with front-line staff\nto co-design an interface for supporting a holistic understanding of client\ncontext and facilitating decision-making. The tool is currently in-use and our\ncollaboration is ongoing. In this paper, we reflect on preliminary findings\nregarding the second iteration of the tool. We find that supporting shelter\nstaff in understanding the human behind the data was a critical component of\ndesign. This work contributes to literature on how data tools may be integrated\ninto homeless shelters in a way that aligns with shelters' values.\n",
                "链接": "https://arxiv.org/abs/2310.13795"
            },
            {
                "文章ID": "67258",
                "标题": "A Benchmark of PDF Information Extraction Tools using a Multi-Task and\n  Multi-Domain Evaluation Framework for Academic Documents",
                "作者": " Norman Meuschke,  Apurva Jagdale,  Timo Spinde,  Jelena Mitrović,  Bela Gipp",
                "发布日期": "2023-03-20",
                "摘要": "  Extracting information from academic PDF documents is crucial for numerous\nindexing, retrieval, and analysis use cases. Choosing the best tool to extract\nspecific content elements is difficult because many, technically diverse tools\nare available, but recent performance benchmarks are rare. Moreover, such\nbenchmarks typically cover only a few content elements like header metadata or\nbibliographic references and use smaller datasets from specific academic\ndisciplines. We provide a large and diverse evaluation framework that supports\nmore extraction tasks than most related datasets. Our framework builds upon\nDocBank, a multi-domain dataset of 1.5M annotated content elements extracted\nfrom 500K pages of research papers on arXiv. Using the new framework, we\nbenchmark ten freely available tools in extracting document metadata,\nbibliographic references, tables, and other content elements from academic PDF\ndocuments. GROBID achieves the best metadata and reference extraction results,\nfollowed by CERMINE and Science Parse. For table extraction, Adobe Extract\noutperforms other tools, even though the performance is much lower than for\nother content elements. All tools struggle to extract lists, footers, and\nequations. We conclude that more research on improving and combining tools is\nnecessary to achieve satisfactory extraction quality for most content elements.\nEvaluation datasets and frameworks like the one we present support this line of\nresearch. We make our data and code publicly available to contribute toward\nthis goal.\n",
                "链接": "https://arxiv.org/abs/2303.09957"
            },
            {
                "文章ID": "59115",
                "标题": "LongEval: Guidelines for Human Evaluation of Faithfulness in Long-form\n  Summarization",
                "作者": " Kalpesh Krishna,  Erin Bransom,  Bailey Kuehl,  Mohit Iyyer,  Pradeep Dasigi,  Arman Cohan,  Kyle Lo",
                "发布日期": "2023-02-01",
                "摘要": "  While human evaluation remains best practice for accurately judging the\nfaithfulness of automatically-generated summaries, few solutions exist to\naddress the increased difficulty and workload when evaluating long-form\nsummaries. Through a survey of 162 papers on long-form summarization, we first\nshed light on current human evaluation practices surrounding long-form\nsummaries. We find that 73% of these papers do not perform any human evaluation\non model-generated summaries, while other works face new difficulties that\nmanifest when dealing with long documents (e.g., low inter-annotator\nagreement). Motivated by our survey, we present LongEval, a set of guidelines\nfor human evaluation of faithfulness in long-form summaries that addresses the\nfollowing challenges: (1) How can we achieve high inter-annotator agreement on\nfaithfulness scores? (2) How can we minimize annotator workload while\nmaintaining accurate faithfulness scores? and (3) Do humans benefit from\nautomated alignment between summary and source snippets? We deploy LongEval in\nannotation studies on two long-form summarization datasets in different domains\n(SQuALITY and PubMed), and we find that switching to a finer granularity of\njudgment (e.g., clause-level) reduces inter-annotator variance in faithfulness\nscores (e.g., std-dev from 18.5 to 6.8). We also show that scores from a\npartial annotation of fine-grained units highly correlates with scores from a\nfull annotation workload (0.89 Kendall's tau using 50% judgments). We release\nour human judgments, annotation templates, and our software as a Python library\nfor future research.\n",
                "链接": "https://arxiv.org/abs/2301.13298"
            },
            {
                "文章ID": "74275",
                "标题": "Methods and datasets for segmentation of minimally invasive surgical\n  instruments in endoscopic images and videos: A review of the state of the art",
                "作者": "2 and 3  Tobias Rueckert, 2 and 3  Daniel Rueckert, 1 and\n  4  Christoph Palm",
                "发布日期": "2023-08-24",
                "摘要": "  In the field of computer- and robot-assisted minimally invasive surgery,\nenormous progress has been made in recent years based on the recognition of\nsurgical instruments in endoscopic images and videos. In particular, the\ndetermination of the position and type of instruments is of great interest.\nCurrent work involves both spatial and temporal information, with the idea that\npredicting the movement of surgical tools over time may improve the quality of\nthe final segmentations. The provision of publicly available datasets has\nrecently encouraged the development of new methods, mainly based on deep\nlearning. In this review, we identify and characterize datasets used for method\ndevelopment and evaluation and quantify their frequency of use in the\nliterature. We further present an overview of the current state of research\nregarding the segmentation and tracking of minimally invasive surgical\ninstruments in endoscopic images and videos. The paper focuses on methods that\nwork purely visually, without markers of any kind attached to the instruments,\nconsidering both single-frame semantic and instance segmentation approaches, as\nwell as those that incorporate temporal information. The publications analyzed\nwere identified through the platforms Google Scholar, Web of Science, and\nPubMed. The search terms used were \"instrument segmentation\", \"instrument\ntracking\", \"surgical tool segmentation\", and \"surgical tool tracking\",\nresulting in a total of 741 articles published between 01/2015 and 07/2023, of\nwhich 123 were included using systematic selection criteria. A discussion of\nthe reviewed literature is provided, highlighting existing shortcomings and\nemphasizing the available potential for future developments.\n",
                "链接": "https://arxiv.org/abs/2304.13014"
            },
            {
                "文章ID": "109104",
                "标题": "Looping LOCI: Developing Object Permanence from Videos",
                "作者": " Manuel Traub,  Frederic Becker,  Sebastian Otte,  Martin V. Butz",
                "发布日期": "2023-10-17",
                "摘要": "  Recent compositional scene representation learning models have become\nremarkably good in segmenting and tracking distinct objects within visual\nscenes. Yet, many of these models require that objects are continuously, at\nleast partially, visible. Moreover, they tend to fail on intuitive physics\ntests, which infants learn to solve over the first months of their life. Our\ngoal is to advance compositional scene representation algorithms with an\nembedded algorithm that fosters the progressive learning of intuitive physics,\nakin to infant development. As a fundamental component for such an algorithm,\nwe introduce Loci-Looped, which advances a recently published unsupervised\nobject location, identification, and tracking neural network architecture\n(Loci, Traub et al., ICLR 2023) with an internal processing loop. The loop is\ndesigned to adaptively blend pixel-space information with anticipations\nyielding information-fused activities as percepts. Moreover, it is designed to\nlearn compositional representations of both individual object dynamics and\nbetween-objects interaction dynamics. We show that Loci-Looped learns to track\nobjects through extended periods of object occlusions, indeed simulating their\nhidden trajectories and anticipating their reappearance, without the need for\nan explicit history buffer. We even find that Loci-Looped surpasses\nstate-of-the-art models on the ADEPT and the CLEVRER dataset, when confronted\nwith object occlusions or temporary sensory data interruptions. This indicates\nthat Loci-Looped is able to learn the physical concepts of object permanence\nand inertia in a fully unsupervised emergent manner. We believe that even\nfurther architectural advancements of the internal loop - also in other\ncompositional scene representation learning models - can be developed in the\nnear future.\n",
                "链接": "https://arxiv.org/abs/2310.10372"
            },
            {
                "文章ID": "120788",
                "标题": "Collaboration or Corporate Capture? Quantifying NLP's Reliance on\n  Industry Artifacts and Contributions",
                "作者": " Will Aitken,  Mohamed Abdalla,  Karen Rudie,  Catherine Stinson",
                "发布日期": "2023-12-08",
                "摘要": "  The advent of transformers, higher computational budgets, and big data has\nengendered remarkable progress in Natural Language Processing (NLP). Impressive\nperformance of industry pre-trained models has garnered public attention in\nrecent years and made news headlines. That these are industry models is\nnoteworthy. Rarely, if ever, are academic institutes producing exciting new NLP\nmodels. Using these models is critical for competing on NLP benchmarks and\ncorrespondingly to stay relevant in NLP research. We surveyed 100 papers\npublished at EMNLP 2022 to determine whether this phenomenon constitutes a\nreliance on industry for NLP publications.\n  We find that there is indeed a substantial reliance. Citations of industry\nartifacts and contributions across categories is at least three times greater\nthan industry publication rates per year. Quantifying this reliance does not\nsettle how we ought to interpret the results. We discuss two possible\nperspectives in our discussion: 1) Is collaboration with industry still\ncollaboration in the absence of an alternative? Or 2) has free NLP inquiry been\ncaptured by the motivations and research direction of private corporations?\n",
                "链接": "https://arxiv.org/abs/2312.03912"
            },
            {
                "文章ID": "90190",
                "标题": "A Call to Reflect on Evaluation Practices for Age Estimation:\n  Comparative Analysis of the State-of-the-Art and a Unified Benchmark",
                "作者": " Jakub Paplham,  Vojtech Franc",
                "发布日期": "2023-09-13",
                "摘要": "  Comparing different age estimation methods poses a challenge due to the\nunreliability of published results stemming from inconsistencies in the\nbenchmarking process. Previous studies have reported continuous performance\nimprovements over the past decade using specialized methods; however, our\nfindings challenge these claims. This paper identifies two trivial, yet\npersistent issues with the currently used evaluation protocol and describes how\nto resolve them. We describe our evaluation protocol in detail and provide\nspecific examples of how the protocol should be used. We utilize the protocol\nto offer an extensive comparative analysis for state-of-the-art facial age\nestimation methods. Surprisingly, we find that the performance differences\nbetween the methods are negligible compared to the effect of other factors,\nsuch as facial alignment, facial coverage, image resolution, model\narchitecture, or the amount of data used for pretraining. We use the gained\ninsights to propose using FaRL as the backbone model and demonstrate its\nefficiency. The results emphasize the importance of consistent data\npreprocessing practices for reliable and meaningful comparisons. We make our\nsource code public at\nhttps://github.com/paplhjak/Facial-Age-Estimation-Benchmark.\n",
                "链接": "https://arxiv.org/abs/2307.04570"
            },
            {
                "文章ID": "76899",
                "标题": "WEIRD FAccTs: How Western, Educated, Industrialized, Rich, and\n  Democratic is FAccT?",
                "作者": " Ali Akbar Septiandri,  Marios Constantinides,  Mohammad Tahaei,  Daniele Quercia",
                "发布日期": "2023-05-12",
                "摘要": "  Studies conducted on Western, Educated, Industrialized, Rich, and Democratic\n(WEIRD) samples are considered atypical of the world's population and may not\naccurately represent human behavior. In this study, we aim to quantify the\nextent to which the ACM FAccT conference, the leading venue in exploring\nArtificial Intelligence (AI) systems' fairness, accountability, and\ntransparency, relies on WEIRD samples. We collected and analyzed 128 papers\npublished between 2018 and 2022, accounting for 30.8% of the overall\nproceedings published at FAccT in those years (excluding abstracts, tutorials,\nand papers without human-subject studies or clear country attribution for the\nparticipants). We found that 84% of the analyzed papers were exclusively based\non participants from Western countries, particularly exclusively from the U.S.\n(63%). Only researchers who undertook the effort to collect data about local\nparticipants through interviews or surveys added diversity to an otherwise\nU.S.-centric view of science. Therefore, we suggest that researchers collect\ndata from under-represented populations to obtain an inclusive worldview. To\nachieve this goal, scientific communities should champion data collection from\nsuch populations and enforce transparent reporting of data biases.\n",
                "链接": "https://arxiv.org/abs/2305.06415"
            },
            {
                "文章ID": "87391",
                "标题": "A Decade of Scholarly Research on Open Knowledge Graphs",
                "作者": " Houcemeddine Turki,  Abraham Toluwase Owodunni,  Mohamed Ali Hadj Taieb,  René Fabrice Bile,  Mohamed Ben Aouicha",
                "发布日期": "2023-09-08",
                "摘要": "  The proliferation of open knowledge graphs has led to a surge in scholarly\nresearch on the topic over the past decade. This paper presents a bibliometric\nanalysis of the scholarly literature on open knowledge graphs published between\n2013 and 2023. The study aims to identify the trends, patterns, and impact of\nresearch in this field, as well as the key topics and research questions that\nhave emerged. The work uses bibliometric techniques to analyze a sample of 4445\nscholarly articles retrieved from Scopus. The findings reveal an\never-increasing number of publications on open knowledge graphs published every\nyear, particularly in developed countries (+50 per year). These outputs are\npublished in highly-referred scholarly journals and conferences. The study\nidentifies three main research themes: (1) knowledge graph construction and\nenrichment, (2) evaluation and reuse, and (3) fusion of knowledge graphs into\nNLP systems. Within these themes, the study identifies specific tasks that have\nreceived considerable attention, including entity linking, knowledge graph\nembedding, and graph neural networks.\n",
                "链接": "https://arxiv.org/abs/2306.13186"
            }
        ]
    },
    {
        "question": {
            "question": "请找到缓和噪声标签影响的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "111388",
                "标题": "Label Propagation for Graph Label Noise",
                "作者": " Yao Cheng,  Caihua Shan,  Yifei Shen,  Xiang Li,  Siqiang Luo,  Dongsheng Li",
                "发布日期": "2023-10-26",
                "摘要": "  Label noise is a common challenge in large datasets, as it can significantly\ndegrade the generalization ability of deep neural networks. Most existing\nstudies focus on noisy labels in computer vision; however, graph models\nencompass both node features and graph topology as input, and become more\nsusceptible to label noise through message-passing mechanisms. Recently, only a\nfew works have been proposed to tackle the label noise on graphs. One major\nlimitation is that they assume the graph is homophilous and the labels are\nsmoothly distributed. Nevertheless, real-world graphs may contain varying\ndegrees of heterophily or even be heterophily-dominated, leading to the\ninadequacy of current methods. In this paper, we study graph label noise in the\ncontext of arbitrary heterophily, with the aim of rectifying noisy labels and\nassigning labels to previously unlabeled nodes. We begin by conducting two\nempirical analyses to explore the impact of graph homophily on graph label\nnoise. Following observations, we propose a simple yet efficient algorithm,\ndenoted as LP4GLN. Specifically, LP4GLN is an iterative algorithm with three\nsteps: (1) reconstruct the graph to recover the homophily property, (2) utilize\nlabel propagation to rectify the noisy labels, (3) select high-confidence\nlabels to retain for the next iteration. By iterating these steps, we obtain a\nset of correct labels, ultimately achieving high accuracy in the node\nclassification task. The theoretical analysis is also provided to demonstrate\nits remarkable denoising \"effect\". Finally, we conduct experiments on 10\nbenchmark datasets under varying graph heterophily levels and noise types,\ncomparing the performance of LP4GLN with 7 typical baselines. Our results\nillustrate the superior performance of the proposed LP4GLN.\n",
                "链接": "https://arxiv.org/abs/2310.16560"
            },
            {
                "文章ID": "94423",
                "标题": "Feature Noise Boosts DNN Generalization under Label Noise",
                "作者": " Lu Zeng,  Xuan Chen,  Xiaoshuang Shi,  Heng Tao Shen",
                "发布日期": "2023-08-04",
                "摘要": "  The presence of label noise in the training data has a profound impact on the\ngeneralization of deep neural networks (DNNs). In this study, we introduce and\ntheoretically demonstrate a simple feature noise method, which directly adds\nnoise to the features of training data, can enhance the generalization of DNNs\nunder label noise. Specifically, we conduct theoretical analyses to reveal that\nlabel noise leads to weakened DNN generalization by loosening the PAC-Bayes\ngeneralization bound, and feature noise results in better DNN generalization by\nimposing an upper bound on the mutual information between the model weights and\nthe features, which constrains the PAC-Bayes generalization bound. Furthermore,\nto ensure effective generalization of DNNs in the presence of label noise, we\nconduct application analyses to identify the optimal types and levels of\nfeature noise to add for obtaining desirable label noise generalization.\nFinally, extensive experimental results on several popular datasets demonstrate\nthe feature noise method can significantly enhance the label noise\ngeneralization of the state-of-the-art label noise method.\n",
                "链接": "https://arxiv.org/abs/2308.01609"
            },
            {
                "文章ID": "22618",
                "标题": "Robustness to Label Noise Depends on the Shape of the Noise Distribution\n  in Feature Space",
                "作者": " Diane Oyen,  Michal Kucer,  Nick Hengartner,  Har Simrat Singh",
                "发布日期": "2022-06-03",
                "摘要": "  Machine learning classifiers have been demonstrated, both empirically and\ntheoretically, to be robust to label noise under certain conditions -- notably\nthe typical assumption is that label noise is independent of the features given\nthe class label. We provide a theoretical framework that generalizes beyond\nthis typical assumption by modeling label noise as a distribution over feature\nspace. We show that both the scale and the shape of the noise distribution\ninfluence the posterior likelihood; and the shape of the noise distribution has\na stronger impact on classification performance if the noise is concentrated in\nfeature space where the decision boundary can be moved. For the special case of\nuniform label noise (independent of features and the class label), we show that\nthe Bayes optimal classifier for $c$ classes is robust to label noise until the\nratio of noisy samples goes above $\\frac{c-1}{c}$ (e.g. 90% for 10 classes),\nwhich we call the tipping point. However, for the special case of\nclass-dependent label noise (independent of features given the class label),\nthe tipping point can be as low as 50%. Most importantly, we show that when the\nnoise distribution targets decision boundaries (label noise is directly\ndependent on feature space), classification robustness can drop off even at a\nsmall scale of noise. Even when evaluating recent label-noise mitigation\nmethods we see reduced accuracy when label noise is dependent on features.\nThese findings explain why machine learning often handles label noise well if\nthe noise distribution is uniform in feature-space; yet it also points to the\ndifficulty of overcoming label noise when it is concentrated in a region of\nfeature space where a decision boundary can move.\n",
                "链接": "https://arxiv.org/abs/2206.01106"
            },
            {
                "文章ID": "50854",
                "标题": "Establishment of Neural Networks Robust to Label Noise",
                "作者": " Pengwei Yang,  Chongyangzi Teng,  Jack George Mangos",
                "发布日期": "2023-04-25",
                "摘要": "  Label noise is a significant obstacle in deep learning model training. It can\nhave a considerable impact on the performance of image classification models,\nparticularly deep neural networks, which are especially susceptible because\nthey have a strong propensity to memorise noisy labels. In this paper, we have\nexamined the fundamental concept underlying related label noise approaches. A\ntransition matrix estimator has been created, and its effectiveness against the\nactual transition matrix has been demonstrated. In addition, we examined the\nlabel noise robustness of two convolutional neural network classifiers with\nLeNet and AlexNet designs. The two FashionMINIST datasets have revealed the\nrobustness of both models. We are not efficiently able to demonstrate the\ninfluence of the transition matrix noise correction on robustness enhancements\ndue to our inability to correctly tune the complex convolutional neural network\nmodel due to time and computing resource constraints. There is a need for\nadditional effort to fine-tune the neural network model and explore the\nprecision of the estimated transition model in future research.\n",
                "链接": "https://arxiv.org/abs/2211.15279"
            },
            {
                "文章ID": "121624",
                "标题": "Regroup Median Loss for Combating Label Noise",
                "作者": " Fengpeng Li,  Kemou Li,  Jinyu Tian,  Jiantao Zhou",
                "发布日期": "2023-12-12",
                "摘要": "  The deep model training procedure requires large-scale datasets of annotated\ndata. Due to the difficulty of annotating a large number of samples, label\nnoise caused by incorrect annotations is inevitable, resulting in low model\nperformance and poor model generalization. To combat label noise, current\nmethods usually select clean samples based on the small-loss criterion and use\nthese samples for training. Due to some noisy samples similar to clean ones,\nthese small-loss criterion-based methods are still affected by label noise. To\naddress this issue, in this work, we propose Regroup Median Loss (RML) to\nreduce the probability of selecting noisy samples and correct losses of noisy\nsamples. RML randomly selects samples with the same label as the training\nsamples based on a new loss processing method. Then, we combine the stable mean\nloss and the robust median loss through a proposed regrouping strategy to\nobtain robust loss estimation for noisy samples. To further improve the model\nperformance against label noise, we propose a new sample selection strategy and\nbuild a semi-supervised method based on RML. Compared to state-of-the-art\nmethods, for both the traditionally trained and semi-supervised models, RML\nachieves a significant improvement on synthetic and complex real-world\ndatasets. The source code of the paper has been released.\n",
                "链接": "https://arxiv.org/abs/2312.06273"
            },
            {
                "文章ID": "67806",
                "标题": "Dynamics-Aware Loss for Learning with Label Noise",
                "作者": " Xiu-Chuan Li,  Xiaobo Xia,  Fei Zhu,  Tongliang Liu,  Xu-Yao Zhang,  Cheng-Lin Liu",
                "发布日期": "2023-08-08",
                "摘要": "  Label noise poses a serious threat to deep neural networks (DNNs). Employing\nrobust loss functions which reconcile fitting ability with robustness is a\nsimple but effective strategy to handle this problem. However, the widely-used\nstatic trade-off between these two factors contradicts the dynamics of DNNs\nlearning with label noise, leading to inferior performance. Therefore, we\npropose a dynamics-aware loss (DAL) to solve this problem. Considering that\nDNNs tend to first learn beneficial patterns, then gradually overfit harmful\nlabel noise, DAL strengthens the fitting ability initially, then gradually\nimproves robustness. Moreover, at the later stage, to further reduce the\nnegative impact of label noise and combat underfitting simultaneously, we let\nDNNs put more emphasis on easy examples than hard ones and introduce a\nbootstrapping term. Both the detailed theoretical analyses and extensive\nexperimental results demonstrate the superiority of our method. Our source code\ncan be found in https://github.com/XiuchuanLi/DAL.\n",
                "链接": "https://arxiv.org/abs/2303.11562"
            },
            {
                "文章ID": "43661",
                "标题": "Pseudo-Label Noise Suppression Techniques for Semi-Supervised Semantic\n  Segmentation",
                "作者": " Sebastian Scherer,  Robin Schön,  Rainer Lienhart",
                "发布日期": "2022-10-20",
                "摘要": "  Semi-supervised learning (SSL) can reduce the need for large labelled\ndatasets by incorporating unlabelled data into the training. This is\nparticularly interesting for semantic segmentation, where labelling data is\nvery costly and time-consuming. Current SSL approaches use an initially\nsupervised trained model to generate predictions for unlabelled images, called\npseudo-labels, which are subsequently used for training a new model from\nscratch. Since the predictions usually do not come from an error-free neural\nnetwork, they are naturally full of errors. However, training with partially\nincorrect labels often reduce the final model performance. Thus, it is crucial\nto manage errors/noise of pseudo-labels wisely. In this work, we use three\nmechanisms to control pseudo-label noise and errors: (1) We construct a solid\nbase framework by mixing images with cow-patterns on unlabelled images to\nreduce the negative impact of wrong pseudo-labels. Nevertheless, wrong\npseudo-labels still have a negative impact on the performance. Therefore, (2)\nwe propose a simple and effective loss weighting scheme for pseudo-labels\ndefined by the feedback of the model trained on these pseudo-labels. This\nallows us to soft-weight the pseudo-label training examples based on their\ndetermined confidence score during training. (3) We also study the common\npractice to ignore pseudo-labels with low confidence and empirically analyse\nthe influence and effect of pseudo-labels with different confidence ranges on\nSSL and the contribution of pseudo-label filtering to the achievable\nperformance gains. We show that our method performs superior to state\nof-the-art alternatives on various datasets. Furthermore, we show that our\nfindings also transfer to other tasks such as human pose estimation. Our code\nis available at https://github.com/ChristmasFan/SSL_Denoising_Segmentation.\n",
                "链接": "https://arxiv.org/abs/2210.10426"
            },
            {
                "文章ID": "92847",
                "标题": "Label Noise: Correcting a Correction",
                "作者": " William Toner,  Amos Storkey",
                "发布日期": "2023-07-26",
                "摘要": "  Training neural network classifiers on datasets with label noise poses a risk\nof overfitting them to the noisy labels. To address this issue, researchers\nhave explored alternative loss functions that aim to be more robust. However,\nmany of these alternatives are heuristic in nature and still vulnerable to\noverfitting or underfitting. In this work, we propose a more direct approach to\ntackling overfitting caused by label noise. We observe that the presence of\nlabel noise implies a lower bound on the noisy generalised risk. Building upon\nthis observation, we propose imposing a lower bound on the empirical risk\nduring training to mitigate overfitting. Our main contribution is providing\ntheoretical results that yield explicit, easily computable bounds on the\nminimum achievable noisy risk for different loss functions. We empirically\ndemonstrate that using these bounds significantly enhances robustness in\nvarious settings, with virtually no additional computational cost.\n",
                "链接": "https://arxiv.org/abs/2307.13100"
            },
            {
                "文章ID": "83903",
                "标题": "Binary Classification with Instance and Label Dependent Label Noise",
                "作者": " Hyungki Im,  Paul Grigas",
                "发布日期": "2023-06-07",
                "摘要": "  Learning with label dependent label noise has been extensively explored in\nboth theory and practice; however, dealing with instance (i.e., feature) and\nlabel dependent label noise continues to be a challenging task. The difficulty\narises from the fact that the noise rate varies for each instance, making it\nchallenging to estimate accurately. The question of whether it is possible to\nlearn a reliable model using only noisy samples remains unresolved. We answer\nthis question with a theoretical analysis that provides matching upper and\nlower bounds. Surprisingly, our results show that, without any additional\nassumptions, empirical risk minimization achieves the optimal excess risk\nbound. Specifically, we derive a novel excess risk bound proportional to the\nnoise level, which holds in very general settings, by comparing the empirical\nrisk minimizers obtained from clean samples and noisy samples. Second, we show\nthat the minimax lower bound for the 0-1 loss is a constant proportional to the\naverage noise rate. Our findings suggest that learning solely with noisy\nsamples is impossible without access to clean samples or strong assumptions on\nthe distribution of the data.\n",
                "链接": "https://arxiv.org/abs/2306.03402"
            },
            {
                "文章ID": "32080",
                "标题": "Noise tolerance of learning to rank under class-conditional label noise",
                "作者": " Dany Haddad",
                "发布日期": "2022-08-18",
                "摘要": "  Often, the data used to train ranking models is subject to label noise. For\nexample, in web-search, labels created from clickstream data are noisy due to\nissues such as insufficient information in item descriptions on the SERP, query\nreformulation by the user, and erratic or unexpected user behavior. In\npractice, it is difficult to handle label noise without making strong\nassumptions about the label generation process. As a result, practitioners\ntypically train their learning-to-rank (LtR) models directly on this noisy data\nwithout additional consideration of the label noise. Surprisingly, we often see\nstrong performance from LtR models trained in this way. In this work, we\ndescribe a class of noise-tolerant LtR losses for which empirical risk\nminimization is a consistent procedure, even in the context of\nclass-conditional label noise. We also develop noise-tolerant analogs of\ncommonly used loss functions. The practical implications of our theoretical\nfindings are further supported by experimental results.\n",
                "链接": "https://arxiv.org/abs/2208.02126"
            }
        ]
    },
    {
        "question": {
            "question": "帮我查找对机器翻译数据集质量进行评估的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "20269",
                "标题": "Prototyping three key properties of specific curiosity in computational\n  reinforcement learning",
                "作者": "University of Alberta Department of Computing Science and Alberta\n  Machine Intelligence Institute  Nadia M. Ady, University of Alberta Department of Computing Science and Alberta\n  Machine Intelligence Institute  Roshan Shariff, University of Alberta Department of Computing Science and Alberta\n  Machine Intelligence Institute  Johannes Günther, University of Alberta Department of Computing Science and Alberta\n  Machine Intelligence Institute  Patrick M. Pilarski",
                "发布日期": "2022-05-24",
                "摘要": "  Curiosity for machine agents has been a focus of intense research. The study\nof human and animal curiosity, particularly specific curiosity, has unearthed\nseveral properties that would offer important benefits for machine learners,\nbut that have not yet been well-explored in machine intelligence. In this work,\nwe introduce three of the most immediate of these properties -- directedness,\ncessation when satisfied, and voluntary exposure -- and show how they may be\nimplemented together in a proof-of-concept reinforcement learning agent;\nfurther, we demonstrate how the properties manifest in the behaviour of this\nagent in a simple non-episodic grid-world environment that includes\ncuriosity-inducing locations and induced targets of curiosity. As we would\nhope, the agent exhibits short-term directed behaviour while updating long-term\npreferences to adaptively seek out curiosity-inducing situations. This work\ntherefore presents a novel view into how specific curiosity operates and in the\nfuture might be integrated into the behaviour of goal-seeking, decision-making\nagents in complex environments.\n",
                "链接": "https://arxiv.org/abs/2205.10407"
            },
            {
                "文章ID": "6201",
                "标题": "PETCI: A Parallel English Translation Dataset of Chinese Idioms",
                "作者": "The University of Chicago  Kenan Tang",
                "发布日期": "2022-02-22",
                "摘要": "  Idioms are an important language phenomenon in Chinese, but idiom translation\nis notoriously hard. Current machine translation models perform poorly on idiom\ntranslation, while idioms are sparse in many translation datasets. We present\nPETCI, a parallel English translation dataset of Chinese idioms, aiming to\nimprove idiom translation by both human and machine. The dataset is built by\nleveraging human and machine effort. Baseline generation models show\nunsatisfactory abilities to improve translation, but structure-aware\nclassification models show good performance on distinguishing good\ntranslations. Furthermore, the size of PETCI can be easily increased without\nexpertise. Overall, PETCI can be helpful to language learners and machine\ntranslation systems.\n",
                "链接": "https://arxiv.org/abs/2202.09509"
            },
            {
                "文章ID": "18582",
                "标题": "ParaCotta: Synthetic Multilingual Paraphrase Corpora from the Most\n  Diverse Translation Sample Pair",
                "作者": " Alham Fikri Aji,  Tirana Noor Fatyanosa,  Radityo Eko Prasojo,  Philip Arthur,  Suci Fitriany,  Salma Qonitah,  Nadhifa Zulfa,  Tomi Santoso,  Mahendra Data",
                "发布日期": "2022-05-11",
                "摘要": "  We release our synthetic parallel paraphrase corpus across 17 languages:\nArabic, Catalan, Czech, German, English, Spanish, Estonian, French, Hindi,\nIndonesian, Italian, Dutch, Romanian, Russian, Swedish, Vietnamese, and\nChinese. Our method relies only on monolingual data and a neural machine\ntranslation system to generate paraphrases, hence simple to apply. We generate\nmultiple translation samples using beam search and choose the most lexically\ndiverse pair according to their sentence BLEU. We compare our generated corpus\nwith the \\texttt{ParaBank2}. According to our evaluation, our synthetic\nparaphrase pairs are semantically similar and lexically diverse.\n",
                "链接": "https://arxiv.org/abs/2205.04651"
            },
            {
                "文章ID": "7306",
                "标题": "OCR Improves Machine Translation for Low-Resource Languages",
                "作者": " Oana Ignat,  Jean Maillard,  Vishrav Chaudhary,  Francisco Guzmán",
                "发布日期": "2022-03-15",
                "摘要": "  We aim to investigate the performance of current OCR systems on low resource\nlanguages and low resource scripts. We introduce and make publicly available a\nnovel benchmark, OCR4MT, consisting of real and synthetic data, enriched with\nnoise, for 60 low-resource languages in low resource scripts. We evaluate\nstate-of-the-art OCR systems on our benchmark and analyse most common errors.\nWe show that OCR monolingual data is a valuable resource that can increase\nperformance of Machine Translation models, when used in backtranslation. We\nthen perform an ablation study to investigate how OCR errors impact Machine\nTranslation performance and determine what is the minimum level of OCR quality\nneeded for the monolingual data to be useful for Machine Translation.\n",
                "链接": "https://arxiv.org/abs/2202.13274"
            },
            {
                "文章ID": "6641",
                "标题": "An Overview on Machine Translation Evaluation",
                "作者": " Lifeng Han",
                "发布日期": "2022-02-23",
                "摘要": "  Since the 1950s, machine translation (MT) has become one of the important\ntasks of AI and development, and has experienced several different periods and\nstages of development, including rule-based methods, statistical methods, and\nrecently proposed neural network-based learning methods. Accompanying these\nstaged leaps is the evaluation research and development of MT, especially the\nimportant role of evaluation methods in statistical translation and neural\ntranslation research. The evaluation task of MT is not only to evaluate the\nquality of machine translation, but also to give timely feedback to machine\ntranslation researchers on the problems existing in machine translation itself,\nhow to improve and how to optimise. In some practical application fields, such\nas in the absence of reference translations, the quality estimation of machine\ntranslation plays an important role as an indicator to reveal the credibility\nof automatically translated target languages. This report mainly includes the\nfollowing contents: a brief history of machine translation evaluation (MTE),\nthe classification of research methods on MTE, and the the cutting-edge\nprogress, including human evaluation, automatic evaluation, and evaluation of\nevaluation methods (meta-evaluation). Manual evaluation and automatic\nevaluation include reference-translation based and reference-translation\nindependent participation; automatic evaluation methods include traditional\nn-gram string matching, models applying syntax and semantics, and deep learning\nmodels; evaluation of evaluation methods includes estimating the credibility of\nhuman evaluations, the reliability of the automatic evaluation, the reliability\nof the test set, etc. Advances in cutting-edge evaluation methods include\ntask-based evaluation, using pre-trained language models based on big data, and\nlightweight optimisation models using distillation techniques.\n",
                "链接": "https://arxiv.org/abs/2202.11027"
            },
            {
                "文章ID": "29403",
                "标题": "Modeling Quality and Machine Learning Pipelines through Extended Feature\n  Models",
                "作者": "University\n  of L'Aquila  Giordano d'Aloisio, University\n  of L'Aquila  Antinisca Di Marco, University\n  of L'Aquila  Giovanni Stilo",
                "发布日期": "2022-07-18",
                "摘要": "  The recently increased complexity of Machine Learning (ML) methods, led to\nthe necessity to lighten both the research and industry development processes.\nML pipelines have become an essential tool for experts of many domains, data\nscientists and researchers, allowing them to easily put together several ML\nmodels to cover the full analytic process starting from raw datasets. Over the\nyears, several solutions have been proposed to automate the building of ML\npipelines, most of them focused on semantic aspects and characteristics of the\ninput dataset. However, an approach taking into account the new quality\nconcerns needed by ML systems (like fairness, interpretability, privacy, etc.)\nis still missing. In this paper, we first identify, from the literature, key\nquality attributes of ML systems. Further, we propose a new engineering\napproach for quality ML pipeline by properly extending the Feature Models\nmeta-model. The presented approach allows to model ML pipelines, their quality\nrequirements (on the whole pipeline and on single phases), and quality\ncharacteristics of algorithms used to implement each pipeline phase. Finally,\nwe demonstrate the expressiveness of our model considering the classification\nproblem.\n",
                "链接": "https://arxiv.org/abs/2207.07528"
            },
            {
                "文章ID": "118650",
                "标题": "Evaluating Optimal Reference Translations",
                "作者": " Vilém Zouhar,  Věra Kloudová,  Martin Popel,  Ondřej Bojar",
                "发布日期": "2023-11-29",
                "摘要": "  The overall translation quality reached by current machine translation (MT)\nsystems for high-resourced language pairs is remarkably good. Standard methods\nof evaluation are not suitable nor intended to uncover the many translation\nerrors and quality deficiencies that still persist. Furthermore, the quality of\nstandard reference translations is commonly questioned and comparable quality\nlevels have been reached by MT alone in several language pairs. Navigating\nfurther research in these high-resource settings is thus difficult. In this\narticle, we propose a methodology for creating more reliable document-level\nhuman reference translations, called \"optimal reference translations,\" with the\nsimple aim to raise the bar of what should be deemed \"human translation\nquality.\" We evaluate the obtained document-level optimal reference\ntranslations in comparison with \"standard\" ones, confirming a significant\nquality increase and also documenting the relationship between evaluation and\ntranslation editing.\n",
                "链接": "https://arxiv.org/abs/2311.16787"
            },
            {
                "文章ID": "62208",
                "标题": "How Good Are GPT Models at Machine Translation? A Comprehensive\n  Evaluation",
                "作者": " Amr Hendy,  Mohamed Abdelrehim,  Amr Sharaf,  Vikas Raunak,  Mohamed Gabr,  Hitokazu Matsushita,  Young Jin Kim,  Mohamed Afify,  Hany Hassan Awadalla",
                "发布日期": "2023-02-21",
                "摘要": "  Generative Pre-trained Transformer (GPT) models have shown remarkable\ncapabilities for natural language generation, but their performance for machine\ntranslation has not been thoroughly investigated. In this paper, we present a\ncomprehensive evaluation of GPT models for machine translation, covering\nvarious aspects such as quality of different GPT models in comparison with\nstate-of-the-art research and commercial systems, effect of prompting\nstrategies, robustness towards domain shifts and document-level translation. We\nexperiment with eighteen different translation directions involving high and\nlow resource languages, as well as non English-centric translations, and\nevaluate the performance of three GPT models: ChatGPT, GPT3.5\n(text-davinci-003), and text-davinci-002. Our results show that GPT models\nachieve very competitive translation quality for high resource languages, while\nhaving limited capabilities for low resource languages. We also show that\nhybrid approaches, which combine GPT models with other translation systems, can\nfurther enhance the translation quality. We perform comprehensive analysis and\nhuman evaluation to further understand the characteristics of GPT translations.\nWe hope that our paper provides valuable insights for researchers and\npractitioners in the field and helps to better understand the potential and\nlimitations of GPT models for translation.\n",
                "链接": "https://arxiv.org/abs/2302.09210"
            },
            {
                "文章ID": "38064",
                "标题": "The first neural machine translation system for the Erzya language",
                "作者": " David Dale",
                "发布日期": "2022-09-21",
                "摘要": "  We present the first neural machine translation system for translation\nbetween the endangered Erzya language and Russian and the dataset collected by\nus to train and evaluate it. The BLEU scores are 17 and 19 for translation to\nErzya and Russian respectively, and more than half of the translations are\nrated as acceptable by native speakers. We also adapt our model to translate\nbetween Erzya and 10 other languages, but without additional parallel data, the\nquality on these directions remains low. We release the translation models\nalong with the collected text corpus, a new language identification model, and\na multilingual sentence encoder adapted for the Erzya language. These resources\nwill be available at https://github.com/slone-nlp/myv-nmt.\n",
                "链接": "https://arxiv.org/abs/2209.09368"
            },
            {
                "文章ID": "88090",
                "标题": "Quality Estimation of Machine Translated Texts based on Direct Evidence\n  from Training Data",
                "作者": " Vibhuti Kumari,  Narayana Murthy Kavi",
                "发布日期": "2023-06-28",
                "摘要": "  Current Machine Translation systems achieve very good results on a growing\nvariety of language pairs and data sets. However, it is now well known that\nthey produce fluent translation outputs that often can contain important\nmeaning errors. Quality Estimation task deals with the estimation of quality of\ntranslations produced by a Machine Translation system without depending on\nReference Translations. A number of approaches have been suggested over the\nyears. In this paper we show that the parallel corpus used as training data for\ntraining the MT system holds direct clues for estimating the quality of\ntranslations produced by the MT system. Our experiments show that this simple\nand direct method holds promise for quality estimation of translations produced\nby any purely data driven machine translation system.\n",
                "链接": "https://arxiv.org/abs/2306.15399"
            }
        ]
    },
    {
        "question": {
            "question": "新的大模型结构相关探索的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "111910",
                "标题": "Autonomous 3D Exploration in Large-Scale Environments with Dynamic\n  Obstacles",
                "作者": " Emil Wiman,  Ludvig Widén,  Mattias Tiger,  Fredrik Heintz",
                "发布日期": "2023-10-30",
                "摘要": "  Exploration in dynamic and uncertain real-world environments is an open\nproblem in robotics and constitutes a foundational capability of autonomous\nsystems operating in most of the real world. While 3D exploration planning has\nbeen extensively studied, the environments are assumed static or only reactive\ncollision avoidance is carried out. We propose a novel approach to not only\navoid dynamic obstacles but also include them in the plan itself, to exploit\nthe dynamic environment in the agent's favor. The proposed planner, Dynamic\nAutonomous Exploration Planner (DAEP), extends AEP to explicitly plan with\nrespect to dynamic obstacles. To thoroughly evaluate exploration planners in\nsuch settings we propose a new enhanced benchmark suite with several dynamic\nenvironments, including large-scale outdoor environments. DAEP outperform\nstate-of-the-art planners in dynamic and large-scale environments. DAEP is\nshown to be more effective at both exploration and collision avoidance.\n",
                "链接": "https://arxiv.org/abs/2310.17977"
            },
            {
                "文章ID": "105928",
                "标题": "Can large language models provide useful feedback on research papers? A\n  large-scale empirical analysis",
                "作者": " Weixin Liang,  Yuhui Zhang,  Hancheng Cao,  Binglu Wang,  Daisy Ding,  Xinyu Yang,  Kailas Vodrahalli,  Siyu He,  Daniel Smith,  Yian Yin,  Daniel McFarland,  James Zou",
                "发布日期": "2023-10-04",
                "摘要": "  Expert feedback lays the foundation of rigorous research. However, the rapid\ngrowth of scholarly production and intricate knowledge specialization challenge\nthe conventional scientific feedback mechanisms. High-quality peer reviews are\nincreasingly difficult to obtain. Researchers who are more junior or from\nunder-resourced settings have especially hard times getting timely feedback.\nWith the breakthrough of large language models (LLM) such as GPT-4, there is\ngrowing interest in using LLMs to generate scientific feedback on research\nmanuscripts. However, the utility of LLM-generated feedback has not been\nsystematically studied. To address this gap, we created an automated pipeline\nusing GPT-4 to provide comments on the full PDFs of scientific papers. We\nevaluated the quality of GPT-4's feedback through two large-scale studies. We\nfirst quantitatively compared GPT-4's generated feedback with human peer\nreviewer feedback in 15 Nature family journals (3,096 papers in total) and the\nICLR machine learning conference (1,709 papers). The overlap in the points\nraised by GPT-4 and by human reviewers (average overlap 30.85% for Nature\njournals, 39.23% for ICLR) is comparable to the overlap between two human\nreviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The\noverlap between GPT-4 and human reviewers is larger for the weaker papers. We\nthen conducted a prospective user study with 308 researchers from 110 US\ninstitutions in the field of AI and computational biology to understand how\nresearchers perceive feedback generated by our GPT-4 system on their own\npapers. Overall, more than half (57.4%) of the users found GPT-4 generated\nfeedback helpful/very helpful and 82.4% found it more beneficial than feedback\nfrom at least some human reviewers. While our findings show that LLM-generated\nfeedback can help researchers, we also identify several limitations.\n",
                "链接": "https://arxiv.org/abs/2310.01783"
            },
            {
                "文章ID": "72308",
                "标题": "PIE: Personalized Interest Exploration for Large-Scale Recommender\n  Systems",
                "作者": " Khushhall Chandra Mahajan,  Amey Porobo Dharwadker,  Romil Shah,  Simeng Qu,  Gaurav Bang,  Brad Schumitsch",
                "发布日期": "2023-04-17",
                "摘要": "  Recommender systems are increasingly successful in recommending personalized\ncontent to users. However, these systems often capitalize on popular content.\nThere is also a continuous evolution of user interests that need to be\ncaptured, but there is no direct way to systematically explore users'\ninterests. This also tends to affect the overall quality of the recommendation\npipeline as training data is generated from the candidates presented to the\nuser. In this paper, we present a framework for exploration in large-scale\nrecommender systems to address these challenges. It consists of three parts,\nfirst the user-creator exploration which focuses on identifying the best\ncreators that users are interested in, second the online exploration framework\nand third a feed composition mechanism that balances explore and exploit to\nensure optimal prevalence of exploratory videos. Our methodology can be easily\nintegrated into an existing large-scale recommender system with minimal\nmodifications. We also analyze the value of exploration by defining relevant\nmetrics around user-creator connections and understanding how this helps the\noverall recommendation pipeline with strong online gains in creator and\necosystem value. In contrast to the regression on user engagement metrics\ngenerally seen while exploring, our method is able to achieve significant\nimprovements of 3.50% in strong creator connections and 0.85% increase in novel\ncreator connections. Moreover, our work has been deployed in production on\nFacebook Watch, a popular video discovery and sharing platform serving billions\nof users.\n",
                "链接": "https://arxiv.org/abs/2304.06844"
            },
            {
                "文章ID": "103034",
                "标题": "Interactive Distillation of Large Single-Topic Corpora of Scientific\n  Papers",
                "作者": " Nicholas Solovyev,  Ryan Barron,  Manish Bhattarai,  Maksim E. Eren,  Kim O. Rasmussen,  Boian S. Alexandrov",
                "发布日期": "2023-09-20",
                "摘要": "  Highly specific datasets of scientific literature are important for both\nresearch and education. However, it is difficult to build such datasets at\nscale. A common approach is to build these datasets reductively by applying\ntopic modeling on an established corpus and selecting specific topics. A more\nrobust but time-consuming approach is to build the dataset constructively in\nwhich a subject matter expert (SME) handpicks documents. This method does not\nscale and is prone to error as the dataset grows. Here we showcase a new tool,\nbased on machine learning, for constructively generating targeted datasets of\nscientific literature. Given a small initial \"core\" corpus of papers, we build\na citation network of documents. At each step of the citation network, we\ngenerate text embeddings and visualize the embeddings through dimensionality\nreduction. Papers are kept in the dataset if they are \"similar\" to the core or\nare otherwise pruned through human-in-the-loop selection. Additional insight\ninto the papers is gained through sub-topic modeling using SeNMFk. We\ndemonstrate our new tool for literature review by applying it to two different\nfields in machine learning.\n",
                "链接": "https://arxiv.org/abs/2309.10772"
            },
            {
                "文章ID": "92138",
                "标题": "Topics, Authors, and Networks in Large Language Model Research: Trends\n  from a Survey of 17K arXiv Papers",
                "作者": " Rajiv Movva,  Sidhika Balachandar,  Kenny Peng,  Gabriel Agostini,  Nikhil Garg,  Emma Pierson",
                "发布日期": "2023-10-24",
                "摘要": "  Large language model (LLM) research is dramatically impacting society, making\nit essential to understand the topics and values it prioritizes, the authors\nand institutions driving it, and its networks of collaboration. Due to the\nrecent growth of the field, many of these fundamental attributes lack\nsystematic description. We gather, annotate, and analyze a new dataset of\n16,979 LLM-related arXiv papers, focusing on changes in 2023 vs. 2018-2022. We\nshow that LLM research increasingly focuses on societal impacts: the Computers\nand Society sub-arXiv has seen 20x growth in its proportion of LLM-related\npapers in 2023. This change is driven in part by an influx of new authors: a\nmajority of 2023 papers are first-authored by researchers who have not\npreviously written an LLM-related paper, and these papers focus particularly on\napplications and societal considerations. While a handful of companies hold\noutsize influence, academia publishes a much larger fraction of papers than\nindustry overall, and this gap widens in 2023. LLM research is also being\nshaped by social dynamics: there are gender and academic/industry differences\nin the topics authors prioritize, and a stark U.S./China schism in the\ncollaboration network. Overall, our analysis documents how LLM research both\nshapes and is shaped by society, attesting to the necessity of sociotechnical\nlenses; we discuss implications for researchers and policymakers.\n",
                "链接": "https://arxiv.org/abs/2307.10700"
            },
            {
                "文章ID": "40120",
                "标题": "Large-scale machine-learning-assisted exploration of the whole materials\n  space",
                "作者": " Jonathan Schmidt,  Noah Hoffmann,  Hai-Chen Wang,  Pedro Borlido,  Pedro J. M. A. Carriço,  Tiago F. T. Cerqueira,  Silvana Botti,  Miguel A. L. Marques",
                "发布日期": "2022-10-04",
                "摘要": "  Crystal-graph attention networks have emerged recently as remarkable tools\nfor the prediction of thermodynamic stability and materials properties from\nunrelaxed crystal structures. Previous networks trained on two million\nmaterials exhibited, however, strong biases originating from underrepresented\nchemical elements and structural prototypes in the available data. We tackled\nthis issue computing additional data to provide better balance across both\nchemical and crystal-symmetry space. Crystal-graph networks trained with this\nnew data show unprecedented generalization accuracy, and allow for reliable,\naccelerated exploration of the whole space of inorganic compounds. We applied\nthis universal network to perform machine-learning assisted high-throughput\nmaterials searches including 2500 binary and ternary structure prototypes and\nspanning about 1 billion compounds. After validation using density-functional\ntheory, we uncover in total 19512 additional materials on the convex hull of\nthermodynamic stability and ~150000 compounds with a distance of less than 50\nmeV/atom from the hull. Combining again machine learning and ab-initio methods,\nwe finally evaluate the discovered materials for applications as\nsuperconductors, superhard materials, and we look for candidates with large gap\ndeformation potentials, finding several compounds with extreme values of these\nproperties.\n",
                "链接": "https://arxiv.org/abs/2210.00579"
            },
            {
                "文章ID": "83621",
                "标题": "Structure-free Graph Condensation: From Large-scale Graphs to Condensed\n  Graph-free Data",
                "作者": " Xin Zheng,  Miao Zhang,  Chunyang Chen,  Quoc Viet Hung Nguyen,  Xingquan Zhu,  Shirui Pan",
                "发布日期": "2023-10-24",
                "摘要": "  Graph condensation, which reduces the size of a large-scale graph by\nsynthesizing a small-scale condensed graph as its substitution, has immediate\nbenefits for various graph learning tasks. However, existing graph condensation\nmethods rely on the joint optimization of nodes and structures in the condensed\ngraph, and overlook critical issues in effectiveness and generalization\nability. In this paper, we advocate a new Structure-Free Graph Condensation\nparadigm, named SFGC, to distill a large-scale graph into a small-scale graph\nnode set without explicit graph structures, i.e., graph-free data. Our idea is\nto implicitly encode topology structure information into the node attributes in\nthe synthesized graph-free data, whose topology is reduced to an identity\nmatrix. Specifically, SFGC contains two collaborative components: (1) a\ntraining trajectory meta-matching scheme for effectively synthesizing\nsmall-scale graph-free data; (2) a graph neural feature score metric for\ndynamically evaluating the quality of the condensed data. Through training\ntrajectory meta-matching, SFGC aligns the long-term GNN learning behaviors\nbetween the large-scale graph and the condensed small-scale graph-free data,\nensuring comprehensive and compact transfer of informative knowledge to the\ngraph-free data. Afterward, the underlying condensed graph-free data would be\ndynamically evaluated with the graph neural feature score, which is a\nclosed-form metric for ensuring the excellent expressiveness of the condensed\ngraph-free data. Extensive experiments verify the superiority of SFGC across\ndifferent condensation ratios.\n",
                "链接": "https://arxiv.org/abs/2306.02664"
            },
            {
                "文章ID": "21246",
                "标题": "Urban Rhapsody: Large-scale exploration of urban soundscapes",
                "作者": " Joao Rulff,  Fabio Miranda,  Maryam Hosseini,  Marcos Lage,  Mark Cartwright,  Graham Dove,  Juan Bello,  Claudio T. Silva",
                "发布日期": "2023-01-11",
                "摘要": "  Noise is one of the primary quality-of-life issues in urban environments. In\naddition to annoyance, noise negatively impacts public health and educational\nperformance. While low-cost sensors can be deployed to monitor ambient noise\nlevels at high temporal resolutions, the amount of data they produce and the\ncomplexity of these data pose significant analytical challenges. One way to\naddress these challenges is through machine listening techniques, which are\nused to extract features in attempts to classify the source of noise and\nunderstand temporal patterns of a city's noise situation. However, the\noverwhelming number of noise sources in the urban environment and the scarcity\nof labeled data makes it nearly impossible to create classification models with\nlarge enough vocabularies that capture the true dynamism of urban soundscapes\nIn this paper, we first identify a set of requirements in the yet unexplored\ndomain of urban soundscape exploration. To satisfy the requirements and tackle\nthe identified challenges, we propose Urban Rhapsody, a framework that combines\nstate-of-the-art audio representation, machine learning, and visual analytics\nto allow users to interactively create classification models, understand noise\npatterns of a city, and quickly retrieve and label audio excerpts in order to\ncreate a large high-precision annotated database of urban sound recordings. We\ndemonstrate the tool's utility through case studies performed by domain experts\nusing data generated over the five-year deployment of a one-of-a-kind sensor\nnetwork in New York City.\n",
                "链接": "https://arxiv.org/abs/2205.13064"
            },
            {
                "文章ID": "6997",
                "标题": "Matching Papers and Reviewers at Large Conferences",
                "作者": " Kevin Leyton-Brown,   Mausam,  Yatin Nandwani,  Hedayat Zarkoob,  Chris Cameron,  Neil Newman,  Dinesh Raghu",
                "发布日期": "2022-08-08",
                "摘要": "  Peer-reviewed conferences, the main publication venues in CS, rely critically\non matching highly qualified reviewers for each paper. Because of the growing\nscale of these conferences, the tight timelines on which they operate, and a\nrecent surge in explicitly dishonest behavior, there is now no alternative to\nperforming this matching in an automated way. This paper studies a novel\nreviewer-paper matching approach that was recently deployed in the 35th AAAI\nConference on Artificial Intelligence (AAAI 2021), and has since been adopted\n(wholly or partially) by other conferences including ICML 2022, AAAI 2022, and\nIJCAI 2022. This approach has three main elements: (1) collecting and\nprocessing input data to identify problematic matches and generate\nreviewer-paper scores; (2) formulating and solving an optimization problem to\nfind good reviewer-paper matchings; and (3) a two-phase reviewing process that\nshifts reviewing resources away from papers likely to be rejected and towards\npapers closer to the decision boundary. This paper also describes an evaluation\nof these innovations based on an extensive post-hoc analysis on real data --\nincluding a comparison with the matching algorithm used in AAAI's previous\n(2020) iteration -- and supplements this with additional numerical\nexperimentation.\n",
                "链接": "https://arxiv.org/abs/2202.12273"
            },
            {
                "文章ID": "119987",
                "标题": "Divide-and-Conquer Strategy for Large-Scale Dynamic Bayesian Network\n  Structure Learning",
                "作者": " Hui Ouyang,  Cheng Chen,  Ke Tang",
                "发布日期": "2023-12-05",
                "摘要": "  Dynamic Bayesian Networks (DBNs), renowned for their interpretability, have\nbecome increasingly vital in representing complex stochastic processes in\nvarious domains such as gene expression analysis, healthcare, and traffic\nprediction. Structure learning of DBNs from data is challenging, particularly\nfor datasets with thousands of variables. Most current algorithms for DBN\nstructure learning are adaptations from those used in static Bayesian Networks\n(BNs), and are typically focused on small-scale problems. In order to solve\nlarge-scale problems while taking full advantage of existing algorithms, this\npaper introduces a novel divide-and-conquer strategy, originally developed for\nstatic BNs, and adapts it for large-scale DBN structure learning. In this work,\nwe specifically concentrate on 2 Time-sliced Bayesian Networks (2-TBNs), a\nspecial class of DBNs. Furthermore, we leverage the prior knowledge of 2-TBNs\nto enhance the performance of the strategy we introduce. Our approach\nsignificantly improves the scalability and accuracy of 2-TBN structure\nlearning. Experimental results demonstrate the effectiveness of our method,\nshowing substantial improvements over existing algorithms in both computational\nefficiency and structure learning accuracy. On problem instances with more than\n1,000 variables, our approach improves two accuracy metrics by 74.45% and\n110.94% on average , respectively, while reducing runtime by 93.65% on average.\n",
                "链接": "https://arxiv.org/abs/2312.01739"
            }
        ]
    },
    {
        "question": {
            "question": "多模态细粒度图片编辑和生成相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "124621",
                "标题": "FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing",
                "作者": " Mingyuan Zhang,  Huirong Li,  Zhongang Cai,  Jiawei Ren,  Lei Yang,  Ziwei Liu",
                "发布日期": "2023-12-27",
                "摘要": "  Text-driven motion generation has achieved substantial progress with the\nemergence of diffusion models. However, existing methods still struggle to\ngenerate complex motion sequences that correspond to fine-grained descriptions,\ndepicting detailed and accurate spatio-temporal actions. This lack of fine\ncontrollability limits the usage of motion generation to a larger audience. To\ntackle these challenges, we present FineMoGen, a diffusion-based motion\ngeneration and editing framework that can synthesize fine-grained motions, with\nspatial-temporal composition to the user instructions. Specifically, FineMoGen\nbuilds upon diffusion model with a novel transformer architecture dubbed\nSpatio-Temporal Mixture Attention (SAMI). SAMI optimizes the generation of the\nglobal attention template from two perspectives: 1) explicitly modeling the\nconstraints of spatio-temporal composition; and 2) utilizing sparsely-activated\nmixture-of-experts to adaptively extract fine-grained features. To facilitate a\nlarge-scale study on this new fine-grained motion generation task, we\ncontribute the HuMMan-MoGen dataset, which consists of 2,968 videos and 102,336\nfine-grained spatio-temporal descriptions. Extensive experiments validate that\nFineMoGen exhibits superior motion generation quality over state-of-the-art\nmethods. Notably, FineMoGen further enables zero-shot motion editing\ncapabilities with the aid of modern large language models (LLM), which\nfaithfully manipulates motion sequences with fine-grained instructions. Project\nPage: https://mingyuan-zhang.github.io/projects/FineMoGen.html\n",
                "链接": "https://arxiv.org/abs/2312.15004"
            },
            {
                "文章ID": "13260",
                "标题": "Flexible Portrait Image Editing with Fine-Grained Control",
                "作者": " Linlin Liu,  Qian Fu,  Fei Hou,  Ying He",
                "发布日期": "2022-04-05",
                "摘要": "  We develop a new method for portrait image editing, which supports\nfine-grained editing of geometries, colors, lights and shadows using a single\nneural network model. We adopt a novel asymmetric conditional GAN architecture:\nthe generators take the transformed conditional inputs, such as edge maps,\ncolor palette, sliders and masks, that can be directly edited by the user; the\ndiscriminators take the conditional inputs in the way that can guide\ncontrollable image generation more effectively. Taking color editing as an\nexample, we feed color palettes (which can be edited easily) into the\ngenerator, and color maps (which contain positional information of colors) into\nthe discriminator. We also design a region-weighted discriminator so that\nhigher weights are assigned to more important regions, like eyes and skin.\nUsing a color palette, the user can directly specify the desired colors of\nhair, skin, eyes, lip and background. Color sliders allow the user to blend\ncolors in an intuitive manner. The user can also edit lights and shadows by\nmodifying the corresponding masks. We demonstrate the effectiveness of our\nmethod by evaluating it on the CelebAMask-HQ dataset with a wide range of\ntasks, including geometry/color/shadow/light editing, hand-drawn sketch to\nimage translation, and color transfer. We also present ablation studies to\njustify our design.\n",
                "链接": "https://arxiv.org/abs/2204.01318"
            },
            {
                "文章ID": "52126",
                "标题": "Fine-grained Image Editing by Pixel-wise Guidance Using Diffusion Models",
                "作者": " Naoki Matsunaga,  Masato Ishii,  Akio Hayakawa,  Kenji Suzuki,  Takuya Narihira",
                "发布日期": "2023-06-01",
                "摘要": "  Our goal is to develop fine-grained real-image editing methods suitable for\nreal-world applications. In this paper, we first summarize four requirements\nfor these methods and propose a novel diffusion-based image editing framework\nwith pixel-wise guidance that satisfies these requirements. Specifically, we\ntrain pixel-classifiers with a few annotated data and then infer the\nsegmentation map of a target image. Users then manipulate the map to instruct\nhow the image will be edited. We utilize a pre-trained diffusion model to\ngenerate edited images aligned with the user's intention with pixel-wise\nguidance. The effective combination of proposed guidance and other techniques\nenables highly controllable editing with preserving the outside of the edited\narea, which results in meeting our requirements. The experimental results\ndemonstrate that our proposal outperforms the GAN-based method for editing\nquality and speed.\n",
                "链接": "https://arxiv.org/abs/2212.02024"
            },
            {
                "文章ID": "26773",
                "标题": "MACSA: A Multimodal Aspect-Category Sentiment Analysis Dataset with\n  Multimodal Fine-grained Aligned Annotations",
                "作者": " Hao Yang,  Yanyan Zhao,  Jianwei Liu,  Yang Wu,  Bing Qin",
                "发布日期": "2022-06-29",
                "摘要": "  Multimodal fine-grained sentiment analysis has recently attracted increasing\nattention due to its broad applications. However, the existing multimodal\nfine-grained sentiment datasets most focus on annotating the fine-grained\nelements in text but ignore those in images, which leads to the fine-grained\nelements in visual content not receiving the full attention they deserve. In\nthis paper, we propose a new dataset, the Multimodal Aspect-Category Sentiment\nAnalysis (MACSA) dataset, which contains more than 21K text-image pairs. The\ndataset provides fine-grained annotations for both textual and visual content\nand firstly uses the aspect category as the pivot to align the fine-grained\nelements between the two modalities. Based on our dataset, we propose the\nMultimodal ACSA task and a multimodal graph-based aligned model (MGAM), which\nadopts a fine-grained cross-modal fusion method. Experimental results show that\nour method can facilitate the baseline comparison for future research on this\ncorpus. We will make the dataset and code publicly available.\n",
                "链接": "https://arxiv.org/abs/2206.13969"
            },
            {
                "文章ID": "124901",
                "标题": "SERF: Fine-Grained Interactive 3D Segmentation and Editing with Radiance\n  Fields",
                "作者": " Kaichen Zhou,  Lanqing Hong,  Enze Xie,  Yongxin Yang,  Zhenguo Li,  Wei Zhang",
                "发布日期": "2023-12-27",
                "摘要": "  Although significant progress has been made in the field of 2D-based\ninteractive editing, fine-grained 3D-based interactive editing remains\nrelatively unexplored. This limitation can be attributed to two main\nchallenges: the lack of an efficient 3D representation robust to different\nmodifications and the absence of an effective 3D interactive segmentation\nmethod. In this paper, we introduce a novel fine-grained interactive 3D\nsegmentation and editing algorithm with radiance fields, which we refer to as\nSERF. Our method entails creating a neural mesh representation by integrating\nmulti-view algorithms with pre-trained 2D models. Building upon this\nrepresentation, we introduce a novel surface rendering technique that preserves\nlocal information and is robust to deformation. Moreover, this representation\nforms the basis for achieving accurate and interactive 3D segmentation without\nrequiring 3D supervision. Harnessing this representation facilitates a range of\ninteractive 3D editing operations, encompassing tasks such as interactive\ngeometry editing and texture painting. Extensive experiments and visualization\nexamples of editing on both real and synthetic data demonstrate the superiority\nof our method on representation quality and editing ability.\n",
                "链接": "https://arxiv.org/abs/2312.15856"
            },
            {
                "文章ID": "41367",
                "标题": "MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language\n  Representation Learning",
                "作者": " Zijia Zhao,  Longteng Guo,  Xingjian He,  Shuai Shao,  Zehuan Yuan,  Jing Liu",
                "发布日期": "2023-06-16",
                "摘要": "  Multimodal representation learning has shown promising improvements on\nvarious vision-language tasks. Most existing methods excel at building\nglobal-level alignment between vision and language while lacking effective\nfine-grained image-text interaction. In this paper, we propose a jointly masked\nmultimodal modeling method to learn fine-grained multimodal representations.\nOur method performs joint masking on image-text input and integrates both\nimplicit and explicit targets for the masked signals to recover. The implicit\ntarget provides a unified and debiased objective for vision and language, where\nthe model predicts latent multimodal representations of the unmasked input. The\nexplicit target further enriches the multimodal representations by recovering\nhigh-level and semantically meaningful information: momentum visual features of\nimage patches and concepts of word tokens. Through such a masked modeling\nprocess, our model not only learns fine-grained multimodal interaction, but\nalso avoids the semantic gap between high-level representations and low- or\nmid-level prediction targets (e.g. image pixels), thus producing semantically\nrich multimodal representations that perform well on both zero-shot and\nfine-tuned settings. Our pre-trained model (named MAMO) achieves\nstate-of-the-art performance on various downstream vision-language tasks,\nincluding image-text retrieval, visual question answering, visual reasoning,\nand weakly-supervised visual grounding.\n",
                "链接": "https://arxiv.org/abs/2210.04183"
            },
            {
                "文章ID": "69760",
                "标题": "Hierarchical Fine-Grained Image Forgery Detection and Localization",
                "作者": " Xiao Guo,  Xiaohong Liu,  Zhiyuan Ren,  Steven Grosz,  Iacopo Masi,  Xiaoming Liu",
                "发布日期": "2023-03-31",
                "摘要": "  Differences in forgery attributes of images generated in CNN-synthesized and\nimage-editing domains are large, and such differences make a unified image\nforgery detection and localization (IFDL) challenging. To this end, we present\na hierarchical fine-grained formulation for IFDL representation learning.\nSpecifically, we first represent forgery attributes of a manipulated image with\nmultiple labels at different levels. Then we perform fine-grained\nclassification at these levels using the hierarchical dependency between them.\nAs a result, the algorithm is encouraged to learn both comprehensive features\nand inherent hierarchical nature of different forgery attributes, thereby\nimproving the IFDL representation. Our proposed IFDL framework contains three\ncomponents: multi-branch feature extractor, localization and classification\nmodules. Each branch of the feature extractor learns to classify forgery\nattributes at one level, while localization and classification modules segment\nthe pixel-level forgery region and detect image-level forgery, respectively.\nLastly, we construct a hierarchical fine-grained dataset to facilitate our\nstudy. We demonstrate the effectiveness of our method on $7$ different\nbenchmarks, for both tasks of IFDL and forgery attribute classification. Our\nsource code and dataset can be found:\n\\href{https://github.com/CHELSEA234/HiFi_IFDL}{github.com/CHELSEA234/HiFi-IFDL}.\n",
                "链接": "https://arxiv.org/abs/2303.17111"
            },
            {
                "文章ID": "73032",
                "标题": "Text-guided Image-and-Shape Editing and Generation: A Short Survey",
                "作者": " Cheng-Kang Ted Chao,  Yotam Gingold",
                "发布日期": "2023-04-20",
                "摘要": "  Image and shape editing are ubiquitous among digital artworks. Graphics\nalgorithms facilitate artists and designers to achieve desired editing intents\nwithout going through manually tedious retouching. In the recent advance of\nmachine learning, artists' editing intents can even be driven by text, using a\nvariety of well-trained neural networks. They have seen to be receiving an\nextensive success on such as generating photorealistic images, artworks and\nhuman poses, stylizing meshes from text, or auto-completion given image and\nshape priors. In this short survey, we provide an overview over 50 papers on\nstate-of-the-art (text-guided) image-and-shape generation techniques. We start\nwith an overview on recent editing algorithms in the introduction. Then, we\nprovide a comprehensive review on text-guided editing techniques for 2D and 3D\nindependently, where each of its sub-section begins with a brief background\nintroduction. We also contextualize editing algorithms under recent implicit\nneural representations. Finally, we conclude the survey with the discussion\nover existing methods and potential research ideas.\n",
                "链接": "https://arxiv.org/abs/2304.09244"
            },
            {
                "文章ID": "80167",
                "标题": "BLIP-Diffusion: Pre-trained Subject Representation for Controllable\n  Text-to-Image Generation and Editing",
                "作者": " Dongxu Li,  Junnan Li,  Steven C. H. Hoi",
                "发布日期": "2023-06-23",
                "摘要": "  Subject-driven text-to-image generation models create novel renditions of an\ninput subject based on text prompts. Existing models suffer from lengthy\nfine-tuning and difficulties preserving the subject fidelity. To overcome these\nlimitations, we introduce BLIP-Diffusion, a new subject-driven image generation\nmodel that supports multimodal control which consumes inputs of subject images\nand text prompts. Unlike other subject-driven generation models, BLIP-Diffusion\nintroduces a new multimodal encoder which is pre-trained to provide subject\nrepresentation. We first pre-train the multimodal encoder following BLIP-2 to\nproduce visual representation aligned with the text. Then we design a subject\nrepresentation learning task which enables a diffusion model to leverage such\nvisual representation and generates new subject renditions. Compared with\nprevious methods such as DreamBooth, our model enables zero-shot subject-driven\ngeneration, and efficient fine-tuning for customized subject with up to 20x\nspeedup. We also demonstrate that BLIP-Diffusion can be flexibly combined with\nexisting techniques such as ControlNet and prompt-to-prompt to enable novel\nsubject-driven generation and editing applications. Code and models will be\nreleased at\nhttps://github.com/salesforce/LAVIS/tree/main/projects/blip-diffusion. Project\npage at https://dxli94.github.io/BLIP-Diffusion-website/.\n",
                "链接": "https://arxiv.org/abs/2305.14720"
            },
            {
                "文章ID": "96626",
                "标题": "DragNUWA: Fine-grained Control in Video Generation by Integrating Text,\n  Image, and Trajectory",
                "作者": " Shengming Yin,  Chenfei Wu,  Jian Liang,  Jie Shi,  Houqiang Li,  Gong Ming,  Nan Duan",
                "发布日期": "2023-08-17",
                "摘要": "  Controllable video generation has gained significant attention in recent\nyears. However, two main limitations persist: Firstly, most existing works\nfocus on either text, image, or trajectory-based control, leading to an\ninability to achieve fine-grained control in videos. Secondly, trajectory\ncontrol research is still in its early stages, with most experiments being\nconducted on simple datasets like Human3.6M. This constraint limits the models'\ncapability to process open-domain images and effectively handle complex curved\ntrajectories. In this paper, we propose DragNUWA, an open-domain\ndiffusion-based video generation model. To tackle the issue of insufficient\ncontrol granularity in existing works, we simultaneously introduce text, image,\nand trajectory information to provide fine-grained control over video content\nfrom semantic, spatial, and temporal perspectives. To resolve the problem of\nlimited open-domain trajectory control in current research, We propose\ntrajectory modeling with three aspects: a Trajectory Sampler (TS) to enable\nopen-domain control of arbitrary trajectories, a Multiscale Fusion (MF) to\ncontrol trajectories in different granularities, and an Adaptive Training (AT)\nstrategy to generate consistent videos following trajectories. Our experiments\nvalidate the effectiveness of DragNUWA, demonstrating its superior performance\nin fine-grained control in video generation. The homepage link is\n\\url{https://www.microsoft.com/en-us/research/project/dragnuwa/}\n",
                "链接": "https://arxiv.org/abs/2308.08089"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下最近用反事实做数据增强的文章",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "80211",
                "标题": "Large Language Models as Counterfactual Generator: Strengths and\n  Weaknesses",
                "作者": " Yongqi Li,  Mayi Xu,  Xin Miao,  Shen Zhou,  Tieyun Qian",
                "发布日期": "2023-05-25",
                "摘要": "  Large language models (LLMs) have demonstrated remarkable performance in a\nrange of natural language understanding and generation tasks. Yet, their\nability to generate counterfactuals, which can be used for areas like data\naugmentation, remains under-explored. This study aims to investigate the\ncounterfactual generation capabilities of LLMs and analysis factors that\ninfluence this ability. First, we evaluate how effective are LLMs in\ncounterfactual generation through data augmentation experiments for small\nlanguage models (SLMs) across four tasks: sentiment analysis, natural language\ninference, named entity recognition, and relation extraction. While LLMs show\npromising enhancements in various settings, they struggle in complex tasks due\nto their self-limitations and the lack of logical guidance to produce\ncounterfactuals that align with commonsense. Second, our analysis reveals the\npivotal role of providing accurate task definitions and detailed step-by-step\ninstructions to LLMs in generating counterfactuals. Interestingly, we also find\nthat LLMs can generate reasonable counterfactuals even with unreasonable\ndemonstrations, which illustrates that demonstrations are primarily to regulate\nthe output format.This study provides the first comprehensive insight into\ncounterfactual generation abilities of LLMs, and offers a novel perspective on\nutilizing LLMs for data augmentation to enhance SLMs.\n",
                "链接": "https://arxiv.org/abs/2305.14791"
            },
            {
                "文章ID": "64938",
                "标题": "Attribution-Scores and Causal Counterfactuals as Explanations in\n  Artificial Intelligence",
                "作者": " Leopoldo Bertossi",
                "发布日期": "2023-03-24",
                "摘要": "  In this expository article we highlight the relevance of explanations for\nartificial intelligence, in general, and for the newer developments in {\\em\nexplainable AI}, referring to origins and connections of and among different\napproaches. We describe in simple terms, explanations in data management and\nmachine learning that are based on attribution-scores, and counterfactuals as\nfound in the area of causality. We elaborate on the importance of logical\nreasoning when dealing with counterfactuals, and their use for score\ncomputation.\n",
                "链接": "https://arxiv.org/abs/2303.02829"
            },
            {
                "文章ID": "44353",
                "标题": "NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer\n  Data Augmentation",
                "作者": " Phillip Howard,  Gadi Singer,  Vasudev Lal,  Yejin Choi,  Swabha Swayamdipta",
                "发布日期": "2022-10-25",
                "摘要": "  While counterfactual data augmentation offers a promising step towards robust\ngeneralization in natural language processing, producing a set of\ncounterfactuals that offer valuable inductive bias for models remains a\nchallenge. Most existing approaches for producing counterfactuals, manual or\nautomated, rely on small perturbations via minimal edits, resulting in\nsimplistic changes. We introduce NeuroCounterfactuals, designed as loose\ncounterfactuals, allowing for larger edits which result in naturalistic\ngenerations containing linguistic diversity, while still bearing similarity to\nthe original document. Our novel generative approach bridges the benefits of\nconstrained decoding, with those of language model adaptation for sentiment\nsteering. Training data augmentation with our generations results in both\nin-domain and out-of-domain improvements for sentiment classification,\noutperforming even manually curated counterfactuals, under select settings. We\nfurther present detailed analyses to show the advantages of\nNeuroCounterfactuals over approaches involving simple, minimal edits.\n",
                "链接": "https://arxiv.org/abs/2210.12365"
            },
            {
                "文章ID": "78047",
                "标题": "Unveiling the Potential of Counterfactuals Explanations in Employability",
                "作者": " Raphael Mazzine Barbosa de Oliveira,  Sofie Goethals,  Dieter Brughmans,  David Martens",
                "发布日期": "2023-05-18",
                "摘要": "  In eXplainable Artificial Intelligence (XAI), counterfactual explanations are\nknown to give simple, short, and comprehensible justifications for complex\nmodel decisions. However, we are yet to see more applied studies in which they\nare applied in real-world cases. To fill this gap, this study focuses on\nshowing how counterfactuals are applied to employability-related problems which\ninvolve complex machine learning algorithms. For these use cases, we use real\ndata obtained from a public Belgian employment institution (VDAB). The use\ncases presented go beyond the mere application of counterfactuals as\nexplanations, showing how they can enhance decision support, comply with legal\nrequirements, guide controlled changes, and analyze novel insights.\n",
                "链接": "https://arxiv.org/abs/2305.10069"
            },
            {
                "文章ID": "104252",
                "标题": "COCO-Counterfactuals: Automatically Constructed Counterfactual Examples\n  for Image-Text Pairs",
                "作者": " Tiep Le,  Vasudev Lal,  Phillip Howard",
                "发布日期": "2023-11-01",
                "摘要": "  Counterfactual examples have proven to be valuable in the field of natural\nlanguage processing (NLP) for both evaluating and improving the robustness of\nlanguage models to spurious correlations in datasets. Despite their\ndemonstrated utility for NLP, multimodal counterfactual examples have been\nrelatively unexplored due to the difficulty of creating paired image-text data\nwith minimal counterfactual changes. To address this challenge, we introduce a\nscalable framework for automatic generation of counterfactual examples using\ntext-to-image diffusion models. We use our framework to create\nCOCO-Counterfactuals, a multimodal counterfactual dataset of paired image and\ntext captions based on the MS-COCO dataset. We validate the quality of\nCOCO-Counterfactuals through human evaluations and show that existing\nmultimodal models are challenged by our counterfactual image-text pairs.\nAdditionally, we demonstrate the usefulness of COCO-Counterfactuals for\nimproving out-of-domain generalization of multimodal vision-language models via\ntraining data augmentation.\n",
                "链接": "https://arxiv.org/abs/2309.14356"
            },
            {
                "文章ID": "876",
                "标题": "Learning Fair Node Representations with Graph Counterfactual Fairness",
                "作者": " Jing Ma,  Ruocheng Guo,  Mengting Wan,  Longqi Yang,  Aidong Zhang,  Jundong Li",
                "发布日期": "2022-01-12",
                "摘要": "  Fair machine learning aims to mitigate the biases of model predictions\nagainst certain subpopulations regarding sensitive attributes such as race and\ngender. Among the many existing fairness notions, counterfactual fairness\nmeasures the model fairness from a causal perspective by comparing the\npredictions of each individual from the original data and the counterfactuals.\nIn counterfactuals, the sensitive attribute values of this individual had been\nmodified. Recently, a few works extend counterfactual fairness to graph data,\nbut most of them neglect the following facts that can lead to biases: 1) the\nsensitive attributes of each node's neighbors may causally affect the\nprediction w.r.t. this node; 2) the sensitive attributes may causally affect\nother features and the graph structure. To tackle these issues, in this paper,\nwe propose a novel fairness notion - graph counterfactual fairness, which\nconsiders the biases led by the above facts. To learn node representations\ntowards graph counterfactual fairness, we propose a novel framework based on\ncounterfactual data augmentation. In this framework, we generate\ncounterfactuals corresponding to perturbations on each node's and their\nneighbors' sensitive attributes. Then we enforce fairness by minimizing the\ndiscrepancy between the representations learned from the original graph and the\ncounterfactuals for each node. Experiments on both synthetic and real-world\ngraphs show that our framework outperforms the state-of-the-art baselines in\ngraph counterfactual fairness, and also achieves comparable prediction\nperformance.\n",
                "链接": "https://arxiv.org/abs/2201.03662"
            },
            {
                "文章ID": "79559",
                "标题": "Improving Classifier Robustness through Active Generation of Pairwise\n  Counterfactuals",
                "作者": " Ananth Balashankar,  Xuezhi Wang,  Yao Qin,  Ben Packer,  Nithum Thain,  Jilin Chen,  Ed H. Chi,  Alex Beutel",
                "发布日期": "2023-05-24",
                "摘要": "  Counterfactual Data Augmentation (CDA) is a commonly used technique for\nimproving robustness in natural language classifiers. However, one fundamental\nchallenge is how to discover meaningful counterfactuals and efficiently label\nthem, with minimal human labeling cost. Most existing methods either completely\nrely on human-annotated labels, an expensive process which limits the scale of\ncounterfactual data, or implicitly assume label invariance, which may mislead\nthe model with incorrect labels. In this paper, we present a novel framework\nthat utilizes counterfactual generative models to generate a large number of\ndiverse counterfactuals by actively sampling from regions of uncertainty, and\nthen automatically label them with a learned pairwise classifier. Our key\ninsight is that we can more correctly label the generated counterfactuals by\ntraining a pairwise classifier that interpolates the relationship between the\noriginal example and the counterfactual. We demonstrate that with a small\namount of human-annotated counterfactual data (10%), we can generate a\ncounterfactual augmentation dataset with learned labels, that provides an\n18-20% improvement in robustness and a 14-21% reduction in errors on 6\nout-of-domain datasets, comparable to that of a fully human-annotated\ncounterfactual dataset for both sentiment classification and question\nparaphrase tasks.\n",
                "链接": "https://arxiv.org/abs/2305.13535"
            },
            {
                "文章ID": "89468",
                "标题": "Beyond Known Reality: Exploiting Counterfactual Explanations for Medical\n  Research",
                "作者": " Toygar Tanyel,  Serkan Ayvaz,  Bilgin Keserci",
                "发布日期": "2023-10-17",
                "摘要": "  The field of explainability in artificial intelligence (AI) has witnessed a\ngrowing number of studies and increasing scholarly interest. However, the lack\nof human-friendly and individual interpretations in explaining the outcomes of\nmachine learning algorithms has significantly hindered the acceptance of these\nmethods by clinicians in their research and clinical practice. To address this\nissue, our study uses counterfactual explanations to explore the applicability\nof \"what if?\" scenarios in medical research. Our aim is to expand our\nunderstanding of magnetic resonance imaging (MRI) features used for diagnosing\npediatric posterior fossa brain tumors beyond existing boundaries. In our case\nstudy, the proposed concept provides a novel way to examine alternative\ndecision-making scenarios that offer personalized and context-specific\ninsights, enabling the validation of predictions and clarification of\nvariations under diverse circumstances. Additionally, we explore the potential\nuse of counterfactuals for data augmentation and evaluate their feasibility as\nan alternative approach in our medical research case. The results demonstrate\nthe promising potential of using counterfactual explanations to enhance\nacceptance of AI-driven methods in clinical research.\n",
                "链接": "https://arxiv.org/abs/2307.02131"
            },
            {
                "文章ID": "110605",
                "标题": "EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Counterfactual Data\n  Augmentation for Multi-hop Fact Verification",
                "作者": " Yingjie Zhu,  Jiasheng Si,  Yibo Zhao,  Haiyang Zhu,  Deyu Zhou,  Yulan He",
                "发布日期": "2023-10-24",
                "摘要": "  Automatic multi-hop fact verification task has gained significant attention\nin recent years. Despite impressive results, these well-designed models perform\npoorly on out-of-domain data. One possible solution is to augment the training\ndata with counterfactuals, which are generated by minimally altering the causal\nfeatures of the original data. However, current counterfactual data\naugmentation techniques fail to handle multi-hop fact verification due to their\nincapability to preserve the complex logical relationships within multiple\ncorrelated texts. In this paper, we overcome this limitation by developing a\nrationale-sensitive method to generate linguistically diverse and\nlabel-flipping counterfactuals while preserving logical relationships. In\nspecific, the diverse and fluent counterfactuals are generated via an\nExplain-Edit-Generate architecture. Moreover, the checking and filtering\nmodules are proposed to regularize the counterfactual data with logical\nrelations and flipped labels. Experimental results show that the proposed\napproach outperforms the SOTA baselines and can generate linguistically diverse\ncounterfactual data without disrupting their logical relationships.\n",
                "链接": "https://arxiv.org/abs/2310.14508"
            },
            {
                "文章ID": "44752",
                "标题": "PseudoAugment: Learning to Use Unlabeled Data for Data Augmentation in\n  Point Clouds",
                "作者": " Zhaoqi Leng,  Shuyang Cheng,  Benjamin Caine,  Weiyue Wang,  Xiao Zhang,  Jonathon Shlens,  Mingxing Tan,  Dragomir Anguelov",
                "发布日期": "2022-10-25",
                "摘要": "  Data augmentation is an important technique to improve data efficiency and\nsave labeling cost for 3D detection in point clouds. Yet, existing augmentation\npolicies have so far been designed to only utilize labeled data, which limits\nthe data diversity. In this paper, we recognize that pseudo labeling and data\naugmentation are complementary, thus propose to leverage unlabeled data for\ndata augmentation to enrich the training data. In particular, we design three\nnovel pseudo-label based data augmentation policies (PseudoAugments) to fuse\nboth labeled and pseudo-labeled scenes, including frames (PseudoFrame), objecta\n(PseudoBBox), and background (PseudoBackground). PseudoAugments outperforms\npseudo labeling by mitigating pseudo labeling errors and generating diverse\nfused training scenes. We demonstrate PseudoAugments generalize across\npoint-based and voxel-based architectures, different model capacity and both\nKITTI and Waymo Open Dataset. To alleviate the cost of hyperparameter tuning\nand iterative pseudo labeling, we develop a population-based data augmentation\nframework for 3D detection, named AutoPseudoAugment. Unlike previous works that\nperform pseudo-labeling offline, our framework performs PseudoAugments and\nhyperparameter tuning in one shot to reduce computational cost. Experimental\nresults on the large-scale Waymo Open Dataset show our method outperforms\nstate-of-the-art auto data augmentation method (PPBA) and self-training method\n(pseudo labeling). In particular, AutoPseudoAugment is about 3X and 2X data\nefficient on vehicle and pedestrian tasks compared to prior arts. Notably,\nAutoPseudoAugment nearly matches the full dataset training results, with just\n10% of the labeled run segments on the vehicle detection task.\n",
                "链接": "https://arxiv.org/abs/2210.13428"
            }
        ]
    },
    {
        "question": {
            "question": "请找到利用clip做开放词汇检测的论文",
            "type": "5"
        },
        "results": []
    },
    {
        "question": {
            "question": "请找到使用自蒸馏加强目标检测性能的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "14018",
                "标题": "Infusing Knowledge from Wikipedia to Enhance Stance Detection",
                "作者": " Zihao He,  Negar Mokhberian,  Kristina Lerman",
                "发布日期": "2022-04-11",
                "摘要": "  Stance detection infers a text author's attitude towards a target. This is\nchallenging when the model lacks background knowledge about the target. Here,\nwe show how background knowledge from Wikipedia can help enhance the\nperformance on stance detection. We introduce Wikipedia Stance Detection BERT\n(WS-BERT) that infuses the knowledge into stance encoding. Extensive results on\nthree benchmark datasets covering social media discussions and online debates\nindicate that our model significantly outperforms the state-of-the-art methods\non target-specific stance detection, cross-target stance detection, and\nzero/few-shot stance detection.\n",
                "链接": "https://arxiv.org/abs/2204.03839"
            },
            {
                "文章ID": "244",
                "标题": "Local Motion and Contrast Priors Driven Deep Network for Infrared Small\n  Target Super-Resolution",
                "作者": " Xinyi Ying,  Yingqian Wang,  Longguang Wang,  Weidong Sheng,  Li Liu,  Zaiping Lin,  Shilin Zhou",
                "发布日期": "2023-04-05",
                "摘要": "  Infrared small target super-resolution (SR) aims to recover reliable and\ndetailed high-resolution image with high-contrast targets from its\nlow-resolution counterparts. Since the infrared small target lacks color and\nfine structure information, it is significant to exploit the supplementary\ninformation among sequence images to enhance the target. In this paper, we\npropose the first infrared small target SR method named local motion and\ncontrast prior driven deep network (MoCoPnet) to integrate the domain knowledge\nof infrared small target into deep network, which can mitigate the intrinsic\nfeature scarcity of infrared small targets. Specifically, motivated by the\nlocal motion prior in the spatio-temporal dimension, we propose a local\nspatio-temporal attention module to perform implicit frame alignment and\nincorporate the local spatio-temporal information to enhance the local features\n(especially for small targets). Motivated by the local contrast prior in the\nspatial dimension, we propose a central difference residual group to\nincorporate the central difference convolution into the feature extraction\nbackbone, which can achieve center-oriented gradient-aware feature extraction\nto further improve the target contrast. Extensive experiments have demonstrated\nthat our method can recover accurate spatial dependency and improve the target\ncontrast. Comparative results show that MoCoPnet can outperform the\nstate-of-the-art video SR and single image SR methods in terms of both SR\nperformance and target enhancement. Based on the SR results, we further\ninvestigate the influence of SR on infrared small target detection and the\nexperimental results demonstrate that MoCoPnet promotes the detection\nperformance. The code is available at https://github.com/XinyiYing/MoCoPnet.\n",
                "链接": "https://arxiv.org/abs/2201.01014"
            },
            {
                "文章ID": "17577",
                "标题": "Cross Domain Object Detection by Target-Perceived Dual Branch\n  Distillation",
                "作者": " Mengzhe He,  Yali Wang,  Jiaxi Wu,  Yiru Wang,  Hanqing Li,  Bo Li,  Weihao Gan,  Wei Wu,  Yu Qiao",
                "发布日期": "2022-05-04",
                "摘要": "  Cross domain object detection is a realistic and challenging task in the\nwild. It suffers from performance degradation due to large shift of data\ndistributions and lack of instance-level annotations in the target domain.\nExisting approaches mainly focus on either of these two difficulties, even\nthough they are closely coupled in cross domain object detection. To solve this\nproblem, we propose a novel Target-perceived Dual-branch Distillation (TDD)\nframework. By integrating detection branches of both source and target domains\nin a unified teacher-student learning scheme, it can reduce domain shift and\ngenerate reliable supervision effectively. In particular, we first introduce a\ndistinct Target Proposal Perceiver between two domains. It can adaptively\nenhance source detector to perceive objects in a target image, by leveraging\ntarget proposal contexts from iterative cross-attention. Afterwards, we design\na concise Dual Branch Self Distillation strategy for model training, which can\nprogressively integrate complementary object knowledge from different domains\nvia self-distillation in two branches. Finally, we conduct extensive\nexperiments on a number of widely-used scenarios in cross domain object\ndetection. The results show that our TDD significantly outperforms the\nstate-of-the-art methods on all the benchmarks. Our code and model will be\navailable at https://github.com/Feobi1999/TDD.\n",
                "链接": "https://arxiv.org/abs/2205.01291"
            },
            {
                "文章ID": "104503",
                "标题": "STANCE-C3: Domain-adaptive Cross-target Stance Detection via Contrastive\n  Learning and Counterfactual Generation",
                "作者": " Nayoung Kim,  David Mosallanezhad,  Lu Cheng,  Michelle V. Mancenido,  Huan Liu",
                "发布日期": "2023-09-28",
                "摘要": "  Stance detection is the process of inferring a person's position or\nstandpoint on a specific issue to deduce prevailing perceptions toward topics\nof general or controversial interest, such as health policies during the\nCOVID-19 pandemic. Existing models for stance detection are trained to perform\nwell for a single domain (e.g., COVID-19) and a specific target topic (e.g.,\nmasking protocols), but are generally ineffectual in other domains or targets\ndue to distributional shifts in the data. However, constructing\nhigh-performing, domain-specific stance detection models requires an extensive\ncorpus of labeled data relevant to the targeted domain, yet such datasets are\nnot readily available. This poses a challenge as the process of annotating data\nis costly and time-consuming. To address these challenges, we introduce a novel\nstance detection model coined domain-adaptive Cross-target STANCE detection via\nContrastive learning and Counterfactual generation (STANCE-C3) that uses\ncounterfactual data augmentation to enhance domain-adaptive training by\nenriching the target domain dataset during the training process and requiring\nsignificantly less information from the new domain. We also propose a modified\nself-supervised contrastive learning as a component of STANCE-C3 to prevent\noverfitting for the existing domain and target and enable cross-target stance\ndetection. Through experiments on various datasets, we show that STANCE-C3\nshows performance improvement over existing state-of-the-art methods.\n",
                "链接": "https://arxiv.org/abs/2309.15176"
            },
            {
                "文章ID": "80761",
                "标题": "Anomaly Detection with Conditioned Denoising Diffusion Models",
                "作者": " Arian Mousakhan,  Thomas Brox,  Jawad Tayyub",
                "发布日期": "2023-12-06",
                "摘要": "  Traditional reconstruction-based methods have struggled to achieve\ncompetitive performance in anomaly detection. In this paper, we introduce\nDenoising Diffusion Anomaly Detection (DDAD), a novel denoising process for\nimage reconstruction conditioned on a target image. This ensures a coherent\nrestoration that closely resembles the target image. Our anomaly detection\nframework employs the conditioning mechanism, where the target image is set as\nthe input image to guide the denoising process, leading to a defectless\nreconstruction while maintaining nominal patterns. Anomalies are then localised\nvia a pixel-wise and feature-wise comparison of the input and reconstructed\nimage. Finally, to enhance the effectiveness of the feature-wise comparison, we\nintroduce a domain adaptation method that utilises nearly identical generated\nexamples from our conditioned denoising process to fine-tune the pretrained\nfeature extractor. The veracity of DDAD is demonstrated on various datasets\nincluding MVTec and VisA benchmarks, achieving state-of-the-art results of\n\\(99.8 \\%\\) and \\(98.9 \\%\\) image-level AUROC respectively.\n",
                "链接": "https://arxiv.org/abs/2305.15956"
            },
            {
                "文章ID": "84646",
                "标题": "Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields",
                "作者": " Qianqiu Tan,  Tao Liu,  Yinling Xie,  Shuwan Yu,  Baohua Zhang",
                "发布日期": "2023-06-09",
                "摘要": "  The quality of three-dimensional reconstruction is a key factor affecting the\neffectiveness of its application in areas such as virtual reality (VR) and\naugmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate\nrealistic images from any viewpoint. It simultaneously reconstructs the shape,\nlighting, and materials of objects, and without surface defects, which breaks\ndown the barrier between virtuality and reality. The potential spatial\ncorrespondences displayed by NeRF between reconstructed scenes and real-world\nscenes offer a wide range of practical applications possibilities. Despite\nsignificant progress in 3D reconstruction since NeRF were introduced, there\nremains considerable room for exploration and experimentation. NeRF-based\nmodels are susceptible to interference issues caused by colored \"fog\" noise.\nAdditionally, they frequently encounter instabilities and failures while\nattempting to reconstruct unbounded scenes. Moreover, the model takes a\nsignificant amount of time to converge, making it even more challenging to use\nin such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color\nto balance low and high reflectivity objects display, utilizes a decoding\narchitecture with prior knowledge to improve recognition, and employs\nmulti-layer performance evaluation mechanisms to enhance learning capacity. It\nachieves reconstruction of outdoor scenes within one hour under single-card\ncondition. Based on experimental results, Enhance-NeRF partially enhances\nfitness capability and provides some support to outdoor scene reconstruction.\nThe Enhance-NeRF method can be used as a plug-and-play component, making it\neasy to integrate with other NeRF-based models. The code is available at:\nhttps://github.com/TANQIanQ/Enhance-NeRF\n",
                "链接": "https://arxiv.org/abs/2306.05303"
            },
            {
                "文章ID": "98993",
                "标题": "Neural Network Training Strategy to Enhance Anomaly Detection\n  Performance: A Perspective on Reconstruction Loss Amplification",
                "作者": " YeongHyeon Park,  Sungho Kang,  Myung Jin Kim,  Hyeonho Jeong,  Hyunkyu Park,  Hyeong Seok Kim,  Juneho Yi",
                "发布日期": "2023-08-29",
                "摘要": "  Unsupervised anomaly detection (UAD) is a widely adopted approach in industry\ndue to rare anomaly occurrences and data imbalance. A desirable characteristic\nof an UAD model is contained generalization ability which excels in the\nreconstruction of seen normal patterns but struggles with unseen anomalies.\nRecent studies have pursued to contain the generalization capability of their\nUAD models in reconstruction from different perspectives, such as design of\nneural network (NN) structure and training strategy. In contrast, we note that\ncontaining of generalization ability in reconstruction can also be obtained\nsimply from steep-shaped loss landscape. Motivated by this, we propose a loss\nlandscape sharpening method by amplifying the reconstruction loss, dubbed Loss\nAMPlification (LAMP). LAMP deforms the loss landscape into a steep shape so the\nreconstruction error on unseen anomalies becomes greater. Accordingly, the\nanomaly detection performance is improved without any change of the NN\narchitecture. Our findings suggest that LAMP can be easily applied to any\nreconstruction error metrics in UAD settings where the reconstruction model is\ntrained with anomaly-free samples only.\n",
                "链接": "https://arxiv.org/abs/2308.14595"
            },
            {
                "文章ID": "52358",
                "标题": "SSDA3D: Semi-supervised Domain Adaptation for 3D Object Detection from\n  Point Cloud",
                "作者": " Yan Wang,  Junbo Yin,  Wei Li,  Pascal Frossard,  Ruigang Yang,  Jianbing Shen",
                "发布日期": "2022-12-07",
                "摘要": "  LiDAR-based 3D object detection is an indispensable task in advanced\nautonomous driving systems. Though impressive detection results have been\nachieved by superior 3D detectors, they suffer from significant performance\ndegeneration when facing unseen domains, such as different LiDAR\nconfigurations, different cities, and weather conditions. The mainstream\napproaches tend to solve these challenges by leveraging unsupervised domain\nadaptation (UDA) techniques. However, these UDA solutions just yield\nunsatisfactory 3D detection results when there is a severe domain shift, e.g.,\nfrom Waymo (64-beam) to nuScenes (32-beam). To address this, we present a novel\nSemi-Supervised Domain Adaptation method for 3D object detection (SSDA3D),\nwhere only a few labeled target data is available, yet can significantly\nimprove the adaptation performance. In particular, our SSDA3D includes an\nInter-domain Adaptation stage and an Intra-domain Generalization stage. In the\nfirst stage, an Inter-domain Point-CutMix module is presented to efficiently\nalign the point cloud distribution across domains. The Point-CutMix generates\nmixed samples of an intermediate domain, thus encouraging to learn\ndomain-invariant knowledge. Then, in the second stage, we further enhance the\nmodel for better generalization on the unlabeled target set. This is achieved\nby exploring Intra-domain Point-MixUp in semi-supervised learning, which\nessentially regularizes the pseudo label distribution. Experiments from Waymo\nto nuScenes show that, with only 10% labeled target data, our SSDA3D can\nsurpass the fully-supervised oracle model with 100% target label. Our code is\navailable at https://github.com/yinjunbo/SSDA3D.\n",
                "链接": "https://arxiv.org/abs/2212.02845"
            },
            {
                "文章ID": "51976",
                "标题": "IDMS: Instance Depth for Multi-scale Monocular 3D Object Detection",
                "作者": " Chao Hu,  Liqiang Zhu,  Weibing Qiu,  Weijie Wu",
                "发布日期": "2023-02-14",
                "摘要": "  Due to the lack of depth information of images and poor detection accuracy in\nmonocular 3D object detection, we proposed the instance depth for multi-scale\nmonocular 3D object detection method. Firstly, to enhance the model's\nprocessing ability for different scale targets, a multi-scale perception module\nbased on dilated convolution is designed, and the depth features containing\nmulti-scale information are re-refined from both spatial and channel directions\nconsidering the inconsistency between feature maps of different scales.\nFirstly, we designed a multi-scale perception module based on dilated\nconvolution to enhance the model's processing ability for different scale\ntargets. The depth features containing multi-scale information are re-refined\nfrom spatial and channel directions considering the inconsistency between\nfeature maps of different scales. Secondly, so as to make the model obtain\nbetter 3D perception, this paper proposed to use the instance depth information\nas an auxiliary learning task to enhance the spatial depth feature of the 3D\ntarget and use the sparse instance depth to supervise the auxiliary task.\nFinally, by verifying the proposed algorithm on the KITTI test set and\nevaluation set, the experimental results show that compared with the baseline\nmethod, the proposed method improves by 5.27\\% in AP40 in the car category,\neffectively improving the detection performance of the monocular 3D object\ndetection algorithm.\n",
                "链接": "https://arxiv.org/abs/2212.01528"
            },
            {
                "文章ID": "97940",
                "标题": "Aggregating Intrinsic Information to Enhance BCI Performance through\n  Federated Learning",
                "作者": " Rui Liu,  Yuanyuan Chen,  Anran Li,  Yi Ding,  Han Yu,  Cuntai Guan",
                "发布日期": "2023-08-24",
                "摘要": "  Insufficient data is a long-standing challenge for Brain-Computer Interface\n(BCI) to build a high-performance deep learning model. Though numerous research\ngroups and institutes collect a multitude of EEG datasets for the same BCI\ntask, sharing EEG data from multiple sites is still challenging due to the\nheterogeneity of devices. The significance of this challenge cannot be\noverstated, given the critical role of data diversity in fostering model\nrobustness. However, existing works rarely discuss this issue, predominantly\ncentering their attention on model training within a single dataset, often in\nthe context of inter-subject or inter-session settings. In this work, we\npropose a hierarchical personalized Federated Learning EEG decoding (FLEEG)\nframework to surmount this challenge. This innovative framework heralds a new\nlearning paradigm for BCI, enabling datasets with disparate data formats to\ncollaborate in the model training process. Each client is assigned a specific\ndataset and trains a hierarchical personalized model to manage diverse data\nformats and facilitate information exchange. Meanwhile, the server coordinates\nthe training procedure to harness knowledge gleaned from all datasets, thus\nelevating overall performance. The framework has been evaluated in Motor\nImagery (MI) classification with nine EEG datasets collected by different\ndevices but implementing the same MI task. Results demonstrate that the\nproposed frame can boost classification performance up to 16.7% by enabling\nknowledge sharing between multiple datasets, especially for smaller datasets.\nVisualization results also indicate that the proposed framework can empower the\nlocal models to put a stable focus on task-related areas, yielding better\nperformance. To the best of our knowledge, this is the first end-to-end\nsolution to address this important challenge.\n",
                "链接": "https://arxiv.org/abs/2308.11636"
            }
        ]
    },
    {
        "question": {
            "question": "查找关于深度学习在医学影像分析中的最新研究。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "69437",
                "标题": "Medical Image Analysis using Deep Relational Learning",
                "作者": " Zhihua Liu",
                "发布日期": "2023-03-29",
                "摘要": "  In the past ten years, with the help of deep learning, especially the rapid\ndevelopment of deep neural networks, medical image analysis has made remarkable\nprogress. However, how to effectively use the relational information between\nvarious tissues or organs in medical images is still a very challenging\nproblem, and it has not been fully studied. In this thesis, we propose two\nnovel solutions to this problem based on deep relational learning. First, we\npropose a context-aware fully convolutional network that effectively models\nimplicit relation information between features to perform medical image\nsegmentation. The network achieves the state-of-the-art segmentation results on\nthe Multi Modal Brain Tumor Segmentation 2017 (BraTS2017) and Multi Modal Brain\nTumor Segmentation 2018 (BraTS2018) data sets. Subsequently, we propose a new\nhierarchical homography estimation network to achieve accurate medical image\nmosaicing by learning the explicit spatial relationship between adjacent\nframes. We use the UCL Fetoscopy Placenta dataset to conduct experiments and\nour hierarchical homography estimation network outperforms the other\nstate-of-the-art mosaicing methods while generating robust and meaningful\nmosaicing result on unseen frames.\n",
                "链接": "https://arxiv.org/abs/2303.16099"
            },
            {
                "文章ID": "107726",
                "标题": "Data efficient deep learning for medical image analysis: A survey",
                "作者": " Suruchi Kumari,  Pravendra Singh",
                "发布日期": "2023-10-11",
                "摘要": "  The rapid evolution of deep learning has significantly advanced the field of\nmedical image analysis. However, despite these achievements, the further\nenhancement of deep learning models for medical image analysis faces a\nsignificant challenge due to the scarcity of large, well-annotated datasets. To\naddress this issue, recent years have witnessed a growing emphasis on the\ndevelopment of data-efficient deep learning methods. This paper conducts a\nthorough review of data-efficient deep learning methods for medical image\nanalysis. To this end, we categorize these methods based on the level of\nsupervision they rely on, encompassing categories such as no supervision,\ninexact supervision, incomplete supervision, inaccurate supervision, and only\nlimited supervision. We further divide these categories into finer\nsubcategories. For example, we categorize inexact supervision into multiple\ninstance learning and learning with weak annotations. Similarly, we categorize\nincomplete supervision into semi-supervised learning, active learning, and\ndomain-adaptive learning and so on. Furthermore, we systematically summarize\ncommonly used datasets for data efficient deep learning in medical image\nanalysis and investigate future research directions to conclude this survey.\n",
                "链接": "https://arxiv.org/abs/2310.06557"
            },
            {
                "文章ID": "110503",
                "标题": "A comprehensive survey on deep active learning and its applications in\n  medical image analysis",
                "作者": " Haoran Wang,  Qiuye Jin,  Shiman Li,  Siyu Liu,  Manning Wang,  Zhijian Song",
                "发布日期": "2023-10-25",
                "摘要": "  Deep learning has achieved widespread success in medical image analysis,\nleading to an increasing demand for large-scale expert-annotated medical image\ndatasets. Yet, the high cost of annotating medical images severely hampers the\ndevelopment of deep learning in this field. To reduce annotation costs, active\nlearning aims to select the most informative samples for annotation and train\nhigh-performance models with as few labeled samples as possible. In this\nsurvey, we review the core methods of active learning, including the evaluation\nof informativeness and sampling strategy. For the first time, we provide a\ndetailed summary of the integration of active learning with other\nlabel-efficient techniques, such as semi-supervised, self-supervised learning,\nand so on. Additionally, we also highlight active learning works that are\nspecifically tailored to medical image analysis. In the end, we offer our\nperspectives on the future trends and challenges of active learning and its\napplications in medical image analysis.\n",
                "链接": "https://arxiv.org/abs/2310.14230"
            },
            {
                "文章ID": "52332",
                "标题": "A Trustworthy Framework for Medical Image Analysis with Deep Learning",
                "作者": " Kai Ma,  Siyuan He,  Pengcheng Xi,  Ashkan Ebadi,  Stéphane Tremblay,  Alexander Wong",
                "发布日期": "2022-12-07",
                "摘要": "  Computer vision and machine learning are playing an increasingly important\nrole in computer-assisted diagnosis; however, the application of deep learning\nto medical imaging has challenges in data availability and data imbalance, and\nit is especially important that models for medical imaging are built to be\ntrustworthy. Therefore, we propose TRUDLMIA, a trustworthy deep learning\nframework for medical image analysis, which adopts a modular design, leverages\nself-supervised pre-training, and utilizes a novel surrogate loss function.\nExperimental evaluations indicate that models generated from the framework are\nboth trustworthy and high-performing. It is anticipated that the framework will\nsupport researchers and clinicians in advancing the use of deep learning for\ndealing with public health crises including COVID-19.\n",
                "链接": "https://arxiv.org/abs/2212.02764"
            },
            {
                "文章ID": "81442",
                "标题": "Trustworthy Deep Learning for Medical Image Segmentation",
                "作者": " Lucas Fidon",
                "发布日期": "2023-05-30",
                "摘要": "  Despite the recent success of deep learning methods at achieving new\nstate-of-the-art accuracy for medical image segmentation, some major\nlimitations are still restricting their deployment into clinics. One major\nlimitation of deep learning-based segmentation methods is their lack of\nrobustness to variability in the image acquisition protocol and in the imaged\nanatomy that were not represented or were underrepresented in the training\ndataset. This suggests adding new manually segmented images to the training\ndataset to better cover the image variability. However, in most cases, the\nmanual segmentation of medical images requires highly skilled raters and is\ntime-consuming, making this solution prohibitively expensive. Even when\nmanually segmented images from different sources are available, they are rarely\nannotated for exactly the same regions of interest. This poses an additional\nchallenge for current state-of-the-art deep learning segmentation methods that\nrely on supervised learning and therefore require all the regions of interest\nto be segmented for all the images to be used for training. This thesis\nintroduces new mathematical and optimization methods to mitigate those\nlimitations.\n",
                "链接": "https://arxiv.org/abs/2305.17456"
            },
            {
                "文章ID": "8249",
                "标题": "Carbon Footprint of Selecting and Training Deep Learning Models for\n  Medical Image Analysis",
                "作者": " Raghavendra Selvan,  Nikhil Bhagwat,  Lasse F. Wolff Anthony,  Benjamin Kanding,  Erik B. Dam",
                "发布日期": "2022-09-16",
                "摘要": "  The increasing energy consumption and carbon footprint of deep learning (DL)\ndue to growing compute requirements has become a cause of concern. In this\nwork, we focus on the carbon footprint of developing DL models for medical\nimage analysis (MIA), where volumetric images of high spatial resolution are\nhandled. In this study, we present and compare the features of four tools from\nliterature to quantify the carbon footprint of DL. Using one of these tools we\nestimate the carbon footprint of medical image segmentation pipelines. We\nchoose nnU-net as the proxy for a medical image segmentation pipeline and\nexperiment on three common datasets. With our work we hope to inform on the\nincreasing energy costs incurred by MIA. We discuss simple strategies to\ncut-down the environmental impact that can make model selection and training\nprocesses more efficient.\n",
                "链接": "https://arxiv.org/abs/2203.02202"
            },
            {
                "文章ID": "84878",
                "标题": "Federated Learning for Medical Image Analysis: A Survey",
                "作者": " Hao Guan,  Pew-Thian Yap,  Andrea Bozoki,  Mingxia Liu",
                "发布日期": "2023-09-13",
                "摘要": "  Machine learning in medical imaging often faces a fundamental dilemma, namely\nthe small sample size problem. Many recent studies suggest using multi-domain\ndata pooled from different acquisition sites/datasets to improve statistical\npower. However, medical images from different sites cannot be easily shared to\nbuild large datasets for model training due to privacy protection reasons. As a\npromising solution, federated learning, which enables collaborative training of\nmachine learning models based on data from different sites without cross-site\ndata sharing, has attracted considerable attention recently. In this paper, we\nconduct a comprehensive survey of the recent development of federated learning\nmethods in medical image analysis. We first introduce the background and\nmotivation of federated learning for dealing with privacy protection and\ncollaborative learning issues in medical imaging. We then present a\ncomprehensive review of recent advances in federated learning methods for\nmedical image analysis. Specifically, existing methods are categorized based on\nthree critical aspects of a federated learning system, including client end,\nserver end, and communication techniques. In each category, we summarize the\nexisting federated learning methods according to specific research problems in\nmedical image analysis and also provide insights into the motivations of\ndifferent approaches. In addition, we provide a review of existing benchmark\nmedical imaging datasets and software platforms for current federated learning\nresearch. We also conduct an experimental study to empirically evaluate typical\nfederated learning methods for medical image analysis. This survey can help to\nbetter understand the current research status, challenges and potential\nresearch opportunities in this promising research field.\n",
                "链接": "https://arxiv.org/abs/2306.05980"
            },
            {
                "文章ID": "65067",
                "标题": "Evaluating the Fairness of Deep Learning Uncertainty Estimates in\n  Medical Image Analysis",
                "作者": " Raghav Mehta,  Changjian Shui,  Tal Arbel",
                "发布日期": "2023-03-07",
                "摘要": "  Although deep learning (DL) models have shown great success in many medical\nimage analysis tasks, deployment of the resulting models into real clinical\ncontexts requires: (1) that they exhibit robustness and fairness across\ndifferent sub-populations, and (2) that the confidence in DL model predictions\nbe accurately expressed in the form of uncertainties. Unfortunately, recent\nstudies have indeed shown significant biases in DL models across demographic\nsubgroups (e.g., race, sex, age) in the context of medical image analysis,\nindicating a lack of fairness in the models. Although several methods have been\nproposed in the ML literature to mitigate a lack of fairness in DL models, they\nfocus entirely on the absolute performance between groups without considering\ntheir effect on uncertainty estimation. In this work, we present the first\nexploration of the effect of popular fairness models on overcoming biases\nacross subgroups in medical image analysis in terms of bottom-line performance,\nand their effects on uncertainty quantification. We perform extensive\nexperiments on three different clinically relevant tasks: (i) skin lesion\nclassification, (ii) brain tumour segmentation, and (iii) Alzheimer's disease\nclinical score regression. Our results indicate that popular ML methods, such\nas data-balancing and distributionally robust optimization, succeed in\nmitigating fairness issues in terms of the model performances for some of the\ntasks. However, this can come at the cost of poor uncertainty estimates\nassociated with the model predictions. This tradeoff must be mitigated if\nfairness models are to be adopted in medical image analysis.\n",
                "链接": "https://arxiv.org/abs/2303.03242"
            },
            {
                "文章ID": "16452",
                "标题": "Masked Image Modeling Advances 3D Medical Image Analysis",
                "作者": " Zekai Chen,  Devansh Agarwal,  Kshitij Aggarwal,  Wiem Safta,  Samit Hirawat,  Venkat Sethuraman,  Mariann Micsinai Balan,  Kevin Brown",
                "发布日期": "2022-08-25",
                "摘要": "  Recently, masked image modeling (MIM) has gained considerable attention due\nto its capacity to learn from vast amounts of unlabeled data and has been\ndemonstrated to be effective on a wide variety of vision tasks involving\nnatural images. Meanwhile, the potential of self-supervised learning in\nmodeling 3D medical images is anticipated to be immense due to the high\nquantities of unlabeled images, and the expense and difficulty of quality\nlabels. However, MIM's applicability to medical images remains uncertain. In\nthis paper, we demonstrate that masked image modeling approaches can also\nadvance 3D medical images analysis in addition to natural images. We study how\nmasked image modeling strategies leverage performance from the viewpoints of 3D\nmedical image segmentation as a representative downstream task: i) when\ncompared to naive contrastive learning, masked image modeling approaches\naccelerate the convergence of supervised training even faster (1.40$\\times$)\nand ultimately produce a higher dice score; ii) predicting raw voxel values\nwith a high masking ratio and a relatively smaller patch size is non-trivial\nself-supervised pretext-task for medical images modeling; iii) a lightweight\ndecoder or projection head design for reconstruction is powerful for masked\nimage modeling on 3D medical images which speeds up training and reduce cost;\niv) finally, we also investigate the effectiveness of MIM methods under\ndifferent practical scenarios where different image resolutions and labeled\ndata ratios are applied.\n",
                "链接": "https://arxiv.org/abs/2204.11716"
            },
            {
                "文章ID": "124706",
                "标题": "Sample selection with noise rate estimation in noise learning of medical\n  image analysis",
                "作者": " Maolin Li,  Giacomo Tarroni,  Vasilis Siomos",
                "发布日期": "2023-12-27",
                "摘要": "  Deep learning techniques have demonstrated remarkable success in the field of\nmedical image analysis. However, the existence of label noise within data\nsignificantly hampers its performance. In this paper, we introduce a novel\nnoise-robust learning method which integrates noise rate estimation into sample\nselection approaches for handling noisy datasets. We first estimate the noise\nrate of a dataset with Linear Regression based on the distribution of loss\nvalues. Then, potentially noisy samples are excluded based on this estimated\nnoise rate, and sparse regularization is further employed to improve the\nrobustness of our deep learning model. Our proposed method is evaluated on five\nbenchmark medical image classification datasets, including two datasets\nfeaturing 3D medical images. Experiments show that our method outperforms other\nexisting noise-robust learning methods, especially when noise rate is very big.\n",
                "链接": "https://arxiv.org/abs/2312.15233"
            }
        ]
    },
    {
        "question": {
            "question": "强化学习在大语言模型领域中应用的相关论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "83947",
                "标题": "Applying Standards to Advance Upstream & Downstream Ethics in Large\n  Language Models",
                "作者": " Jose Berengueres,  Marybeth Sandell",
                "发布日期": "2023-06-13",
                "摘要": "  This paper explores how AI-owners can develop safeguards for AI-generated\ncontent by drawing from established codes of conduct and ethical standards in\nother content-creation industries. It delves into the current state of ethical\nawareness on Large Language Models (LLMs). By dissecting the mechanism of\ncontent generation by LLMs, four key areas (upstream/downstream and at user\nprompt/answer), where safeguards could be effectively applied, are identified.\nA comparative analysis of these four areas follows and includes an evaluation\nof the existing ethical safeguards in terms of cost, effectiveness, and\nalignment with established industry practices. The paper's key argument is that\nexisting IT-related ethical codes, while adequate for traditional IT\nengineering, are inadequate for the challenges posed by LLM-based content\ngeneration. Drawing from established practices within journalism, we propose\npotential standards for businesses involved in distributing and selling\nLLM-generated content. Finally, potential conflicts of interest between dataset\ncuration at upstream and ethical benchmarking downstream are highlighted to\nunderscore the need for a broader evaluation beyond mere output. This study\nprompts a nuanced conversation around ethical implications in this rapidly\nevolving field of content generation.\n",
                "链接": "https://arxiv.org/abs/2306.03503"
            },
            {
                "文章ID": "115286",
                "标题": "Applying Large Language Models for Causal Structure Learning in Non\n  Small Cell Lung Cancer",
                "作者": " Narmada Naik,  Ayush Khandelwal,  Mohit Joshi,  Madhusudan Atre,  Hollis Wright,  Kavya Kannan,  Scott Hill,  Giridhar Mamidipudi,  Ganapati Srinivasa,  Carlo Bifulco,  Brian Piening,  Kevin Matlock",
                "发布日期": "2023-11-14",
                "摘要": "  Causal discovery is becoming a key part in medical AI research. These methods\ncan enhance healthcare by identifying causal links between biomarkers,\ndemographics, treatments and outcomes. They can aid medical professionals in\nchoosing more impactful treatments and strategies. In parallel, Large Language\nModels (LLMs) have shown great potential in identifying patterns and generating\ninsights from text data. In this paper we investigate applying LLMs to the\nproblem of determining the directionality of edges in causal discovery.\nSpecifically, we test our approach on a deidentified set of Non Small Cell Lung\nCancer(NSCLC) patients that have both electronic health record and genomic\npanel data. Graphs are validated using Bayesian Dirichlet estimators using\ntabular data. Our result shows that LLMs can accurately predict the\ndirectionality of edges in causal graphs, outperforming existing\nstate-of-the-art methods. These findings suggests that LLMs can play a\nsignificant role in advancing causal discovery and help us better understand\ncomplex systems.\n",
                "链接": "https://arxiv.org/abs/2311.07191"
            },
            {
                "文章ID": "50738",
                "标题": "Applying Deep Reinforcement Learning to the HP Model for Protein\n  Structure Prediction",
                "作者": " Kaiyuan Yang,  Houjing Huang,  Olafs Vandans,  Adithya Murali,  Fujia Tian,  Roland H. C. Yap,  Liang Dai",
                "发布日期": "2023-03-14",
                "摘要": "  A central problem in computational biophysics is protein structure\nprediction, i.e., finding the optimal folding of a given amino acid sequence.\nThis problem has been studied in a classical abstract model, the HP model,\nwhere the protein is modeled as a sequence of H (hydrophobic) and P (polar)\namino acids on a lattice. The objective is to find conformations maximizing H-H\ncontacts. It is known that even in this reduced setting, the problem is\nintractable (NP-hard). In this work, we apply deep reinforcement learning (DRL)\nto the two-dimensional HP model. We can obtain the conformations of best known\nenergies for benchmark HP sequences with lengths from 20 to 50. Our DRL is\nbased on a deep Q-network (DQN). We find that a DQN based on long short-term\nmemory (LSTM) architecture greatly enhances the RL learning ability and\nsignificantly improves the search process. DRL can sample the state space\nefficiently, without the need of manual heuristics. Experimentally we show that\nit can find multiple distinct best-known solutions per trial. This study\ndemonstrates the effectiveness of deep reinforcement learning in the HP model\nfor protein folding.\n",
                "链接": "https://arxiv.org/abs/2211.14939"
            },
            {
                "文章ID": "92458",
                "标题": "Domain Knowledge Distillation from Large Language Model: An Empirical\n  Study in the Autonomous Driving Domain",
                "作者": " Yun Tang,  Antonio A. Bruto da Costa,  Jason Zhang,  Irvine Patrick,  Siddartha Khastgir,  Paul Jennings",
                "发布日期": "2023-07-25",
                "摘要": "  Engineering knowledge-based (or expert) systems require extensive manual\neffort and domain knowledge. As Large Language Models (LLMs) are trained using\nan enormous amount of cross-domain knowledge, it becomes possible to automate\nsuch engineering processes. This paper presents an empirical automation and\nsemi-automation framework for domain knowledge distillation using prompt\nengineering and the LLM ChatGPT. We assess the framework empirically in the\nautonomous driving domain and present our key observations. In our\nimplementation, we construct the domain knowledge ontology by \"chatting\" with\nChatGPT. The key finding is that while fully automated domain ontology\nconstruction is possible, human supervision and early intervention typically\nimprove efficiency and output quality as they lessen the effects of response\nrandomness and the butterfly effect. We, therefore, also develop a web-based\ndistillation assistant enabling supervision and flexible intervention at\nruntime. We hope our findings and tools could inspire future research toward\nrevolutionizing the engineering of knowledge-based systems across application\ndomains.\n",
                "链接": "https://arxiv.org/abs/2307.11769"
            },
            {
                "文章ID": "61474",
                "标题": "Guiding Pretraining in Reinforcement Learning with Large Language Models",
                "作者": " Yuqing Du,  Olivia Watkins,  Zihan Wang,  Cédric Colas,  Trevor Darrell,  Pieter Abbeel,  Abhishek Gupta,  Jacob Andreas",
                "发布日期": "2023-09-18",
                "摘要": "  Reinforcement learning algorithms typically struggle in the absence of a\ndense, well-shaped reward function. Intrinsically motivated exploration methods\naddress this limitation by rewarding agents for visiting novel states or\ntransitions, but these methods offer limited benefits in large environments\nwhere most discovered novelty is irrelevant for downstream tasks. We describe a\nmethod that uses background knowledge from text corpora to shape exploration.\nThis method, called ELLM (Exploring with LLMs) rewards an agent for achieving\ngoals suggested by a language model prompted with a description of the agent's\ncurrent state. By leveraging large-scale language model pretraining, ELLM\nguides agents toward human-meaningful and plausibly useful behaviors without\nrequiring a human in the loop. We evaluate ELLM in the Crafter game environment\nand the Housekeep robotic simulator, showing that ELLM-trained agents have\nbetter coverage of common-sense behaviors during pretraining and usually match\nor improve performance on a range of downstream tasks. Code available at\nhttps://github.com/yuqingd/ellm.\n",
                "链接": "https://arxiv.org/abs/2302.06692"
            },
            {
                "文章ID": "118959",
                "标题": "AviationGPT: A Large Language Model for the Aviation Domain",
                "作者": " Liya Wang,  Jason Chou,  Xin Zhou,  Alex Tien,  Diane M Baumgartner",
                "发布日期": "2023-11-30",
                "摘要": "  The advent of ChatGPT and GPT-4 has captivated the world with large language\nmodels (LLMs), demonstrating exceptional performance in question-answering,\nsummarization, and content generation. The aviation industry is characterized\nby an abundance of complex, unstructured text data, replete with technical\njargon and specialized terminology. Moreover, labeled data for model building\nare scarce in this domain, resulting in low usage of aviation text data. The\nemergence of LLMs presents an opportunity to transform this situation, but\nthere is a lack of LLMs specifically designed for the aviation domain. To\naddress this gap, we propose AviationGPT, which is built on open-source LLaMA-2\nand Mistral architectures and continuously trained on a wealth of carefully\ncurated aviation datasets. Experimental results reveal that AviationGPT offers\nusers multiple advantages, including the versatility to tackle diverse natural\nlanguage processing (NLP) problems (e.g., question-answering, summarization,\ndocument writing, information extraction, report querying, data cleaning, and\ninteractive data exploration). It also provides accurate and contextually\nrelevant responses within the aviation domain and significantly improves\nperformance (e.g., over a 40% performance gain in tested cases). With\nAviationGPT, the aviation industry is better equipped to address more complex\nresearch problems and enhance the efficiency and safety of National Airspace\nSystem (NAS) operations.\n",
                "链接": "https://arxiv.org/abs/2311.17686"
            },
            {
                "文章ID": "106817",
                "标题": "Applying Reinforcement Learning to Option Pricing and Hedging",
                "作者": " Zoran Stoiljkovic",
                "发布日期": "2023-10-09",
                "摘要": "  This thesis provides an overview of the recent advances in reinforcement\nlearning in pricing and hedging financial instruments, with a primary focus on\na detailed explanation of the Q-Learning Black Scholes approach, introduced by\nHalperin (2017). This reinforcement learning approach bridges the traditional\nBlack and Scholes (1973) model with novel artificial intelligence algorithms,\nenabling option pricing and hedging in a completely model-free and data-driven\nway. This paper also explores the algorithm's performance under different state\nvariables and scenarios for a European put option. The results reveal that the\nmodel is an accurate estimator under different levels of volatility and hedging\nfrequency. Moreover, this method exhibits robust performance across various\nlevels of option's moneyness. Lastly, the algorithm incorporates proportional\ntransaction costs, indicating diverse impacts on profit and loss, affected by\ndifferent statistical properties of the state variables.\n",
                "链接": "https://arxiv.org/abs/2310.04336"
            },
            {
                "文章ID": "117473",
                "标题": "Applying Large Language Models to Power Systems: Potential Security\n  Threats",
                "作者": " Jiaqi Ruan,  Gaoqi Liang,  Huan Zhao,  Guolong Liu,  Jing Qiu,  Junhua Zhao,  Zhao Xu,  Fushuan Wen,  Zhao Yang Dong",
                "发布日期": "2023-11-23",
                "摘要": "  Applying large language models (LLMs) to power systems presents a promising\navenue for enhancing decision-making and operational efficiency. However, this\naction may also incur potential security threats, which have not been fully\nrecognized so far. To this end, this letter analyzes potential threats incurred\nby applying LLMs to power systems, emphasizing the need for urgent research and\ndevelopment of countermeasures.\n",
                "链接": "https://arxiv.org/abs/2311.13361"
            },
            {
                "文章ID": "97795",
                "标题": "Federated Learning in Big Model Era: Domain-Specific Multimodal Large\n  Models",
                "作者": " Zengxiang Li,  Zhaoxiang Hou,  Hui Liu,  Ying Wang,  Tongzhi Li,  Longfei Xie,  Chao Shi,  Chengyi Yang,  Weishan Zhang,  Zelei Liu,  Liang Xu",
                "发布日期": "2023-08-25",
                "摘要": "  Multimodal data, which can comprehensively perceive and recognize the\nphysical world, has become an essential path towards general artificial\nintelligence. However, multimodal large models trained on public datasets often\nunderperform in specific industrial domains. This paper proposes a multimodal\nfederated learning framework that enables multiple enterprises to utilize\nprivate domain data to collaboratively train large models for vertical domains,\nachieving intelligent services across scenarios. The authors discuss in-depth\nthe strategic transformation of federated learning in terms of intelligence\nfoundation and objectives in the era of big model, as well as the new\nchallenges faced in heterogeneous data, model aggregation, performance and cost\ntrade-off, data privacy, and incentive mechanism. The paper elaborates a case\nstudy of leading enterprises contributing multimodal data and expert knowledge\nto city safety operation management , including distributed deployment and\nefficient coordination of the federated learning platform, technical\ninnovations on data quality improvement based on large model capabilities and\nefficient joint fine-tuning approaches. Preliminary experiments show that\nenterprises can enhance and accumulate intelligent capabilities through\nmultimodal model federated learning, thereby jointly creating an smart city\nmodel that provides high-quality intelligent services covering energy\ninfrastructure safety, residential community security, and urban operation\nmanagement. The established federated learning cooperation ecosystem is\nexpected to further aggregate industry, academia, and research resources,\nrealize large models in multiple vertical domains, and promote the large-scale\nindustrial application of artificial intelligence and cutting-edge research on\nmultimodal federated learning.\n",
                "链接": "https://arxiv.org/abs/2308.11217"
            },
            {
                "文章ID": "101716",
                "标题": "Self-Refined Large Language Model as Automated Reward Function Designer\n  for Deep Reinforcement Learning in Robotics",
                "作者": " Jiayang Song,  Zhehua Zhou,  Jiawei Liu,  Chunrong Fang,  Zhan Shu,  Lei Ma",
                "发布日期": "2023-10-03",
                "摘要": "  Although Deep Reinforcement Learning (DRL) has achieved notable success in\nnumerous robotic applications, designing a high-performing reward function\nremains a challenging task that often requires substantial manual input.\nRecently, Large Language Models (LLMs) have been extensively adopted to address\ntasks demanding in-depth common-sense knowledge, such as reasoning and\nplanning. Recognizing that reward function design is also inherently linked to\nsuch knowledge, LLM offers a promising potential in this context. Motivated by\nthis, we propose in this work a novel LLM framework with a self-refinement\nmechanism for automated reward function design. The framework commences with\nthe LLM formulating an initial reward function based on natural language\ninputs. Then, the performance of the reward function is assessed, and the\nresults are presented back to the LLM for guiding its self-refinement process.\nWe examine the performance of our proposed framework through a variety of\ncontinuous robotic control tasks across three diverse robotic systems. The\nresults indicate that our LLM-designed reward functions are able to rival or\neven surpass manually designed reward functions, highlighting the efficacy and\napplicability of our approach.\n",
                "链接": "https://arxiv.org/abs/2309.06687"
            }
        ]
    },
    {
        "question": {
            "question": "2023年以后关于大语言模型和人文学科交叉的研究",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "114592",
                "标题": "An Interdisciplinary Outlook on Large Language Models for Scientific\n  Research",
                "作者": " James Boyko,  Joseph Cohen,  Nathan Fox,  Maria Han Veiga,  Jennifer I-Hsiu Li,  Jing Liu,  Bernardo Modenesi,  Andreas H. Rauch,  Kenneth N. Reid,  Soumi Tribedi,  Anastasia Visheratina,  Xin Xie",
                "发布日期": "2023-11-10",
                "摘要": "  In this paper, we describe the capabilities and constraints of Large Language\nModels (LLMs) within disparate academic disciplines, aiming to delineate their\nstrengths and limitations with precision. We examine how LLMs augment\nscientific inquiry, offering concrete examples such as accelerating literature\nreview by summarizing vast numbers of publications, enhancing code development\nthrough automated syntax correction, and refining the scientific writing\nprocess. Simultaneously, we articulate the challenges LLMs face, including\ntheir reliance on extensive and sometimes biased datasets, and the potential\nethical dilemmas stemming from their use. Our critical discussion extends to\nthe varying impacts of LLMs across fields, from the natural sciences, where\nthey help model complex biological sequences, to the social sciences, where\nthey can parse large-scale qualitative data. We conclude by offering a nuanced\nperspective on how LLMs can be both a boon and a boundary to scientific\nprogress.\n",
                "链接": "https://arxiv.org/abs/2311.04929"
            },
            {
                "文章ID": "112477",
                "标题": "Artificial intelligence and the limits of the humanities",
                "作者": " Włodzisław Duch",
                "发布日期": "2023-10-31",
                "摘要": "  The complexity of cultures in the modern world is now beyond human\ncomprehension. Cognitive sciences cast doubts on the traditional explanations\nbased on mental models. The core subjects in humanities may lose their\nimportance. Humanities have to adapt to the digital age. New, interdisciplinary\nbranches of humanities emerge. Instant access to information will be replaced\nby instant access to knowledge. Understanding the cognitive limitations of\nhumans and the opportunities opened by the development of artificial\nintelligence and interdisciplinary research necessary to address global\nchallenges is the key to the revitalization of humanities. Artificial\nintelligence will radically change humanities, from art to political sciences\nand philosophy, making these disciplines attractive to students and enabling\nthem to go beyond current limitations.\n",
                "链接": "https://arxiv.org/abs/2310.19425"
            },
            {
                "文章ID": "35258",
                "标题": "Labeling of Cultural Heritage Collections on the Intersection of Visual\n  Analytics and Digital Humanities",
                "作者": " Christofer Meinecke",
                "发布日期": "2022-08-30",
                "摘要": "  Engaging in interdisciplinary projects on the intersection between\nvisualization and humanities research can be a challenging endeavor. Challenges\ncan be finding valuable outcomes for both domains, or how to apply\nstate-of-the-art visual analytics methods like supervised machine learning\nalgorithms. We discuss these challenges when working with cultural heritage\ndata. Further, there is a gap in applying these methods to intangible heritage.\nTo give a reflection on some interdisciplinary projects, we present three case\nstudies focusing on the labeling of cultural heritage collections, the problems\nand challenges with the data, the participatory design process, and takeaways\nfor the visualization scholars from these collaborations.\n",
                "链接": "https://arxiv.org/abs/2208.13512"
            },
            {
                "文章ID": "70754",
                "标题": "A Bibliometric Review of Large Language Models Research from 2017 to\n  2023",
                "作者": " Lizhou Fan,  Lingyao Li,  Zihui Ma,  Sanggyu Lee,  Huizi Yu,  Libby Hemphill",
                "发布日期": "2023-04-06",
                "摘要": "  Large language models (LLMs) are a class of language models that have\ndemonstrated outstanding performance across a range of natural language\nprocessing (NLP) tasks and have become a highly sought-after research area,\nbecause of their ability to generate human-like language and their potential to\nrevolutionize science and technology. In this study, we conduct bibliometric\nand discourse analyses of scholarly literature on LLMs. Synthesizing over 5,000\npublications, this paper serves as a roadmap for researchers, practitioners,\nand policymakers to navigate the current landscape of LLMs research. We present\nthe research trends from 2017 to early 2023, identifying patterns in research\nparadigms and collaborations. We start with analyzing the core algorithm\ndevelopments and NLP tasks that are fundamental in LLMs research. We then\ninvestigate the applications of LLMs in various fields and domains including\nmedicine, engineering, social science, and humanities. Our review also reveals\nthe dynamic, fast-paced evolution of LLMs research. Overall, this paper offers\nvaluable insights into the current state, impact, and potential of LLMs\nresearch and its applications.\n",
                "链接": "https://arxiv.org/abs/2304.02020"
            },
            {
                "文章ID": "102321",
                "标题": "ChatGPT v Bard v Bing v Claude 2 v Aria v human-expert. How good are AI\n  chatbots at scientific writing?",
                "作者": " Edisa Lozić,  Benjamin Štular",
                "发布日期": "2023-10-17",
                "摘要": "  Historical emphasis on writing mastery has shifted with advances in\ngenerative AI, especially in scientific writing. This study analysed six AI\nchatbots for scholarly writing in humanities and archaeology. Using methods\nthat assessed factual correctness and scientific contribution, ChatGPT-4 showed\nthe highest quantitative accuracy, closely followed by ChatGPT-3.5, Bing, and\nBard. However, Claude 2 and Aria scored considerably lower. Qualitatively, all\nAIs exhibited proficiency in merging existing knowledge, but none produced\noriginal scientific content. Inter-estingly, our findings suggest ChatGPT-4\nmight represent a plateau in large language model size. This research\nemphasizes the unique, intricate nature of human research, suggesting that AI's\nemulation of human originality in scientific writing is challenging. As of\n2023, while AI has transformed content generation, it struggles with original\ncontributions in humanities. This may change as AI chatbots continue to evolve\ninto LLM-powered software.\n",
                "链接": "https://arxiv.org/abs/2309.08636"
            },
            {
                "文章ID": "87950",
                "标题": "Large Multimodal Models: Notes on CVPR 2023 Tutorial",
                "作者": " Chunyuan Li",
                "发布日期": "2023-06-27",
                "摘要": "  This tutorial note summarizes the presentation on ``Large Multimodal Models:\nTowards Building and Surpassing Multimodal GPT-4'', a part of CVPR 2023\ntutorial on ``Recent Advances in Vision Foundation Models''. The tutorial\nconsists of three parts. We first introduce the background on recent GPT-like\nlarge models for vision-and-language modeling to motivate the research in\ninstruction-tuned large multimodal models (LMMs). As a pre-requisite, we\ndescribe the basics of instruction-tuning in large language models, which is\nfurther extended to the multimodal space. Lastly, we illustrate how to build\nthe minimum prototype of multimodal GPT-4 like models with the open-source\nresource, and review the recently emerged topics.\n",
                "链接": "https://arxiv.org/abs/2306.14895"
            },
            {
                "文章ID": "80006",
                "标题": "Having Beer after Prayer? Measuring Cultural Bias in Large Language\n  Models",
                "作者": " Tarek Naous,  Michael J. Ryan,  Alan Ritter,  Wei Xu",
                "发布日期": "2023-11-17",
                "摘要": "  It is important that language models appropriately adapt to specific cultural\ncontexts. However, as we show in this paper, multilingual and Arabic\nmonolingual language models default to Western culture even when prompted in\nArabic and contextualized by an Arab cultural setting. To measure this Western\nbias, we introduce CAMeL, a dataset of naturally occurring Arabic prompts\nspanning eight diverse cultural aspects and an extensive list of 20,504\ncultural targets corresponding to Arab or Western culture. Using CAMeL, we show\nthat models favor Western targets and demonstrate cultural unfairness on\ndownstream tasks such as named entity recognition and sentiment analysis. Our\nanalyses of pretraining corpora also reveal that commonly used sources such as\nWikipedia may not be suited to build culturally aware models, underscoring the\nimportance of carefully curating pretraining data in constructing language\nmodels to serve a global population.\n",
                "链接": "https://arxiv.org/abs/2305.14456"
            },
            {
                "文章ID": "98834",
                "标题": "Large Language Models Streamline Automated Machine Learning for Clinical\n  Studies",
                "作者": " Soroosh Tayebi Arasteh,  Tianyu Han,  Mahshad Lotfinia,  Christiane Kuhl,  Jakob Nikolas Kather,  Daniel Truhn,  Sven Nebelung",
                "发布日期": "2023-10-11",
                "摘要": "  A knowledge gap persists between machine learning (ML) developers (e.g., data\nscientists) and practitioners (e.g., clinicians), hampering the full\nutilization of ML for clinical data analysis. We investigated the potential of\nthe ChatGPT Advanced Data Analysis (ADA), an extension of GPT-4, to bridge this\ngap and perform ML analyses efficiently. Real-world clinical datasets and study\ndetails from large trials across various medical specialties were presented to\nChatGPT ADA without specific guidance. ChatGPT ADA autonomously developed\nstate-of-the-art ML models based on the original study's training data to\npredict clinical outcomes such as cancer development, cancer progression,\ndisease complications, or biomarkers such as pathogenic gene sequences.\nFollowing the re-implementation and optimization of the published models, the\nhead-to-head comparison of the ChatGPT ADA-crafted ML models and their\nrespective manually crafted counterparts revealed no significant differences in\ntraditional performance metrics (P>0.474). Strikingly, the ChatGPT ADA-crafted\nML models often outperformed their counterparts. In conclusion, ChatGPT ADA\noffers a promising avenue to democratize ML in medicine by simplifying complex\ndata analyses, yet should enhance, not replace, specialized training and\nresources, to promote broader applications in medical research and practice.\n",
                "链接": "https://arxiv.org/abs/2308.14120"
            },
            {
                "文章ID": "106494",
                "标题": "Benchmarking Large Language Models As AI Research Agents",
                "作者": " Qian Huang,  Jian Vora,  Percy Liang,  Jure Leskovec",
                "发布日期": "2023-10-06",
                "摘要": "  Scientific experimentation involves an iterative process of creating\nhypotheses, designing experiments, running experiments, and analyzing the\nresults. Can we build AI research agents to perform these long-horizon tasks?\nTo take a step towards building and evaluating research agents on such\nopen-ended decision-making tasks, we focus on the problem of machine learning\nengineering: given a task description and a dataset, build a high-performing\nmodel. In this paper, we propose MLAgentBench, a suite of ML tasks for\nbenchmarking AI research agents. Agents can perform actions like\nreading/writing files, executing code, and inspecting outputs. With these\nactions, agents could run experiments, analyze the results, and modify the code\nof entire machine learning pipelines, such as data processing, architecture,\ntraining processes, etc. The benchmark then automatically evaluates the agent's\nperformance objectively over various metrics related to performance and\nefficiency. We also design an LLM-based research agent to automatically perform\nexperimentation loops in such an environment. Empirically, we find that a\nGPT-4-based research agent can feasibly build compelling ML models over many\ntasks in MLAgentBench, displaying highly interpretable plans and actions.\nHowever, the success rates vary considerably; they span from almost 90\\% on\nwell-established older datasets to as low as 10\\% on recent Kaggle Challenges\n-- unavailable during the LLM model's pretraining -- and even 0\\% on newer\nresearch challenges like BabyLM. Finally, we identify several key challenges\nfor LLM-based research agents such as long-term planning and hallucination. Our\ncode is released at https://github.com/snap-stanford/MLAgentBench.\n",
                "链接": "https://arxiv.org/abs/2310.03302"
            },
            {
                "文章ID": "112605",
                "标题": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as\n  Explainable Metrics",
                "作者": " Christoph Leiter,  Juri Opitz,  Daniel Deutsch,  Yang Gao,  Rotem Dror,  Steffen Eger",
                "发布日期": "2023-10-31",
                "摘要": "  With an increasing number of parameters and pre-training data, generative\nlarge language models (LLMs) have shown remarkable capabilities to solve tasks\nwith minimal or no task-related examples. Notably, LLMs have been successfully\nemployed as evaluation metrics in text generation tasks. Within this context,\nwe introduce the Eval4NLP 2023 shared task that asks participants to explore\nprompting and score extraction for machine translation (MT) and summarization\nevaluation. Specifically, we propose a novel competition setting in which we\nselect a list of allowed LLMs and disallow fine-tuning to ensure a focus on\nprompting. We present an overview of participants' approaches and evaluate them\non a new reference-free test set spanning three language pairs for MT and a\nsummarization dataset. Notably, despite the task's restrictions, the\nbest-performing systems achieve results on par with or even surpassing recent\nreference-free metrics developed using larger models, including GEMBA and\nComet-Kiwi-XXL. Finally, as a separate track, we perform a small-scale human\nevaluation of the plausibility of explanations given by the LLMs.\n",
                "链接": "https://arxiv.org/abs/2310.19792"
            }
        ]
    },
    {
        "question": {
            "question": "对比解码综述",
            "type": "6"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找一下近三个月有关语言模型rlhf的arxiv上的全部文章。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "106830",
                "标题": "Confronting Reward Model Overoptimization with Constrained RLHF",
                "作者": " Ted Moskovitz,  Aaditya K. Singh,  DJ Strouse,  Tuomas Sandholm,  Ruslan Salakhutdinov,  Anca D. Dragan,  Stephen McAleer",
                "发布日期": "2023-10-11",
                "摘要": "  Large language models are typically aligned with human preferences by\noptimizing $\\textit{reward models}$ (RMs) fitted to human feedback. However,\nhuman preferences are multi-faceted, and it is increasingly common to derive\nreward from a composition of simpler reward models which each capture a\ndifferent aspect of language quality. This itself presents a challenge, as it\nis difficult to appropriately weight these component RMs when combining them.\nCompounding this difficulty, because any RM is only a proxy for human\nevaluation, this process is vulnerable to $\\textit{overoptimization}$, wherein\npast a certain point, accumulating higher reward is associated with worse human\nratings. In this paper, we perform, to our knowledge, the first study on\noveroptimization in composite RMs, showing that correlation between component\nRMs has a significant effect on the locations of these points. We then\nintroduce an approach to solve this issue using constrained reinforcement\nlearning as a means of preventing the agent from exceeding each RM's threshold\nof usefulness. Our method addresses the problem of weighting component RMs by\nlearning dynamic weights, naturally expressed by Lagrange multipliers. As a\nresult, each RM stays within the range at which it is an effective proxy,\nimproving evaluation performance. Finally, we introduce an adaptive method\nusing gradient-free optimization to identify and optimize towards these points\nduring a single run.\n",
                "链接": "https://arxiv.org/abs/2310.04373"
            },
            {
                "文章ID": "102832",
                "标题": "Stabilizing RLHF through Advantage Model and Selective Rehearsal",
                "作者": " Baolin Peng,  Linfeng Song,  Ye Tian,  Lifeng Jin,  Haitao Mi,  Dong Yu",
                "发布日期": "2023-09-20",
                "摘要": "  Large Language Models (LLMs) have revolutionized natural language processing,\nyet aligning these models with human values and preferences using RLHF remains\na significant challenge. This challenge is characterized by various\ninstabilities, such as reward hacking and catastrophic forgetting. In this\ntechnical report, we propose two innovations to stabilize RLHF training: 1)\nAdvantage Model, which directly models advantage score i.e., extra reward\ncompared to the expected rewards and regulates score distributions across tasks\nto prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic\nforgetting by strategically selecting data for PPO training and knowledge\nrehearsing. Our experimental analysis on public and proprietary datasets\nreveals that the proposed methods not only increase stability in RLHF training\nbut also achieve higher reward scores and win rates.\n",
                "链接": "https://arxiv.org/abs/2309.10202"
            },
            {
                "文章ID": "90293",
                "标题": "Secrets of RLHF in Large Language Models Part I: PPO",
                "作者": " Rui Zheng,  Shihan Dou,  Songyang Gao,  Yuan Hua,  Wei Shen,  Binghai Wang,  Yan Liu,  Senjie Jin,  Qin Liu,  Yuhao Zhou,  Limao Xiong,  Lu Chen,  Zhiheng Xi,  Nuo Xu,  Wenbin Lai,  Minghao Zhu,  Cheng Chang,  Zhangyue Yin,  Rongxiang Weng,  Wensen Cheng,  Haoran Huang,  Tianxiang Sun,  Hang Yan,  Tao Gui,  Qi Zhang,  Xipeng Qiu,  Xuanjing Huang",
                "发布日期": "2023-07-19",
                "摘要": "  Large language models (LLMs) have formulated a blueprint for the advancement\nof artificial general intelligence. Its primary objective is to function as a\nhuman-centric (helpful, honest, and harmless) assistant. Alignment with humans\nassumes paramount significance, and reinforcement learning with human feedback\n(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.\nCurrent technical routes usually include \\textbf{reward models} to measure\nhuman preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize\npolicy model outputs, and \\textbf{process supervision} to improve step-by-step\nreasoning capabilities. However, due to the challenges of reward design,\nenvironment interaction, and agent training, coupled with huge trial and error\ncost of large language models, there is a significant barrier for AI\nresearchers to motivate the development of technical alignment and safe landing\nof LLMs. The stable training of RLHF has still been a puzzle. In the first\nreport, we dissect the framework of RLHF, re-evaluate the inner workings of\nPPO, and explore how the parts comprising PPO algorithms impact policy agent\ntraining. We identify policy constraints being the key factor for the effective\nimplementation of the PPO algorithm. Therefore, we explore the PPO-max, an\nadvanced version of PPO algorithm, to efficiently improve the training\nstability of the policy model. Based on our main results, we perform a\ncomprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.\nThe absence of open-source implementations has posed significant challenges to\nthe investigation of LLMs alignment. Therefore, we are eager to release\ntechnical reports, reward models and PPO codes, aiming to make modest\ncontributions to the advancement of LLMs.\n",
                "链接": "https://arxiv.org/abs/2307.04964"
            },
            {
                "文章ID": "108320",
                "标题": "Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse\n  Autoencoders",
                "作者": " Luke Marks,  Amir Abdullah,  Luna Mendez,  Rauno Arike,  Philip Torr,  Fazl Barez",
                "发布日期": "2023-11-29",
                "摘要": "  Large language models (LLMs) aligned to human preferences via reinforcement\nlearning from human feedback (RLHF) underpin many commercial applications of\nLLM technology. Despite this, the impacts of RLHF on LLM internals remain\nopaque. We propose a novel method for interpreting implicit reward models\n(IRMs) in LLMs learned through RLHF. Our approach trains pairs of autoencoders\non activations from a base LLM and its RLHF-tuned variant. Through a comparison\nof autoencoder hidden spaces, we identify features that reflect the accuracy of\nthe learned IRM. To illustrate our method, we fine-tune an LLM via RLHF to\nlearn a token-utility mapping and maximize the aggregate utility of generated\ntext. This is the first application of sparse autoencoders to interpreting\nIRMs. Our method provides an abstract approximation of reward integrity and\nholds promise for measuring alignment between specified objectives and learned\nmodel behaviors.\n",
                "链接": "https://arxiv.org/abs/2310.08164"
            },
            {
                "文章ID": "119747",
                "标题": "RLHF and IIA: Perverse Incentives",
                "作者": " Wanqiao Xu,  Shi Dong,  Xiuyuan Lu,  Grace Lam,  Zheng Wen,  Benjamin Van Roy",
                "发布日期": "2023-12-22",
                "摘要": "  Existing algorithms for reinforcement learning from human feedback (RLHF) can\nincentivize responses at odds with preferences because they are based on models\nthat assume independence of irrelevant alternatives (IIA). The perverse\nincentives induced by IIA give rise to egregious behavior when innovating on\nquery formats or learning algorithms.\n",
                "链接": "https://arxiv.org/abs/2312.01057"
            },
            {
                "文章ID": "105766",
                "标题": "UltraFeedback: Boosting Language Models with High-quality Feedback",
                "作者": " Ganqu Cui,  Lifan Yuan,  Ning Ding,  Guanming Yao,  Wei Zhu,  Yuan Ni,  Guotong Xie,  Zhiyuan Liu,  Maosong Sun",
                "发布日期": "2023-10-03",
                "摘要": "  Reinforcement learning from human feedback (RLHF) has become a pivot\ntechnique in aligning large language models (LLMs) with human preferences. In\nRLHF practice, preference data plays a crucial role in bridging human\nproclivity and LLMs. However, the scarcity of diverse, naturalistic datasets of\nhuman preferences on LLM outputs at scale poses a great challenge to RLHF as\nwell as feedback learning research within the open-source community. Current\npreference datasets, either proprietary or limited in size and prompt variety,\nresult in limited RLHF adoption in open-source models and hinder further\nexploration. In this study, we propose ULTRAFEEDBACK, a large-scale,\nhigh-quality, and diversified preference dataset designed to overcome these\nlimitations and foster RLHF development. To create ULTRAFEEDBACK, we compile a\ndiverse array of instructions and models from multiple sources to produce\ncomparative data. We meticulously devise annotation instructions and employ\nGPT-4 to offer detailed feedback in both numerical and textual forms.\nULTRAFEEDBACK establishes a reproducible and expandable preference data\nconstruction pipeline, serving as a solid foundation for future RLHF and\nfeedback learning research. Utilizing ULTRAFEEDBACK, we train various models to\ndemonstrate its effectiveness, including the reward model UltraRM, chat\nlanguage model UltraLM-13B-PPO, and critique model UltraCM. Experimental\nresults indicate that our models outperform existing open-source models,\nachieving top performance across multiple benchmarks. Our data and models are\navailable at https://github.com/thunlp/UltraFeedback.\n",
                "链接": "https://arxiv.org/abs/2310.01377"
            },
            {
                "文章ID": "111183",
                "标题": "AI Alignment and Social Choice: Fundamental Limitations and Policy\n  Implications",
                "作者": " Abhilash Mishra",
                "发布日期": "2023-10-25",
                "摘要": "  Aligning AI agents to human intentions and values is a key bottleneck in\nbuilding safe and deployable AI applications. But whose values should AI agents\nbe aligned with? Reinforcement learning with human feedback (RLHF) has emerged\nas the key framework for AI alignment. RLHF uses feedback from human\nreinforcers to fine-tune outputs; all widely deployed large language models\n(LLMs) use RLHF to align their outputs to human values. It is critical to\nunderstand the limitations of RLHF and consider policy challenges arising from\nthese limitations. In this paper, we investigate a specific challenge in\nbuilding RLHF systems that respect democratic norms. Building on impossibility\nresults in social choice theory, we show that, under fairly broad assumptions,\nthere is no unique voting protocol to universally align AI systems using RLHF\nthrough democratic processes. Further, we show that aligning AI agents with the\nvalues of all individuals will always violate certain private ethical\npreferences of an individual user i.e., universal AI alignment using RLHF is\nimpossible. We discuss policy implications for the governance of AI systems\nbuilt using RLHF: first, the need for mandating transparent voting rules to\nhold model builders accountable. Second, the need for model builders to focus\non developing AI agents that are narrowly aligned to specific user groups.\n",
                "链接": "https://arxiv.org/abs/2310.16048"
            },
            {
                "文章ID": "111439",
                "标题": "BabyStories: Can Reinforcement Learning Teach Baby Language Models to\n  Write Better Stories?",
                "作者": " Xingmeng Zhao,  Tongnian Wang,  Sheri Osborn,  Anthony Rios",
                "发布日期": "2023-10-26",
                "摘要": "  Language models have seen significant growth in the size of their corpus,\nleading to notable performance improvements. Yet, there has been limited\nprogress in developing models that handle smaller, more human-like datasets. As\npart of the BabyLM shared task, this study explores the impact of reinforcement\nlearning from human feedback (RLHF) on language models pretrained from scratch\nwith a limited training corpus. Comparing two GPT-2 variants, the larger model\nperforms better in storytelling tasks after RLHF fine-tuning. These findings\nsuggest that RLHF techniques may be more advantageous for larger models due to\ntheir higher learning and adaptation capacity, though more experiments are\nneeded to confirm this finding. These insights highlight the potential benefits\nof RLHF fine-tuning for language models within limited data, enhancing their\nability to maintain narrative focus and coherence while adhering better to\ninitial instructions in storytelling tasks. The code for this work is publicly\nat https://github.com/Zephyr1022/BabyStories-UTSA.\n",
                "链接": "https://arxiv.org/abs/2310.16681"
            },
            {
                "文章ID": "119683",
                "标题": "RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from\n  Fine-grained Correctional Human Feedback",
                "作者": " Tianyu Yu,  Yuan Yao,  Haoye Zhang,  Taiwen He,  Yifeng Han,  Ganqu Cui,  Jinyi Hu,  Zhiyuan Liu,  Hai-Tao Zheng,  Maosong Sun,  Tat-Seng Chua",
                "发布日期": "2023-12-05",
                "摘要": "  Multimodal Large Language Models (MLLMs) have recently demonstrated\nimpressive capabilities in multimodal understanding, reasoning, and\ninteraction. However, existing MLLMs prevalently suffer from serious\nhallucination problems, generating text that is not factually grounded in\nassociated images. The problem makes existing MLLMs untrustworthy and thus\nimpractical in real-world (especially high-stakes) applications. To address the\nchallenge, we present RLHF-V, which enhances MLLM trustworthiness via behavior\nalignment from fine-grained correctional human feedback. Specifically, RLHF-V\ncollects human preference in the form of segment-level corrections on\nhallucinations, and performs dense direct preference optimization over the\nhuman feedback. Comprehensive experiments on five benchmarks in both automatic\nand human evaluation show that, RLHF-V can enable substantially more\ntrustworthy MLLM behaviors with promising data and computation efficiency.\nRemarkably, using 1.4k annotated data samples, RLHF-V significantly reduces the\nhallucination rate of the base MLLM by 34.8%, outperforming the concurrent\nLLaVA-RLHF trained on 10k annotated data. The final model achieves\nstate-of-the-art performance in trustworthiness among open-source MLLMs, and\nshows better robustness than GPT-4V in preventing hallucinations aroused from\nover-generalization. We open-source our code, model, and data at\nhttps://github.com/RLHF-V/RLHF-V.\n",
                "链接": "https://arxiv.org/abs/2312.00849"
            },
            {
                "文章ID": "123656",
                "标题": "An Adaptive Placement and Parallelism Framework for Accelerating RLHF\n  Training",
                "作者": " Youshao Xiao,  Weichang Wu,  Zhenglei Zhou,  Fagui Mao,  Shangchun Zhao,  Lin Ju,  Lei Liang,  Xiaolu Zhang,  Jun Zhou",
                "发布日期": "2023-12-20",
                "摘要": "  Recently, ChatGPT or InstructGPT like large language models (LLM) has made a\nsignificant impact in the AI world. These models are incredibly versatile,\ncapable of performing language tasks on par or even exceeding the capabilities\nof human experts. Many works have attempted to reproduce the complex\nInstructGPT's RLHF (Reinforcement Learning with Human Feedback) training\npipeline. However, the mainstream distributed RLHF training methods typically\nadopt a fixed model placement strategy, referred to as the Flattening strategy.\nThis strategy treats all four models involved in RLHF as a single entity and\nplaces them on all devices, regardless of their differences. Unfortunately,\nthis strategy exacerbates the generation bottlenecks in the RLHF training and\ndegrades the overall training efficiency. To address these issues, we propose\nan adaptive model placement framework that offers two flexible model placement\nstrategies. These strategies allow for the agile allocation of models across\ndevices in a fine-grained manner. The Interleaving strategy helps reduce memory\nredundancy and communication costs during RLHF training. On the other hand, the\nSeparation strategy improves the throughput of model training by separating the\ntraining and generation stages of the RLHF pipeline. Notably, this framework\nseamlessly integrates with other mainstream techniques for acceleration and\nenables automatic hyperparameter search. Extensive experiments have\ndemonstrated that our Interleaving and Separation strategies can achieve\nnotable improvements up to 11x, compared to the current state-of-the-art (SOTA)\napproaches. These experiments encompassed a wide range of training scenarios,\ninvolving models of varying sizes and devices of different scales. The results\nhighlight the effectiveness and superiority of our approaches in accelerating\nthe training of distributed RLHF.\n",
                "链接": "https://arxiv.org/abs/2312.11819"
            }
        ]
    },
    {
        "question": {
            "question": "利用大模型做数学题生成的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "21205",
                "标题": "NaturalProver: Grounded Mathematical Proof Generation with Language\n  Models",
                "作者": " Sean Welleck,  Jiacheng Liu,  Ximing Lu,  Hannaneh Hajishirzi,  Yejin Choi",
                "发布日期": "2022-11-02",
                "摘要": "  Theorem proving in natural mathematical language - the mixture of symbolic\nand natural language used by humans - plays a central role in mathematical\nadvances and education, and tests aspects of reasoning that are core to\nintelligence. Yet it has remained underexplored with modern generative models.\nWe study large-scale language models on two new generation tasks: suggesting\nthe next step in a mathematical proof, and full proof generation. We develop\nNaturalProver, a language model that generates proofs by conditioning on\nbackground references (e.g. theorems and definitions that are either retrieved\nor human-provided), and optionally enforces their presence with constrained\ndecoding. On theorems from the NaturalProofs benchmark, NaturalProver improves\nthe quality of next-step suggestions and generated proofs over fine-tuned\nGPT-3, according to human evaluations from university-level mathematics\nstudents. NaturalProver is capable of proving some theorems that require short\n(2-6 step) proofs, and providing next-step suggestions that are rated as\ncorrect and useful over 40% of the time, which is to our knowledge the first\ndemonstration of these capabilities using neural language models.\n",
                "链接": "https://arxiv.org/abs/2205.12910"
            },
            {
                "文章ID": "85371",
                "标题": "Mathematical conjecture generation using machine intelligence",
                "作者": " Challenger Mishra,  Subhayan Roy Moulik,  Rahul Sarkar",
                "发布日期": "2023-06-13",
                "摘要": "  Conjectures have historically played an important role in the development of\npure mathematics. We propose a systematic approach to finding abstract patterns\nin mathematical data, in order to generate conjectures about mathematical\ninequalities, using machine intelligence. We focus on strict inequalities of\ntype f < g and associate them with a vector space. By geometerising this space,\nwhich we refer to as a conjecture space, we prove that this space is isomorphic\nto a Banach manifold. We develop a structural understanding of this conjecture\nspace by studying linear automorphisms of this manifold and show that this\nspace admits several free group actions. Based on these insights, we propose an\nalgorithmic pipeline to generate novel conjectures using geometric gradient\ndescent, where the metric is informed by the invariances of the conjecture\nspace. As proof of concept, we give a toy algorithm to generate novel\nconjectures about the prime counting function and diameters of Cayley graphs of\nnon-abelian simple groups. We also report private communications with\ncolleagues in which some conjectures were proved, and highlight that some\nconjectures generated using this procedure are still unproven. Finally, we\npropose a pipeline of mathematical discovery in this space and highlight the\nimportance of domain expertise in this pipeline.\n",
                "链接": "https://arxiv.org/abs/2306.07277"
            },
            {
                "文章ID": "61811",
                "标题": "Tree-Based Representation and Generation of Natural and Mathematical\n  Language",
                "作者": " Alexander Scarlatos,  Andrew Lan",
                "发布日期": "2023-02-17",
                "摘要": "  Mathematical language in scientific communications and educational scenarios\nis important yet relatively understudied compared to natural languages. Recent\nworks on mathematical language focus either on representing stand-alone\nmathematical expressions, especially in their natural tree format, or\nmathematical reasoning in pre-trained natural language models. Existing works\non jointly modeling and generating natural and mathematical languages simply\ntreat mathematical expressions as text, without accounting for the rigid\nstructural properties of mathematical expressions. In this paper, we propose a\nseries of modifications to existing language models to jointly represent and\ngenerate text and math: representing mathematical expressions as sequences of\nnode tokens in their operator tree format, using math symbol and tree position\nembeddings to preserve the semantic and structural properties of mathematical\nexpressions, and using a constrained decoding method to generate mathematically\nvalid expressions. We ground our modifications in GPT-2, resulting in a model\nMathGPT, and demonstrate that it outperforms baselines on mathematical\nexpression generation tasks.\n",
                "链接": "https://arxiv.org/abs/2302.07974"
            },
            {
                "文章ID": "91891",
                "标题": "Generating Mathematical Derivations with Large Language Models",
                "作者": " Jordan Meadows,  Marco Valentino,  Andre Freitas",
                "发布日期": "2023-08-09",
                "摘要": "  The derivation of mathematical results in specialised fields, using Large\nLanguage Models (LLMs), is an emerging research direction that can help\nidentify models' limitations, and potentially support mathematical discovery.\nIn this paper, we leverage a symbolic engine to generate derivations of\nequations at scale, and investigate the capabilities of LLMs when deriving goal\nequations from premises. Specifically, we employ in-context learning for GPT\nand fine-tune a range of T5 models to compare the robustness and generalisation\nof pre-training strategies to specialised models. Empirical results show that\nfine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and\nout-of-distribution test sets in conventional scores. However, an in-depth\nanalysis reveals that the fine-tuned models are more sensitive to perturbations\ninvolving unseen symbols and (to a lesser extent) changes to equation\nstructure. In addition, we analyse 1.7K equations, and over 200 derivations, to\nhighlight common reasoning errors such as the inclusion of incorrect,\nirrelevant, and redundant equations. Finally, we explore the suitability of\nexisting metrics for evaluating mathematical derivations and find evidence\nthat, while they can capture general properties such as sensitivity to\nperturbations, they fail to highlight fine-grained reasoning errors and\nessential differences between models. Overall, this work demonstrates that\ntraining models on synthetic data may improve their math capabilities beyond\nmuch larger LLMs, but current metrics are not appropriately assessing the\nquality of generated mathematical text.\n",
                "链接": "https://arxiv.org/abs/2307.09998"
            },
            {
                "文章ID": "89573",
                "标题": "Math Agents: Computational Infrastructure, Mathematical Embedding, and\n  Genomics",
                "作者": " Melanie Swan,  Takashi Kido,  Eric Roland,  Renato P. dos Santos",
                "发布日期": "2023-07-07",
                "摘要": "  The advancement in generative AI could be boosted with more accessible\nmathematics. Beyond human-AI chat, large language models (LLMs) are emerging in\nprogramming, algorithm discovery, and theorem proving, yet their genomics\napplication is limited. This project introduces Math Agents and mathematical\nembedding as fresh entries to the \"Moore's Law of Mathematics\", using a\nGPT-based workflow to convert equations from literature into LaTeX and Python\nformats. While many digital equation representations exist, there's a lack of\nautomated large-scale evaluation tools. LLMs are pivotal as linguistic user\ninterfaces, providing natural language access for human-AI chat and formal\nlanguages for large-scale AI-assisted computational infrastructure. Given the\ninfinite formal possibility spaces, Math Agents, which interact with math,\ncould potentially shift us from \"big data\" to \"big math\". Math, unlike the more\nflexible natural language, has properties subject to proof, enabling its use\nbeyond traditional applications like high-validation math-certified icons for\nAI alignment aims. This project aims to use Math Agents and mathematical\nembeddings to address the ageing issue in information systems biology by\napplying multiscalar physics mathematics to disease models and genomic data.\nGenerative AI with episodic memory could help analyse causal relations in\nlongitudinal health records, using SIR Precision Health models. Genomic data is\nsuggested for addressing the unsolved Alzheimer's disease problem.\n",
                "链接": "https://arxiv.org/abs/2307.02502"
            },
            {
                "文章ID": "59308",
                "标题": "Mathematical Capabilities of ChatGPT",
                "作者": " Simon Frieder,  Luca Pinchetti,  Alexis Chevalier,  Ryan-Rhys Griffiths,  Tommaso Salvatori,  Thomas Lukasiewicz,  Philipp Christian Petersen,  Julius Berner",
                "发布日期": "2023-12-12",
                "摘要": "  We investigate the mathematical capabilities of two iterations of ChatGPT\n(released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on\npublicly available datasets, as well as hand-crafted ones, using a novel\nmethodology. In contrast to formal mathematics, where large databases of formal\nproofs are available (e.g., the Lean Mathematical Library), current datasets of\nnatural-language mathematics, used to benchmark language models, either cover\nonly elementary mathematics or are very small. We address this by publicly\nreleasing two new datasets: GHOSTS and miniGHOSTS. These are the first\nnatural-language datasets curated by working researchers in mathematics that\n(1) aim to cover graduate-level mathematics, (2) provide a holistic overview of\nthe mathematical capabilities of language models, and (3) distinguish multiple\ndimensions of mathematical reasoning. These datasets also test whether ChatGPT\nand GPT-4 can be helpful assistants to professional mathematicians by emulating\nuse cases that arise in the daily professional activities of mathematicians. We\nbenchmark the models on a range of fine-grained performance metrics. For\nadvanced mathematics, this is the most detailed evaluation effort to date. We\nfind that ChatGPT can be used most successfully as a mathematical assistant for\nquerying facts, acting as a mathematical search engine and knowledge base\ninterface. GPT-4 can additionally be used for undergraduate-level mathematics\nbut fails on graduate-level difficulty. Contrary to many positive reports in\nthe media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of\nselection bias), their overall mathematical performance is well below the level\nof a graduate student. Hence, if your goal is to use ChatGPT to pass a\ngraduate-level math exam, you would be better off copying from your average\npeer!\n",
                "链接": "https://arxiv.org/abs/2301.13867"
            },
            {
                "文章ID": "99865",
                "标题": "Extracting Mathematical Concepts with Large Language Models",
                "作者": " Valeria de Paiva,  Qiyue Gao,  Pavel Kovalev,  Lawrence S. Moss",
                "发布日期": "2023-09-06",
                "摘要": "  We extract mathematical concepts from mathematical text using generative\nlarge language models (LLMs) like ChatGPT, contributing to the field of\nautomatic term extraction (ATE) and mathematical text processing, and also to\nthe study of LLMs themselves. Our work builds on that of others in that we aim\nfor automatic extraction of terms (keywords) in one mathematical field,\ncategory theory, using as a corpus the 755 abstracts from a snapshot of the\nonline journal \"Theory and Applications of Categories\", circa 2020. Where our\nstudy diverges from previous work is in (1) providing a more thorough analysis\nof what makes mathematical term extraction a difficult problem to begin with;\n(2) paying close attention to inter-annotator disagreements; (3) providing a\nset of guidelines which both human and machine annotators could use to\nstandardize the extraction process; (4) introducing a new annotation tool to\nhelp humans with ATE, applicable to any mathematical field and even beyond\nmathematics; (5) using prompts to ChatGPT as part of the extraction process,\nand proposing best practices for such prompts; and (6) raising the question of\nwhether ChatGPT could be used as an annotator on the same level as human\nexperts. Our overall findings are that the matter of mathematical ATE is an\ninteresting field which can benefit from participation by LLMs, but LLMs\nthemselves cannot at this time surpass human performance on it.\n",
                "链接": "https://arxiv.org/abs/2309.00642"
            },
            {
                "文章ID": "103794",
                "标题": "LPML: LLM-Prompting Markup Language for Mathematical Reasoning",
                "作者": " Ryutaro Yamauchi,  Sho Sonoda,  Akiyoshi Sannai,  Wataru Kumagai",
                "发布日期": "2023-10-12",
                "摘要": "  In utilizing large language models (LLMs) for mathematical reasoning,\naddressing the errors in the reasoning and calculation present in the generated\ntext by LLMs is a crucial challenge. In this paper, we propose a novel\nframework that integrates the Chain-of-Thought (CoT) method with an external\ntool (Python REPL). We discovered that by prompting LLMs to generate structured\ntext in XML-like markup language, we could seamlessly integrate CoT and the\nexternal tool and control the undesired behaviors of LLMs. With our approach,\nLLMs can utilize Python computation to rectify errors within CoT. We applied\nour method to ChatGPT (GPT-3.5) to solve challenging mathematical problems and\ndemonstrated that combining CoT and Python REPL through the markup language\nenhances the reasoning capability of LLMs. Our approach enables LLMs to write\nthe markup language and perform advanced mathematical reasoning using only\nzero-shot prompting.\n",
                "链接": "https://arxiv.org/abs/2309.13078"
            },
            {
                "文章ID": "98013",
                "标题": "Algorithm-assisted discovery of an intrinsic order among mathematical\n  constants",
                "作者": " Rotem Elimelech,  Ofir David,  Carlos De la Cruz Mengual,  Rotem Kalisch,  Wolfgang Berndt,  Michael Shalyt,  Mark Silberstein,  Yaron Hadad,  Ido Kaminer",
                "发布日期": "2023-10-17",
                "摘要": "  In recent decades, a growing number of discoveries in fields of mathematics\nhave been assisted by computer algorithms, primarily for exploring large\nparameter spaces that humans would take too long to investigate. As computers\nand algorithms become more powerful, an intriguing possibility arises - the\ninterplay between human intuition and computer algorithms can lead to\ndiscoveries of novel mathematical concepts that would otherwise remain elusive.\nTo realize this perspective, we have developed a massively parallel computer\nalgorithm that discovers an unprecedented number of continued fraction formulas\nfor fundamental mathematical constants. The sheer number of formulas discovered\nby the algorithm unveils a novel mathematical structure that we call the\nconservative matrix field. Such matrix fields (1) unify thousands of existing\nformulas, (2) generate infinitely many new formulas, and most importantly, (3)\nlead to unexpected relations between different mathematical constants,\nincluding multiple integer values of the Riemann zeta function. Conservative\nmatrix fields also enable new mathematical proofs of irrationality. In\nparticular, we can use them to generalize the celebrated proof by Ap\\'ery for\nthe irrationality of $\\zeta(3)$. Utilizing thousands of personal computers\nworldwide, our computer-supported research strategy demonstrates the power of\nexperimental mathematics, highlighting the prospects of large-scale\ncomputational approaches to tackle longstanding open problems and discover\nunexpected connections across diverse fields of science.\n",
                "链接": "https://arxiv.org/abs/2308.11829"
            },
            {
                "文章ID": "48502",
                "标题": "Semantic Representations of Mathematical Expressions in a Continuous\n  Vector Space",
                "作者": " Neeraj Gangwar,  Nickvash Kani",
                "发布日期": "2023-09-06",
                "摘要": "  Mathematical notation makes up a large portion of STEM literature, yet\nfinding semantic representations for formulae remains a challenging problem.\nBecause mathematical notation is precise, and its meaning changes significantly\nwith small character shifts, the methods that work for natural text do not\nnecessarily work well for mathematical expressions. This work describes an\napproach for representing mathematical expressions in a continuous vector\nspace. We use the encoder of a sequence-to-sequence architecture, trained on\nvisually different but mathematically equivalent expressions, to generate\nvector representations (or embeddings). We compare this approach with a\nstructural approach that considers visual layout to embed an expression and\nshow that our proposed approach is better at capturing mathematical semantics.\nFinally, to expedite future research, we publish a corpus of equivalent\ntranscendental and algebraic expression pairs.\n",
                "链接": "https://arxiv.org/abs/2211.08142"
            }
        ]
    },
    {
        "question": {
            "question": "帮我找一下用大模型进行论文查找的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "110649",
                "标题": "Large Search Model: Redefining Search Stack in the Era of LLMs",
                "作者": " Liang Wang,  Nan Yang,  Xiaolong Huang,  Linjun Yang,  Rangan Majumder,  Furu Wei",
                "发布日期": "2023-10-24",
                "摘要": "  Modern search engines are built on a stack of different components, including\nquery understanding, retrieval, multi-stage ranking, and question answering,\namong others. These components are often optimized and deployed independently.\nIn this paper, we introduce a novel conceptual framework called large search\nmodel, which redefines the conventional search stack by unifying search tasks\nwith one large language model (LLM). All tasks are formulated as autoregressive\ntext generation problems, allowing for the customization of tasks through the\nuse of natural language prompts. This proposed framework capitalizes on the\nstrong language understanding and reasoning capabilities of LLMs, offering the\npotential to enhance search result quality while simultaneously simplifying the\nexisting cumbersome search stack. To substantiate the feasibility of this\nframework, we present a series of proof-of-concept experiments and discuss the\npotential challenges associated with implementing this approach within\nreal-world search systems.\n",
                "链接": "https://arxiv.org/abs/2310.14587"
            },
            {
                "文章ID": "91534",
                "标题": "Federated Large Language Model: A Position Paper",
                "作者": " Chaochao Chen,  Xiaohua Feng,  Jun Zhou,  Jianwei Yin,  Xiaolin Zheng",
                "发布日期": "2023-07-19",
                "摘要": "  Large scale language models (LLM) have received significant attention and\nfound diverse applications across various domains, but their development\nencounters challenges in real-world scenarios. These challenges arise due to\nthe scarcity of public domain data availability and the need to maintain\nprivacy with respect to private domain data. To address these issues, federated\nlearning (FL) has emerged as a promising technology that enables collaborative\ntraining of shared models while preserving decentralized data. We propose the\nconcept of federated LLM, which comprises three key components, i.e., federated\nLLM pre-training, federated LLM fine-tuning, and federated LLM prompt\nengineering. For each component, we discuss its advantage over traditional LLM\ntraining methods and propose specific engineering strategies for\nimplementation. Furthermore, we explore the novel challenges introduced by the\nintegration of FL and LLM. We analyze existing solutions and identify potential\nobstacles faced by these solutions within the context of federated LLM.\n",
                "链接": "https://arxiv.org/abs/2307.08925"
            },
            {
                "文章ID": "110564",
                "标题": "Monte Carlo Thought Search: Large Language Model Querying for Complex\n  Scientific Reasoning in Catalyst Design",
                "作者": " Henry W. Sprueill,  Carl Edwards,  Mariefel V. Olarte,  Udishnu Sanyal,  Heng Ji,  Sutanay Choudhury",
                "发布日期": "2023-11-07",
                "摘要": "  Discovering novel catalysts requires complex reasoning involving multiple\nchemical properties and resultant trade-offs, leading to a combinatorial growth\nin the search space. While large language models (LLM) have demonstrated novel\ncapabilities for chemistry through complex instruction following capabilities\nand high quality reasoning, a goal-driven combinatorial search using LLMs has\nnot been explored in detail. In this work, we present a Monte Carlo Tree\nSearch-based approach that improves beyond state-of-the-art chain-of-thought\nprompting variants to augment scientific reasoning. We introduce two new\nreasoning datasets: 1) a curation of computational chemistry simulations, and\n2) diverse questions written by catalysis researchers for reasoning about novel\nchemical conversion processes. We improve over the best baseline by 25.8\\% and\nfind that our approach can augment scientist's reasoning and discovery process\nwith novel insights.\n",
                "链接": "https://arxiv.org/abs/2310.14420"
            },
            {
                "文章ID": "34",
                "标题": "Semantic Search for Large Scale Clinical Ontologies",
                "作者": " Duy-Hoa Ngo,  Madonna Kemp,  Donna Truran,  Bevan Koopman,  Alejandro Metke-Jimenez",
                "发布日期": "2022-01-04",
                "摘要": "  Finding concepts in large clinical ontologies can be challenging when queries\nuse different vocabularies. A search algorithm that overcomes this problem is\nuseful in applications such as concept normalisation and ontology matching,\nwhere concepts can be referred to in different ways, using different synonyms.\nIn this paper, we present a deep learning based approach to build a semantic\nsearch system for large clinical ontologies. We propose a Triplet-BERT model\nand a method that generates training data directly from the ontologies. The\nmodel is evaluated using five real benchmark data sets and the results show\nthat our approach achieves high results on both free text to concept and\nconcept to concept searching tasks, and outperforms all baseline methods.\n",
                "链接": "https://arxiv.org/abs/2201.00118"
            },
            {
                "文章ID": "114213",
                "标题": "Large Language Model based Long-tail Query Rewriting in Taobao Search",
                "作者": " Wenjun Peng,  Guiyang Li,  Yue Jiang,  Zilong Wang,  Dan Ou,  Xiaoyi Zeng,  Derong Xu,   Tongxu,  Enhong Chen",
                "发布日期": "2023-11-14",
                "摘要": "  In the realm of e-commerce search, the significance of semantic matching\ncannot be overstated, as it directly impacts both user experience and company\nrevenue. Along this line, query rewriting, serving as an important technique to\nbridge the semantic gaps inherent in the semantic matching process, has\nattached wide attention from the industry and academia. However, existing query\nrewriting methods often struggle to effectively optimize long-tail queries and\nalleviate the phenomenon of \"few-recall\" caused by semantic gap. In this paper,\nwe present BEQUE, a comprehensive framework that Bridges the sEmantic gap for\nlong-tail QUEries. In detail, BEQUE comprises three stages: multi-instruction\nsupervised fine tuning (SFT), offline feedback, and objective alignment. We\nfirst construct a rewriting dataset based on rejection sampling and auxiliary\ntasks mixing to fine-tune our large language model (LLM) in a supervised\nfashion. Subsequently, with the well-trained LLM, we employ beam search to\ngenerate multiple candidate rewrites, and feed them into Taobao offline system\nto obtain the partial order. Leveraging the partial order of rewrites, we\nintroduce a contrastive learning method to highlight the distinctions between\nrewrites, and align the model with the Taobao online objectives. Offline\nexperiments prove the effectiveness of our method in bridging semantic gap.\nOnline A/B tests reveal that our method can significantly boost gross\nmerchandise volume (GMV), number of transaction (#Trans) and unique visitor\n(UV) for long-tail queries. BEQUE has been deployed on Taobao, one of most\npopular online shopping platforms in China, since October 2023.\n",
                "链接": "https://arxiv.org/abs/2311.03758"
            },
            {
                "文章ID": "77883",
                "标题": "Large Language Models are Built-in Autoregressive Search Engines",
                "作者": " Noah Ziems,  Wenhao Yu,  Zhihan Zhang,  Meng Jiang",
                "发布日期": "2023-05-17",
                "摘要": "  Document retrieval is a key stage of standard Web search engines. Existing\ndual-encoder dense retrievers obtain representations for questions and\ndocuments independently, allowing for only shallow interactions between them.\nTo overcome this limitation, recent autoregressive search engines replace the\ndual-encoder architecture by directly generating identifiers for relevant\ndocuments in the candidate pool. However, the training cost of such\nautoregressive search engines rises sharply as the number of candidate\ndocuments increases. In this paper, we find that large language models (LLMs)\ncan follow human instructions to directly generate URLs for document retrieval.\n  Surprisingly, when providing a few {Query-URL} pairs as in-context\ndemonstrations, LLMs can generate Web URLs where nearly 90\\% of the\ncorresponding documents contain correct answers to open-domain questions. In\nthis way, LLMs can be thought of as built-in search engines, since they have\nnot been explicitly trained to map questions to document identifiers.\nExperiments demonstrate that our method can consistently achieve better\nretrieval performance than existing retrieval approaches by a significant\nmargin on three open-domain question answering benchmarks, under both zero and\nfew-shot settings. The code for this work can be found at\n\\url{https://github.com/Ziems/llm-url}.\n",
                "链接": "https://arxiv.org/abs/2305.09612"
            },
            {
                "文章ID": "105172",
                "标题": "Alphazero-like Tree-Search can Guide Large Language Model Decoding and\n  Training",
                "作者": " Xidong Feng,  Ziyu Wan,  Muning Wen,  Ying Wen,  Weinan Zhang,  Jun Wang",
                "发布日期": "2023-10-02",
                "摘要": "  Large language models (LLMs) typically employ sampling or beam search,\naccompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and\ndecoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via\nPlanning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing\ntree-search algorithms to guide multi-step reasoning. These methods mainly\nfocus on LLMs' reasoning ability during inference and heavily rely on\nhuman-designed prompts to activate LLM as a value function, which lacks general\napplicability and scalability. To address these limitations, we present an\nAlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically\nillustrating how tree-search with a learned value function can guide LLMs'\ndecoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a\nlearned value function, our approach can be generally applied to different\ntasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without\nprompting advanced, large-scale models. (2) It can guide LLM's decoding during\nboth inference and training. Empirical evaluations across reasoning, planning,\nand RLHF alignment tasks validate the effectiveness of TS-LLM, even on trees\nwith a depth of 64.\n",
                "链接": "https://arxiv.org/abs/2309.17179"
            },
            {
                "文章ID": "36968",
                "标题": "An Embedding-Based Grocery Search Model at Instacart",
                "作者": " Yuqing Xie,  Taesik Na,  Xiao Xiao,  Saurav Manchanda,  Young Rao,  Zhihong Xu,  Guanghua Shu,  Esther Vasiete,  Tejaswi Tenneti,  Haixun Wang",
                "发布日期": "2022-09-14",
                "摘要": "  The key to e-commerce search is how to best utilize the large yet noisy log\ndata. In this paper, we present our embedding-based model for grocery search at\nInstacart. The system learns query and product representations with a two-tower\ntransformer-based encoder architecture. To tackle the cold-start problem, we\nfocus on content-based features. To train the model efficiently on noisy data,\nwe propose a self-adversarial learning method and a cascade training method.\nAccOn an offline human evaluation dataset, we achieve 10% relative improvement\nin RECALL@20, and for online A/B testing, we achieve 4.1% cart-adds per search\n(CAPS) and 1.5% gross merchandise value (GMV) improvement. We describe how we\ntrain and deploy the embedding based search model and give a detailed analysis\nof the effectiveness of our method.\n",
                "链接": "https://arxiv.org/abs/2209.05555"
            },
            {
                "文章ID": "52542",
                "标题": "You Don't Know Search: Helping Users Find Code by Automatically\n  Evaluating Alternative Queries",
                "作者": " Rijnard van Tonder",
                "发布日期": "2022-12-08",
                "摘要": "  Tens of thousands of engineers use Sourcegraph day-to-day to search for code\nand rely on it to make progress on software development tasks. We face a key\nchallenge in designing a query language that accommodates the needs of a broad\nspectrum of users. Our experience shows that users express different and often\ncontradictory preferences for how queries should be interpreted. These\npreferences stem from users with differing usage contexts, technical\nexperience, and implicit expectations from using prior tools. At the same time,\ndesigning a code search query language poses unique challenges because it\nintersects traditional search engines and full-fledged programming languages.\nFor example, code search queries adopt certain syntactic conventions in the\ninterest of simplicity and terseness but invariably risk encoding implicit\nsemantics that are ambiguous at face-value (a single space in a query could\nmean three or more semantically different things depending on surrounding\nterms). Users often need to disambiguate intent with additional syntax so that\na query expresses what they actually want to search. This need to disambiguate\nis one of the primary frustrations we've seen users experience with writing\nsearch queries in the last three years. We share our observations that lead us\nto a fresh perspective where code search behavior can straddle seemingly\nambiguous queries. We develop Automated Query Evaluation (AQE), a new technique\nthat automatically generates and adaptively runs alternative query\ninterpretations in frustration-prone conditions. We evaluate AQE with an A/B\ntest across more than 10,000 unique users on our publicly-available code search\ninstance. Our main result shows that relative to the control group, users are\non average 22% more likely to click on a search result at all on any given day\nwhen AQE is active.\n",
                "链接": "https://arxiv.org/abs/2212.03459"
            },
            {
                "文章ID": "58115",
                "标题": "How search engine marketing influences user knowledge gain: Development\n  and empirical testing of an information search behavior model",
                "作者": " Sebastian Schultheiß",
                "发布日期": "2023-01-25",
                "摘要": "  People use search engines to find answers to questions related to their\nhealth, finances, or other socially relevant issues. However, most users are\nunaware that search results are considerably influenced by search engine\nmarketing (SEM). SEM measures are driven by commercial, political, or other\nmotives. Due to these motivations, two questions arise: What information\nquality is mediated through SEM? And how is collecting documents of different\nquality affecting user knowledge gain? Both questions are not considered by\nexisting models of information behavior. Hence, the doctoral research project\ndescribed in this paper aims to develop and empirically test an information\nsearch behavior model on the influences of SEM on user knowledge gain and\nthereby contribute to the search as learning body of research.\n",
                "链接": "https://arxiv.org/abs/2301.10086"
            }
        ]
    },
    {
        "question": {
            "question": "请帮我找到，最早提出分布式词表示的论文是哪一篇？",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "87597",
                "标题": "IERL: Interpretable Ensemble Representation Learning -- Combining\n  CrowdSourced Knowledge and Distributed Semantic Representations",
                "作者": " Yuxin Zi,  Kaushik Roy,  Vignesh Narayanan,  Manas Gaur,  Amit Sheth",
                "发布日期": "2023-06-27",
                "摘要": "  Large Language Models (LLMs) encode meanings of words in the form of\ndistributed semantics. Distributed semantics capture common statistical\npatterns among language tokens (words, phrases, and sentences) from large\namounts of data. LLMs perform exceedingly well across General Language\nUnderstanding Evaluation (GLUE) tasks designed to test a model's understanding\nof the meanings of the input tokens. However, recent studies have shown that\nLLMs tend to generate unintended, inconsistent, or wrong texts as outputs when\nprocessing inputs that were seen rarely during training, or inputs that are\nassociated with diverse contexts (e.g., well-known hallucination phenomenon in\nlanguage generation tasks). Crowdsourced and expert-curated knowledge graphs\nsuch as ConceptNet are designed to capture the meaning of words from a compact\nset of well-defined contexts. Thus LLMs may benefit from leveraging such\nknowledge contexts to reduce inconsistencies in outputs. We propose a novel\nensemble learning method, Interpretable Ensemble Representation Learning\n(IERL), that systematically combines LLM and crowdsourced knowledge\nrepresentations of input tokens. IERL has the distinct advantage of being\ninterpretable by design (when was the LLM context used vs. when was the\nknowledge context used?) over state-of-the-art (SOTA) methods, allowing\nscrutiny of the inputs in conjunction with the parameters of the model,\nfacilitating the analysis of models' inconsistent or irrelevant outputs.\nAlthough IERL is agnostic to the choice of LLM and crowdsourced knowledge, we\ndemonstrate our approach using BERT and ConceptNet. We report improved or\ncompetitive results with IERL across GLUE tasks over current SOTA methods and\nsignificantly enhanced model interpretability.\n",
                "链接": "https://arxiv.org/abs/2306.13865"
            },
            {
                "文章ID": "1090",
                "标题": "Local2Global: A distributed approach for scaling representation learning\n  on graphs",
                "作者": " Lucas G. S. Jeub,  Giovanni Colavizza,  Xiaowen Dong,  Marya Bazzi,  Mihai Cucuringu",
                "发布日期": "2022-01-14",
                "摘要": "  We propose a decentralised \"local2global\"' approach to graph representation\nlearning, that one can a-priori use to scale any embedding technique. Our\nlocal2global approach proceeds by first dividing the input graph into\noverlapping subgraphs (or \"patches\") and training local representations for\neach patch independently. In a second step, we combine the local\nrepresentations into a globally consistent representation by estimating the set\nof rigid motions that best align the local representations using information\nfrom the patch overlaps, via group synchronization. A key distinguishing\nfeature of local2global relative to existing work is that patches are trained\nindependently without the need for the often costly parameter synchronization\nduring distributed training. This allows local2global to scale to large-scale\nindustrial applications, where the input graph may not even fit into memory and\nmay be stored in a distributed manner. We apply local2global on data sets of\ndifferent sizes and show that our approach achieves a good trade-off between\nscale and accuracy on edge reconstruction and semi-supervised classification.\nWe also consider the downstream task of anomaly detection and show how one can\nuse local2global to highlight anomalies in cybersecurity networks.\n",
                "链接": "https://arxiv.org/abs/2201.04729"
            },
            {
                "文章ID": "69845",
                "标题": "Efficient distributed representations beyond negative sampling",
                "作者": " Lorenzo Dall'Amico,  Enrico Maria Belliardo",
                "发布日期": "2023-10-31",
                "摘要": "  This article describes an efficient method to learn distributed\nrepresentations, also known as embeddings. This is accomplished minimizing an\nobjective function similar to the one introduced in the Word2Vec algorithm and\nlater adopted in several works. The optimization computational bottleneck is\nthe calculation of the softmax normalization constants for which a number of\noperations scaling quadratically with the sample size is required. This\ncomplexity is unsuited for large datasets and negative sampling is a popular\nworkaround, allowing one to obtain distributed representations in linear time\nwith respect to the sample size. Negative sampling consists, however, in a\nchange of the loss function and hence solves a different optimization problem\nfrom the one originally proposed. Our contribution is to show that the sotfmax\nnormalization constants can be estimated in linear time, allowing us to design\nan efficient optimization strategy to learn distributed representations. We\ntest our approximation on two popular applications related to word and node\nembeddings. The results evidence competing performance in terms of accuracy\nwith respect to negative sampling with a remarkably lower computational time.\n",
                "链接": "https://arxiv.org/abs/2303.17475"
            },
            {
                "文章ID": "110199",
                "标题": "Learning Successor Representations with Distributed Hebbian Temporal\n  Memory",
                "作者": " Evgenii Dzhivelikian,  Petr Kuderov,  Aleksandr I. Panov",
                "发布日期": "2023-10-23",
                "摘要": "  This paper presents a novel approach to address the challenge of online\nhidden representation learning for decision-making under uncertainty in\nnon-stationary, partially observable environments. The proposed algorithm,\nDistributed Hebbian Temporal Memory (DHTM), is based on factor graph formalism\nand a multicomponent neuron model. DHTM aims to capture sequential data\nrelationships and make cumulative predictions about future observations,\nforming Successor Representation (SR). Inspired by neurophysiological models of\nthe neocortex, the algorithm utilizes distributed representations, sparse\ntransition matrices, and local Hebbian-like learning rules to overcome the\ninstability and slow learning process of traditional temporal memory algorithms\nlike RNN and HMM. Experimental results demonstrate that DHTM outperforms\nclassical LSTM and performs comparably to more advanced RNN-like algorithms,\nspeeding up Temporal Difference learning for SR in changing environments.\nAdditionally, we compare the SRs produced by DHTM to another biologically\ninspired HMM-like algorithm, CSCG. Our findings suggest that DHTM is a\npromising approach for addressing the challenges of online hidden\nrepresentation learning in dynamic environments.\n",
                "链接": "https://arxiv.org/abs/2310.13391"
            },
            {
                "文章ID": "73435",
                "标题": "Distributed Neural Representation for Reactive in situ Visualization",
                "作者": " Qi Wu,  Joseph A. Insley,  Victor A. Mateevitsi,  Silvio Rizzi,  Michael E. Papka,  Kwan-Liu Ma",
                "发布日期": "2023-04-21",
                "摘要": "  In situ visualization and steering of computational modeling can be\neffectively achieved using reactive programming, which leverages temporal\nabstraction and data caching mechanisms to create dynamic workflows. However,\nimplementing a temporal cache for large-scale simulations can be challenging.\nImplicit neural networks have proven effective in compressing large volume\ndata. However, their application to distributed data has yet to be fully\nexplored. In this work, we develop an implicit neural representation for\ndistributed volume data and incorporate it into the DIVA reactive programming\nsystem. This implementation enables us to build an in situ temporal caching\nsystem with a capacity 100 times larger than previously achieved. We integrate\nour implementation into the Ascent infrastructure and evaluate its performance\nusing real-world simulations.\n",
                "链接": "https://arxiv.org/abs/2304.10516"
            },
            {
                "文章ID": "23809",
                "标题": "Extreme Masking for Learning Instance and Distributed Visual\n  Representations",
                "作者": " Zhirong Wu,  Zihang Lai,  Xiao Sun,  Stephen Lin",
                "发布日期": "2023-03-09",
                "摘要": "  The paper presents a scalable approach for learning spatially distributed\nvisual representations over individual tokens and a holistic instance\nrepresentation simultaneously. We use self-attention blocks to represent\nspatially distributed tokens, followed by cross-attention blocks to aggregate\nthe holistic image instance. The core of the approach is the use of extremely\nlarge token masking (75\\%-90\\%) as the data augmentation for supervision. Our\nmodel, named ExtreMA, follows the plain BYOL approach where the instance\nrepresentation from the unmasked subset is trained to predict that from the\nintact input. Instead of encouraging invariance across inputs, the model is\nrequired to capture informative variations in an image. The paper makes three\ncontributions: 1) It presents random masking as a strong and computationally\nefficient data augmentation for siamese representation learning. 2) With\nmultiple sampling per instance, extreme masking greatly speeds up learning and\nimproves performance with more data. 3) ExtreMA obtains stronger linear probing\nperformance than masked modeling methods, and better transfer performance than\nprior contrastive models.\n",
                "链接": "https://arxiv.org/abs/2206.04667"
            },
            {
                "文章ID": "38067",
                "标题": "Distributed representations of graphs for drug pair scoring",
                "作者": " Paul Scherer,  Pietro Liò,  Mateja Jamnik",
                "发布日期": "2022-11-28",
                "摘要": "  In this paper we study the practicality and usefulness of incorporating\ndistributed representations of graphs into models within the context of drug\npair scoring. We argue that the real world growth and update cycles of drug\npair scoring datasets subvert the limitations of transductive learning\nassociated with distributed representations. Furthermore, we argue that the\nvocabulary of discrete substructure patterns induced over drug sets is not\ndramatically large due to the limited set of atom types and constraints on\nbonding patterns enforced by chemistry. Under this pretext, we explore the\neffectiveness of distributed representations of the molecular graphs of drugs\nin drug pair scoring tasks such as drug synergy, polypharmacy, and drug-drug\ninteraction prediction. To achieve this, we present a methodology for learning\nand incorporating distributed representations of graphs within a unified\nframework for drug pair scoring. Subsequently, we augment a number of recent\nand state-of-the-art models to utilise our embeddings. We empirically show that\nthe incorporation of these embeddings improves downstream performance of almost\nevery model across different drug pair scoring tasks, even those the original\nmodel was not designed for. We publicly release all of our drug embeddings for\nthe DrugCombDB, DrugComb, DrugbankDDI, and TwoSides datasets.\n",
                "链接": "https://arxiv.org/abs/2209.09383"
            },
            {
                "文章ID": "22285",
                "标题": "Distributed Graph Neural Network Training with Periodic Stale\n  Representation Synchronization",
                "作者": " Zheng Chai,  Guangji Bai,  Liang Zhao,  Yue Cheng",
                "发布日期": "2022-10-04",
                "摘要": "  Despite the recent success of Graph Neural Networks, it remains challenging\nto train a GNN on large graphs with millions of nodes and billions of edges,\nwhich are prevalent in many graph-based applications. Traditional\nsampling-based methods accelerate GNN training by dropping edges and nodes,\nwhich impairs the graph integrity and model performance. Differently,\ndistributed GNN algorithms accelerate GNN training by utilizing multiple\ncomputing devices and can be classified into two types: \"partition-based\"\nmethods enjoy low communication costs but suffer from information loss due to\ndropped edges, while \"propagation-based\" methods avoid information loss but\nsuffer from prohibitive communication overhead caused by the neighbor\nexplosion. To jointly address these problems, this paper proposes DIGEST\n(DIstributed Graph reprEsentation SynchronizaTion), a novel distributed GNN\ntraining framework that synergizes the complementary strength of both\ncategories of existing methods. We propose to allow each device to utilize the\nstale representations of its neighbors in other subgraphs during subgraph\nparallel training. This way, our method preserves global graph information from\nneighbors to avoid information loss and reduce communication costs. Our\nconvergence analysis demonstrates that DIGEST enjoys a state-of-the-art\nconvergence rate. Extensive experimental evaluation on large, real-world graph\ndatasets shows that DIGEST achieves up to 21.82 speedups without compromising\nperformance compared to state-of-the-art distributed GNN training frameworks.\n",
                "链接": "https://arxiv.org/abs/2206.00057"
            },
            {
                "文章ID": "86603",
                "标题": "Distributed Marker Representation for Ambiguous Discourse Markers and\n  Entangled Relations",
                "作者": " Dongyu Ru,  Lin Qiu,  Xipeng Qiu,  Yue Zhang,  Zheng Zhang",
                "发布日期": "2023-06-21",
                "摘要": "  Discourse analysis is an important task because it models intrinsic semantic\nstructures between sentences in a document. Discourse markers are natural\nrepresentations of discourse in our daily language. One challenge is that the\nmarkers as well as pre-defined and human-labeled discourse relations can be\nambiguous when describing the semantics between sentences. We believe that a\nbetter approach is to use a contextual-dependent distribution over the markers\nto express discourse information. In this work, we propose to learn a\nDistributed Marker Representation (DMR) by utilizing the (potentially)\nunlimited discourse marker data with a latent discourse sense, thereby bridging\nmarkers with sentence pairs. Such representations can be learned automatically\nfrom data without supervision, and in turn provide insights into the data\nitself. Experiments show the SOTA performance of our DMR on the implicit\ndiscourse relation recognition task and strong interpretability. Our method\nalso offers a valuable tool to understand complex ambiguity and entanglement\namong discourse markers and manually defined discourse relations.\n",
                "链接": "https://arxiv.org/abs/2306.10658"
            },
            {
                "文章ID": "118082",
                "标题": "Probabilistic Transformer: A Probabilistic Dependency Model for\n  Contextual Word Representation",
                "作者": " Haoyi Wu,  Kewei Tu",
                "发布日期": "2023-11-28",
                "摘要": "  Syntactic structures used to play a vital role in natural language processing\n(NLP), but since the deep learning revolution, NLP has been gradually dominated\nby neural models that do not consider syntactic structures in their design. One\nvastly successful class of neural models is transformers. When used as an\nencoder, a transformer produces contextual representation of words in the input\nsentence. In this work, we propose a new model of contextual word\nrepresentation, not from a neural perspective, but from a purely syntactic and\nprobabilistic perspective. Specifically, we design a conditional random field\nthat models discrete latent representations of all words in a sentence as well\nas dependency arcs between them; and we use mean field variational inference\nfor approximate inference. Strikingly, we find that the computation graph of\nour model resembles transformers, with correspondences between dependencies and\nself-attention and between distributions over latent representations and\ncontextual embeddings of words. Experiments show that our model performs\ncompetitively to transformers on small to medium sized datasets. We hope that\nour work could help bridge the gap between traditional syntactic and\nprobabilistic approaches and cutting-edge neural approaches to NLP, and inspire\nmore linguistically-principled neural approaches in the future.\n",
                "链接": "https://arxiv.org/abs/2311.15211"
            }
        ]
    },
    {
        "question": {
            "question": "闭源大模型用户数据隐私保护研究",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "52013",
                "标题": "AI-driven Mobile Apps: an Explorative Study",
                "作者": " Yinghua Li,  Xueqi Dang,  Haoye Tian,  Tiezhu Sun,  Zhijie Wang,  Lei Ma,  Jacques Klein,  Tegawende F. Bissyande",
                "发布日期": "2022-12-06",
                "摘要": "  Recent years have witnessed an astonishing explosion in the evolution of\nmobile applications powered by AI technologies. The rapid growth of AI\nframeworks enables the transition of AI technologies to mobile devices,\nsignificantly prompting the adoption of AI apps (i.e., apps that integrate AI\ninto their functions) among smartphone devices. In this paper, we conduct the\nmost extensive empirical study on 56,682 published AI apps from three\nperspectives: dataset characteristics, development issues, and user feedback\nand privacy. To this end, we build an automated AI app identification tool, AI\nDiscriminator, that detects eligible AI apps from 7,259,232 mobile apps. First,\nwe carry out a dataset analysis, where we explore the AndroZoo large repository\nto identify AI apps and their core characteristics. Subsequently, we pinpoint\nkey issues in AI app development (e.g., model protection). Finally, we focus on\nuser reviews and user privacy protection. Our paper provides several notable\nfindings. Some essential ones involve revealing the issue of insufficient model\nprotection by presenting the lack of model encryption, and demonstrating the\nrisk of user privacy data being leaked. We published our large-scale AI app\ndatasets to inspire more future research.\n",
                "链接": "https://arxiv.org/abs/2212.01635"
            },
            {
                "文章ID": "85712",
                "标题": "Protecting User Privacy in Remote Conversational Systems: A\n  Privacy-Preserving framework based on text sanitization",
                "作者": " Zhigang Kan,  Linbo Qiao,  Hao Yu,  Liwen Peng,  Yifu Gao,  Dongsheng Li",
                "发布日期": "2023-06-16",
                "摘要": "  Large Language Models (LLMs) are gaining increasing attention due to their\nexceptional performance across numerous tasks. As a result, the general public\nutilize them as an influential tool for boosting their productivity while\nnatural language processing researchers endeavor to employ them in solving\nexisting or new research problems. Unfortunately, individuals can only access\nsuch powerful AIs through APIs, which ultimately leads to the transmission of\nraw data to the models' providers and increases the possibility of privacy data\nleakage. Current privacy-preserving methods for cloud-deployed language models\naim to protect privacy information in the pre-training dataset or during the\nmodel training phase. However, they do not meet the specific challenges\npresented by the remote access approach of new large-scale language models.\n  This paper introduces a novel task, \"User Privacy Protection for Dialogue\nModels,\" which aims to safeguard sensitive user information from any possible\ndisclosure while conversing with chatbots. We also present an evaluation scheme\nfor this task, which covers evaluation metrics for privacy protection, data\navailability, and resistance to simulation attacks. Moreover, we propose the\nfirst framework for this task, namely privacy protection through text\nsanitization. Before sending the input to remote large models, it filters out\nthe sensitive information, using several rounds of text sanitization based on\nprivacy types that users define. Upon receiving responses from the larger\nmodel, our framework automatically restores privacy to ensure that the\nconversation goes smoothly, without intervention from the privacy filter.\nExperiments based on real-world datasets demonstrate the efficacy of our\nprivacy-preserving approach against eavesdropping from potential attackers.\n",
                "链接": "https://arxiv.org/abs/2306.08223"
            },
            {
                "文章ID": "119126",
                "标题": "Toward the Tradeoffs between Privacy, Fairness and Utility in Federated\n  Learning",
                "作者": " Kangkang Sun,  Xiaojin Zhang,  Xi Lin,  Gaolei Li,  Jing Wang,  Jianhua Li",
                "发布日期": "2023-12-01",
                "摘要": "  Federated Learning (FL) is a novel privacy-protection distributed machine\nlearning paradigm that guarantees user privacy and prevents the risk of data\nleakage due to the advantage of the client's local training. Researchers have\nstruggled to design fair FL systems that ensure fairness of results. However,\nthe interplay between fairness and privacy has been less studied. Increasing\nthe fairness of FL systems can have an impact on user privacy, while an\nincrease in user privacy can affect fairness. In this work, on the client side,\nwe use fairness metrics, such as Demographic Parity (DemP), Equalized Odds\n(EOs), and Disparate Impact (DI), to construct the local fair model. To protect\nthe privacy of the client model, we propose a privacy-protection fairness FL\nmethod. The results show that the accuracy of the fair model with privacy\nincreases because privacy breaks the constraints of the fairness metrics. In\nour experiments, we conclude the relationship between privacy, fairness and\nutility, and there is a tradeoff between these.\n",
                "链接": "https://arxiv.org/abs/2311.18190"
            },
            {
                "文章ID": "36560",
                "标题": "A Framework for Evaluating Privacy-Utility Trade-off in Vertical\n  Federated Learning",
                "作者": " Yan Kang,  Jiahuan Luo,  Yuanqin He,  Xiaojin Zhang,  Lixin Fan,  Qiang Yang",
                "发布日期": "2022-09-12",
                "摘要": "  Federated learning (FL) has emerged as a practical solution to tackle data\nsilo issues without compromising user privacy. One of its variants, vertical\nfederated learning (VFL), has recently gained increasing attention as the VFL\nmatches the enterprises' demands of leveraging more valuable features to build\nbetter machine learning models while preserving user privacy. Current works in\nVFL concentrate on developing a specific protection or attack mechanism for a\nparticular VFL algorithm. In this work, we propose an evaluation framework that\nformulates the privacy-utility evaluation problem. We then use this framework\nas a guide to comprehensively evaluate a broad range of protection mechanisms\nagainst most of the state-of-the-art privacy attacks for three widely-deployed\nVFL algorithms. These evaluations may help FL practitioners select appropriate\nprotection mechanisms given specific requirements. Our evaluation results\ndemonstrate that: the model inversion and most of the label inference attacks\ncan be thwarted by existing protection mechanisms; the model completion (MC)\nattack is difficult to be prevented, which calls for more advanced MC-targeted\nprotection mechanisms. Based on our evaluation results, we offer concrete\nadvice on improving the privacy-preserving capability of VFL systems.\n",
                "链接": "https://arxiv.org/abs/2209.03885"
            },
            {
                "文章ID": "84894",
                "标题": "SoK: Analysis of User-Centered Studies Focusing on Healthcare Privacy &\n  Security",
                "作者": " Faiza Tazi,  Archana Nandakumar,  Josiah Dykstra,  Prashanth Rajivan,  Sanchari Das",
                "发布日期": "2023-06-27",
                "摘要": "  Sensitive information is intrinsically tied to interactions in healthcare,\nand its protection is of paramount importance for achieving high-quality\npatient outcomes. Research in healthcare privacy and security is predominantly\nfocused on understanding the factors that increase the susceptibility of users\nto privacy and security breaches. To understand further, we systematically\nreview 26 research papers in this domain to explore the existing user studies\nin healthcare privacy and security. Following the review, we conducted a\ncard-sorting exercise, allowing us to identify 12 themes integral to this\nsubject such as \"Data Sharing,\" \"Risk Awareness,\" and \"Privacy.\" Further to the\nidentification of these themes, we performed an in-depth analysis of the 26\nresearch papers report on the insights into the discourse within the research\ncommunity about healthcare privacy and security, particularly from the user\nperspective.\n",
                "链接": "https://arxiv.org/abs/2306.06033"
            },
            {
                "文章ID": "76087",
                "标题": "An Overview of AI and Blockchain Integration for Privacy-Preserving",
                "作者": " Zongwei Li,  Dechao Kong,  Yuanzheng Niu,  Hongli Peng,  Xiaoqi Li,  Wenkai Li",
                "发布日期": "2023-05-09",
                "摘要": "  With the widespread attention and application of artificial intelligence (AI)\nand blockchain technologies, privacy protection techniques arising from their\nintegration are of notable significance. In addition to protecting privacy of\nindividuals, these techniques also guarantee security and dependability of\ndata. This paper initially presents an overview of AI and blockchain,\nsummarizing their combination along with derived privacy protection\ntechnologies. It then explores specific application scenarios in data\nencryption, de-identification, multi-tier distributed ledgers, and k-anonymity\nmethods. Moreover, the paper evaluates five critical aspects of\nAI-blockchain-integration privacy protection systems, including authorization\nmanagement, access control, data protection, network security, and scalability.\nFurthermore, it analyzes the deficiencies and their actual cause, offering\ncorresponding suggestions. This research also classifies and summarizes privacy\nprotection techniques based on AI-blockchain application scenarios and\ntechnical schemes. In conclusion, this paper outlines the future directions of\nprivacy protection technologies emerging from AI and blockchain integration,\nincluding enhancing efficiency and security to achieve a more comprehensive\nprivacy protection of privacy.\n",
                "链接": "https://arxiv.org/abs/2305.03928"
            },
            {
                "文章ID": "36093",
                "标题": "How Much User Context Do We Need? Privacy by Design in Mental Health NLP\n  Application",
                "作者": " Ramit Sawhney,  Atula Tejaswi Neerkaje,  Ivan Habernal,  Lucie Flek",
                "发布日期": "2022-09-07",
                "摘要": "  Clinical NLP tasks such as mental health assessment from text, must take\nsocial constraints into account - the performance maximization must be\nconstrained by the utmost importance of guaranteeing privacy of user data.\nConsumer protection regulations, such as GDPR, generally handle privacy by\nrestricting data availability, such as requiring to limit user data to 'what is\nnecessary' for a given purpose. In this work, we reason that providing stricter\nformal privacy guarantees, while increasing the volume of user data in the\nmodel, in most cases increases benefit for all parties involved, especially for\nthe user. We demonstrate our arguments on two existing suicide risk assessment\ndatasets of Twitter and Reddit posts. We present the first analysis juxtaposing\nuser history length and differential privacy budgets and elaborate how modeling\nadditional user context enables utility preservation while maintaining\nacceptable user privacy guarantees.\n",
                "链接": "https://arxiv.org/abs/2209.02022"
            },
            {
                "文章ID": "78730",
                "标题": "A Path to Holistic Privacy in Stream Processing Systems",
                "作者": " Mikhail Fomichev",
                "发布日期": "2023-05-22",
                "摘要": "  The massive streams of Internet of Things (IoT) data require a timely\nanalysis to retain data usefulness. Stream processing systems (SPSs) enable\nthis task, deriving knowledge from the IoT data in real-time. Such real-time\nanalytics benefits many applications but can also be used to violate user\nprivacy, as the IoT data collected from users or their vicinity is inherently\nsensitive. In this paper, we present our systematic look into privacy issues\narising from the intersection of SPSs and IoT, identifying key research\nchallenges towards achieving holistic privacy protection in SPSs and proposing\nthe solutions.\n",
                "链接": "https://arxiv.org/abs/2305.11638"
            },
            {
                "文章ID": "46467",
                "标题": "User-Entity Differential Privacy in Learning Natural Language Models",
                "作者": " Phung Lai,  NhatHai Phan,  Tong Sun,  Rajiv Jain,  Franck Dernoncourt,  Jiuxiang Gu,  Nikolaos Barmpalios",
                "发布日期": "2022-11-10",
                "摘要": "  In this paper, we introduce a novel concept of user-entity differential\nprivacy (UeDP) to provide formal privacy protection simultaneously to both\nsensitive entities in textual data and data owners in learning natural language\nmodels (NLMs). To preserve UeDP, we developed a novel algorithm, called\nUeDP-Alg, optimizing the trade-off between privacy loss and model utility with\na tight sensitivity bound derived from seamlessly combining user and sensitive\nentity sampling processes. An extensive theoretical analysis and evaluation\nshow that our UeDP-Alg outperforms baseline approaches in model utility under\nthe same privacy budget consumption on several NLM tasks, using benchmark\ndatasets.\n",
                "链接": "https://arxiv.org/abs/2211.01141"
            },
            {
                "文章ID": "104121",
                "标题": "Characterising User Transfer Amid Industrial Resource Variation: A\n  Bayesian Nonparametric Approach",
                "作者": " Dongxu Lei,  Xiaotian Lin,  Xinghu Yu,  Zhan Li,  Weichao Sun,  Jianbin Qiu,  Songlin Zhuang,  Huijun Gao",
                "发布日期": "2023-09-26",
                "摘要": "  In a multitude of industrial fields, a key objective entails optimising\nresource management whilst satisfying user requirements. Resource management by\nindustrial practitioners can result in a passive transfer of user loads across\nresource providers, a phenomenon whose accurate characterisation is both\nchallenging and crucial. This research reveals the existence of user clusters,\nwhich capture macro-level user transfer patterns amid resource variation. We\nthen propose CLUSTER, an interpretable hierarchical Bayesian nonparametric\nmodel capable of automating cluster identification, and thereby predicting user\ntransfer in response to resource variation. Furthermore, CLUSTER facilitates\nuncertainty quantification for further reliable decision-making. Our method\nenables privacy protection by functioning independently of personally\nidentifiable information. Experiments with simulated and real-world data from\nthe communications industry reveal a pronounced alignment between prediction\nresults and empirical observations across a spectrum of resource management\nscenarios. This research establishes a solid groundwork for advancing resource\nmanagement strategy development.\n",
                "链接": "https://arxiv.org/abs/2309.13949"
            }
        ]
    },
    {
        "question": {
            "question": "推荐与AutoGPT相似的10篇文献",
            "type": "1"
        },
        "results": []
    }
]