[
    {
        "question": {
            "question": "与大模型工具学习相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "81272",
                "标题": "Large Language Models as Tool Makers",
                "作者": " Tianle Cai,  Xuezhi Wang,  Tengyu Ma,  Xinyun Chen,  Denny Zhou",
                "发布日期": "2023-05-29",
                "摘要": "  Recent research shows the potential of enhancing the problem-solving ability\nof large language models (LLMs) through the use of external tools. However,\nprior work along this line depends on the availability of existing tools. In\nthis work, we take an initial step towards removing this dependency by\nproposing a closed-loop framework, referred to as LLMs As Tool Makers (LATM),\nwhere LLMs create their own reusable tools for problem-solving. Our approach\nconsists of two key phases: 1) tool making: an LLM acts as the tool maker that\ncrafts tools for given tasks, where a tool is implemented as a Python utility\nfunction. 2) tool using: an LLM acts as the tool user, which applies the tool\nbuilt by the tool maker for problem-solving. The tool user can be either the\nsame or a different LLM from the tool maker. Tool-making enables an LLM to\ncontinually generate tools that can be applied to different requests so that\nfuture requests can call the corresponding APIs when beneficial for solving the\ntasks. Furthermore, the division of labor among LLMs for tool-making and\ntool-using phases introduces the opportunity to achieve cost effectiveness\nwithout degrading the quality of generated tools and problem solutions. For\nexample, recognizing that tool-making demands more sophisticated capabilities\nthan tool-using, we can apply a powerful yet resource-intensive model as the\ntool maker, and a lightweight while cost-effective model as the tool user. We\nvalidate the effectiveness of our approach across a variety of complex\nreasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and\nGPT-3.5 as the tool user, LATM can achieve performance that is on par with\nusing GPT-4 for both tool making and tool using, while the inference cost is\nsignificantly reduced.\n",
                "链接": "https://arxiv.org/abs/2305.17126"
            },
            {
                "文章ID": "94169",
                "标题": "Tool Documentation Enables Zero-Shot Tool-Usage with Large Language\n  Models",
                "作者": " Cheng-Yu Hsieh,  Si-An Chen,  Chun-Liang Li,  Yasuhisa Fujii,  Alexander Ratner,  Chen-Yu Lee,  Ranjay Krishna,  Tomas Pfister",
                "发布日期": "2023-08-02",
                "摘要": "  Today, large language models (LLMs) are taught to use new tools by providing\na few demonstrations of the tool's usage. Unfortunately, demonstrations are\nhard to acquire, and can result in undesirable biased usage if the wrong\ndemonstration is chosen. Even in the rare scenario that demonstrations are\nreadily available, there is no principled selection protocol to determine how\nmany and which ones to provide. As tasks grow more complex, the selection\nsearch grows combinatorially and invariably becomes intractable. Our work\nprovides an alternative to demonstrations: tool documentation. We advocate the\nuse of tool documentation, descriptions for the individual tool usage, over\ndemonstrations. We substantiate our claim through three main empirical findings\non 6 tasks across both vision and language modalities. First, on existing\nbenchmarks, zero-shot prompts with only tool documentation are sufficient for\neliciting proper tool usage, achieving performance on par with few-shot\nprompts. Second, on a newly collected realistic tool-use dataset with hundreds\nof available tool APIs, we show that tool documentation is significantly more\nvaluable than demonstrations, with zero-shot documentation significantly\noutperforming few-shot without documentation. Third, we highlight the benefits\nof tool documentations by tackling image generation and video tracking using\njust-released unseen state-of-the-art models as tools. Finally, we highlight\nthe possibility of using tool documentation to automatically enable new\napplications: by using nothing more than the documentation of GroundingDino,\nStable Diffusion, XMem, and SAM, LLMs can re-invent the functionalities of the\njust-released Grounded-SAM and Track Anything models.\n",
                "链接": "https://arxiv.org/abs/2308.00675"
            },
            {
                "文章ID": "110068",
                "标题": "Creative Robot Tool Use with Large Language Models",
                "作者": " Mengdi Xu,  Peide Huang,  Wenhao Yu,  Shiqi Liu,  Xilun Zhang,  Yaru Niu,  Tingnan Zhang,  Fei Xia,  Jie Tan,  Ding Zhao",
                "发布日期": "2023-10-23",
                "摘要": "  Tool use is a hallmark of advanced intelligence, exemplified in both animal\nbehavior and robotic capabilities. This paper investigates the feasibility of\nimbuing robots with the ability to creatively use tools in tasks that involve\nimplicit physical constraints and long-term planning. Leveraging Large Language\nModels (LLMs), we develop RoboTool, a system that accepts natural language\ninstructions and outputs executable code for controlling robots in both\nsimulated and real-world environments. RoboTool incorporates four pivotal\ncomponents: (i) an \"Analyzer\" that interprets natural language to discern key\ntask-related concepts, (ii) a \"Planner\" that generates comprehensive strategies\nbased on the language input and key concepts, (iii) a \"Calculator\" that\ncomputes parameters for each skill, and (iv) a \"Coder\" that translates these\nplans into executable Python code. Our results show that RoboTool can not only\ncomprehend explicit or implicit physical constraints and environmental factors\nbut also demonstrate creative tool use. Unlike traditional Task and Motion\nPlanning (TAMP) methods that rely on explicit optimization, our LLM-based\nsystem offers a more flexible, efficient, and user-friendly solution for\ncomplex robotics tasks. Through extensive experiments, we validate that\nRoboTool is proficient in handling tasks that would otherwise be infeasible\nwithout the creative use of tools, thereby expanding the capabilities of\nrobotic systems. Demos are available on our project page:\nhttps://creative-robotool.github.io/.\n",
                "链接": "https://arxiv.org/abs/2310.13065"
            },
            {
                "文章ID": "95072",
                "标题": "TPTU: Large Language Model-based AI Agents for Task Planning and Tool\n  Usage",
                "作者": " Jingqing Ruan,  Yihong Chen,  Bin Zhang,  Zhiwei Xu,  Tianpeng Bao,  Guoqing Du,  Shiwei Shi,  Hangyu Mao,  Ziyue Li,  Xingyu Zeng,  Rui Zhao",
                "发布日期": "2023-11-08",
                "摘要": "  With recent advancements in natural language processing, Large Language\nModels (LLMs) have emerged as powerful tools for various real-world\napplications. Despite their prowess, the intrinsic generative abilities of LLMs\nmay prove insufficient for handling complex tasks which necessitate a\ncombination of task planning and the usage of external tools. In this paper, we\nfirst propose a structured framework tailored for LLM-based AI Agents and\ndiscuss the crucial capabilities necessary for tackling intricate problems.\nWithin this framework, we design two distinct types of agents (i.e., one-step\nagent and sequential agent) to execute the inference process. Subsequently, we\ninstantiate the framework using various LLMs and evaluate their Task Planning\nand Tool Usage (TPTU) abilities on typical tasks. By highlighting key findings\nand challenges, our goal is to provide a helpful resource for researchers and\npractitioners to leverage the power of LLMs in their AI applications. Our study\nemphasizes the substantial potential of these models, while also identifying\nareas that need more investigation and improvement.\n",
                "链接": "https://arxiv.org/abs/2308.03427"
            },
            {
                "文章ID": "94691",
                "标题": "A large language model-assisted education tool to provide feedback on\n  open-ended responses",
                "作者": " Jordan K. Matelsky,  Felipe Parodi,  Tony Liu,  Richard D. Lange,  Konrad P. Kording",
                "发布日期": "2023-08-07",
                "摘要": "  Open-ended questions are a favored tool among instructors for assessing\nstudent understanding and encouraging critical exploration of course material.\nProviding feedback for such responses is a time-consuming task that can lead to\noverwhelmed instructors and decreased feedback quality. Many instructors resort\nto simpler question formats, like multiple-choice questions, which provide\nimmediate feedback but at the expense of personalized and insightful comments.\nHere, we present a tool that uses large language models (LLMs), guided by\ninstructor-defined criteria, to automate responses to open-ended questions. Our\ntool delivers rapid personalized feedback, enabling students to quickly test\ntheir knowledge and identify areas for improvement. We provide open-source\nreference implementations both as a web application and as a Jupyter Notebook\nwidget that can be used with instructional coding or math notebooks. With\ninstructor guidance, LLMs hold promise to enhance student learning outcomes and\nelevate instructional methodologies.\n",
                "链接": "https://arxiv.org/abs/2308.02439"
            },
            {
                "文章ID": "78764",
                "标题": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive\n  Critiquing",
                "作者": " Zhibin Gou,  Zhihong Shao,  Yeyun Gong,  Yelong Shen,  Yujiu Yang,  Nan Duan,  Weizhu Chen",
                "发布日期": "2023-10-03",
                "摘要": "  Recent developments in large language models (LLMs) have been impressive.\nHowever, these models sometimes show inconsistencies and problematic behavior,\nsuch as hallucinating facts, generating flawed code, or creating offensive and\ntoxic content. Unlike these models, humans typically utilize external tools to\ncross-check and refine their initial content, like using a search engine for\nfact-checking, or a code interpreter for debugging. Inspired by this\nobservation, we introduce a framework called CRITIC that allows LLMs, which are\nessentially \"black boxes\" to validate and progressively amend their own outputs\nin a manner similar to human interaction with tools. More specifically,\nstarting with an initial output, CRITIC interacts with appropriate tools to\nevaluate certain aspects of the text, and then revises the output based on the\nfeedback obtained during this validation process. Comprehensive evaluations\ninvolving free-form question answering, mathematical program synthesis, and\ntoxicity reduction demonstrate that CRITIC consistently enhances the\nperformance of LLMs. Meanwhile, our research highlights the crucial importance\nof external feedback in promoting the ongoing self-improvement of LLMs.\n",
                "链接": "https://arxiv.org/abs/2305.11738"
            },
            {
                "文章ID": "80331",
                "标题": "LLMDet: A Third Party Large Language Models Generated Text Detection\n  Tool",
                "作者": " Kangxi Wu,  Liang Pang,  Huawei Shen,  Xueqi Cheng,  Tat-Seng Chua",
                "发布日期": "2023-11-06",
                "摘要": "  Generated texts from large language models (LLMs) are remarkably close to\nhigh-quality human-authored text, raising concerns about their potential misuse\nin spreading false information and academic misconduct. Consequently, there is\nan urgent need for a highly practical detection tool capable of accurately\nidentifying the source of a given text. However, existing detection tools\ntypically rely on access to LLMs and can only differentiate between\nmachine-generated and human-authored text, failing to meet the requirements of\nfine-grained tracing, intermediary judgment, and rapid detection. Therefore, we\npropose LLMDet, a model-specific, secure, efficient, and extendable detection\ntool, that can source text from specific LLMs, such as GPT-2, OPT, LLaMA, and\nothers. In LLMDet, we record the next-token probabilities of salient n-grams as\nfeatures to calculate proxy perplexity for each LLM. By jointly analyzing the\nproxy perplexities of LLMs, we can determine the source of the generated text.\nExperimental results show that LLMDet yields impressive detection performance\nwhile ensuring speed and security, achieving 98.54% precision and x5.0 faster\nfor recognizing human-authored text. Additionally, LLMDet can effortlessly\nextend its detection capabilities to a new open-source model. We will provide\nan open-source tool at https://github.com/TrustedLLM/LLMDet.\n",
                "链接": "https://arxiv.org/abs/2305.15004"
            },
            {
                "文章ID": "80992",
                "标题": "On the Tool Manipulation Capability of Open-source Large Language Models",
                "作者": " Qiantong Xu,  Fenglu Hong,  Bo Li,  Changran Hu,  Zhengyu Chen,  Jian Zhang",
                "发布日期": "2023-05-29",
                "摘要": "  Recent studies on software tool manipulation with large language models\n(LLMs) mostly rely on closed model APIs. The industrial adoption of these\nmodels is substantially constrained due to the security and robustness risks in\nexposing information to closed LLM API services. In this paper, we ask can we\nenhance open-source LLMs to be competitive to leading closed LLM APIs in tool\nmanipulation, with practical amount of human supervision. By analyzing common\ntool manipulation failures, we first demonstrate that open-source LLMs may\nrequire training with usage examples, in-context demonstration and generation\nstyle regulation to resolve failures. These insights motivate us to revisit\nclassical methods in LLM literature, and demonstrate that we can adapt them as\nmodel alignment with programmatic data generation, system prompts and\nin-context demonstration retrievers to enhance open-source LLMs for tool\nmanipulation. To evaluate these techniques, we create the ToolBench, a tool\nmanipulation benchmark consisting of diverse software tools for real-world\ntasks. We demonstrate that our techniques can boost leading open-source LLMs by\nup to 90% success rate, showing capabilities competitive to OpenAI GPT-4 in 4\nout of 8 ToolBench tasks. We show that such enhancement typically requires\nabout one developer day to curate data for each tool, rendering a recipe with\npractical amount of human supervision.\n",
                "链接": "https://arxiv.org/abs/2305.16504"
            },
            {
                "文章ID": "109237",
                "标题": "Large Language Model Unlearning",
                "作者": " Yuanshun Yao,  Xiaojun Xu,  Yang Liu",
                "发布日期": "2023-10-18",
                "摘要": "  We study how to perform unlearning, i.e. forgetting undesirable\n(mis)behaviors, on large language models (LLMs). We show at least three\nscenarios of aligning LLMs with human preferences can benefit from unlearning:\n(1) removing harmful responses, (2) erasing copyright-protected content as\nrequested, and (3) eliminating hallucinations. Unlearning, as an alignment\ntechnique, has three advantages. (1) It only requires negative (e.g. harmful)\nexamples, which are much easier and cheaper to collect (e.g. via red teaming or\nuser reporting) than positive (e.g. helpful and often human-written) examples\nrequired in RLHF (RL from human feedback). (2) It is computationally efficient.\n(3) It is especially effective when we know which training samples cause the\nmisbehavior. To the best of our knowledge, our work is among the first to\nexplore LLM unlearning. We are also among the first to formulate the settings,\ngoals, and evaluations in LLM unlearning. We show that if practitioners only\nhave limited resources, and therefore the priority is to stop generating\nundesirable outputs rather than to try to generate desirable outputs,\nunlearning is particularly appealing. Despite only having negative samples, our\nablation study shows that unlearning can still achieve better alignment\nperformance than RLHF with just 2% of its computational time.\n",
                "链接": "https://arxiv.org/abs/2310.10683"
            },
            {
                "文章ID": "76562",
                "标题": "Large Language Model Programs",
                "作者": " Imanol Schlag,  Sainbayar Sukhbaatar,  Asli Celikyilmaz,  Wen-tau Yih,  Jason Weston,  Jürgen Schmidhuber,  Xian Li",
                "发布日期": "2023-05-10",
                "摘要": "  In recent years, large pre-trained language models (LLMs) have demonstrated\nthe ability to follow instructions and perform novel tasks from a few examples.\nThe possibility to parameterise an LLM through such in-context examples widens\ntheir capability at a much lower cost than finetuning. We extend this line of\nreasoning and present a method which further expands the capabilities of an LLM\nby embedding it within an algorithm or program. To demonstrate the benefits of\nthis approach, we present an illustrative example of evidence-supported\nquestion-answering. We obtain a 6.4\\% improvement over the chain of thought\nbaseline through a more algorithmic approach without any finetuning.\nFurthermore, we highlight recent work from this perspective and discuss the\nadvantages and disadvantages in comparison to the standard approaches.\n",
                "链接": "https://arxiv.org/abs/2305.05364"
            }
        ]
    },
    {
        "question": {
            "question": "查询近一年模型推理加速相关的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "124676",
                "标题": "Understanding the Potential of FPGA-Based Spatial Acceleration for Large\n  Language Model Inference",
                "作者": " Hongzheng Chen,  Jiahao Zhang,  Yixiao Du,  Shaojie Xiang,  Zichao Yue,  Niansong Zhang,  Yaohui Cai,  Zhiru Zhang",
                "发布日期": "2023-12-27",
                "摘要": "  Recent advancements in large language models (LLMs) boasting billions of\nparameters have generated a significant demand for efficient deployment in\ninference workloads. The majority of existing approaches rely on temporal\narchitectures that reuse hardware units for different network layers and\noperators. However, these methods often encounter challenges in achieving low\nlatency due to considerable memory access overhead. This paper investigates the\nfeasibility and potential of model-specific spatial acceleration for LLM\ninference on FPGAs. Our approach involves the specialization of distinct\nhardware units for specific operators or layers, facilitating direct\ncommunication between them through a dataflow architecture while minimizing\noff-chip memory accesses. We introduce a comprehensive analytical model for\nestimating the performance of a spatial LLM accelerator, taking into account\nthe on-chip compute and memory resources available on an FPGA. Through our\nanalysis, we can determine the scenarios in which FPGA-based spatial\nacceleration can outperform its GPU-based counterpart. To enable more\nproductive implementations of an LLM model on FPGAs, we further provide a\nlibrary of high-level synthesis (HLS) kernels that are composable and reusable.\nThis library will be made available as open-source. To validate the\neffectiveness of both our analytical model and HLS library, we have implemented\nBERT and GPT2 on an AMD Alveo U280 FPGA device. Experimental results\ndemonstrate our approach can achieve up to 16.1x speedup when compared to\nprevious FPGA-based accelerators for the BERT model. For GPT generative\ninference, we attain a 2.2x speedup compared to DFX, an FPGA overlay, in the\nprefill stage, while achieving a 1.9x speedup and a 5.7x improvement in energy\nefficiency compared to the NVIDIA A100 GPU in the decode stage.\n",
                "链接": "https://arxiv.org/abs/2312.15159"
            },
            {
                "文章ID": "87082",
                "标题": "Edge Devices Inference Performance Comparison",
                "作者": " R. Tobiasz,  G. Wilczyński,  P. Graszka,  N. Czechowski,  S. Łuczak",
                "发布日期": "2023-07-03",
                "摘要": "  In this work, we investigate the inference time of the MobileNet family,\nEfficientNet V1 and V2 family, VGG models, Resnet family, and InceptionV3 on\nfour edge platforms. Specifically NVIDIA Jetson Nano, Intel Neural Stick,\nGoogle Coral USB Dongle, and Google Coral PCIe. Our main contribution is a\nthorough analysis of the aforementioned models in multiple settings, especially\nas a function of input size, the presence of the classification head, its size,\nand the scale of the model. Since throughout the industry, those architectures\nare mainly utilized as feature extractors we put our main focus on analyzing\nthem as such. We show that Google platforms offer the fastest average inference\ntime, especially for newer models like MobileNet or EfficientNet family, while\nIntel Neural Stick is the most universal accelerator allowing to run most\narchitectures. These results should provide guidance for engineers in the early\nstages of AI edge systems development. All of them are accessible at\nhttps://bulletprove.com/research/edge_inference_results.csv\n",
                "链接": "https://arxiv.org/abs/2306.12093"
            },
            {
                "文章ID": "108944",
                "标题": "Chameleon: a heterogeneous and disaggregated accelerator system for\n  retrieval-augmented language models",
                "作者": " Wenqi Jiang,  Marco Zeller,  Roger Waleffe,  Torsten Hoefler,  Gustavo Alonso",
                "发布日期": "2023-11-30",
                "摘要": "  A Retrieval-Augmented Language Model (RALM) augments a generative language\nmodel by retrieving context-specific knowledge from an external database. This\nstrategy facilitates impressive text generation quality even with smaller\nmodels, thus reducing orders of magnitude of computational demands. However,\nRALMs introduce unique system design challenges due to (a) the diverse workload\ncharacteristics between LM inference and retrieval and (b) the various system\nrequirements and bottlenecks for different RALM configurations such as model\nsizes, database sizes, and retrieval frequencies. We propose Chameleon, a\nheterogeneous accelerator system that integrates both LM and retrieval\naccelerators in a disaggregated architecture. The heterogeneity ensures\nefficient acceleration of both LM inference and retrieval, while the\naccelerator disaggregation enables the system to independently scale both types\nof accelerators to fulfill diverse RALM requirements. Our Chameleon prototype\nimplements retrieval accelerators on FPGAs and assigns LM inference to GPUs,\nwith a CPU server orchestrating these accelerators over the network. Compared\nto CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x\nspeedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon\nexhibits up to 2.16x reduction in latency and 3.18x speedup in throughput\ncompared to the hybrid CPU-GPU architecture. These promising results pave the\nway for bringing accelerator heterogeneity and disaggregation into future RALM\nsystems.\n",
                "链接": "https://arxiv.org/abs/2310.09949"
            },
            {
                "文章ID": "71550",
                "标题": "Inference with Reference: Lossless Acceleration of Large Language Models",
                "作者": " Nan Yang,  Tao Ge,  Liang Wang,  Binxing Jiao,  Daxin Jiang,  Linjun Yang,  Rangan Majumder,  Furu Wei",
                "发布日期": "2023-04-11",
                "摘要": "  We propose LLMA, an LLM accelerator to losslessly speed up Large Language\nModel (LLM) inference with references. LLMA is motivated by the observation\nthat there are abundant identical text spans between the decoding result by an\nLLM and the reference that is available in many real world scenarios (e.g.,\nretrieved documents). LLMA first selects a text span from the reference and\ncopies its tokens to the decoder and then efficiently checks the tokens'\nappropriateness as the decoding result in parallel within one decoding step.\nThe improved computational parallelism allows LLMA to achieve over 2x speed-up\nfor LLMs with identical generation results as greedy decoding in many practical\ngeneration scenarios where significant overlap between in-context reference and\noutputs exists (e.g., search engines and multi-turn conversations).\n",
                "链接": "https://arxiv.org/abs/2304.04487"
            },
            {
                "文章ID": "123966",
                "标题": "Lookahead: An Inference Acceleration Framework for Large Language Model\n  with Lossless Generation Accuracy",
                "作者": " Yao Zhao,  Zhitian Xie,  Chenyi Zhuang,  Jinjie Gu",
                "发布日期": "2023-12-21",
                "摘要": "  As Large Language Models (LLMs) have made significant advancements across\nvarious tasks, such as question answering, translation, text summarization, and\ndialogue systems, the need for accuracy in information becomes crucial,\nespecially for serious financial products serving billions of users like\nAlipay. To address this, Alipay has developed a Retrieval-Augmented Generation\n(RAG) system that grounds LLMs on the most accurate and up-to-date information.\nHowever, for a real-world product serving millions of users, the inference\nspeed of LLMs becomes a critical factor compared to a mere experimental model.\n  Hence, this paper presents a generic framework for accelerating the inference\nprocess, resulting in a substantial increase in speed and cost reduction for\nour RAG system, with lossless generation accuracy. In the traditional inference\nprocess, each token is generated sequentially by the LLM, leading to a time\nconsumption proportional to the number of generated tokens. To enhance this\nprocess, our framework, named \\textit{lookahead}, introduces a\n\\textit{multi-branch} strategy. Instead of generating a single token at a time,\nwe propose a \\textit{Trie-based Retrieval} (TR) process that enables the\ngeneration of multiple branches simultaneously, each of which is a sequence of\ntokens. Subsequently, for each branch, a \\textit{Verification and Accept} (VA)\nprocess is performed to identify the longest correct sub-sequence as the final\noutput. Our strategy offers two distinct advantages: (1) it guarantees absolute\ncorrectness of the output, avoiding any approximation algorithms, and (2) the\nworst-case performance of our approach is equivalent to the conventional\nprocess. We conduct extensive experiments to demonstrate the significant\nimprovements achieved by applying our inference acceleration framework.\n",
                "链接": "https://arxiv.org/abs/2312.12728"
            },
            {
                "文章ID": "111604",
                "标题": "Hierarchical Semi-Implicit Variational Inference with Application to\n  Diffusion Model Acceleration",
                "作者": " Longlin Yu,  Tianyu Xie,  Yu Zhu,  Tong Yang,  Xiangyu Zhang,  Cheng Zhang",
                "发布日期": "2023-10-27",
                "摘要": "  Semi-implicit variational inference (SIVI) has been introduced to expand the\nanalytical variational families by defining expressive semi-implicit\ndistributions in a hierarchical manner. However, the single-layer architecture\ncommonly used in current SIVI methods can be insufficient when the target\nposterior has complicated structures. In this paper, we propose hierarchical\nsemi-implicit variational inference, called HSIVI, which generalizes SIVI to\nallow more expressive multi-layer construction of semi-implicit distributions.\nBy introducing auxiliary distributions that interpolate between a simple base\ndistribution and the target distribution, the conditional layers can be trained\nby progressively matching these auxiliary distributions one layer after\nanother. Moreover, given pre-trained score networks, HSIVI can be used to\naccelerate the sampling process of diffusion models with the score matching\nobjective. We show that HSIVI significantly enhances the expressiveness of SIVI\non several Bayesian inference problems with complicated target distributions.\nWhen used for diffusion model acceleration, we show that HSIVI can produce high\nquality samples comparable to or better than the existing fast diffusion model\nbased samplers with a small number of function evaluations on various datasets.\n",
                "链接": "https://arxiv.org/abs/2310.17153"
            },
            {
                "文章ID": "88446",
                "标题": "An Efficient Sparse Inference Software Accelerator for Transformer-based\n  Language Models on CPUs",
                "作者": " Haihao Shen,  Hengyu Meng,  Bo Dong,  Zhe Wang,  Ofir Zafrir,  Yi Ding,  Yu Luo,  Hanwen Chang,  Qun Gao,  Ziheng Wang,  Guy Boudoukh,  Moshe Wasserblat",
                "发布日期": "2023-06-30",
                "摘要": "  In recent years, Transformer-based language models have become the standard\napproach for natural language processing tasks. However, stringent throughput\nand latency requirements in industrial applications are limiting their\nadoption. To mitigate the gap, model compression techniques such as structured\npruning are being used to improve inference efficiency. However, most existing\nneural network inference runtimes lack adequate support for structured\nsparsity. In this paper, we propose an efficient sparse deep learning inference\nsoftware stack for Transformer-based language models where the weights are\npruned with constant block size. Our sparse software accelerator leverages\nIntel Deep Learning Boost to maximize the performance of sparse matrix - dense\nmatrix multiplication (commonly abbreviated as SpMM) on CPUs. Our SpMM kernel\noutperforms the existing sparse libraries (oneMKL, TVM, and LIBXSMM) by an\norder of magnitude on a wide range of GEMM shapes under 5 representative\nsparsity ratios (70%, 75%, 80%, 85%, 90%). Moreover, our SpMM kernel shows up\nto 5x speedup over dense GEMM kernel of oneDNN, a well-optimized dense library\nwidely used in industry. We apply our sparse accelerator on widely-used\nTransformer-based language models including Bert-Mini, DistilBERT, Bert-Base,\nand BERT-Large. Our sparse inference software shows up to 1.5x speedup over\nNeural Magic's Deepsparse under same configurations on Xeon on Amazon Web\nServices under proxy production latency constraints. We also compare our\nsolution with two framework-based inference solutions, ONNX Runtime and\nPyTorch, and demonstrate up to 37x speedup over ONNX Runtime and 345x over\nPyTorch on Xeon under the latency constraints. All the source code is publicly\navailable on Github: https://github.com/intel/intel-extension-for-transformers.\n",
                "链接": "https://arxiv.org/abs/2306.16601"
            },
            {
                "文章ID": "109962",
                "标题": "Boosting Inference Efficiency: Unleashing the Power of Parameter-Shared\n  Pre-trained Language Models",
                "作者": " Weize Chen,  Xiaoyue Xu,  Xu Han,  Yankai Lin,  Ruobing Xie,  Zhiyuan Liu,  Maosong Sun,  Jie Zhou",
                "发布日期": "2023-10-20",
                "摘要": "  Parameter-shared pre-trained language models (PLMs) have emerged as a\nsuccessful approach in resource-constrained environments, enabling substantial\nreductions in model storage and memory costs without significant performance\ncompromise. However, it is important to note that parameter sharing does not\nalleviate computational burdens associated with inference, thus impeding its\npracticality in situations characterized by limited stringent latency\nrequirements or computational resources. Building upon neural ordinary\ndifferential equations (ODEs), we introduce a straightforward technique to\nenhance the inference efficiency of parameter-shared PLMs. Additionally, we\npropose a simple pre-training technique that leads to fully or partially shared\nmodels capable of achieving even greater inference acceleration. The\nexperimental results demonstrate the effectiveness of our methods on both\nautoregressive and autoencoding PLMs, providing novel insights into more\nefficient utilization of parameter-shared models in resource-constrained\nsettings.\n",
                "链接": "https://arxiv.org/abs/2310.12818"
            },
            {
                "文章ID": "67885",
                "标题": "DIPPM: a Deep Learning Inference Performance Predictive Model using\n  Graph Neural Networks",
                "作者": " Karthick Panner Selvam,  Mats Brorsson",
                "发布日期": "2023-03-22",
                "摘要": "  Deep Learning (DL) has developed to become a corner-stone in many everyday\napplications that we are now relying on. However, making sure that the DL model\nuses the underlying hardware efficiently takes a lot of effort. Knowledge about\ninference characteristics can help to find the right match so that enough\nresources are given to the model, but not too much. We have developed a DL\nInference Performance Predictive Model (DIPPM) that predicts the inference\nlatency, energy, and memory usage of a given input DL model on the NVIDIA A100\nGPU. We also devised an algorithm to suggest the appropriate A100\nMulti-Instance GPU profile from the output of DIPPM. We developed a methodology\nto convert DL models expressed in multiple frameworks to a generalized graph\nstructure that is used in DIPPM. It means DIPPM can parse input DL models from\nvarious frameworks. Our DIPPM can be used not only helps to find suitable\nhardware configurations but also helps to perform rapid design-space\nexploration for the inference performance of a model. We constructed a graph\nmulti-regression dataset consisting of 10,508 different DL models to train and\nevaluate the performance of DIPPM, and reached a resulting Mean Absolute\nPercentage Error (MAPE) as low as 1.9%.\n",
                "链接": "https://arxiv.org/abs/2303.11733"
            },
            {
                "文章ID": "124897",
                "标题": "High Efficiency Inference Accelerating Algorithm for NOMA-based Mobile\n  Edge Computing",
                "作者": " Xin Yuan,  Ning Li,  Tuo Zhang,  Muqing Li,  Yuwen Chen,  Jose Fernan Martinez Ortega,  Song Guo",
                "发布日期": "2023-12-27",
                "摘要": "  Splitting the inference model between device, edge server, and cloud can\nimprove the performance of EI greatly. Additionally, the non-orthogonal\nmultiple access (NOMA), which is the key supporting technologies of B5G/6G, can\nachieve massive connections and high spectrum efficiency. Motivated by the\nbenefits of NOMA, integrating NOMA with model split in MEC to reduce the\ninference latency further becomes attractive. However, the NOMA based\ncommunication during split inference has not been properly considered in\nprevious works. Therefore, in this paper, we integrate the NOMA into split\ninference in MEC, and propose the effective communication and computing\nresource allocation algorithm to accelerate the model inference at edge.\nSpecifically, when the mobile user has a large model inference task needed to\nbe calculated in the NOMA-based MEC, it will take the energy consumption of\nboth device and edge server and the inference latency into account to find the\noptimal model split strategy, subchannel allocation strategy (uplink and\ndownlink), and transmission power allocation strategy (uplink and downlink).\nSince the minimum inference delay and energy consumption cannot be satisfied\nsimultaneously, and the variables of subchannel allocation and model split are\ndiscrete, the gradient descent (GD) algorithm is adopted to find the optimal\ntradeoff between them. Moreover, the loop iteration GD approach (Li-GD) is\nproposed to reduce the complexity of GD algorithm that caused by the parameter\ndiscrete. Additionally, the properties of the proposed algorithm are also\ninvestigated, which demonstrate the effectiveness of the proposed algorithms.\n",
                "链接": "https://arxiv.org/abs/2312.15850"
            }
        ]
    },
    {
        "question": {
            "question": "查找论文中包含指令微调细节描述的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "87849",
                "标题": "Mitigating Hallucination in Large Multi-Modal Models via Robust\n  Instruction Tuning",
                "作者": " Fuxiao Liu,  Kevin Lin,  Linjie Li,  Jianfeng Wang,  Yaser Yacoob,  Lijuan Wang",
                "发布日期": "2023-10-02",
                "摘要": "  Despite the promising progress in multi-modal tasks, current large\nmulti-modal models (LMMs) are prone to hallucinating inconsistent descriptions\nwith respect to the associated image and human instructions. This paper\naddresses this issue by introducing the first large and diverse visual\ninstruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction.\nOur dataset comprises 400k visual instructions generated by GPT4, covering 16\nvision-and-language tasks with open-ended instructions and answers. Unlike\nexisting studies that primarily focus on positive instruction samples, we\ndesign LRV-Instruction to include both positive and negative instructions for\nmore robust visual instruction tuning. Our negative instructions are designed\nat three semantic levels: (i) Nonexistent Object Manipulation, (ii) Existent\nObject Manipulation and (iii) Knowledge Manipulation. To efficiently measure\nthe hallucination generated by LMMs, we propose GPT4-Assisted Visual\nInstruction Evaluation (GAVIE), a stable approach to evaluate visual\ninstruction tuning like human experts. GAVIE does not require human-annotated\ngroundtruth answers and can adapt to diverse instruction formats. We conduct\ncomprehensive experiments to investigate the hallucination of LMMs. Our results\ndemonstrate existing LMMs exhibit significant hallucinations when presented\nwith our negative instructions, particularly Existent Object and Knowledge\nManipulation instructions. Moreover, we successfully mitigate hallucination by\nfinetuning MiniGPT4 and mPLUG-Owl on LRV-Instruction while improving\nperformance on several public datasets compared to state-of-the-art methods.\nAdditionally, we observed that a balanced ratio of positive and negative\ninstances in the training data leads to a more robust model.\n",
                "链接": "https://arxiv.org/abs/2306.14565"
            },
            {
                "文章ID": "79942",
                "标题": "Dynosaur: A Dynamic Growth Paradigm for Instruction-Tuning Data Curation",
                "作者": " Da Yin,  Xiao Liu,  Fan Yin,  Ming Zhong,  Hritik Bansal,  Jiawei Han,  Kai-Wei Chang",
                "发布日期": "2023-10-27",
                "摘要": "  Instruction tuning has emerged to enhance the capabilities of large language\nmodels (LLMs) to comprehend instructions and generate appropriate responses.\nExisting methods either manually annotate or employ LLM (e.g., GPT-series) to\ngenerate data for instruction tuning. However, they often overlook associating\ninstructions with existing annotated datasets. In this paper, we propose\nDynosaur, a dynamic growth paradigm for the automatic curation of\ninstruction-tuning data. Based on the metadata of existing datasets, we use\nLLMs to automatically construct instruction-tuning data by identifying relevant\ndata fields and generating appropriate instructions.\n  By leveraging the existing annotated datasets, Dynosaur offers several\nadvantages: 1) it reduces the API cost for generating instructions (e.g., it\ncosts less than $12 USD by calling GPT-3.5-turbo for generating 800K\ninstruction tuning samples; 2) it provides high-quality data for instruction\ntuning (e.g., it performs better than Alpaca and Flan on Super-NI and Longform\nwith comparable data sizes); and 3) it supports the continuous improvement of\nmodels by generating instruction-tuning data when a new annotated dataset\nbecomes available. We further investigate a continual learning scheme for\nlearning with the ever-growing instruction-tuning dataset, and demonstrate that\nreplaying tasks with diverse instruction embeddings not only helps mitigate\nforgetting issues but generalizes to unseen tasks better.\n  Code and data are available at https://github.com/WadeYin9712/Dynosaur.\n",
                "链接": "https://arxiv.org/abs/2305.14327"
            },
            {
                "文章ID": "93561",
                "标题": "Exploring Format Consistency for Instruction Tuning",
                "作者": " Shihao Liang,  Kunlun Zhu,  Runchu Tian,  Yujia Qin,  Huadong Wang,  Xin Cong,  Zhiyuan Liu,  Xiaojiang Liu,  Maosong Sun",
                "发布日期": "2023-07-31",
                "摘要": "  Instruction tuning has emerged as a promising approach to enhancing large\nlanguage models in following human instructions. It is shown that increasing\nthe diversity and number of instructions in the training data can consistently\nenhance generalization performance, which facilitates a recent endeavor to\ncollect various instructions and integrate existing instruction tuning datasets\ninto larger collections. However, different users have their unique ways of\nexpressing instructions, and there often exist variations across different\ndatasets in the instruction styles and formats, i.e., format inconsistency. In\nthis work, we study how format inconsistency may impact the performance of\ninstruction tuning. We propose a framework called \"Unified Instruction Tuning\"\n(UIT), which calls OpenAI APIs for automatic format transfer among different\ninstruction tuning datasets. We show that UIT successfully improves the\ngeneralization performance on unseen instructions, which highlights the\nimportance of format consistency for instruction tuning. To make the UIT\nframework more practical, we further propose a novel perplexity-based denoising\nmethod to reduce the noise of automatic format transfer. We also train a\nsmaller offline model that achieves comparable format transfer capability than\nOpenAI APIs to reduce costs in practice.\n",
                "链接": "https://arxiv.org/abs/2307.15504"
            },
            {
                "文章ID": "86420",
                "标题": "Differentiable Instruction Optimization for Cross-Task Generalization",
                "作者": " Masaru Isonuma,  Junichiro Mori,  Ichiro Sakata",
                "发布日期": "2023-06-21",
                "摘要": "  Instruction tuning has been attracting much attention to achieve\ngeneralization ability across a wide variety of tasks. Although various types\nof instructions have been manually created for instruction tuning, it is still\nunclear what kind of instruction is optimal to obtain cross-task generalization\nability. This work presents instruction optimization, which optimizes training\ninstructions with respect to generalization ability. Rather than manually\ntuning instructions, we introduce learnable instructions and optimize them with\ngradient descent by leveraging bilevel optimization. Experimental results show\nthat the learned instruction enhances the diversity of instructions and\nimproves the generalization ability compared to using only manually created\ninstructions.\n",
                "链接": "https://arxiv.org/abs/2306.10098"
            },
            {
                "文章ID": "115412",
                "标题": "To See is to Believe: Prompting GPT-4V for Better Visual Instruction\n  Tuning",
                "作者": " Junke Wang,  Lingchen Meng,  Zejia Weng,  Bo He,  Zuxuan Wu,  Yu-Gang Jiang",
                "发布日期": "2023-11-30",
                "摘要": "  Existing visual instruction tuning methods typically prompt large language\nmodels with textual descriptions to generate instruction-following data.\nDespite the promising performance achieved, these descriptions are derived from\nimage annotations, which are oftentimes coarse-grained. Furthermore, the\ninstructions might even contradict the visual content without observing the\nentire visual context. To address this challenge, we introduce a fine-grained\nvisual instruction dataset, LVIS-Instruct4V, which contains 220K visually\naligned and context-aware instructions produced by prompting the powerful\nGPT-4V with images from LVIS. Through experimental validation and case studies,\nwe demonstrate that high-quality visual instructional data could improve the\nperformance of LLaVA-1.5, a state-of-the-art large multimodal model, across a\nwide spectrum of benchmarks by clear margins. Notably, by simply replacing the\nLLaVA-Instruct with our LVIS-Instruct4V, we achieve better results than LLaVA\non most challenging LMM benchmarks, e.g., LLaVA$^w$ (76.7 vs. 70.7) and MM-Vet\n(40.2 vs. 35.4). We release our data and model at\nhttps://github.com/X2FD/LVIS-INSTRUCT4V.\n",
                "链接": "https://arxiv.org/abs/2311.07574"
            },
            {
                "文章ID": "125155",
                "标题": "Visual Instruction Tuning towards General-Purpose Multimodal Model: A\n  Survey",
                "作者": " Jiaxing Huang,  Jingyi Zhang,  Kai Jiang,  Han Qiu,  Shijian Lu",
                "发布日期": "2023-12-29",
                "摘要": "  Traditional computer vision generally solves each single task independently\nby a dedicated model with the task instruction implicitly designed in the model\narchitecture, arising two limitations: (1) it leads to task-specific models,\nwhich require multiple models for different tasks and restrict the potential\nsynergies from diverse tasks; (2) it leads to a pre-defined and fixed model\ninterface that has limited interactivity and adaptability in following user'\ntask instructions. To address them, Visual Instruction Tuning (VIT) has been\nintensively studied recently, which finetunes a large vision model with\nlanguage as task instructions, aiming to learn from a wide range of vision\ntasks described by language instructions a general-purpose multimodal model\nthat can follow arbitrary instructions and thus solve arbitrary tasks specified\nby the user. This work aims to provide a systematic review of visual\ninstruction tuning, covering (1) the background that presents computer vision\ntask paradigms and the development of VIT; (2) the foundations of VIT that\nintroduce commonly used network architectures, visual instruction tuning\nframeworks and objectives, and evaluation setups and tasks; (3) the commonly\nused datasets in visual instruction tuning and evaluation; (4) the review of\nexisting VIT methods that categorizes them with a taxonomy according to both\nthe studied vision task and the method design and highlights the major\ncontributions, strengths, and shortcomings of them; (5) the comparison and\ndiscussion of VIT methods over various instruction-following benchmarks; (6)\nseveral challenges, open directions and possible future works in visual\ninstruction tuning research.\n",
                "链接": "https://arxiv.org/abs/2312.16602"
            },
            {
                "文章ID": "90044",
                "标题": "SVIT: Scaling up Visual Instruction Tuning",
                "作者": " Bo Zhao,  Boya Wu,  Muyang He,  Tiejun Huang",
                "发布日期": "2023-12-29",
                "摘要": "  Thanks to the emerging of foundation models, the large language and vision\nmodels are integrated to acquire the multimodal ability of visual captioning,\nquestion answering, etc. Although existing multimodal models present impressive\nperformance of visual understanding and reasoning, their limits are still\nlargely under-explored due to the scarcity of high-quality instruction tuning\ndata. To push the limits of multimodal capability, we Scale up Visual\nInstruction Tuning (SVIT) by constructing a dataset of 4.2 million visual\ninstruction tuning data including 1.6M conversation question-answer (QA) pairs,\n1.6M complex reasoning QA pairs, 1.0M referring QA pairs and 106K detailed\nimage descriptions. Besides the volume, the proposed dataset is also featured\nby the high quality and rich diversity, which is generated by prompting GPT-4\nwith the abundant manual annotations of images. We also propose a new data\nrecipe to select subset with better diversity and balance, which evokes model's\nsuperior capabilities. Extensive experiments verify that SVIT-v1.5, trained on\nthe proposed dataset, outperforms state-of-the-art Multimodal Large Language\nModels on popular benchmarks. The data and code are publicly available at\nhttps://github.com/BAAI-DCAI/Visual-Instruction-Tuning.\n",
                "链接": "https://arxiv.org/abs/2307.04087"
            },
            {
                "文章ID": "77770",
                "标题": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low\n  Training Data Instruction Tuning",
                "作者": " Hao Chen,  Yiming Zhang,  Qi Zhang,  Hantao Yang,  Xiaomeng Hu,  Xuetao Ma,  Yifan Yanggong,  Junbo Zhao",
                "发布日期": "2023-05-17",
                "摘要": "  Instruction tuning for large language models (LLMs) has gained attention from\nresearchers due to its ability to unlock the potential of LLMs in following\ninstructions. While instruction tuning offers advantages for facilitating the\nadaptation of large language models (LLMs) to downstream tasks as a fine-tuning\napproach, training models with tens of millions or even billions of parameters\non large amounts of data results in unaffordable computational costs. To\naddress this, we focus on reducing the data used in LLM instruction tuning to\ndecrease training costs and improve data efficiency, dubbed as Low Training\nData Instruction Tuning (LTD Instruction Tuning). Specifically, this paper\nconducts a preliminary exploration into reducing the data used in LLM training\nand identifies several observations regarding task specialization for LLM\ntraining, such as the optimization of performance for a specific task, the\nnumber of instruction types required for instruction tuning, and the amount of\ndata required for task-specific models. The results suggest that task-specific\nmodels can be trained using less than 0.5% of the original dataset, with a 2%\nimprovement in performance over those trained on full task-related data.\n",
                "链接": "https://arxiv.org/abs/2305.09246"
            },
            {
                "文章ID": "21130",
                "标题": "InstructDial: Improving Zero and Few-shot Generalization in Dialogue\n  through Instruction Tuning",
                "作者": " Prakhar Gupta,  Cathy Jiao,  Yi-Ting Yeh,  Shikib Mehri,  Maxine Eskenazi,  Jeffrey P. Bigham",
                "发布日期": "2022-10-27",
                "摘要": "  Instruction tuning is an emergent paradigm in NLP wherein natural language\ninstructions are leveraged with language models to induce zero-shot performance\non unseen tasks. Instructions have been shown to enable good performance on\nunseen tasks and datasets in both large and small language models. Dialogue is\nan especially interesting area to explore instruction tuning because dialogue\nsystems perform multiple kinds of tasks related to language (e.g., natural\nlanguage understanding and generation, domain-specific interaction), yet\ninstruction tuning has not been systematically explored for dialogue-related\ntasks. We introduce InstructDial, an instruction tuning framework for dialogue,\nwhich consists of a repository of 48 diverse dialogue tasks in a unified\ntext-to-text format created from 59 openly available dialogue datasets. Next,\nwe explore cross-task generalization ability on models tuned on InstructDial\nacross diverse dialogue tasks. Our analysis reveals that InstructDial enables\ngood zero-shot performance on unseen datasets and tasks such as dialogue\nevaluation and intent detection, and even better performance in a few-shot\nsetting. To ensure that models adhere to instructions, we introduce novel\nmeta-tasks. We establish benchmark zero-shot and few-shot performance of models\ntrained using the proposed framework on multiple dialogue tasks.\n",
                "链接": "https://arxiv.org/abs/2205.12673"
            },
            {
                "文章ID": "54694",
                "标题": "MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction\n  Tuning",
                "作者": " Zhiyang Xu,  Ying Shen,  Lifu Huang",
                "发布日期": "2023-06-13",
                "摘要": "  Instruction tuning, a new learning paradigm that fine-tunes pre-trained\nlanguage models on tasks specified through instructions, has shown promising\nzero-shot performance on various natural language processing tasks. However, it\nhas yet to be explored for vision and multimodal tasks. In this work, we\nintroduce MUL-TIINSTRUCT, the first multimodal instruction tuning benchmark\ndataset that consists of 62 diverse multimodal tasks in a unified seq-to-seq\nformat covering 10 broad categories. The tasks are derived from 21 existing\nopen-source datasets and each task is equipped with 5 expert-written\ninstructions. We take OFA as the base pre-trained model for multimodal\ninstruction tuning, and to further improve its zero-shot performance, we\nexplore multiple transfer learning strategies to leverage the large-scale\nNATURAL INSTRUCTIONS dataset. Experimental results demonstrate strong zero-shot\nperformance on various unseen multimodal tasks and the benefit of transfer\nlearning from a text-only instruction dataset. We also design a new evaluation\nmetric - Sensitivity, to evaluate how sensitive the model is to the variety of\ninstructions. Our results indicate that fine-tuning the model on a diverse set\nof tasks and instructions leads to a reduced sensitivity to variations in\ninstructions for each task.\n",
                "链接": "https://arxiv.org/abs/2212.10773"
            }
        ]
    },
    {
        "question": {
            "question": "查找OCR文本检测最新进展",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "100439",
                "标题": "STEP -- Towards Structured Scene-Text Spotting",
                "作者": " Sergi Garcia-Bordils,  Dimosthenis Karatzas,  Marçal Rusiñol",
                "发布日期": "2023-12-12",
                "摘要": "  We introduce the structured scene-text spotting task, which requires a\nscene-text OCR system to spot text in the wild according to a query regular\nexpression. Contrary to generic scene text OCR, structured scene-text spotting\nseeks to dynamically condition both scene text detection and recognition on\nuser-provided regular expressions. To tackle this task, we propose the\nStructured TExt sPotter (STEP), a model that exploits the provided text\nstructure to guide the OCR process. STEP is able to deal with regular\nexpressions that contain spaces and it is not bound to detection at the\nword-level granularity. Our approach enables accurate zero-shot structured text\nspotting in a wide variety of real-world reading scenarios and is solely\ntrained on publicly available data. To demonstrate the effectiveness of our\napproach, we introduce a new challenging test dataset that contains several\ntypes of out-of-vocabulary structured text, reflecting important reading\napplications of fields such as prices, dates, serial numbers, license plates\netc. We demonstrate that STEP can provide specialised OCR performance on demand\nin all tested scenarios.\n",
                "链接": "https://arxiv.org/abs/2309.02356"
            },
            {
                "文章ID": "94665",
                "标题": "Universal Defensive Underpainting Patch: Making Your Text Invisible to\n  Optical Character Recognition",
                "作者": " JiaCheng Deng,  Li Dong,  Jiahao Chen,  Diqun Yan,  Rangding Wang,  Dengpan Ye,  Lingchen Zhao,  Jinyu Tian",
                "发布日期": "2023-08-07",
                "摘要": "  Optical Character Recognition (OCR) enables automatic text extraction from\nscanned or digitized text images, but it also makes it easy to pirate valuable\nor sensitive text from these images. Previous methods to prevent OCR piracy by\ndistorting characters in text images are impractical in real-world scenarios,\nas pirates can capture arbitrary portions of the text images, rendering the\ndefenses ineffective. In this work, we propose a novel and effective defense\nmechanism termed the Universal Defensive Underpainting Patch (UDUP) that\nmodifies the underpainting of text images instead of the characters. UDUP is\ncreated through an iterative optimization process to craft a small, fixed-size\ndefensive patch that can generate non-overlapping underpainting for text images\nof any size. Experimental results show that UDUP effectively defends against\nunauthorized OCR under the setting of any screenshot range or complex image\nbackground. It is agnostic to the content, size, colors, and languages of\ncharacters, and is robust to typical image operations such as scaling and\ncompressing. In addition, the transferability of UDUP is demonstrated by\nevading several off-the-shelf OCRs. The code is available at\nhttps://github.com/QRICKDD/UDUP.\n",
                "链接": "https://arxiv.org/abs/2308.02369"
            },
            {
                "文章ID": "45166",
                "标题": "A Late Multi-Modal Fusion Model for Detecting Hybrid Spam E-mail",
                "作者": " Zhibo Zhang,  Ernesto Damiani,  Hussam Al Hamadi,  Chan Yeob Yeun,  Fatma Taher",
                "发布日期": "2023-05-16",
                "摘要": "  In recent years, spammers are now trying to obfuscate their intents by\nintroducing hybrid spam e-mail combining both image and text parts, which is\nmore challenging to detect in comparison to e-mails containing text or image\nonly. The motivation behind this research is to design an effective approach\nfiltering out hybrid spam e-mails to avoid situations where traditional\ntext-based or image-baesd only filters fail to detect hybrid spam e-mails. To\nthe best of our knowledge, a few studies have been conducted with the goal of\ndetecting hybrid spam e-mails. Ordinarily, Optical Character Recognition (OCR)\ntechnology is used to eliminate the image parts of spam by transforming images\ninto text. However, the research questions are that although OCR scanning is a\nvery successful technique in processing text-and-image hybrid spam, it is not\nan effective solution for dealing with huge quantities due to the CPU power\nrequired and the execution time it takes to scan e-mail files. And the OCR\ntechniques are not always reliable in the transformation processes. To address\nsuch problems, we propose new late multi-modal fusion training frameworks for a\ntext-and-image hybrid spam e-mail filtering system compared to the classical\nearly fusion detection frameworks based on the OCR method. Convolutional Neural\nNetwork (CNN) and Continuous Bag of Words were implemented to extract features\nfrom image and text parts of hybrid spam respectively, whereas generated\nfeatures were fed to sigmoid layer and Machine Learning based classifiers\nincluding Random Forest (RF), Decision Tree (DT), Naive Bayes (NB) and Support\nVector Machine (SVM) to determine the e-mail ham or spam.\n",
                "链接": "https://arxiv.org/abs/2210.14616"
            },
            {
                "文章ID": "98478",
                "标题": "DISGO: Automatic End-to-End Evaluation for Scene Text OCR",
                "作者": " Mei-Yuh Hwang,  Yangyang Shi,  Ankit Ramchandani,  Guan Pang,  Praveen Krishnan,  Lucas Kabela,  Frank Seide,  Samyak Datta,  Jun Liu",
                "发布日期": "2023-08-28",
                "摘要": "  This paper discusses the challenges of optical character recognition (OCR) on\nnatural scenes, which is harder than OCR on documents due to the wild content\nand various image backgrounds. We propose to uniformly use word error rates\n(WER) as a new measurement for evaluating scene-text OCR, both end-to-end (e2e)\nperformance and individual system component performances. Particularly for the\ne2e metric, we name it DISGO WER as it considers Deletion, Insertion,\nSubstitution, and Grouping/Ordering errors. Finally we propose to utilize the\nconcept of super blocks to automatically compute BLEU scores for e2e OCR\nmachine translation. The small SCUT public test set is used to demonstrate WER\nperformance by a modularized OCR system.\n",
                "链接": "https://arxiv.org/abs/2308.13173"
            },
            {
                "文章ID": "53136",
                "标题": "Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned\n  Receipt Images",
                "作者": " Hongkuan Zhang,  Edward Whittaker,  Ikuo Kitagishi",
                "发布日期": "2023-10-17",
                "摘要": "  Digitization of scanned receipts aims to extract text from receipt images and\nsave it into structured documents. This is usually split into two sub-tasks:\ntext localization and optical character recognition (OCR). Most existing OCR\nmodels only focus on the cropped text instance images, which require the\nbounding box information provided by a text region detection model. Introducing\nan additional detector to identify the text instance images in advance adds\ncomplexity, however instance-level OCR models have very low accuracy when\nprocessing the whole image for the document-level OCR, such as receipt images\ncontaining multiple text lines arranged in various layouts. To this end, we\npropose a localization-free document-level OCR model for transcribing all the\ncharacters in a receipt image into an ordered sequence end-to-end.\nSpecifically, we finetune the pretrained instance-level model TrOCR with\nrandomly cropped image chunks, and gradually increase the image chunk size to\ngeneralize the recognition ability from instance images to full-page images. In\nour experiments on the SROIE receipt OCR dataset, the model finetuned with our\nstrategy achieved 64.4 F1-score and a 22.8% character error rate (CER),\nrespectively, which outperforms the baseline results with 48.5 F1-score and\n50.6% CER. The best model, which splits the full image into 15 equally sized\nchunks, gives 87.8 F1-score and 4.98% CER with minimal additional pre or\npost-processing of the output. Moreover, the characters in the generated\ndocument-level sequences are arranged in the reading order, which is practical\nfor real-world applications.\n",
                "链接": "https://arxiv.org/abs/2212.05525"
            },
            {
                "文章ID": "73951",
                "标题": "ICDAR 2023 Competition on Reading the Seal Title",
                "作者": " Wenwen Yu,  Mingyu Liu,  Mingrui Chen,  Ning Lu,  Yinlong Wen,  Yuliang Liu,  Dimosthenis Karatzas,  Xiang Bai",
                "发布日期": "2023-06-07",
                "摘要": "  Reading seal title text is a challenging task due to the variable shapes of\nseals, curved text, background noise, and overlapped text. However, this\nimportant element is commonly found in official and financial scenarios, and\nhas not received the attention it deserves in the field of OCR technology. To\npromote research in this area, we organized ICDAR 2023 competition on reading\nthe seal title (ReST), which included two tasks: seal title text detection\n(Task 1) and end-to-end seal title recognition (Task 2). We constructed a\ndataset of 10,000 real seal data, covering the most common classes of seals,\nand labeled all seal title texts with text polygons and text contents. The\ncompetition opened on 30th December, 2022 and closed on 20th March, 2023. The\ncompetition attracted 53 participants from academia and industry including 28\nsubmissions for Task 1 and 25 submissions for Task 2, which demonstrated\nsignificant interest in this challenging task. In this report, we present an\noverview of the competition, including the organization, challenges, and\nresults. We describe the dataset and tasks, and summarize the submissions and\nevaluation results. The results show that significant progress has been made in\nthe field of seal title text reading, and we hope that this competition will\ninspire further research and development in this important area of OCR\ntechnology.\n",
                "链接": "https://arxiv.org/abs/2304.11966"
            },
            {
                "文章ID": "123651",
                "标题": "Advancements and Challenges in Arabic Optical Character Recognition: A\n  Comprehensive Survey",
                "作者": " Mahmoud SalahEldin Kasem,  Mohamed Mahmoud,  Hyun-Soo Kang",
                "发布日期": "2023-12-20",
                "摘要": "  Optical character recognition (OCR) is a vital process that involves the\nextraction of handwritten or printed text from scanned or printed images,\nconverting it into a format that can be understood and processed by machines.\nThis enables further data processing activities such as searching and editing.\nThe automatic extraction of text through OCR plays a crucial role in digitizing\ndocuments, enhancing productivity, improving accessibility, and preserving\nhistorical records. This paper seeks to offer an exhaustive review of\ncontemporary applications, methodologies, and challenges associated with Arabic\nOptical Character Recognition (OCR). A thorough analysis is conducted on\nprevailing techniques utilized throughout the OCR process, with a dedicated\neffort to discern the most efficacious approaches that demonstrate enhanced\noutcomes. To ensure a thorough evaluation, a meticulous keyword-search\nmethodology is adopted, encompassing a comprehensive analysis of articles\nrelevant to Arabic OCR, including both backward and forward citation reviews.\nIn addition to presenting cutting-edge techniques and methods, this paper\ncritically identifies research gaps within the realm of Arabic OCR. By\nhighlighting these gaps, we shed light on potential areas for future\nexploration and development, thereby guiding researchers toward promising\navenues in the field of Arabic OCR. The outcomes of this study provide valuable\ninsights for researchers, practitioners, and stakeholders involved in Arabic\nOCR, ultimately fostering advancements in the field and facilitating the\ncreation of more accurate and efficient OCR systems for the Arabic language.\n",
                "链接": "https://arxiv.org/abs/2312.11812"
            },
            {
                "文章ID": "90096",
                "标题": "A Novel Pipeline for Improving Optical Character Recognition through\n  Post-processing Using Natural Language Processing",
                "作者": " Aishik Rakshit,  Samyak Mehta,  Anirban Dasgupta",
                "发布日期": "2023-07-11",
                "摘要": "  Optical Character Recognition (OCR) technology finds applications in\ndigitizing books and unstructured documents, along with applications in other\ndomains such as mobility statistics, law enforcement, traffic, security\nsystems, etc. The state-of-the-art methods work well with the OCR with printed\ntext on license plates, shop names, etc. However, applications such as printed\ntextbooks and handwritten texts have limited accuracy with existing techniques.\nThe reason may be attributed to similar-looking characters and variations in\nhandwritten characters. Since these issues are challenging to address with OCR\ntechnologies exclusively, we propose a post-processing approach using Natural\nLanguage Processing (NLP) tools. This work presents an end-to-end pipeline that\nfirst performs OCR on the handwritten or printed text and then improves its\naccuracy using NLP.\n",
                "链接": "https://arxiv.org/abs/2307.04245"
            },
            {
                "文章ID": "115320",
                "标题": "What Large Language Models Bring to Text-rich VQA?",
                "作者": " Xuejing Liu,  Wei Tang,  Xinzhe Ni,  Jinghui Lu,  Rui Zhao,  Zechao Li,  Fei Tan",
                "发布日期": "2023-11-14",
                "摘要": "  Text-rich VQA, namely Visual Question Answering based on text recognition in\nthe images, is a cross-modal task that requires both image comprehension and\ntext recognition. In this work, we focus on investigating the advantages and\nbottlenecks of LLM-based approaches in addressing this problem. To address the\nabove concern, we separate the vision and language modules, where we leverage\nexternal OCR models to recognize texts in the image and Large Language Models\n(LLMs) to answer the question given texts. The whole framework is training-free\nbenefiting from the in-context ability of LLMs. This pipeline achieved superior\nperformance compared to the majority of existing Multimodal Large Language\nModels (MLLM) on four text-rich VQA datasets. Besides, based on the ablation\nstudy, we find that LLM brings stronger comprehension ability and may introduce\nhelpful knowledge for the VQA problem. The bottleneck for LLM to address\ntext-rich VQA problems may primarily lie in visual part. We also combine the\nOCR module with MLLMs and pleasantly find that the combination of OCR module\nwith MLLM also works. It's worth noting that not all MLLMs can comprehend the\nOCR information, which provides insights into how to train an MLLM that\npreserves the abilities of LLM.\n",
                "链接": "https://arxiv.org/abs/2311.07306"
            },
            {
                "文章ID": "113971",
                "标题": "AnyText: Multilingual Visual Text Generation And Editing",
                "作者": " Yuxiang Tuo,  Wangmeng Xiang,  Jun-Yan He,  Yifeng Geng,  Xuansong Xie",
                "发布日期": "2023-12-18",
                "摘要": "  Diffusion model based Text-to-Image has achieved impressive achievements\nrecently. Although current technology for synthesizing images is highly\nadvanced and capable of generating images with high fidelity, it is still\npossible to give the show away when focusing on the text area in the generated\nimage. To address this issue, we introduce AnyText, a diffusion-based\nmultilingual visual text generation and editing model, that focuses on\nrendering accurate and coherent text in the image. AnyText comprises a\ndiffusion pipeline with two primary elements: an auxiliary latent module and a\ntext embedding module. The former uses inputs like text glyph, position, and\nmasked image to generate latent features for text generation or editing. The\nlatter employs an OCR model for encoding stroke data as embeddings, which blend\nwith image caption embeddings from the tokenizer to generate texts that\nseamlessly integrate with the background. We employed text-control diffusion\nloss and text perceptual loss for training to further enhance writing accuracy.\nAnyText can write characters in multiple languages, to the best of our\nknowledge, this is the first work to address multilingual visual text\ngeneration. It is worth mentioning that AnyText can be plugged into existing\ndiffusion models from the community for rendering or editing text accurately.\nAfter conducting extensive evaluation experiments, our method has outperformed\nall other approaches by a significant margin. Additionally, we contribute the\nfirst large-scale multilingual text images dataset, AnyWord-3M, containing 3\nmillion image-text pairs with OCR annotations in multiple languages. Based on\nAnyWord-3M dataset, we propose AnyText-benchmark for the evaluation of visual\ntext generation accuracy and quality. Our project will be open-sourced on\nhttps://github.com/tyxsspa/AnyText to improve and promote the development of\ntext generation technology.\n",
                "链接": "https://arxiv.org/abs/2311.03054"
            }
        ]
    },
    {
        "question": {
            "question": "查找OCR文本识别最新进展。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "42779",
                "标题": "Text Detection Forgot About Document OCR",
                "作者": " Krzysztof Olejniczak,  Milan Šulc",
                "发布日期": "2023-01-24",
                "摘要": "  Detection and recognition of text from scans and other images, commonly\ndenoted as Optical Character Recognition (OCR), is a widely used form of\nautomated document processing with a number of methods available. Yet OCR\nsystems still do not achieve 100% accuracy, requiring human corrections in\napplications where correct readout is essential. Advances in machine learning\nenabled even more challenging scenarios of text detection and recognition\n\"in-the-wild\" - such as detecting text on objects from photographs of complex\nscenes. While the state-of-the-art methods for in-the-wild text recognition are\ntypically evaluated on complex scenes, their performance in the domain of\ndocuments is typically not published, and a comprehensive comparison with\nmethods for document OCR is missing. This paper compares several methods\ndesigned for in-the-wild text recognition and for document text recognition,\nand provides their evaluation on the domain of structured documents. The\nresults suggest that state-of-the-art methods originally proposed for\nin-the-wild text detection also achieve competitive results on document text\ndetection, outperforming available OCR methods. We argue that the application\nof document OCR should not be omitted in evaluation of text detection and\nrecognition methods.\n",
                "链接": "https://arxiv.org/abs/2210.07903"
            },
            {
                "文章ID": "90438",
                "标题": "Handwritten Text Recognition Using Convolutional Neural Network",
                "作者": " Atman Mishra,  A. Sharath Ram,  Kavyashree C",
                "发布日期": "2023-07-12",
                "摘要": "  OCR (Optical Character Recognition) is a technology that offers comprehensive\nalphanumeric recognition of handwritten and printed characters at electronic\nspeed by merely scanning the document. Recently, the understanding of visual\ndata has been termed Intelligent Character Recognition (ICR). Intelligent\nCharacter Recognition (ICR) is the OCR module that can convert scans of\nhandwritten or printed characters into ASCII text. ASCII data is the standard\nformat for data encoding in electronic communication. ASCII assigns standard\nnumeric values to letters, numeral, symbols, white-spaces and other characters.\nIn more technical terms, OCR is the process of using an electronic device to\ntransform 2-Dimensional textual information into machine-encoded text. Anything\nthat contains text both machine written or handwritten can be scanned either\nthrough a scanner or just simply a picture of the text is enough for the\nrecognition system to distinguish the text. The goal of this papers is to show\nthe results of a Convolutional Neural Network model which has been trained on\nNational Institute of Science and Technology (NIST) dataset containing over a\n100,000 images. The network learns from the features extracted from the images\nand use it to generate the probability of each class to which the picture\nbelongs to. We have achieved an accuracy of 90.54% with a loss of 2.53%.\n",
                "链接": "https://arxiv.org/abs/2307.05396"
            },
            {
                "文章ID": "53136",
                "标题": "Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned\n  Receipt Images",
                "作者": " Hongkuan Zhang,  Edward Whittaker,  Ikuo Kitagishi",
                "发布日期": "2023-10-17",
                "摘要": "  Digitization of scanned receipts aims to extract text from receipt images and\nsave it into structured documents. This is usually split into two sub-tasks:\ntext localization and optical character recognition (OCR). Most existing OCR\nmodels only focus on the cropped text instance images, which require the\nbounding box information provided by a text region detection model. Introducing\nan additional detector to identify the text instance images in advance adds\ncomplexity, however instance-level OCR models have very low accuracy when\nprocessing the whole image for the document-level OCR, such as receipt images\ncontaining multiple text lines arranged in various layouts. To this end, we\npropose a localization-free document-level OCR model for transcribing all the\ncharacters in a receipt image into an ordered sequence end-to-end.\nSpecifically, we finetune the pretrained instance-level model TrOCR with\nrandomly cropped image chunks, and gradually increase the image chunk size to\ngeneralize the recognition ability from instance images to full-page images. In\nour experiments on the SROIE receipt OCR dataset, the model finetuned with our\nstrategy achieved 64.4 F1-score and a 22.8% character error rate (CER),\nrespectively, which outperforms the baseline results with 48.5 F1-score and\n50.6% CER. The best model, which splits the full image into 15 equally sized\nchunks, gives 87.8 F1-score and 4.98% CER with minimal additional pre or\npost-processing of the output. Moreover, the characters in the generated\ndocument-level sequences are arranged in the reading order, which is practical\nfor real-world applications.\n",
                "链接": "https://arxiv.org/abs/2212.05525"
            },
            {
                "文章ID": "6983",
                "标题": "BLPnet: A new DNN model and Bengali OCR engine for Automatic License\n  Plate Recognition",
                "作者": " Md. Saif Hassan Onim,  Hussain Nyeem,  Koushik Roy,  Mahmudul Hasan,  Abtahi Ishmam,  Md. Akiful Hoque Akif,  Tareque Bashar Ovi",
                "发布日期": "2022-02-25",
                "摘要": "  The development of the Automatic License Plate Recognition (ALPR) system has\nreceived much attention for the English license plate. However, despite being\nthe sixth largest population around the world, no significant progress can be\ntracked in the Bengali language countries or states for the ALPR system\naddressing their more alarming traffic management with inadequate road-safety\nmeasures. This paper reports a computationally efficient and reasonably\naccurate Automatic License Plate Recognition (ALPR) system for Bengali\ncharacters with a new end-to-end DNN model that we call Bengali License Plate\nNetwork(BLPnet). The cascaded architecture for detecting vehicle regions prior\nto vehicle license plate (VLP) in the model is proposed to eliminate false\npositives resulting in higher detection accuracy of VLP. Besides, a lower set\nof trainable parameters is considered for reducing the computational cost\nmaking the system faster and more compatible for a real-time application. With\na Computational Neural Network (CNN)based new Bengali OCR engine and\nword-mapping process, the model is characters rotation invariant, and can\nreadily extract, detect and output the complete license plate number of a\nvehicle. The model feeding with17 frames per second (fps) on real-time video\nfootage can detect a vehicle with the Mean Squared Error (MSE) of 0.0152, and\nthe mean license plate character recognition accuracy of 95%. While compared to\nthe other models, an improvement of 5% and 20% were recorded for the BLPnetover\nthe prominent YOLO-based ALPR model and the Tesseract model for the\nnumber-plate detection accuracy and time requirement, respectively.\n",
                "链接": "https://arxiv.org/abs/2202.12250"
            },
            {
                "文章ID": "50811",
                "标题": "Recent Advances in RecBole: Extensions with more Practical\n  Considerations",
                "作者": " Lanling Xu,  Zhen Tian,  Gaowei Zhang,  Lei Wang,  Junjie Zhang,  Bowen Zheng,  Yifan Li,  Yupeng Hou,  Xingyu Pan,  Yushuo Chen,  Wayne Xin Zhao,  Xu Chen,  Ji-Rong Wen",
                "发布日期": "2022-11-29",
                "摘要": "  RecBole has recently attracted increasing attention from the research\ncommunity. As the increase of the number of users, we have received a number of\nsuggestions and update requests. This motivates us to make some significant\nimprovements on our library, so as to meet the user requirements and contribute\nto the research community. In order to show the recent update in RecBole, we\nwrite this technical report to introduce our latest improvements on RecBole. In\ngeneral, we focus on the flexibility and efficiency of RecBole in the past few\nmonths. More specifically, we have four development targets: (1) more flexible\ndata processing, (2) more efficient model training, (3) more reproducible\nconfigurations, and (4) more comprehensive user documentation. Readers can\ndownload the above updates at: https://github.com/RUCAIBox/RecBole.\n",
                "链接": "https://arxiv.org/abs/2211.15148"
            },
            {
                "文章ID": "8471",
                "标题": "Recent Advances in Neural Text Generation: A Task-Agnostic Survey",
                "作者": " Chen Tang,  Frank Guerin,  Chenghua Lin",
                "发布日期": "2023-06-13",
                "摘要": "  In recent years, considerable research has been dedicated to the application\nof neural models in the field of natural language generation (NLG). The primary\nobjective is to generate text that is both linguistically natural and\nhuman-like, while also exerting control over the generation process. This paper\noffers a comprehensive and task-agnostic survey of the recent advancements in\nneural text generation. These advancements have been facilitated through a\nmultitude of developments, which we categorize into four key areas: data\nconstruction, neural frameworks, training and inference strategies, and\nevaluation metrics. By examining these different aspects, we aim to provide a\nholistic overview of the progress made in the field. Furthermore, we explore\nthe future directions for the advancement of neural text generation, which\nencompass the utilization of neural pipelines and the incorporation of\nbackground knowledge. These avenues present promising opportunities to further\nenhance the capabilities of NLG systems. Overall, this survey serves to\nconsolidate the current state of the art in neural text generation and\nhighlights potential avenues for future research and development in this\ndynamic field.\n",
                "链接": "https://arxiv.org/abs/2203.03047"
            },
            {
                "文章ID": "60474",
                "标题": "A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and\n  Future Trends",
                "作者": " Xiaoye Qu,  Yingjie Gu,  Qingrong Xia,  Zechang Li,  Zhefeng Wang,  Baoxing Huai",
                "发布日期": "2023-08-09",
                "摘要": "  As more and more Arabic texts emerged on the Internet, extracting important\ninformation from these Arabic texts is especially useful. As a fundamental\ntechnology, Named entity recognition (NER) serves as the core component in\ninformation extraction technology, while also playing a critical role in many\nother Natural Language Processing (NLP) systems, such as question answering and\nknowledge graph building. In this paper, we provide a comprehensive review of\nthe development of Arabic NER, especially the recent advances in deep learning\nand pre-trained language model. Specifically, we first introduce the background\nof Arabic NER, including the characteristics of Arabic and existing resources\nfor Arabic NER. Then, we systematically review the development of Arabic NER\nmethods. Traditional Arabic NER systems focus on feature engineering and\ndesigning domain-specific rules. In recent years, deep learning methods achieve\nsignificant progress by representing texts via continuous vector\nrepresentations. With the growth of pre-trained language model, Arabic NER\nyields better performance. Finally, we conclude the method gap between Arabic\nNER and NER methods from other languages, which helps outline future directions\nfor Arabic NER.\n",
                "链接": "https://arxiv.org/abs/2302.03512"
            },
            {
                "文章ID": "23385",
                "标题": "Recent Advances in Bayesian Optimization",
                "作者": " Xilu Wang,  Yaochu Jin,  Sebastian Schmitt,  Markus Olhofer",
                "发布日期": "2022-11-14",
                "摘要": "  Bayesian optimization has emerged at the forefront of expensive black-box\noptimization due to its data efficiency. Recent years have witnessed a\nproliferation of studies on the development of new Bayesian optimization\nalgorithms and their applications. Hence, this paper attempts to provide a\ncomprehensive and updated survey of recent advances in Bayesian optimization\nand identify interesting open problems. We categorize the existing work on\nBayesian optimization into nine main groups according to the motivations and\nfocus of the proposed algorithms. For each category, we present the main\nadvances with respect to the construction of surrogate models and adaptation of\nthe acquisition functions. Finally, we discuss the open questions and suggest\npromising future research directions, in particular with regard to\nheterogeneity, privacy preservation, and fairness in distributed and federated\noptimization systems.\n",
                "链接": "https://arxiv.org/abs/2206.03301"
            },
            {
                "文章ID": "1390",
                "标题": "Recent Progress in the CUHK Dysarthric Speech Recognition System",
                "作者": " Shansong Liu,  Mengzhe Geng,  Shoukang Hu,  Xurong Xie,  Mingyu Cui,  Jianwei Yu,  Xunying Liu,  Helen Meng",
                "发布日期": "2022-03-01",
                "摘要": "  Despite the rapid progress of automatic speech recognition (ASR) technologies\nin the past few decades, recognition of disordered speech remains a highly\nchallenging task to date. Disordered speech presents a wide spectrum of\nchallenges to current data intensive deep neural networks (DNNs) based ASR\ntechnologies that predominantly target normal speech. This paper presents\nrecent research efforts at the Chinese University of Hong Kong (CUHK) to\nimprove the performance of disordered speech recognition systems on the largest\npublicly available UASpeech dysarthric speech corpus. A set of novel modelling\ntechniques including neural architectural search, data augmentation using\nspectra-temporal perturbation, model based speaker adaptation and cross-domain\ngeneration of visual features within an audio-visual speech recognition (AVSR)\nsystem framework were employed to address the above challenges. The combination\nof these techniques produced the lowest published word error rate (WER) of\n25.21% on the UASpeech test set 16 dysarthric speakers, and an overall WER\nreduction of 5.4% absolute (17.6% relative) over the CUHK 2018 dysarthric\nspeech recognition system featuring a 6-way DNN system combination and cross\nadaptation of out-of-domain normal speech data trained systems. Bayesian model\nadaptation further allows rapid adaptation to individual dysarthric speakers to\nbe performed using as little as 3.06 seconds of speech. The efficacy of these\ntechniques were further demonstrated on a CUDYS Cantonese dysarthric speech\nrecognition task.\n",
                "链接": "https://arxiv.org/abs/2201.05845"
            },
            {
                "文章ID": "94665",
                "标题": "Universal Defensive Underpainting Patch: Making Your Text Invisible to\n  Optical Character Recognition",
                "作者": " JiaCheng Deng,  Li Dong,  Jiahao Chen,  Diqun Yan,  Rangding Wang,  Dengpan Ye,  Lingchen Zhao,  Jinyu Tian",
                "发布日期": "2023-08-07",
                "摘要": "  Optical Character Recognition (OCR) enables automatic text extraction from\nscanned or digitized text images, but it also makes it easy to pirate valuable\nor sensitive text from these images. Previous methods to prevent OCR piracy by\ndistorting characters in text images are impractical in real-world scenarios,\nas pirates can capture arbitrary portions of the text images, rendering the\ndefenses ineffective. In this work, we propose a novel and effective defense\nmechanism termed the Universal Defensive Underpainting Patch (UDUP) that\nmodifies the underpainting of text images instead of the characters. UDUP is\ncreated through an iterative optimization process to craft a small, fixed-size\ndefensive patch that can generate non-overlapping underpainting for text images\nof any size. Experimental results show that UDUP effectively defends against\nunauthorized OCR under the setting of any screenshot range or complex image\nbackground. It is agnostic to the content, size, colors, and languages of\ncharacters, and is robust to typical image operations such as scaling and\ncompressing. In addition, the transferability of UDUP is demonstrated by\nevading several off-the-shelf OCRs. The code is available at\nhttps://github.com/QRICKDD/UDUP.\n",
                "链接": "https://arxiv.org/abs/2308.02369"
            }
        ]
    },
    {
        "question": {
            "question": "查找大模型推理的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "116170",
                "标题": "Large Language Model Inference with Lexical Shortlisting",
                "作者": " Nikolay Bogoychev,  Pinzhen Chen,  Barry Haddow,  Alexandra Birch",
                "发布日期": "2023-11-17",
                "摘要": "  Large language model (LLM) inference is computation and memory intensive, so\nwe adapt lexical shortlisting to it hoping to improve both. While lexical\nshortlisting is well-explored in tasks like machine translation, it requires\nmodifications before being suitable for LLMs as the intended applications vary\nsignificantly. Our work studies two heuristics to shortlist sub-vocabulary at\nLLM inference time: Unicode-based script filtering and corpus-based selection.\nWe explore different LLM families and sizes, and we find that lexical\nshortlisting can reduce the memory usage of some models by nearly 50\\% and has\nan upper bound of 25\\% improvement in generation speed. In this pilot study, we\nalso identify the drawbacks of such vocabulary selection methods and propose\navenues for future research.\n",
                "链接": "https://arxiv.org/abs/2311.09709"
            },
            {
                "文章ID": "71550",
                "标题": "Inference with Reference: Lossless Acceleration of Large Language Models",
                "作者": " Nan Yang,  Tao Ge,  Liang Wang,  Binxing Jiao,  Daxin Jiang,  Linjun Yang,  Rangan Majumder,  Furu Wei",
                "发布日期": "2023-04-11",
                "摘要": "  We propose LLMA, an LLM accelerator to losslessly speed up Large Language\nModel (LLM) inference with references. LLMA is motivated by the observation\nthat there are abundant identical text spans between the decoding result by an\nLLM and the reference that is available in many real world scenarios (e.g.,\nretrieved documents). LLMA first selects a text span from the reference and\ncopies its tokens to the decoder and then efficiently checks the tokens'\nappropriateness as the decoding result in parallel within one decoding step.\nThe improved computational parallelism allows LLMA to achieve over 2x speed-up\nfor LLMs with identical generation results as greedy decoding in many practical\ngeneration scenarios where significant overlap between in-context reference and\noutputs exists (e.g., search engines and multi-turn conversations).\n",
                "链接": "https://arxiv.org/abs/2304.04487"
            },
            {
                "文章ID": "83366",
                "标题": "On Optimal Caching and Model Multiplexing for Large Model Inference",
                "作者": " Banghua Zhu,  Ying Sheng,  Lianmin Zheng,  Clark Barrett,  Michael I. Jordan,  Jiantao Jiao",
                "发布日期": "2023-08-30",
                "摘要": "  Large Language Models (LLMs) and other large foundation models have achieved\nnoteworthy success, but their size exacerbates existing resource consumption\nand latency challenges. In particular, the large-scale deployment of these\nmodels is hindered by the significant resource requirements during inference.\nIn this paper, we study two approaches for mitigating these challenges:\nemploying a cache to store previous queries and learning a model multiplexer to\nchoose from an ensemble of models for query processing.\n  Theoretically, we provide an optimal algorithm for jointly optimizing both\napproaches to reduce the inference cost in both offline and online tabular\nsettings. By combining a caching algorithm, namely Greedy Dual Size with\nFrequency (GDSF) or Least Expected Cost (LEC), with a model multiplexer, we\nachieve optimal rates in both offline and online settings. Empirically,\nsimulations show that the combination of our caching and model multiplexing\nalgorithms greatly improves over the baselines, with up to $50\\times$\nimprovement over the baseline when the ratio between the maximum cost and\nminimum cost is $100$. Experiments on real datasets show a $4.3\\times$\nimprovement in FLOPs over the baseline when the ratio for FLOPs is $10$, and a\n$1.8\\times$ improvement in latency when the ratio for average latency is\n$1.85$.\n",
                "链接": "https://arxiv.org/abs/2306.02003"
            },
            {
                "文章ID": "99195",
                "标题": "Papeos: Augmenting Research Papers with Talk Videos",
                "作者": " Tae Soo Kim,  Matt Latzke,  Jonathan Bragg,  Amy X. Zhang,  Joseph Chee Chang",
                "发布日期": "2023-08-30",
                "摘要": "  Research consumption has been traditionally limited to the reading of\nacademic papers-a static, dense, and formally written format. Alternatively,\npre-recorded conference presentation videos, which are more dynamic, concise,\nand colloquial, have recently become more widely available but potentially\nunder-utilized. In this work, we explore the design space and benefits for\ncombining academic papers and talk videos to leverage their complementary\nnature to provide a rich and fluid research consumption experience. Based on\nformative and co-design studies, we present Papeos, a novel reading and\nauthoring interface that allow authors to augment their papers by segmenting\nand localizing talk videos alongside relevant paper passages with automatically\ngenerated suggestions. With Papeos, readers can visually skim a paper through\nclip thumbnails, and fluidly switch between consuming dense text in the paper\nor visual summaries in the video. In a comparative lab study (n=16), Papeos\nreduced mental load, scaffolded navigation, and facilitated more comprehensive\nreading of papers.\n",
                "链接": "https://arxiv.org/abs/2308.15224"
            },
            {
                "文章ID": "113368",
                "标题": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
                "作者": " Ke Hong,  Guohao Dai,  Jiaming Xu,  Qiuli Mao,  Xiuhong Li,  Jun Liu,  Kangdi Chen,  Yuhan Dong,  Yu Wang",
                "发布日期": "2023-11-13",
                "摘要": "  As the Large Language Model (LLM) becomes increasingly important in various\ndomains. However, the following challenges still remain unsolved in\naccelerating LLM inference: (1) Synchronized partial softmax update. The\nsoftmax operation requires a synchronized update operation among each partial\nsoftmax result, leading to ~20% overheads for the attention computation in\nLLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices\nperforming GEMM in LLM inference is flat, leading to under-utilized computation\nand >50% performance loss after padding zeros in previous designs. (3)\nPerformance loss due to static dataflow. Kernel performance in LLM depends on\nvaried input data features, hardware configurations, etc. A single and static\ndataflow may lead to a 50.25% performance loss for GEMMs of different shapes in\nLLM inference.\n  We present FlashDecoding++, a fast LLM inference engine supporting mainstream\nLLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++\ncreatively proposes: (1) Asynchronized softmax with unified max value.\nFlashDecoding++ introduces a unified max value technique for different partial\nsoftmax computations to avoid synchronization. (2) Flat GEMM optimization with\ndouble buffering. FlashDecoding++ points out that flat GEMMs with different\nshapes face varied bottlenecks. Then, techniques like double buffering are\nintroduced. (3) Heuristic dataflow with hardware resource adaptation.\nFlashDecoding++ heuristically optimizes dataflow using different hardware\nresource considering input dynamics. Due to the versatility of optimizations in\nFlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on\nboth NVIDIA and AMD GPUs compared to Hugging Face implementations.\nFlashDecoding++ also achieves an average speedup of 1.37x compared to\nstate-of-the-art LLM inference engines on mainstream LLMs.\n",
                "链接": "https://arxiv.org/abs/2311.01282"
            },
            {
                "文章ID": "120482",
                "标题": "A Hardware Evaluation Framework for Large Language Model Inference",
                "作者": " Hengrui Zhang,  August Ning,  Rohan Prabhakar,  David Wentzlaff",
                "发布日期": "2023-12-07",
                "摘要": "  The past year has witnessed the increasing popularity of Large Language\nModels (LLMs). Their unprecedented scale and associated high hardware cost have\nimpeded their broader adoption, calling for efficient hardware designs. With\nthe large hardware needed to simply run LLM inference, evaluating different\nhardware designs becomes a new bottleneck.\n  This work introduces LLMCompass, a hardware evaluation framework for LLM\ninference workloads. LLMCompass is fast, accurate, versatile, and able to\ndescribe and evaluate different hardware designs. LLMCompass includes a mapper\nto automatically find performance-optimal mapping and scheduling. It also\nincorporates an area-based cost model to help architects reason about their\ndesign choices. Compared to real-world hardware, LLMCompass' estimated latency\nachieves an average 10.4% error rate across various operators with various\ninput sizes and an average 4.1% error rate for LLM inference. With LLMCompass,\nsimulating a 4-NVIDIA A100 GPU node running GPT-3 175B inference can be done\nwithin 16 minutes on commodity hardware, including 26,400 rounds of the\nmapper's parameter search.\n  With the aid of LLMCompass, this work draws architectural implications and\nexplores new cost-effective hardware designs. By reducing the compute\ncapability or replacing High Bandwidth Memory (HBM) with traditional DRAM,\nthese new designs can achieve as much as 3.41x improvement in performance/cost\ncompared to an NVIDIA A100, making them promising choices for democratizing\nLLMs.\n  LLMCompass is planned to be fully open-source.\n",
                "链接": "https://arxiv.org/abs/2312.03134"
            },
            {
                "文章ID": "100993",
                "标题": "LLMCad: Fast and Scalable On-device Large Language Model Inference",
                "作者": " Daliang Xu,  Wangsong Yin,  Xin Jin,  Ying Zhang,  Shiyun Wei,  Mengwei Xu,  Xuanzhe Liu",
                "发布日期": "2023-09-11",
                "摘要": "  Generative tasks, such as text generation and question answering, hold a\ncrucial position in the realm of mobile applications. Due to their sensitivity\nto privacy concerns, there is a growing demand for their execution directly on\nmobile devices. Currently, the execution of these generative tasks heavily\ndepends on Large Language Models (LLMs). Nevertheless, the limited memory\ncapacity of these devices presents a formidable challenge to the scalability of\nsuch models.\n  In our research, we introduce LLMCad, an innovative on-device inference\nengine specifically designed for efficient generative Natural Language\nProcessing (NLP) tasks. The core idea behind LLMCad revolves around model\ncollaboration: a compact LLM, residing in memory, takes charge of generating\nthe most straightforward tokens, while a high-precision LLM steps in to\nvalidate these tokens and rectify any identified errors. LLMCad incorporates\nthree novel techniques: (1) Instead of generating candidate tokens in a\nsequential manner, LLMCad employs the smaller LLM to construct a token tree,\nencompassing a wider range of plausible token pathways. Subsequently, the\nlarger LLM can efficiently validate all of these pathways simultaneously. (2)\nIt employs a self-adjusting fallback strategy, swiftly initiating the\nverification process whenever the smaller LLM generates an erroneous token. (3)\nTo ensure a continuous flow of token generation, LLMCad speculatively generates\ntokens during the verification process by implementing a compute-IO pipeline.\nThrough an extensive series of experiments, LLMCad showcases an impressive\ntoken generation speed, achieving rates up to 9.3x faster than existing\ninference engines.\n",
                "链接": "https://arxiv.org/abs/2309.04255"
            },
            {
                "文章ID": "119522",
                "标题": "LinguaLinked: A Distributed Large Language Model Inference System for\n  Mobile Devices",
                "作者": " Junchen Zhao,  Yurun Song,  Simeng Liu,  Ian G. Harris,  Sangeetha Abdu Jyothi",
                "发布日期": "2023-12-04",
                "摘要": "  Deploying Large Language Models (LLMs) locally on mobile devices presents a\nsignificant challenge due to their extensive memory requirements. In this\npaper, we introduce LinguaLinked, a system for decentralized, distributed LLM\ninference on mobile devices. LinguaLinked enables collaborative execution of\nthe inference task across multiple trusted devices. LinguaLinked ensures data\nprivacy by processing information locally. LinguaLinked uses three key\nstrategies. First, an optimized model assignment technique segments LLMs and\nuses linear optimization to align segments with each device's capabilities.\nSecond, an optimized data transmission mechanism ensures efficient and\nstructured data flow between model segments while also maintaining the\nintegrity of the original model structure. Finally, LinguaLinked incorporates a\nruntime load balancer that actively monitors and redistributes tasks among\nmobile devices to prevent bottlenecks, enhancing the system's overall\nefficiency and responsiveness. We demonstrate that LinguaLinked facilitates\nefficient LLM inference while maintaining consistent throughput and minimal\nlatency through extensive testing across various mobile devices, from high-end\nto low-end Android devices. In our evaluations, compared to the baseline,\nLinguaLinked achieves an inference performance acceleration of $1.11\\times$ to\n$1.61\\times$ in single-threaded settings, $1.73\\times$ to $2.65\\times$ with\nmulti-threading. Additionally, runtime load balancing yields an overall\ninference acceleration of $1.29\\times$ to $1.32\\times$.\n",
                "链接": "https://arxiv.org/abs/2312.00388"
            },
            {
                "文章ID": "123538",
                "标题": "LLM in a flash: Efficient Large Language Model Inference with Limited\n  Memory",
                "作者": " Keivan Alizadeh,  Iman Mirzadeh,  Dmitry Belenko,  Karen Khatamifard,  Minsik Cho,  Carlo C Del Mundo,  Mohammad Rastegari,  Mehrdad Farajtabar",
                "发布日期": "2023-12-20",
                "摘要": "  Large language models (LLMs) are central to modern natural language\nprocessing, delivering exceptional performance in various tasks. However, their\nintensive computational and memory requirements present challenges, especially\nfor devices with limited DRAM capacity. This paper tackles the challenge of\nefficiently running LLMs that exceed the available DRAM capacity by storing the\nmodel parameters on flash memory but bringing them on demand to DRAM. Our\nmethod involves constructing an inference cost model that harmonizes with the\nflash memory behavior, guiding us to optimize in two critical areas: reducing\nthe volume of data transferred from flash and reading data in larger, more\ncontiguous chunks. Within this flash memory-informed framework, we introduce\ntwo principal techniques. First, \"windowing'\" strategically reduces data\ntransfer by reusing previously activated neurons, and second, \"row-column\nbundling\", tailored to the sequential data access strengths of flash memory,\nincreases the size of data chunks read from flash memory. These methods\ncollectively enable running models up to twice the size of the available DRAM,\nwith a 4-5x and 20-25x increase in inference speed compared to naive loading\napproaches in CPU and GPU, respectively. Our integration of sparsity awareness,\ncontext-adaptive loading, and a hardware-oriented design paves the way for\neffective inference of LLMs on devices with limited memory.\n",
                "链接": "https://arxiv.org/abs/2312.11514"
            },
            {
                "文章ID": "55730",
                "标题": "Rethinking with Retrieval: Faithful Large Language Model Inference",
                "作者": " Hangfeng He,  Hongming Zhang,  Dan Roth",
                "发布日期": "2023-01-03",
                "摘要": "  Despite the success of large language models (LLMs) in various natural\nlanguage processing (NLP) tasks, the stored knowledge in these models may\ninevitably be incomplete, out-of-date, or incorrect. This motivates the need to\nutilize external knowledge to assist LLMs. Unfortunately, current methods for\nincorporating external knowledge often require additional training or\nfine-tuning, which can be costly and may not be feasible for LLMs. To address\nthis issue, we propose a novel post-processing approach, rethinking with\nretrieval (RR), which retrieves relevant external knowledge based on the\ndecomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.\nThis lightweight approach does not require additional training or fine-tuning\nand is not limited by the input length of LLMs. We evaluate the effectiveness\nof RR through extensive experiments with GPT-3 on three complex reasoning\ntasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our\nresults show that RR can produce more faithful explanations and improve the\nperformance of LLMs.\n",
                "链接": "https://arxiv.org/abs/2301.00303"
            }
        ]
    },
    {
        "question": {
            "question": "近几个月自然语言处理相关的文章。",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找多语言情感分析的最新论文。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "2025",
                "标题": "NaijaSenti: A Nigerian Twitter Sentiment Corpus for Multilingual\n  Sentiment Analysis",
                "作者": " Shamsuddeen Hassan Muhammad,  David Ifeoluwa Adelani,  Sebastian Ruder,  Ibrahim Said Ahmad,  Idris Abdulmumin,  Bello Shehu Bello,  Monojit Choudhury,  Chris Chinenye Emezue,  Saheed Salahudeen Abdullahi,  Anuoluwapo Aremu,  Alipio Jeorge,  Pavel Brazdil",
                "发布日期": "2022-06-22",
                "摘要": "  Sentiment analysis is one of the most widely studied applications in NLP, but\nmost work focuses on languages with large amounts of data. We introduce the\nfirst large-scale human-annotated Twitter sentiment dataset for the four most\nwidely spoken languages in Nigeria (Hausa, Igbo, Nigerian-Pidgin, and\nYor\\`ub\\'a ) consisting of around 30,000 annotated tweets per language (and\n14,000 for Nigerian-Pidgin), including a significant fraction of code-mixed\ntweets. We propose text collection, filtering, processing and labeling methods\nthat enable us to create datasets for these low-resource languages. We evaluate\na rangeof pre-trained models and transfer strategies on the dataset. We find\nthat language-specific models and language-adaptivefine-tuning generally\nperform best. We release the datasets, trained models, sentiment lexicons, and\ncode to incentivizeresearch on sentiment analysis in under-represented\nlanguages.\n",
                "链接": "https://arxiv.org/abs/2201.08277"
            },
            {
                "文章ID": "74935",
                "标题": "NLNDE at SemEval-2023 Task 12: Adaptive Pretraining and Source Language\n  Selection for Low-Resource Multilingual Sentiment Analysis",
                "作者": " Mingyang Wang,  Heike Adel,  Lukas Lange,  Jannik Strötgen,  Hinrich Schütze",
                "发布日期": "2023-05-02",
                "摘要": "  This paper describes our system developed for the SemEval-2023 Task 12\n\"Sentiment Analysis for Low-resource African Languages using Twitter Dataset\".\nSentiment analysis is one of the most widely studied applications in natural\nlanguage processing. However, most prior work still focuses on a small number\nof high-resource languages. Building reliable sentiment analysis systems for\nlow-resource languages remains challenging, due to the limited training data in\nthis task. In this work, we propose to leverage language-adaptive and\ntask-adaptive pretraining on African texts and study transfer learning with\nsource language selection on top of an African language-centric pretrained\nlanguage model. Our key findings are: (1) Adapting the pretrained model to the\ntarget language and task using a small yet relevant corpus improves performance\nremarkably by more than 10 F1 score points. (2) Selecting source languages with\npositive transfer gains during training can avoid harmful interference from\ndissimilar languages, leading to better results in multilingual and\ncross-lingual settings. In the shared task, our system wins 8 out of 15 tracks\nand, in particular, performs best in the multilingual evaluation.\n",
                "链接": "https://arxiv.org/abs/2305.00090"
            },
            {
                "文章ID": "85580",
                "标题": "Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted\n  Sentiment Classification Benchmark",
                "作者": " Łukasz Augustyniak,  Szymon Woźniak,  Marcin Gruza,  Piotr Gramacki,  Krzysztof Rajda,  Mikołaj Morzy,  Tomasz Kajdanowicz",
                "发布日期": "2023-06-14",
                "摘要": "  Despite impressive advancements in multilingual corpora collection and model\ntraining, developing large-scale deployments of multilingual models still\npresents a significant challenge. This is particularly true for language tasks\nthat are culture-dependent. One such example is the area of multilingual\nsentiment analysis, where affective markers can be subtle and deeply ensconced\nin culture. This work presents the most extensive open massively multilingual\ncorpus of datasets for training sentiment models. The corpus consists of 79\nmanually selected datasets from over 350 datasets reported in the scientific\nliterature based on strict quality criteria. The corpus covers 27 languages\nrepresenting 6 language families. Datasets can be queried using several\nlinguistic and functional features. In addition, we present a multi-faceted\nsentiment classification benchmark summarizing hundreds of experiments\nconducted on different base models, training objectives, dataset collections,\nand fine-tuning strategies.\n",
                "链接": "https://arxiv.org/abs/2306.07902"
            },
            {
                "文章ID": "52169",
                "标题": "Video Games as a Corpus: Sentiment Analysis using Fallout New Vegas\n  Dialog",
                "作者": " Mika Hämäläinen,  Khalid Alnajjar,  Thierry Poibeau",
                "发布日期": "2022-12-06",
                "摘要": "  We present a method for extracting a multilingual sentiment annotated dialog\ndata set from Fallout New Vegas. The game developers have preannotated every\nline of dialog in the game in one of the 8 different sentiments: \\textit{anger,\ndisgust, fear, happy, neutral, pained, sad } and \\textit{surprised}. The game\nhas been translated into English, Spanish, German, French and Italian. We\nconduct experiments on multilingual, multilabel sentiment analysis on the\nextracted data set using multilingual BERT, XLMRoBERTa and language specific\nBERT models. In our experiments, multilingual BERT outperformed XLMRoBERTa for\nmost of the languages, also language specific models were slightly better than\nmultilingual BERT for most of the languages. The best overall accuracy was 54\\%\nand it was achieved by using multilingual BERT on Spanish data. The extracted\ndata set presents a challenging task for sentiment analysis. We have released\nthe data, including the testing and training splits, openly on Zenodo. The data\nset has been shuffled for copyright reasons.\n",
                "链接": "https://arxiv.org/abs/2212.02168"
            },
            {
                "文章ID": "14369",
                "标题": "Assessment of Massively Multilingual Sentiment Classifiers",
                "作者": " Krzysztof Rajda,  Łukasz Augustyniak,  Piotr Gramacki,  Marcin Gruza,  Szymon Woźniak,  Tomasz Kajdanowicz",
                "发布日期": "2022-04-12",
                "摘要": "  Models are increasing in size and complexity in the hunt for SOTA. But what\nif those 2\\% increase in performance does not make a difference in a production\nuse case? Maybe benefits from a smaller, faster model outweigh those slight\nperformance gains. Also, equally good performance across languages in\nmultilingual tasks is more important than SOTA results on a single one. We\npresent the biggest, unified, multilingual collection of sentiment analysis\ndatasets. We use these to assess 11 models and 80 high-quality sentiment\ndatasets (out of 342 raw datasets collected) in 27 languages and included\nresults on the internally annotated datasets. We deeply evaluate multiple\nsetups, including fine-tuning transformer-based models for measuring\nperformance. We compare results in numerous dimensions addressing the imbalance\nin both languages coverage and dataset sizes. Finally, we present some best\npractices for working with such a massive collection of datasets and models\nfrom a multilingual perspective.\n",
                "链接": "https://arxiv.org/abs/2204.04937"
            },
            {
                "文章ID": "82977",
                "标题": "UCAS-IIE-NLP at SemEval-2023 Task 12: Enhancing Generalization of\n  Multilingual BERT for Low-resource Sentiment Analysis",
                "作者": " Dou Hu,  Lingwei Wei,  Yaxin Liu,  Wei Zhou,  Songlin Hu",
                "发布日期": "2023-06-05",
                "摘要": "  This paper describes our system designed for SemEval-2023 Task 12: Sentiment\nanalysis for African languages. The challenge faced by this task is the\nscarcity of labeled data and linguistic resources in low-resource settings. To\nalleviate these, we propose a generalized multilingual system SACL-XLMR for\nsentiment analysis on low-resource languages. Specifically, we design a\nlexicon-based multilingual BERT to facilitate language adaptation and\nsentiment-aware representation learning. Besides, we apply a supervised\nadversarial contrastive learning technique to learn sentiment-spread structured\nrepresentations and enhance model generalization. Our system achieved\ncompetitive results, largely outperforming baselines on both multilingual and\nzero-shot sentiment classification subtasks. Notably, the system obtained the\n1st rank on the zero-shot classification subtask in the official ranking.\nExtensive experiments demonstrate the effectiveness of our system.\n",
                "链接": "https://arxiv.org/abs/2306.01093"
            },
            {
                "文章ID": "59246",
                "标题": "Automated Sentiment and Hate Speech Analysis of Facebook Data by\n  Employing Multilingual Transformer Models",
                "作者": " Ritumbra Manuvie,  Saikat Chatterjee",
                "发布日期": "2023-02-01",
                "摘要": "  In recent years, there has been a heightened consensus within academia and in\nthe public discourse that Social Media Platforms (SMPs), amplify the spread of\nhateful and negative sentiment content. Researchers have identified how hateful\ncontent, political propaganda, and targeted messaging contributed to real-world\nharms including insurrections against democratically elected governments,\ngenocide, and breakdown of social cohesion due to heightened negative discourse\ntowards certain communities in parts of the world. To counter these issues,\nSMPs have created semi-automated systems that can help identify toxic speech.\nIn this paper we analyse the statistical distribution of hateful and negative\nsentiment contents within a representative Facebook dataset (n= 604,703)\nscrapped through 648 public Facebook pages which identify themselves as\nproponents (and followers) of far-right Hindutva actors. These pages were\nidentified manually using keyword searches on Facebook and on CrowdTangleand\nclassified as far-right Hindutva pages based on page names, page descriptions,\nand discourses shared on these pages. We employ state-of-the-art, open-source\nXLM-T multilingual transformer-based language models to perform sentiment and\nhate speech analysis of the textual contents shared on these pages over a\nperiod of 5.5 years. The result shows the statistical distributions of the\npredicted sentiment and the hate speech labels; top actors, and top page\ncategories. We further discuss the benchmark performances and limitations of\nthese pre-trained language models.\n",
                "链接": "https://arxiv.org/abs/2301.13668"
            },
            {
                "文章ID": "19141",
                "标题": "Sentiment Analysis of Covid-related Reddits",
                "作者": " Yilin Yang,  Tomas Fieg,  Marina Sokolova",
                "发布日期": "2022-05-17",
                "摘要": "  This paper focuses on Sentiment Analysis of Covid-19 related messages from\nthe r/Canada and r/Unitedkingdom subreddits of Reddit. We apply manual\nannotation and three Machine Learning algorithms to analyze sentiments conveyed\nin those messages. We use VADER and TextBlob to label messages for Machine\nLearning experiments. Our results show that removal of shortest and longest\nmessages improves VADER and TextBlob agreement on positive sentiments and\nF-score of sentiment classification by all the three algorithms\n",
                "链接": "https://arxiv.org/abs/2205.06863"
            },
            {
                "文章ID": "102692",
                "标题": "The ParlaSent multilingual training dataset for sentiment identification\n  in parliamentary proceedings",
                "作者": " Michal Mochtak,  Peter Rupnik,  Nikola Ljubešić",
                "发布日期": "2023-09-19",
                "摘要": "  Sentiments inherently drive politics. How we receive and process information\nplays an essential role in political decision-making, shaping our judgment with\nstrategic consequences both on the level of legislators and the masses. If\nsentiment plays such an important role in politics, how can we study and\nmeasure it systematically? The paper presents a new dataset of\nsentiment-annotated sentences, which are used in a series of experiments\nfocused on training a robust sentiment classifier for parliamentary\nproceedings. The paper also introduces the first domain-specific LLM for\npolitical science applications additionally pre-trained on 1.72 billion\ndomain-specific words from proceedings of 27 European parliaments. We present\nexperiments demonstrating how the additional pre-training of LLM on\nparliamentary data can significantly improve the model downstream performance\non the domain-specific tasks, in our case, sentiment detection in parliamentary\nproceedings. We further show that multilingual models perform very well on\nunseen languages and that additional data from other languages significantly\nimproves the target parliament's results. The paper makes an important\ncontribution to multiple domains of social sciences and bridges them with\ncomputer science and computational linguistics. Lastly, it sets up a more\nrobust approach to sentiment analysis of political texts in general, which\nallows scholars to study political sentiment from a comparative perspective\nusing standardized tools and techniques.\n",
                "链接": "https://arxiv.org/abs/2309.09783"
            },
            {
                "文章ID": "43460",
                "标题": "Sentiment-Aware Word and Sentence Level Pre-training for Sentiment\n  Analysis",
                "作者": " Shuai Fan,  Chen Lin,  Haonan Li,  Zhenghao Lin,  Jinsong Su,  Hang Zhang,  Yeyun Gong,  Jian Guo,  Nan Duan",
                "发布日期": "2022-10-20",
                "摘要": "  Most existing pre-trained language representation models (PLMs) are\nsub-optimal in sentiment analysis tasks, as they capture the sentiment\ninformation from word-level while under-considering sentence-level information.\nIn this paper, we propose SentiWSP, a novel Sentiment-aware pre-trained\nlanguage model with combined Word-level and Sentence-level Pre-training tasks.\nThe word level pre-training task detects replaced sentiment words, via a\ngenerator-discriminator framework, to enhance the PLM's knowledge about\nsentiment words. The sentence level pre-training task further strengthens the\ndiscriminator via a contrastive learning framework, with similar sentences as\nnegative samples, to encode sentiments in a sentence. Extensive experimental\nresults show that SentiWSP achieves new state-of-the-art performance on various\nsentence-level and aspect-level sentiment classification benchmarks. We have\nmade our code and model publicly available at\nhttps://github.com/XMUDM/SentiWSP.\n",
                "链接": "https://arxiv.org/abs/2210.09803"
            }
        ]
    },
    {
        "question": {
            "question": "2023年以后关于NLP领域的持续性学习论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "76320",
                "标题": "Putting Natural in Natural Language Processing",
                "作者": " Grzegorz Chrupała",
                "发布日期": "2023-05-24",
                "摘要": "  Human language is firstly spoken and only secondarily written. Text, however,\nis a very convenient and efficient representation of language, and modern\ncivilization has made it ubiquitous. Thus the field of NLP has overwhelmingly\nfocused on processing written rather than spoken language. Work on spoken\nlanguage, on the other hand, has been siloed off within the largely separate\nspeech processing community which has been inordinately preoccupied with\ntranscribing speech into text. Recent advances in deep learning have led to a\nfortuitous convergence in methods between speech processing and mainstream NLP.\nArguably, the time is ripe for a unification of these two fields, and for\nstarting to take spoken language seriously as the primary mode of human\ncommunication. Truly natural language processing could lead to better\nintegration with the rest of language science and could lead to systems which\nare more data-efficient and more human-like, and which can communicate beyond\nthe textual modality.\n",
                "链接": "https://arxiv.org/abs/2305.04572"
            },
            {
                "文章ID": "115729",
                "标题": "Natural Language Processing for Financial Regulation",
                "作者": " Ixandra Achitouv,  Dragos Gorduza,  Antoine Jacquier",
                "发布日期": "2023-11-16",
                "摘要": "  This article provides an understanding of Natural Language Processing\ntechniques in the framework of financial regulation, more specifically in order\nto perform semantic matching search between rules and policy when no dataset is\navailable for supervised learning. We outline how to outperform simple\npre-trained sentences-transformer models using freely available resources and\nexplain the mathematical concepts behind the key building blocks of Natural\nLanguage Processing.\n",
                "链接": "https://arxiv.org/abs/2311.08533"
            },
            {
                "文章ID": "43559",
                "标题": "A Survey of Active Learning for Natural Language Processing",
                "作者": " Zhisong Zhang,  Emma Strubell,  Eduard Hovy",
                "发布日期": "2023-02-06",
                "摘要": "  In this work, we provide a survey of active learning (AL) for its\napplications in natural language processing (NLP). In addition to a\nfine-grained categorization of query strategies, we also investigate several\nother important aspects of applying AL to NLP problems. These include AL for\nstructured prediction tasks, annotation cost, model learning (especially with\ndeep neural models), and starting and stopping AL. Finally, we conclude with a\ndiscussion of related topics and future directions.\n",
                "链接": "https://arxiv.org/abs/2210.10109"
            },
            {
                "文章ID": "68861",
                "标题": "Natural Language Processing in Ethiopian Languages: Current State,\n  Challenges, and Opportunities",
                "作者": " Atnafu Lambebo Tonja,  Tadesse Destaw Belay,  Israel Abebe Azime,  Abinew Ali Ayele,  Moges Ahmed Mehamed,  Olga Kolesnikova,  Seid Muhie Yimam",
                "发布日期": "2023-03-28",
                "摘要": "  This survey delves into the current state of natural language processing\n(NLP) for four Ethiopian languages: Amharic, Afaan Oromo, Tigrinya, and\nWolaytta. Through this paper, we identify key challenges and opportunities for\nNLP research in Ethiopia. Furthermore, we provide a centralized repository on\nGitHub that contains publicly available resources for various NLP tasks in\nthese languages. This repository can be updated periodically with contributions\nfrom other researchers. Our objective is to identify research gaps and\ndisseminate the information to NLP researchers interested in Ethiopian\nlanguages and encourage future research in this domain.\n",
                "链接": "https://arxiv.org/abs/2303.14406"
            },
            {
                "文章ID": "112711",
                "标题": "Partial Tensorized Transformers for Natural Language Processing",
                "作者": " Subhadra Vadlamannati,  Ryan Solgi",
                "发布日期": "2023-11-01",
                "摘要": "  The transformer architecture has revolutionized Natural Language Processing\n(NLP) and other machine-learning tasks, due to its unprecedented accuracy.\nHowever, their extensive memory and parameter requirements often hinder their\npractical applications. In this work, we study the effect of tensor-train\ndecomposition to improve the accuracy and compress transformer vision-language\nneural networks, namely BERT and ViT. We focus both on embedding-layer\ncompression and partial tensorization of neural networks (PTNN) through an\nalgorithmic approach. Our novel PTNN approach significantly improves the\naccuracy of existing models by up to 5%, all without the need for post-training\nadjustments, breaking new ground in the field of tensor decomposition.\n",
                "链接": "https://arxiv.org/abs/2310.20077"
            },
            {
                "文章ID": "119807",
                "标题": "Enabling Quantum Natural Language Processing for Hindi Language",
                "作者": " Naman Srivastava,  Gaurang Belekar,  Sunil Saumya,  Aswath Babu H",
                "发布日期": "2023-12-05",
                "摘要": "  Quantum Natural Language Processing (QNLP) is taking huge leaps in solving\nthe shortcomings of classical Natural Language Processing (NLP) techniques and\nmoving towards a more \"Explainable\" NLP system. The current literature around\nQNLP focuses primarily on implementing QNLP techniques in sentences in the\nEnglish language. In this paper, we propose to enable the QNLP approach to\nHINDI, which is the third most spoken language in South Asia. We present the\nprocess of building the parameterized quantum circuits required to undertake\nQNLP on Hindi sentences. We use the pregroup representation of Hindi and the\nDisCoCat framework to draw sentence diagrams. Later, we translate these\ndiagrams to Parameterised Quantum Circuits based on Instantaneous Quantum\nPolynomial (IQP) style ansatz. Using these parameterized quantum circuits\nallows one to train grammar and topic-aware sentence classifiers for the Hindi\nLanguage.\n",
                "链接": "https://arxiv.org/abs/2312.01221"
            },
            {
                "文章ID": "5528",
                "标题": "A Survey on Dynamic Neural Networks for Natural Language Processing",
                "作者": " Canwen Xu,  Julian McAuley",
                "发布日期": "2023-02-27",
                "摘要": "  Effectively scaling large Transformer models is a main driver of recent\nadvances in natural language processing. Dynamic neural networks, as an\nemerging research direction, are capable of scaling up neural networks with\nsub-linear increases in computation and time by dynamically adjusting their\ncomputational path based on the input. Dynamic neural networks could be a\npromising solution to the growing parameter numbers of pretrained language\nmodels, allowing both model pretraining with trillions of parameters and faster\ninference on mobile devices. In this survey, we summarize progress of three\ntypes of dynamic neural networks in NLP: skimming, mixture of experts, and\nearly exit. We also highlight current challenges in dynamic neural networks and\ndirections for future research.\n",
                "链接": "https://arxiv.org/abs/2202.07101"
            },
            {
                "文章ID": "49978",
                "标题": "Continual Learning of Natural Language Processing Tasks: A Survey",
                "作者": " Zixuan Ke,  Bing Liu",
                "发布日期": "2023-05-12",
                "摘要": "  Continual learning (CL) is a learning paradigm that emulates the human\ncapability of learning and accumulating knowledge continually without\nforgetting the previously learned knowledge and also transferring the learned\nknowledge to help learn new tasks better. This survey presents a comprehensive\nreview and analysis of the recent progress of CL in NLP, which has significant\ndifferences from CL in computer vision and machine learning. It covers (1) all\nCL settings with a taxonomy of existing techniques; (2) catastrophic forgetting\n(CF) prevention, (3) knowledge transfer (KT), which is particularly important\nfor NLP tasks; and (4) some theory and the hidden challenge of inter-task class\nseparation (ICS). (1), (3) and (4) have not been included in the existing\nsurvey. Finally, a list of future directions is discussed.\n",
                "链接": "https://arxiv.org/abs/2211.12701"
            },
            {
                "文章ID": "107590",
                "标题": "Evolution of Natural Language Processing Technology: Not Just Language\n  Processing Towards General Purpose AI",
                "作者": " Masahiro Yamamoto",
                "发布日期": "2023-10-11",
                "摘要": "  Since the invention of computers, communication through natural language\n(actual human language) has been a dream technology. However, natural language\nis extremely difficult to mathematically formulate, making it difficult to\nrealize as an algorithm without considering programming. While there have been\nnumerous technological developments, one cannot say that any results allowing\nfree utilization have been achieved thus far. In the case of language learning\nin humans, for instance when learning one's mother tongue or foreign language,\none must admit that this process is similar to the adage \"practice makes\nperfect\" in principle, even though the learning method is significant up to a\npoint. Deep learning has played a central role in contemporary AI technology in\nrecent years. When applied to natural language processing (NLP), this produced\nunprecedented results. Achievements exceeding the initial predictions have been\nreported from the results of learning vast amounts of textual data using deep\nlearning. For instance, four arithmetic operations could be performed without\nexplicit learning, thereby enabling the explanation of complex images and the\ngeneration of images from corresponding explanatory texts. It is an accurate\nexample of the learner embodying the concept of \"practice makes perfect\" by\nusing vast amounts of textual data. This report provides a technological\nexplanation of how cutting-edge NLP has made it possible to realize the\n\"practice makes perfect\" principle. Additionally, examples of how this can be\napplied to business are provided. We reported in June 2022 in Japanese on the\nNLP movement from late 2021 to early 2022. We would like to summarize this as a\nmemorandum since this is just the initial movement leading to the current large\nlanguage models (LLMs).\n",
                "链接": "https://arxiv.org/abs/2310.06228"
            },
            {
                "文章ID": "5542",
                "标题": "Integrating AI Planning with Natural Language Processing: A Combination\n  of Explicit and Tacit Knowledge",
                "作者": " Kebing Jin,  Hankz Hankui Zhuo",
                "发布日期": "2023-04-14",
                "摘要": "  Natural language processing (NLP) aims at investigating the interactions\nbetween agents and humans, processing and analyzing large amounts of natural\nlanguage data. Large-scale language models play an important role in current\nnatural language processing. However, the challenges of explainability and\ncomplexity come along with the developments of language models. One way is to\nintroduce logical relations and rules into natural language processing models,\nsuch as making use of Automated Planning. Automated planning (AI planning)\nfocuses on building symbolic domain models and synthesizing plans to transit\ninitial states to goals based on domain models. Recently, there have been\nplenty of works related to these two fields, which have the abilities to\ngenerate explicit knowledge, e.g., preconditions and effects of action models,\nand learn from tacit knowledge, e.g., neural models, respectively. Integrating\nAI planning and natural language processing effectively improves the\ncommunication between human and intelligent agents. This paper outlines the\ncommons and relations between AI planning and natural language processing,\nargues that each of them can effectively impact on the other one by five areas:\n(1) planning-based text understanding, (2) planning-based natural language\nprocessing, (3) planning-based explainability, (4) text-based human-robot\ninteraction, and (5) applications. We also explore some potential future issues\nbetween AI planning and natural language processing. To the best of our\nknowledge, this survey is the first work that addresses the deep connections\nbetween AI planning and Natural language processing.\n",
                "链接": "https://arxiv.org/abs/2202.07138"
            }
        ]
    },
    {
        "question": {
            "question": "查找近六个月工具学习评测数据集的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "108993",
                "标题": "A Comprehensive Evaluation of Tool-Assisted Generation Strategies",
                "作者": " Alon Jacovi,  Avi Caciularu,  Jonathan Herzig,  Roee Aharoni,  Bernd Bohnet,  Mor Geva",
                "发布日期": "2023-12-29",
                "摘要": "  A growing area of research investigates augmenting language models with tools\n(e.g., search engines, calculators) to overcome their shortcomings (e.g.,\nmissing or incorrect knowledge, incorrect logical inferences). Various few-shot\ntool-usage strategies have been proposed. However, there is no systematic and\nfair comparison across different strategies, or between these strategies and\nstrong baselines that do not leverage tools. We conduct an extensive empirical\nanalysis, finding that (1) across various datasets, example difficulty levels,\nand models, strong no-tool baselines are competitive to tool-assisted\nstrategies, implying that effectively using tools with in-context\ndemonstrations is a difficult unsolved problem; (2) for knowledge-retrieval\ntasks, strategies that *refine* incorrect outputs with tools outperform\nstrategies that retrieve relevant information *ahead of* or *during\ngeneration*; (3) tool-assisted strategies are expensive in the number of tokens\nthey require to work -- incurring additional costs by orders of magnitude --\nwhich does not translate into significant improvement in performance. Overall,\nour findings suggest that few-shot tool integration is still an open challenge,\nemphasizing the need for comprehensive evaluations of future strategies to\naccurately assess their *benefits* and *costs*.\n",
                "链接": "https://arxiv.org/abs/2310.10062"
            },
            {
                "文章ID": "98794",
                "标题": "Confucius: Iterative Tool Learning from Introspection Feedback by\n  Easy-to-Difficult Curriculum",
                "作者": " Shen Gao,  Zhengliang Shi,  Minghang Zhu,  Bowen Fang,  Xin Xin,  Pengjie Ren,  Zhumin Chen,  Jun Ma,  Zhaochun Ren",
                "发布日期": "2023-12-22",
                "摘要": "  Augmenting large language models (LLMs) with external tools has emerged as a\npromising approach to extending the capability of LLMs. Although some works\nemploy open-source LLMs for the tool learning task, most of them are trained in\na controlled environment in which LLMs only learn to execute the human-provided\ntools. However, selecting proper tools from the large toolset is also a crucial\nability for the tool learning model to be applied in real-world applications.\nExisting methods usually directly employ self-instruction methods to train the\nmodel, which ignores differences in tool complexity. In this paper, we propose\nthe Confucius, a novel tool learning framework to train LLM to use complicated\ntools in real-world scenarios, which contains two main phases: (1) We first\npropose a multi-stage learning method to teach the LLM to use various tools\nfrom an easy-to-difficult curriculum; (2) thenceforth, we propose the Iterative\nSelf-instruct from Introspective Feedback (ISIF) to dynamically construct the\ndataset to improve the ability to use the complicated tool. Extensive\nexperiments conducted on both controlled and real-world settings demonstrate\nthe superiority of our tool learning framework in the real-world application\nscenarios compared to both tuning-free (e.g. ChatGPT, Claude) and tuning-based\nbaselines (e.g. GPT4Tools).\n",
                "链接": "https://arxiv.org/abs/2308.14034"
            },
            {
                "文章ID": "117825",
                "标题": "MRxaI: Black-Box Explainability for Image Classifiers in a Medical\n  Setting",
                "作者": " Nathan Blake,  Hana Chockler,  David A. Kelly,  Santiago Calderon Pena,  Akchunya Chanchal",
                "发布日期": "2023-11-27",
                "摘要": "  Existing tools for explaining the output of image classifiers can be divided\ninto white-box, which rely on access to the model internals, and black-box,\nagnostic to the model. As the usage of AI in the medical domain grows, so too\ndoes the usage of explainability tools. Existing work on medical image\nexplanations focuses on white-box tools, such as gradcam. However, there are\nclear advantages to switching to a black-box tool, including the ability to use\nit with any classifier and the wide selection of black-box tools available. On\nstandard images, black-box tools are as precise as white-box. In this paper we\ncompare the performance of several black-box methods against gradcam on a brain\ncancer MRI dataset. We demonstrate that most black-box tools are not suitable\nfor explaining medical image classifications and present a detailed analysis of\nthe reasons for their shortcomings. We also show that one black-box tool, a\ncausal explainability-based rex, performs as well as \\gradcam.\n",
                "链接": "https://arxiv.org/abs/2311.14471"
            },
            {
                "文章ID": "79938",
                "标题": "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large\n  Language Models",
                "作者": " Zhipeng Chen,  Kun Zhou,  Beichen Zhang,  Zheng Gong,  Wayne Xin Zhao,  Ji-Rong Wen",
                "发布日期": "2023-11-07",
                "摘要": "  Although large language models (LLMs) have achieved excellent performance in\na variety of evaluation benchmarks, they still struggle in complex reasoning\ntasks which require specific knowledge and multi-hop reasoning. To improve the\nreasoning abilities, we propose ChatCoT, a tool-augmented chain-of-thought\nreasoning framework for chat-based LLMs (e.g., ChatGPT). In ChatCoT, we model\nthe chain-of-thought (CoT) reasoning as multi-turn conversations, to utilize\ntools in a more natural way through chatting. At each turn, LLMs can either\ninteract with tools or perform the reasoning. Our approach can effectively\nleverage the multi-turn conversation ability of chat-based LLMs, and integrate\nthe thought chain following and tools manipulation in a unified way. Specially,\nwe initialize the early turns of the conversation by the knowledge about tools,\ntasks, and reasoning format, and propose an iterative tool-augmented reasoning\nstep to perform step-by-step tool-augmented reasoning. The experiment results\non two complex reasoning datasets (MATH and HotpotQA) have shown the\neffectiveness of ChatCoT on complex reasoning tasks, achieving a 7.9% relative\nimprovement over the state-of-the-art baseline. Our code and data are available\nat: \\url{https://github.com/RUCAIBOX/ChatCoT}.\n",
                "链接": "https://arxiv.org/abs/2305.14323"
            },
            {
                "文章ID": "117142",
                "标题": "CSMeD: Bridging the Dataset Gap in Automated Citation Screening for\n  Systematic Literature Reviews",
                "作者": " Wojciech Kusa,  Oscar E. Mendoza,  Matthias Samwald,  Petr Knoth,  Allan Hanbury",
                "发布日期": "2023-11-22",
                "摘要": "  Systematic literature reviews (SLRs) play an essential role in summarising,\nsynthesising and validating scientific evidence. In recent years, there has\nbeen a growing interest in using machine learning techniques to automate the\nidentification of relevant studies for SLRs. However, the lack of standardised\nevaluation datasets makes comparing the performance of such automated\nliterature screening systems difficult. In this paper, we analyse the citation\nscreening evaluation datasets, revealing that many of the available datasets\nare either too small, suffer from data leakage or have limited applicability to\nsystems treating automated literature screening as a classification task, as\nopposed to, for example, a retrieval or question-answering task. To address\nthese challenges, we introduce CSMeD, a meta-dataset consolidating nine\npublicly released collections, providing unified access to 325 SLRs from the\nfields of medicine and computer science. CSMeD serves as a comprehensive\nresource for training and evaluating the performance of automated citation\nscreening models. Additionally, we introduce CSMeD-FT, a new dataset designed\nexplicitly for evaluating the full text publication screening task. To\ndemonstrate the utility of CSMeD, we conduct experiments and establish\nbaselines on new datasets.\n",
                "链接": "https://arxiv.org/abs/2311.12474"
            },
            {
                "文章ID": "124318",
                "标题": "T-Eval: Evaluating the Tool Utilization Capability Step by Step",
                "作者": " Zehui Chen,  Weihua Du,  Wenwei Zhang,  Kuikun Liu,  Jiangning Liu,  Miao Zheng,  Jingming Zhuo,  Songyang Zhang,  Dahua Lin,  Kai Chen,  Feng Zhao",
                "发布日期": "2023-12-22",
                "摘要": "  Large language models (LLM) have achieved remarkable performance on various\nNLP tasks and are augmented by tools for broader applications. Yet, how to\nevaluate and analyze the tool-utilization capability of LLMs is still\nunder-explored. In contrast to previous works that evaluate models\nholistically, we comprehensively decompose the tool utilization into multiple\nsub-processes, including instruction following, planning, reasoning, retrieval,\nunderstanding, and review. Based on that, we further introduce \\shortname~to\nevaluate the tool utilization capability step by step. \\shortname~disentangles\nthe tool utilization evaluation into several sub-domains along model\ncapabilities, facilitating the inner understanding of both holistic and\nisolated competency of LLMs. We conduct extensive experiments on \\shortname~and\nin-depth analysis of various LLMs. \\shortname~ not only exhibits consistency\nwith the outcome-oriented evaluation but also provides a more fine-grained\nanalysis of the capabilities of LLMs, providing a new perspective in LLM\nevaluation on tool-utilization ability. The benchmark will be available at\n\\href{https://github.com/open-compass/T-Eval}{https://github.com/open-compass/T-Eval}.\n",
                "链接": "https://arxiv.org/abs/2312.14033"
            },
            {
                "文章ID": "115006",
                "标题": "Knowledge-Based Support for Adhesive Selection: Will it Stick?",
                "作者": " Simon Vandevelde,  Jeroen Jordens,  Bart Van Doninck,  Maarten Witters,  Joost Vennekens",
                "发布日期": "2023-11-14",
                "摘要": "  As the popularity of adhesive joints in industry increases, so does the need\nfor tools to support the process of selecting a suitable adhesive. While some\nsuch tools already exist, they are either too limited in scope, or offer too\nlittle flexibility in use. This work presents a more advanced tool, that was\ndeveloped together with a team of adhesive experts. We first extract the\nexperts' knowledge about this domain and formalize it in a Knowledge Base (KB).\nThe IDP-Z3 reasoning system can then be used to derive the necessary\nfunctionality from this KB. Together with a user-friendly interactive\ninterface, this creates an easy-to-use tool capable of assisting the adhesive\nexperts. To validate our approach, we performed user testing in the form of\nqualitative interviews. The experts are very positive about the tool, stating\nthat, among others, it will help save time and find more suitable adhesives.\nUnder consideration in Theory and Practice of Logic Programming (TPLP).\n",
                "链接": "https://arxiv.org/abs/2311.06302"
            },
            {
                "文章ID": "119311",
                "标题": "TaskBench: Benchmarking Large Language Models for Task Automation",
                "作者": " Yongliang Shen,  Kaitao Song,  Xu Tan,  Wenqi Zhang,  Kan Ren,  Siyu Yuan,  Weiming Lu,  Dongsheng Li,  Yueting Zhuang",
                "发布日期": "2023-12-12",
                "摘要": "  Recently, the incredible progress of large language models (LLMs) has ignited\nthe spark of task automation, which decomposes the complex tasks described by\nuser instructions into sub-tasks, and invokes external tools to execute them,\nand plays a central role in autonomous agents. However, there lacks a\nsystematic and standardized benchmark to foster the development of LLMs in task\nautomation. To this end, we introduce TaskBench to evaluate the capability of\nLLMs in task automation. Specifically, task automation can be formulated into\nthree critical stages: task decomposition, tool invocation, and parameter\nprediction to fulfill user intent. This complexity makes data collection and\nevaluation more challenging compared to common NLP tasks. To generate\nhigh-quality evaluation datasets, we introduce the concept of Tool Graph to\nrepresent the decomposed tasks in user intent, and adopt a back-instruct method\nto simulate user instruction and annotations. Furthermore, we propose TaskEval\nto evaluate the capability of LLMs from different aspects, including task\ndecomposition, tool invocation, and parameter prediction. Experimental results\ndemonstrate that TaskBench can effectively reflects the capability of LLMs in\ntask automation. Benefiting from the mixture of automated data construction and\nhuman verification, TaskBench achieves a high consistency compared to the human\nevaluation, which can be utilized as a comprehensive and faithful benchmark for\nLLM-based autonomous agents.\n",
                "链接": "https://arxiv.org/abs/2311.18760"
            },
            {
                "文章ID": "117724",
                "标题": "When is Off-Policy Evaluation Useful? A Data-Centric Perspective",
                "作者": " Hao Sun,  Alex J. Chan,  Nabeel Seedat,  Alihan Hüyük,  Mihaela van der Schaar",
                "发布日期": "2023-11-27",
                "摘要": "  Evaluating the value of a hypothetical target policy with only a logged\ndataset is important but challenging. On the one hand, it brings opportunities\nfor safe policy improvement under high-stakes scenarios like clinical\nguidelines. On the other hand, such opportunities raise a need for precise\noff-policy evaluation (OPE). While previous work on OPE focused on improving\nthe algorithm in value estimation, in this work, we emphasize the importance of\nthe offline dataset, hence putting forward a data-centric framework for\nevaluating OPE problems. We propose DataCOPE, a data-centric framework for\nevaluating OPE, that answers the questions of whether and to what extent we can\nevaluate a target policy given a dataset. DataCOPE (1) forecasts the overall\nperformance of OPE algorithms without access to the environment, which is\nespecially useful before real-world deployment where evaluating OPE is\nimpossible; (2) identifies the sub-group in the dataset where OPE can be\ninaccurate; (3) permits evaluations of datasets or data-collection strategies\nfor OPE problems. Our empirical analysis of DataCOPE in the logged contextual\nbandit settings using healthcare datasets confirms its ability to evaluate both\nmachine-learning and human expert policies like clinical guidelines.\n",
                "链接": "https://arxiv.org/abs/2311.14110"
            },
            {
                "文章ID": "122009",
                "标题": "CholecTrack20: A Dataset for Multi-Class Multiple Tool Tracking in\n  Laparoscopic Surgery",
                "作者": " Chinedu Innocent Nwoye,  Kareem Elgohary,  Anvita Srinivas,  Fauzan Zaid,  Joël L. Lavanchy,  Nicolas Padoy",
                "发布日期": "2023-12-13",
                "摘要": "  Tool tracking in surgical videos is vital in computer-assisted intervention\nfor tasks like surgeon skill assessment, safety zone estimation, and\nhuman-machine collaboration during minimally invasive procedures. The lack of\nlarge-scale datasets hampers Artificial Intelligence implementation in this\ndomain. Current datasets exhibit overly generic tracking formalization, often\nlacking surgical context: a deficiency that becomes evident when tools move out\nof the camera's scope, resulting in rigid trajectories that hinder realistic\nsurgical representation. This paper addresses the need for a more precise and\nadaptable tracking formalization tailored to the intricacies of endoscopic\nprocedures by introducing CholecTrack20, an extensive dataset meticulously\nannotated for multi-class multi-tool tracking across three perspectives\nrepresenting the various ways of considering the temporal duration of a tool\ntrajectory: (1) intraoperative, (2) intracorporeal, and (3) visibility within\nthe camera's scope. The dataset comprises 20 laparoscopic videos with over\n35,000 frames and 65,000 annotated tool instances with details on spatial\nlocation, category, identity, operator, phase, and surgical visual conditions.\nThis detailed dataset caters to the evolving assistive requirements within a\nprocedure.\n",
                "链接": "https://arxiv.org/abs/2312.07352"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下工具评测相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "81377",
                "标题": "Counterfactual Evaluation of Peer-Review Assignment Policies",
                "作者": " Martin Saveski,  Steven Jecmen,  Nihar B. Shah,  Johan Ugander",
                "发布日期": "2023-05-30",
                "摘要": "  Peer review assignment algorithms aim to match research papers to suitable\nexpert reviewers, working to maximize the quality of the resulting reviews. A\nkey challenge in designing effective assignment policies is evaluating how\nchanges to the assignment algorithm map to changes in review quality. In this\nwork, we leverage recently proposed policies that introduce randomness in\npeer-review assignment--in order to mitigate fraud--as a valuable opportunity\nto evaluate counterfactual assignment policies. Specifically, we exploit how\nsuch randomized assignments provide a positive probability of observing the\nreviews of many assignment policies of interest. To address challenges in\napplying standard off-policy evaluation methods, such as violations of\npositivity, we introduce novel methods for partial identification based on\nmonotonicity and Lipschitz smoothness assumptions for the mapping between\nreviewer-paper covariates and outcomes. We apply our methods to peer-review\ndata from two computer science venues: the TPDP'21 workshop (95 papers and 35\nreviewers) and the AAAI'22 conference (8,450 papers and 3,145 reviewers). We\nconsider estimates of (i) the effect on review quality when changing weights in\nthe assignment algorithm, e.g., weighting reviewers' bids vs. textual\nsimilarity (between the review's past papers and the submission), and (ii) the\n\"cost of randomization\", capturing the difference in expected quality between\nthe perturbed and unperturbed optimal match. We find that placing higher weight\non text similarity results in higher review quality and that introducing\nrandomization in the reviewer-paper assignment only marginally reduces the\nreview quality. Our methods for partial identification may be of independent\ninterest, while our off-policy approach can likely find use evaluating a broad\nclass of algorithmic matching systems.\n",
                "链接": "https://arxiv.org/abs/2305.17339"
            },
            {
                "文章ID": "90453",
                "标题": "Testing for Reviewer Anchoring in Peer Review: A Randomized Controlled\n  Trial",
                "作者": " Ryan Liu,  Steven Jecmen,  Vincent Conitzer,  Fei Fang,  Nihar B. Shah",
                "发布日期": "2023-07-12",
                "摘要": "  Peer review frequently follows a process where reviewers first provide\ninitial reviews, authors respond to these reviews, then reviewers update their\nreviews based on the authors' response. There is mixed evidence regarding\nwhether this process is useful, including frequent anecdotal complaints that\nreviewers insufficiently update their scores. In this study, we aim to\ninvestigate whether reviewers anchor to their original scores when updating\ntheir reviews, which serves as a potential explanation for the lack of updates\nin reviewer scores.\n  We design a novel randomized controlled trial to test if reviewers exhibit\nanchoring. In the experimental condition, participants initially see a flawed\nversion of a paper that is later corrected, while in the control condition,\nparticipants only see the correct version. We take various measures to ensure\nthat in the absence of anchoring, reviewers in the experimental group should\nrevise their scores to be identically distributed to the scores from the\ncontrol group. Furthermore, we construct the reviewed paper to maximize the\ndifference between the flawed and corrected versions, and employ deception to\nhide the true experiment purpose.\n  Our randomized controlled trial consists of 108 researchers as participants.\nFirst, we find that our intervention was successful at creating a difference in\nperceived paper quality between the flawed and corrected versions: Using a\npermutation test with the Mann-Whitney U statistic, we find that the\nexperimental group's initial scores are lower than the control group's scores\nin both the Evaluation category (Vargha-Delaney A=0.64, p=0.0096) and Overall\nscore (A=0.59, p=0.058). Next, we test for anchoring by comparing the\nexperimental group's revised scores with the control group's scores. We find no\nsignificant evidence of anchoring in either the Overall (A=0.50, p=0.61) or\nEvaluation category (A=0.49, p=0.61).\n",
                "链接": "https://arxiv.org/abs/2307.05443"
            },
            {
                "文章ID": "124318",
                "标题": "T-Eval: Evaluating the Tool Utilization Capability Step by Step",
                "作者": " Zehui Chen,  Weihua Du,  Wenwei Zhang,  Kuikun Liu,  Jiangning Liu,  Miao Zheng,  Jingming Zhuo,  Songyang Zhang,  Dahua Lin,  Kai Chen,  Feng Zhao",
                "发布日期": "2023-12-22",
                "摘要": "  Large language models (LLM) have achieved remarkable performance on various\nNLP tasks and are augmented by tools for broader applications. Yet, how to\nevaluate and analyze the tool-utilization capability of LLMs is still\nunder-explored. In contrast to previous works that evaluate models\nholistically, we comprehensively decompose the tool utilization into multiple\nsub-processes, including instruction following, planning, reasoning, retrieval,\nunderstanding, and review. Based on that, we further introduce \\shortname~to\nevaluate the tool utilization capability step by step. \\shortname~disentangles\nthe tool utilization evaluation into several sub-domains along model\ncapabilities, facilitating the inner understanding of both holistic and\nisolated competency of LLMs. We conduct extensive experiments on \\shortname~and\nin-depth analysis of various LLMs. \\shortname~ not only exhibits consistency\nwith the outcome-oriented evaluation but also provides a more fine-grained\nanalysis of the capabilities of LLMs, providing a new perspective in LLM\nevaluation on tool-utilization ability. The benchmark will be available at\n\\href{https://github.com/open-compass/T-Eval}{https://github.com/open-compass/T-Eval}.\n",
                "链接": "https://arxiv.org/abs/2312.14033"
            },
            {
                "文章ID": "60466",
                "标题": "Can ChatGPT Write a Good Boolean Query for Systematic Review Literature\n  Search?",
                "作者": " Shuai Wang,  Harrisen Scells,  Bevan Koopman,  Guido Zuccon",
                "发布日期": "2023-02-10",
                "摘要": "  Systematic reviews are comprehensive reviews of the literature for a highly\nfocused research question. These reviews are often treated as the highest form\nof evidence in evidence-based medicine, and are the key strategy to answer\nresearch questions in the medical field. To create a high-quality systematic\nreview, complex Boolean queries are often constructed to retrieve studies for\nthe review topic. However, it often takes a long time for systematic review\nresearchers to construct a high quality systematic review Boolean query, and\noften the resulting queries are far from effective. Poor queries may lead to\nbiased or invalid reviews, because they missed to retrieve key evidence, or to\nextensive increase in review costs, because they retrieved too many irrelevant\nstudies. Recent advances in Transformer-based generative models have shown\ngreat potential to effectively follow instructions from users and generate\nanswers based on the instructions being made. In this paper, we investigate the\neffectiveness of the latest of such models, ChatGPT, in generating effective\nBoolean queries for systematic review literature search. Through a number of\nextensive experiments on standard test collections for the task, we find that\nChatGPT is capable of generating queries that lead to high search precision,\nalthough trading-off this for recall. Overall, our study demonstrates the\npotential of ChatGPT in generating effective Boolean queries for systematic\nreview literature search. The ability of ChatGPT to follow complex instructions\nand generate queries with high precision makes it a valuable tool for\nresearchers conducting systematic reviews, particularly for rapid reviews where\ntime is a constraint and often trading-off higher precision for lower recall is\nacceptable.\n",
                "链接": "https://arxiv.org/abs/2302.03495"
            },
            {
                "文章ID": "72776",
                "标题": "Tool Learning with Foundation Models",
                "作者": " Yujia Qin,  Shengding Hu,  Yankai Lin,  Weize Chen,  Ning Ding,  Ganqu Cui,  Zheni Zeng,  Yufei Huang,  Chaojun Xiao,  Chi Han,  Yi Ren Fung,  Yusheng Su,  Huadong Wang,  Cheng Qian,  Runchu Tian,  Kunlun Zhu,  Shihao Liang,  Xingyu Shen,  Bokai Xu,  Zhen Zhang,  Yining Ye,  Bowen Li,  Ziwei Tang,  Jing Yi,  Yuzhang Zhu,  Zhenning Dai,  Lan Yan,  Xin Cong,  Yaxi Lu,  Weilin Zhao,  Yuxiang Huang,  Junxi Yan,  Xu Han,  Xian Sun,  Dahai Li,  Jason Phang,  Cheng Yang,  Tongshuang Wu,  Heng Ji,  Zhiyuan Liu,  Maosong Sun",
                "发布日期": "2023-06-16",
                "摘要": "  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. Overall, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n",
                "链接": "https://arxiv.org/abs/2304.08354"
            },
            {
                "文章ID": "88345",
                "标题": "More efficient manual review of automatically transcribed tabular data",
                "作者": " Bjørn-Richard Pedersen,  Rigmor Katrine Johansen,  Einar Holsbø,  Hilde Sommerseth,  Lars Ailo Bongo",
                "发布日期": "2023-06-29",
                "摘要": "  Machine learning methods have proven useful in transcribing historical data.\nHowever, results from even highly accurate methods require manual verification\nand correction. Such manual review can be time-consuming and expensive,\ntherefore the objective of this paper was to make it more efficient.\nPreviously, we used machine learning to transcribe 2.3 million handwritten\noccupation codes from the Norwegian 1950 census with high accuracy (97%). We\nmanually reviewed the 90,000 (3%) codes with the lowest model confidence. We\nallocated those 90,000 codes to human reviewers, who used our annotation tool\nto review the codes. To assess reviewer agreement, some codes were assigned to\nmultiple reviewers. We then analyzed the review results to understand the\nrelationship between accuracy improvements and effort. Additionally, we\ninterviewed the reviewers to improve the workflow. The reviewers corrected\n62.8% of the labels and agreed with the model label in 31.9% of cases. About\n0.2% of the images could not be assigned a label, while for 5.1% the reviewers\nwere uncertain, or they assigned an invalid label. 9,000 images were\nindependently reviewed by multiple reviewers, resulting in an agreement of\n86.43% and disagreement of 8.96%. We learned that our automatic transcription\nis biased towards the most frequent codes, with a higher degree of\nmisclassification for the lowest frequency codes. Our interview findings show\nthat the reviewers did internal quality control and found our custom tool\nwell-suited. So, only one reviewer is needed, but they should report\nuncertainty.\n",
                "链接": "https://arxiv.org/abs/2306.16126"
            },
            {
                "文章ID": "53578",
                "标题": "How Researchers Could Obtain Quick and Cheap User Feedback on their\n  Algorithms Without Having to Operate their Own Recommender System",
                "作者": " Tobias Eichinger,  Ananta Lamichhane",
                "发布日期": "2022-12-15",
                "摘要": "  The majority of recommendation algorithms are evaluated on the basis of\nhistoric benchmark datasets. Evaluation on historic benchmark datasets is quick\nand cheap to conduct, yet excludes the viewpoint of users who actually consume\nrecommendations. User feedback is seldom collected, since it requires access to\nan operational recommender system. Establishing and maintaining an operational\nrecommender system imposes a timely and financial burden that a majority of\nresearchers cannot shoulder. We aim to reduce this burden in order to promote\nwidespread user-centric evaluations of recommendation algorithms, in particular\nfor novice researchers in the field. We present work in progress on an\nevaluation tool that implements a novel paradigm that enables user-centric\nevaluations of recommendation algorithms without access to an operational\nrecommender system. Finally, we sketch the experiments we plan to conduct with\nthe help of the evaluation tool.\n",
                "链接": "https://arxiv.org/abs/2212.07177"
            },
            {
                "文章ID": "95678",
                "标题": "A Unified Interactive Model Evaluation for Classification, Object\n  Detection, and Instance Segmentation in Computer Vision",
                "作者": " Changjian Chen,  Yukai Guo,  Fengyuan Tian,  Shilong Liu,  Weikai Yang,  Zhaowei Wang,  Jing Wu,  Hang Su,  Hanspeter Pfister,  Shixia Liu",
                "发布日期": "2023-08-11",
                "摘要": "  Existing model evaluation tools mainly focus on evaluating classification\nmodels, leaving a gap in evaluating more complex models, such as object\ndetection. In this paper, we develop an open-source visual analysis tool,\nUni-Evaluator, to support a unified model evaluation for classification, object\ndetection, and instance segmentation in computer vision. The key idea behind\nour method is to formulate both discrete and continuous predictions in\ndifferent tasks as unified probability distributions. Based on these\ndistributions, we develop 1) a matrix-based visualization to provide an\noverview of model performance; 2) a table visualization to identify the\nproblematic data subsets where the model performs poorly; 3) a grid\nvisualization to display the samples of interest. These visualizations work\ntogether to facilitate the model evaluation from a global overview to\nindividual samples. Two case studies demonstrate the effectiveness of\nUni-Evaluator in evaluating model performance and making informed improvements.\n",
                "链接": "https://arxiv.org/abs/2308.05168"
            },
            {
                "文章ID": "70216",
                "标题": "Reviewer Assignment Problem: A Systematic Review of the Literature",
                "作者": " Meltem Aksoy,  Seda Yanik,  Mehmet Fatih Amasyali",
                "发布日期": "2023-04-05",
                "摘要": "  Appropriate reviewer assignment significantly impacts the quality of proposal\nevaluation, as accurate and fair reviews are contingent on their assignment to\nrelevant reviewers. The crucial task of assigning reviewers to submitted\nproposals is the starting point of the review process and is also known as the\nreviewer assignment problem (RAP). Due to the obvious restrictions of manual\nassignment, journal editors, conference organizers, and grant managers demand\nautomatic reviewer assignment approaches. Many studies have proposed assignment\nsolutions in response to the demand for automated procedures since 1992. The\nprimary objective of this survey paper is to provide scholars and practitioners\nwith a comprehensive overview of available research on the RAP. To achieve this\ngoal, this article presents an in-depth systematic review of 103 publications\nin the field of reviewer assignment published in the past three decades and\navailable in the Web of Science, Scopus, ScienceDirect, Google Scholar, and\nSemantic Scholar databases. This review paper classified and discussed the RAP\napproaches into two broad categories and numerous subcategories based on their\nunderlying techniques. Furthermore, potential future research directions for\neach category are presented. This survey shows that the research on the RAP is\nbecoming more significant and that more effort is required to develop new\napproaches and a framework.\n",
                "链接": "https://arxiv.org/abs/2304.00353"
            },
            {
                "文章ID": "71217",
                "标题": "Hierarchical Catalogue Generation for Literature Review: A Benchmark",
                "作者": " Kun Zhu,  Xiaocheng Feng,  Xiachong Feng,  Yingsheng Wu,  Bing Qin",
                "发布日期": "2023-11-20",
                "摘要": "  Scientific literature review generation aims to extract and organize\nimportant information from an abundant collection of reference papers and\nproduces corresponding reviews while lacking a clear and logical hierarchy. We\nobserve that a high-quality catalogue-guided generation process can effectively\nalleviate this problem. Therefore, we present an atomic and challenging task\nnamed Hierarchical Catalogue Generation for Literature Review as the first step\nfor review generation, which aims to produce a hierarchical catalogue of a\nreview paper given various references. We construct a novel English\nHierarchical Catalogues of Literature Reviews Dataset with 7.6k literature\nreview catalogues and 389k reference papers. To accurately assess the model\nperformance, we design two evaluation metrics for informativeness and\nsimilarity to ground truth from semantics and structure.Our extensive analyses\nverify the high quality of our dataset and the effectiveness of our evaluation\nmetrics. We further benchmark diverse experiments on state-of-the-art\nsummarization models like BART and large language models like ChatGPT to\nevaluate their capabilities. We further discuss potential directions for this\ntask to motivate future research.\n",
                "链接": "https://arxiv.org/abs/2304.03512"
            }
        ]
    },
    {
        "question": {
            "question": "对比解码相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "115879",
                "标题": "Speculative Contrastive Decoding",
                "作者": " Hongyi Yuan,  Keming Lu,  Fei Huang,  Zheng Yuan,  Chang Zhou",
                "发布日期": "2023-11-16",
                "摘要": "  Large language models (LLMs) have shown extraordinary performance in various\nlanguage tasks, but high computational requirements hinder their widespread\ndeployment. Speculative decoding, which uses amateur models to predict the\ngeneration of expert models, has been proposed as a way to accelerate LLM\ninference. However, speculative decoding focuses on acceleration instead of\nmaking the best use of the token distribution from amateur models. We proposed\nSpeculative Contrastive Decoding (SCD), an accelerated decoding method\nleveraging the natural contrast between expert and amateur models in\nspeculative decoding. Comprehensive evaluations on four benchmarks show that\nSCD can achieve similar acceleration factors as speculative decoding while\nfurther improving the generation quality as the contrastive decoding. The\nanalysis of token probabilities further demonstrates the compatibility between\nspeculative and contrastive decoding. Overall, SCD provides an effective\napproach to enhance the decoding quality of LLMs while saving computational\nresources.\n",
                "链接": "https://arxiv.org/abs/2311.08981"
            },
            {
                "文章ID": "17492",
                "标题": "Quality-Aware Decoding for Neural Machine Translation",
                "作者": " Patrick Fernandes,  António Farinhas,  Ricardo Rei,  José G. C. de Souza,  Perez Ogayo,  Graham Neubig,  André F. T. Martins",
                "发布日期": "2022-05-03",
                "摘要": "  Despite the progress in machine translation quality estimation and evaluation\nin the last years, decoding in neural machine translation (NMT) is mostly\noblivious to this and centers around finding the most probable translation\naccording to the model (MAP decoding), approximated with beam search. In this\npaper, we bring together these two lines of research and propose quality-aware\ndecoding for NMT, by leveraging recent breakthroughs in reference-free and\nreference-based MT evaluation through various inference methods like $N$-best\nreranking and minimum Bayes risk decoding. We perform an extensive comparison\nof various possible candidate generation and ranking methods across four\ndatasets and two model classes and find that quality-aware decoding\nconsistently outperforms MAP-based decoding according both to state-of-the-art\nautomatic metrics (COMET and BLEURT) and to human assessments. Our code is\navailable at https://github.com/deep-spin/qaware-decode.\n",
                "链接": "https://arxiv.org/abs/2205.00978"
            },
            {
                "文章ID": "13791",
                "标题": "Knowledge Infused Decoding",
                "作者": " Ruibo Liu,  Guoqing Zheng,  Shashank Gupta,  Radhika Gaonkar,  Chongyang Gao,  Soroush Vosoughi,  Milad Shokouhi,  Ahmed Hassan Awadallah",
                "发布日期": "2022-04-08",
                "摘要": "  Pre-trained language models (LMs) have been shown to memorize a substantial\namount of knowledge from the pre-training corpora; however, they are still\nlimited in recalling factually correct knowledge given a certain context.\nHence, they tend to suffer from counterfactual or hallucinatory generation when\nused in knowledge-intensive natural language generation (NLG) tasks. Recent\nremedies to this problem focus on modifying either the pre-training or task\nfine-tuning objectives to incorporate knowledge, which normally require\nadditional costly training or architecture modification of LMs for practical\napplications. We present Knowledge Infused Decoding (KID) -- a novel decoding\nalgorithm for generative LMs, which dynamically infuses external knowledge into\neach step of the LM decoding. Specifically, we maintain a local knowledge\nmemory based on the current context, interacting with a dynamically created\nexternal knowledge trie, and continuously update the local memory as a\nknowledge-aware constraint to guide decoding via reinforcement learning. On six\ndiverse knowledge-intensive NLG tasks, task-agnostic LMs (e.g., GPT-2 and BART)\narmed with KID outperform many task-optimized state-of-the-art models, and show\nparticularly strong performance in few-shot scenarios over seven related\nknowledge-infusion techniques. Human evaluation confirms KID's ability to\ngenerate more relevant and factual language for the input context when compared\nwith multiple baselines. Finally, KID also alleviates exposure bias and\nprovides stable generation quality when generating longer sequences. Code for\nKID is available at https://github.com/microsoft/KID.\n",
                "链接": "https://arxiv.org/abs/2204.03084"
            },
            {
                "文章ID": "90598",
                "标题": "Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM\n  Decoding",
                "作者": " Seongjun Yang,  Gibbeum Lee,  Jaewoong Cho,  Dimitris Papailiopoulos,  Kangwook Lee",
                "发布日期": "2023-07-13",
                "摘要": "  This paper presents \"Predictive Pipelined Decoding (PPD),\" an approach that\nspeeds up greedy decoding in Large Language Models (LLMs) while maintaining the\nexact same output as the original decoding. Unlike conventional strategies, PPD\nemploys additional compute resources to parallelize the initiation of\nsubsequent token decoding during the current token decoding. This innovative\nmethod reduces decoding latency and reshapes the understanding of trade-offs in\nLLM decoding strategies. We have developed a theoretical framework that allows\nus to analyze the trade-off between computation and latency. Using this\nframework, we can analytically estimate the potential reduction in latency\nassociated with our proposed method, achieved through the assessment of the\nmatch rate, represented as p_correct. The results demonstrate that the use of\nextra computational resources has the potential to accelerate LLM greedy\ndecoding.\n",
                "链接": "https://arxiv.org/abs/2307.05908"
            },
            {
                "文章ID": "107928",
                "标题": "Online Speculative Decoding",
                "作者": " Xiaoxuan Liu,  Lanxiang Hu,  Peter Bailis,  Ion Stoica,  Zhijie Deng,  Alvin Cheung,  Hao Zhang",
                "发布日期": "2023-10-19",
                "摘要": "  Speculative decoding is a pivotal technique to accelerate the inference of\nlarge language models (LLMs) by employing a smaller draft model to predict the\ntarget model's outputs. However, its efficacy can be limited due to the low\npredictive accuracy of the draft model, particularly when faced with diverse\ntext inputs and a significant capability gap between the draft and target\nmodels. We introduce online speculative decoding (OSD) to address this\nchallenge. The main idea is to continually update (multiple) draft model(s) on\nobserved user query data using the abundant excess computational power in an\nLLM serving cluster. Given that LLM inference is memory-bounded, the surplus\ncomputational power in a typical LLM serving cluster can be repurposed for\nonline retraining of draft models, thereby making the training cost-neutral.\nSince the query distribution of an LLM service is relatively simple, retraining\non query distribution enables the draft model to more accurately predict the\ntarget model's outputs, particularly on data originating from query\ndistributions. As the draft model evolves online, it aligns with the query\ndistribution in real time, mitigating distribution shifts. We develop a\nprototype of online speculative decoding based on online knowledge distillation\nand evaluate it using both synthetic and real query data on several popular\nLLMs. The results show a substantial increase in the token acceptance rate by\n0.1 to 0.65, which translates into 1.22x to 3.06x latency reduction.\n",
                "链接": "https://arxiv.org/abs/2310.07177"
            },
            {
                "文章ID": "110770",
                "标题": "Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time\n  Controllable Text Generation",
                "作者": " Tianqi Zhong,  Quan Wang,  Jingxuan Han,  Yongdong Zhang,  Zhendong Mao",
                "发布日期": "2023-11-03",
                "摘要": "  Controllable text generation (CTG) aims to generate text with desired\nattributes, and decoding-time-based methods have shown promising performance on\nthis task. However, in this paper, we identify the phenomenon of Attribute\nCollapse for the first time. It causes the fluency of generated text to rapidly\ndecrease when the control strength exceeds a critical value, rendering the text\ncompletely unusable. This limitation hinders the effectiveness of decoding\nmethods in achieving high levels of controllability. To address this problem,\nwe propose a novel lightweight decoding framework named Air-Decoding. Its main\nidea is reconstructing the attribute distributions to balance the weights\nbetween attribute words and non-attribute words to generate more fluent text.\nSpecifically, we train prefixes by prefix-tuning to obtain attribute\ndistributions. Then we design a novel attribute distribution reconstruction\nmethod to balance the obtained distributions and use the reconstructed\ndistributions to guide language models for generation, effectively avoiding the\nissue of Attribute Collapse. Experiments on multiple CTG tasks prove that our\nmethod achieves a new state-of-the-art control performance.\n",
                "链接": "https://arxiv.org/abs/2310.14892"
            },
            {
                "文章ID": "40637",
                "标题": "Nonparametric Decoding for Generative Retrieval",
                "作者": " Hyunji Lee,  Jaeyoung Kim,  Hoyeon Chang,  Hanseok Oh,  Sohee Yang,  Vlad Karpukhin,  Yi Lu,  Minjoon Seo",
                "发布日期": "2023-05-30",
                "摘要": "  The generative retrieval model depends solely on the information encoded in\nits model parameters without external memory, its information capacity is\nlimited and fixed. To overcome the limitation, we propose Nonparametric\nDecoding (Np Decoding) which can be applied to existing generative retrieval\nmodels. Np Decoding uses nonparametric contextualized vocab embeddings\n(external memory) rather than vanilla vocab embeddings as decoder vocab\nembeddings. By leveraging the contextualized vocab embeddings, the generative\nretrieval model is able to utilize both the parametric and nonparametric space.\nEvaluation over 9 datasets (8 single-hop and 1 multi-hop) in the document\nretrieval task shows that applying Np Decoding to generative retrieval models\nsignificantly improves the performance. We also show that Np Decoding is data-\nand parameter-efficient, and shows high performance in the zero-shot setting.\n",
                "链接": "https://arxiv.org/abs/2210.02068"
            },
            {
                "文章ID": "15783",
                "标题": "Cross-view Brain Decoding",
                "作者": " Subba Reddy Oota,  Jashn Arora,  Manish Gupta,  Raju S. Bapi",
                "发布日期": "2022-04-21",
                "摘要": "  How the brain captures the meaning of linguistic stimuli across multiple\nviews is still a critical open question in neuroscience. Consider three\ndifferent views of the concept apartment: (1) picture (WP) presented with the\ntarget word label, (2) sentence (S) using the target word, and (3) word cloud\n(WC) containing the target word along with other semantically related words.\nUnlike previous efforts, which focus only on single view analysis, in this\npaper, we study the effectiveness of brain decoding in a zero-shot cross-view\nlearning setup. Further, we propose brain decoding in the novel context of\ncross-view-translation tasks like image captioning (IC), image tagging (IT),\nkeyword extraction (KE), and sentence formation (SF). Using extensive\nexperiments, we demonstrate that cross-view zero-shot brain decoding is\npractical leading to ~0.68 average pairwise accuracy across view pairs. Also,\nthe decoded representations are sufficiently detailed to enable high accuracy\nfor cross-view-translation tasks with following pairwise accuracy: IC (78.0),\nIT (83.0), KE (83.7) and SF (74.5). Analysis of the contribution of different\nbrain networks reveals exciting cognitive insights: (1) A high percentage of\nvisual voxels are involved in image captioning and image tagging tasks, and a\nhigh percentage of language voxels are involved in the sentence formation and\nkeyword extraction tasks. (2) Zero-shot accuracy of the model trained on S view\nand tested on WC view is better than same-view accuracy of the model trained\nand tested on WC view.\n",
                "链接": "https://arxiv.org/abs/2204.09564"
            },
            {
                "文章ID": "14514",
                "标题": "Beam Decoding with Controlled Patience",
                "作者": " Jungo Kasai,  Keisuke Sakaguchi,  Ronan Le Bras,  Dragomir Radev,  Yejin Choi,  Noah A. Smith",
                "发布日期": "2022-05-02",
                "摘要": "  Text generation with beam search has proven successful in a wide range of\napplications. The commonly-used implementation of beam decoding follows a first\ncome, first served heuristic: it keeps a set of already completed sequences\nover time steps and stops when the size of this set reaches the beam size. We\nintroduce a patience factor, a simple modification to this decoding algorithm,\nthat generalizes the stopping criterion and provides flexibility to the depth\nof search. Extensive empirical results demonstrate that the patience factor\nimproves decoding performance of strong pretrained models on news text\nsummarization and machine translation over diverse language pairs, with a\nnegligible inference slowdown. Our approach only modifies one line of code and\ncan be thus readily incorporated in any implementation.\n",
                "链接": "https://arxiv.org/abs/2204.05424"
            },
            {
                "文章ID": "31513",
                "标题": "Graph Neural Networks for Channel Decoding",
                "作者": " Sebastian Cammerer,  Jakob Hoydis,  Fayçal Aït Aoudia,  Alexander Keller",
                "发布日期": "2022-10-13",
                "摘要": "  In this work, we propose a fully differentiable graph neural network\n(GNN)-based architecture for channel decoding and showcase a competitive\ndecoding performance for various coding schemes, such as low-density\nparity-check (LDPC) and BCH codes. The idea is to let a neural network (NN)\nlearn a generalized message passing algorithm over a given graph that\nrepresents the forward error correction (FEC) code structure by replacing node\nand edge message updates with trainable functions. Contrary to many other deep\nlearning-based decoding approaches, the proposed solution enjoys scalability to\narbitrary block lengths and the training is not limited by the curse of\ndimensionality. We benchmark our proposed decoder against state-of-the-art in\nconventional channel decoding as well as against recent deep learning-based\nresults. For the (63,45) BCH code, our solution outperforms weighted belief\npropagation (BP) decoding by approximately 0.4 dB with significantly less\ndecoding iterations and even for 5G NR LDPC codes, we observe a competitive\nperformance when compared to conventional BP decoding. For the BCH codes, the\nresulting GNN decoder can be fully parametrized with only 9640 weights.\n",
                "链接": "https://arxiv.org/abs/2207.14742"
            }
        ]
    },
    {
        "question": {
            "question": "请找到缓和噪声标签影响的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "79671",
                "标题": "Mitigating Label Noise through Data Ambiguation",
                "作者": " Julian Lienen,  Eyke Hüllermeier",
                "发布日期": "2023-05-24",
                "摘要": "  Label noise poses an important challenge in machine learning, especially in\ndeep learning, in which large models with high expressive power dominate the\nfield. Models of that kind are prone to memorizing incorrect labels, thereby\nharming generalization performance. Many methods have been proposed to address\nthis problem, including robust loss functions and more complex label correction\napproaches. Robust loss functions are appealing due to their simplicity, but\ntypically lack flexibility, while label correction usually adds substantial\ncomplexity to the training setup. In this paper, we suggest to address the\nshortcomings of both methodologies by \"ambiguating\" the target information,\nadding additional, complementary candidate labels in case the learner is not\nsufficiently convinced of the observed training label. More precisely, we\nleverage the framework of so-called superset learning to construct set-valued\ntargets based on a confidence threshold, which deliver imprecise yet more\nreliable beliefs about the ground-truth, effectively helping the learner to\nsuppress the memorization effect. In an extensive empirical evaluation, our\nmethod demonstrates favorable learning behavior on synthetic and real-world\nnoise, confirming the effectiveness in detecting and correcting erroneous\ntraining labels.\n",
                "链接": "https://arxiv.org/abs/2305.13764"
            },
            {
                "文章ID": "52026",
                "标题": "CrossSplit: Mitigating Label Noise Memorization through Data Splitting",
                "作者": " Jihye Kim,  Aristide Baratin,  Yan Zhang,  Simon Lacoste-Julien",
                "发布日期": "2023-04-27",
                "摘要": "  We approach the problem of improving robustness of deep learning algorithms\nin the presence of label noise. Building upon existing label correction and\nco-teaching methods, we propose a novel training procedure to mitigate the\nmemorization of noisy labels, called CrossSplit, which uses a pair of neural\nnetworks trained on two disjoint parts of the labelled dataset. CrossSplit\ncombines two main ingredients: (i) Cross-split label correction. The idea is\nthat, since the model trained on one part of the data cannot memorize\nexample-label pairs from the other part, the training labels presented to each\nnetwork can be smoothly adjusted by using the predictions of its peer network;\n(ii) Cross-split semi-supervised training. A network trained on one part of the\ndata also uses the unlabeled inputs of the other part. Extensive experiments on\nCIFAR-10, CIFAR-100, Tiny-ImageNet and mini-WebVision datasets demonstrate that\nour method can outperform the current state-of-the-art in a wide range of noise\nratios.\n",
                "链接": "https://arxiv.org/abs/2212.01674"
            },
            {
                "文章ID": "48397",
                "标题": "Quantifying the Impact of Label Noise on Federated Learning",
                "作者": " Shuqi Ke,  Chao Huang,  Xin Liu",
                "发布日期": "2023-04-04",
                "摘要": "  Federated Learning (FL) is a distributed machine learning paradigm where\nclients collaboratively train a model using their local (human-generated)\ndatasets. While existing studies focus on FL algorithm development to tackle\ndata heterogeneity across clients, the important issue of data quality (e.g.,\nlabel noise) in FL is overlooked. This paper aims to fill this gap by providing\na quantitative study on the impact of label noise on FL. We derive an upper\nbound for the generalization error that is linear in the clients' label noise\nlevel. Then we conduct experiments on MNIST and CIFAR-10 datasets using various\nFL algorithms. Our empirical results show that the global model accuracy\nlinearly decreases as the noise level increases, which is consistent with our\ntheoretical analysis. We further find that label noise slows down the\nconvergence of FL training, and the global model tends to overfit when the\nnoise level is high.\n",
                "链接": "https://arxiv.org/abs/2211.07816"
            },
            {
                "文章ID": "111388",
                "标题": "Label Propagation for Graph Label Noise",
                "作者": " Yao Cheng,  Caihua Shan,  Yifei Shen,  Xiang Li,  Siqiang Luo,  Dongsheng Li",
                "发布日期": "2023-10-26",
                "摘要": "  Label noise is a common challenge in large datasets, as it can significantly\ndegrade the generalization ability of deep neural networks. Most existing\nstudies focus on noisy labels in computer vision; however, graph models\nencompass both node features and graph topology as input, and become more\nsusceptible to label noise through message-passing mechanisms. Recently, only a\nfew works have been proposed to tackle the label noise on graphs. One major\nlimitation is that they assume the graph is homophilous and the labels are\nsmoothly distributed. Nevertheless, real-world graphs may contain varying\ndegrees of heterophily or even be heterophily-dominated, leading to the\ninadequacy of current methods. In this paper, we study graph label noise in the\ncontext of arbitrary heterophily, with the aim of rectifying noisy labels and\nassigning labels to previously unlabeled nodes. We begin by conducting two\nempirical analyses to explore the impact of graph homophily on graph label\nnoise. Following observations, we propose a simple yet efficient algorithm,\ndenoted as LP4GLN. Specifically, LP4GLN is an iterative algorithm with three\nsteps: (1) reconstruct the graph to recover the homophily property, (2) utilize\nlabel propagation to rectify the noisy labels, (3) select high-confidence\nlabels to retain for the next iteration. By iterating these steps, we obtain a\nset of correct labels, ultimately achieving high accuracy in the node\nclassification task. The theoretical analysis is also provided to demonstrate\nits remarkable denoising \"effect\". Finally, we conduct experiments on 10\nbenchmark datasets under varying graph heterophily levels and noise types,\ncomparing the performance of LP4GLN with 7 typical baselines. Our results\nillustrate the superior performance of the proposed LP4GLN.\n",
                "链接": "https://arxiv.org/abs/2310.16560"
            },
            {
                "文章ID": "88300",
                "标题": "Systematic analysis of the impact of label noise correction on ML\n  Fairness",
                "作者": " I. Oliveira e Silva,  C. Soares,  I. Sousa,  R. Ghani",
                "发布日期": "2023-06-29",
                "摘要": "  Arbitrary, inconsistent, or faulty decision-making raises serious concerns,\nand preventing unfair models is an increasingly important challenge in Machine\nLearning. Data often reflect past discriminatory behavior, and models trained\non such data may reflect bias on sensitive attributes, such as gender, race, or\nage. One approach to developing fair models is to preprocess the training data\nto remove the underlying biases while preserving the relevant information, for\nexample, by correcting biased labels. While multiple label noise correction\nmethods are available, the information about their behavior in identifying\ndiscrimination is very limited. In this work, we develop an empirical\nmethodology to systematically evaluate the effectiveness of label noise\ncorrection techniques in ensuring the fairness of models trained on biased\ndatasets. Our methodology involves manipulating the amount of label noise and\ncan be used with fairness benchmarks but also with standard ML datasets. We\napply the methodology to analyze six label noise correction methods according\nto several fairness metrics on standard OpenML datasets. Our results suggest\nthat the Hybrid Label Noise Correction method achieves the best trade-off\nbetween predictive performance and fairness. Clustering-Based Correction can\nreduce discrimination the most, however, at the cost of lower predictive\nperformance.\n",
                "链接": "https://arxiv.org/abs/2306.15994"
            },
            {
                "文章ID": "33646",
                "标题": "Investigating the Impact of Model Width and Density on Generalization in\n  Presence of Label Noise",
                "作者": " Yihao Xue,  Kyle Whitecross,  Baharan Mirzasoleiman",
                "发布日期": "2023-06-16",
                "摘要": "  Increasing the size of overparameterized neural networks has been a key in\nachieving state-of-the-art performance. This is captured by the double descent\nphenomenon, where the test loss follows a decreasing-increasing-decreasing\npattern as model width increases. However, the effect of label noise on the\ntest loss curve has not been fully explored. In this work, we uncover an\nintriguing phenomenon where label noise leads to a \\textit{final ascent} in the\noriginally observed double descent curve. Specifically, under a sufficiently\nlarge noise-to-sample-size ratio, optimal generalization is achieved at\nintermediate widths. Through theoretical analysis, we attribute this phenomenon\nto the shape transition of test loss variance induced by label noise.\nFurthermore, we extend the final ascent phenomenon to model density and provide\nthe first theoretical characterization showing that reducing density by\nrandomly dropping trainable parameters improves generalization under label\nnoise. We also thoroughly examine the roles of regularization and sample size.\nSurprisingly, we find that larger $\\ell_2$ regularization and robust learning\nmethods against label noise exacerbate the final ascent. We confirm the\nvalidity of our findings through extensive experiments on ReLu networks trained\non MNIST, ResNets trained on CIFAR-10/100, and InceptionResNet-v2 trained on\nStanford Cars with real-world noisy labels.\n",
                "链接": "https://arxiv.org/abs/2208.08003"
            },
            {
                "文章ID": "92847",
                "标题": "Label Noise: Correcting a Correction",
                "作者": " William Toner,  Amos Storkey",
                "发布日期": "2023-07-26",
                "摘要": "  Training neural network classifiers on datasets with label noise poses a risk\nof overfitting them to the noisy labels. To address this issue, researchers\nhave explored alternative loss functions that aim to be more robust. However,\nmany of these alternatives are heuristic in nature and still vulnerable to\noverfitting or underfitting. In this work, we propose a more direct approach to\ntackling overfitting caused by label noise. We observe that the presence of\nlabel noise implies a lower bound on the noisy generalised risk. Building upon\nthis observation, we propose imposing a lower bound on the empirical risk\nduring training to mitigate overfitting. Our main contribution is providing\ntheoretical results that yield explicit, easily computable bounds on the\nminimum achievable noisy risk for different loss functions. We empirically\ndemonstrate that using these bounds significantly enhances robustness in\nvarious settings, with virtually no additional computational cost.\n",
                "链接": "https://arxiv.org/abs/2307.13100"
            },
            {
                "文章ID": "32080",
                "标题": "Noise tolerance of learning to rank under class-conditional label noise",
                "作者": " Dany Haddad",
                "发布日期": "2022-08-18",
                "摘要": "  Often, the data used to train ranking models is subject to label noise. For\nexample, in web-search, labels created from clickstream data are noisy due to\nissues such as insufficient information in item descriptions on the SERP, query\nreformulation by the user, and erratic or unexpected user behavior. In\npractice, it is difficult to handle label noise without making strong\nassumptions about the label generation process. As a result, practitioners\ntypically train their learning-to-rank (LtR) models directly on this noisy data\nwithout additional consideration of the label noise. Surprisingly, we often see\nstrong performance from LtR models trained in this way. In this work, we\ndescribe a class of noise-tolerant LtR losses for which empirical risk\nminimization is a consistent procedure, even in the context of\nclass-conditional label noise. We also develop noise-tolerant analogs of\ncommonly used loss functions. The practical implications of our theoretical\nfindings are further supported by experimental results.\n",
                "链接": "https://arxiv.org/abs/2208.02126"
            },
            {
                "文章ID": "94423",
                "标题": "Feature Noise Boosts DNN Generalization under Label Noise",
                "作者": " Lu Zeng,  Xuan Chen,  Xiaoshuang Shi,  Heng Tao Shen",
                "发布日期": "2023-08-04",
                "摘要": "  The presence of label noise in the training data has a profound impact on the\ngeneralization of deep neural networks (DNNs). In this study, we introduce and\ntheoretically demonstrate a simple feature noise method, which directly adds\nnoise to the features of training data, can enhance the generalization of DNNs\nunder label noise. Specifically, we conduct theoretical analyses to reveal that\nlabel noise leads to weakened DNN generalization by loosening the PAC-Bayes\ngeneralization bound, and feature noise results in better DNN generalization by\nimposing an upper bound on the mutual information between the model weights and\nthe features, which constrains the PAC-Bayes generalization bound. Furthermore,\nto ensure effective generalization of DNNs in the presence of label noise, we\nconduct application analyses to identify the optimal types and levels of\nfeature noise to add for obtaining desirable label noise generalization.\nFinally, extensive experimental results on several popular datasets demonstrate\nthe feature noise method can significantly enhance the label noise\ngeneralization of the state-of-the-art label noise method.\n",
                "链接": "https://arxiv.org/abs/2308.01609"
            },
            {
                "文章ID": "105104",
                "标题": "Understanding and Mitigating the Label Noise in Pre-training on\n  Downstream Tasks",
                "作者": " Hao Chen,  Jindong Wang,  Ankit Shah,  Ran Tao,  Hongxin Wei,  Xing Xie,  Masashi Sugiyama,  Bhiksha Raj",
                "发布日期": "2023-10-02",
                "摘要": "  Pre-training on large-scale datasets and then fine-tuning on downstream tasks\nhave become a standard practice in deep learning. However, pre-training data\noften contain label noise that may adversely affect the generalization of the\nmodel. This paper aims to understand the nature of noise in pre-training\ndatasets and to mitigate its impact on downstream tasks. More specifically,\nthrough extensive experiments of supervised pre-training models on synthetic\nnoisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise\nin pre-training can benefit in-domain (ID) transfer performance, where the\ntraining and testing data share the same distribution, it always deteriorates\nout-of-domain (OOD) performance, where training and testing data distribution\nare different. We empirically verify that the reason behind is noise in\npre-training shapes the feature space differently. We then propose a\nlightweight black-box tuning method (NMTune) to affine the feature space to\nmitigate the malignant effect of noise and improve generalization on both ID\nand OOD tasks, considering one may not be able to fully fine-tune or even\naccess the pre-trained models. We conduct practical experiments on popular\nvision and language models that are pre-trained on noisy data for evaluation of\nour approach. Our analysis and results show the importance of this interesting\nand novel research direction, which we term Noisy Model Learning.\n",
                "链接": "https://arxiv.org/abs/2309.17002"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下文本检索任务上，是否有关于大模型在语义坍缩问题上的研究",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "65811",
                "标题": "Semantic-Preserving Augmentation for Robust Image-Text Retrieval",
                "作者": " Sunwoo Kim,  Kyuhong Shim,  Luong Trung Nguyen,  Byonghyo Shim",
                "发布日期": "2023-03-13",
                "摘要": "  Image text retrieval is a task to search for the proper textual descriptions\nof the visual world and vice versa. One challenge of this task is the\nvulnerability to input image and text corruptions. Such corruptions are often\nunobserved during the training, and degrade the retrieval model decision\nquality substantially. In this paper, we propose a novel image text retrieval\ntechnique, referred to as robust visual semantic embedding (RVSE), which\nconsists of novel image-based and text-based augmentation techniques called\nsemantic preserving augmentation for image (SPAugI) and text (SPAugT). Since\nSPAugI and SPAugT change the original data in a way that its semantic\ninformation is preserved, we enforce the feature extractors to generate\nsemantic aware embedding vectors regardless of the corruption, improving the\nmodel robustness significantly. From extensive experiments using benchmark\ndatasets, we show that RVSE outperforms conventional retrieval schemes in terms\nof image-text retrieval performance.\n",
                "链接": "https://arxiv.org/abs/2303.05692"
            },
            {
                "文章ID": "50715",
                "标题": "Dense Text Retrieval based on Pretrained Language Models: A Survey",
                "作者": " Wayne Xin Zhao,  Jing Liu,  Ruiyang Ren,  Ji-Rong Wen",
                "发布日期": "2022-11-29",
                "摘要": "  Text retrieval is a long-standing research topic on information seeking,\nwhere a system is required to return relevant information resources to user's\nqueries in natural language. From classic retrieval methods to learning-based\nranking functions, the underlying retrieval models have been continually\nevolved with the ever-lasting technical innovation. To design effective\nretrieval models, a key point lies in how to learn the text representation and\nmodel the relevance matching. The recent success of pretrained language models\n(PLMs) sheds light on developing more capable text retrieval approaches by\nleveraging the excellent modeling capacity of PLMs. With powerful PLMs, we can\neffectively learn the representations of queries and texts in the latent\nrepresentation space, and further construct the semantic matching function\nbetween the dense vectors for relevance modeling. Such a retrieval approach is\nreferred to as dense retrieval, since it employs dense vectors (a.k.a.,\nembeddings) to represent the texts. Considering the rapid progress on dense\nretrieval, in this survey, we systematically review the recent advances on\nPLM-based dense retrieval. Different from previous surveys on dense retrieval,\nwe take a new perspective to organize the related work by four major aspects,\nincluding architecture, training, indexing and integration, and summarize the\nmainstream techniques for each aspect. We thoroughly survey the literature, and\ninclude 300+ related reference papers on dense retrieval. To support our\nsurvey, we create a website for providing useful resources, and release a code\nrepertory and toolkit for implementing dense retrieval models. This survey aims\nto provide a comprehensive, practical reference focused on the major progress\nfor dense text retrieval.\n",
                "链接": "https://arxiv.org/abs/2211.14876"
            },
            {
                "文章ID": "79916",
                "标题": "Query Rewriting for Retrieval-Augmented Large Language Models",
                "作者": " Xinbei Ma,  Yeyun Gong,  Pengcheng He,  Hai Zhao,  Nan Duan",
                "发布日期": "2023-10-24",
                "摘要": "  Large Language Models (LLMs) play powerful, black-box readers in the\nretrieve-then-read pipeline, making remarkable progress in knowledge-intensive\ntasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of\nthe previous retrieve-then-read for the retrieval-augmented LLMs from the\nperspective of the query rewriting. Unlike prior studies focusing on adapting\neither the retriever or the reader, our approach pays attention to the\nadaptation of the search query itself, for there is inevitably a gap between\nthe input text and the needed knowledge in retrieval. We first prompt an LLM to\ngenerate the query, then use a web search engine to retrieve contexts.\nFurthermore, to better align the query to the frozen modules, we propose a\ntrainable scheme for our pipeline. A small language model is adopted as a\ntrainable rewriter to cater to the black-box LLM reader. The rewriter is\ntrained using the feedback of the LLM reader by reinforcement learning.\nEvaluation is conducted on downstream tasks, open-domain QA and multiple-choice\nQA. Experiments results show consistent performance improvement, indicating\nthat our framework is proven effective and scalable, and brings a new framework\nfor retrieval-augmented LLM.\n",
                "链接": "https://arxiv.org/abs/2305.14283"
            },
            {
                "文章ID": "96698",
                "标题": "Pre-training with Large Language Model-based Document Expansion for\n  Dense Passage Retrieval",
                "作者": " Guangyuan Ma,  Xing Wu,  Peng Wang,  Zijia Lin,  Songlin Hu",
                "发布日期": "2023-08-17",
                "摘要": "  In this paper, we systematically study the potential of pre-training with\nLarge Language Model(LLM)-based document expansion for dense passage retrieval.\nConcretely, we leverage the capabilities of LLMs for document expansion, i.e.\nquery generation, and effectively transfer expanded knowledge to retrievers\nusing pre-training strategies tailored for passage retrieval. These strategies\ninclude contrastive learning and bottlenecked query generation. Furthermore, we\nincorporate a curriculum learning strategy to reduce the reliance on LLM\ninferences. Experimental results demonstrate that pre-training with LLM-based\ndocument expansion significantly boosts the retrieval performance on\nlarge-scale web-search tasks. Our work shows strong zero-shot and out-of-domain\nretrieval abilities, making it more widely applicable for retrieval when\ninitializing with no human-labeled data.\n",
                "链接": "https://arxiv.org/abs/2308.08285"
            },
            {
                "文章ID": "80481",
                "标题": "Enhancing Retrieval-Augmented Large Language Models with Iterative\n  Retrieval-Generation Synergy",
                "作者": " Zhihong Shao,  Yeyun Gong,  Yelong Shen,  Minlie Huang,  Nan Duan,  Weizhu Chen",
                "发布日期": "2023-10-24",
                "摘要": "  Large language models are powerful text processors and reasoners, but are\nstill subject to limitations including outdated knowledge and hallucinations,\nwhich necessitates connecting them to the world. Retrieval-augmented large\nlanguage models have raised extensive attention for grounding model generation\non external knowledge. However, retrievers struggle to capture relevance,\nespecially for queries with complex information needs. Recent work has proposed\nto improve relevance modeling by having large language models actively involved\nin retrieval, i.e., to improve retrieval with generation. In this paper, we\nshow that strong performance can be achieved by a method we call Iter-RetGen,\nwhich synergizes retrieval and generation in an iterative manner. A model\noutput shows what might be needed to finish a task, and thus provides an\ninformative context for retrieving more relevant knowledge which in turn helps\ngenerate a better output in the next iteration. Compared with recent work which\ninterleaves retrieval with generation when producing an output, Iter-RetGen\nprocesses all retrieved knowledge as a whole and largely preserves the\nflexibility in generation without structural constraints. We evaluate\nIter-RetGen on multi-hop question answering, fact verification, and commonsense\nreasoning, and show that it can flexibly leverage parametric knowledge and\nnon-parametric knowledge, and is superior to or competitive with\nstate-of-the-art retrieval-augmented baselines while causing fewer overheads of\nretrieval and generation. We can further improve performance via\ngeneration-augmented retrieval adaptation.\n",
                "链接": "https://arxiv.org/abs/2305.15294"
            },
            {
                "文章ID": "77185",
                "标题": "Synergistic Interplay between Search and Large Language Models for\n  Information Retrieval",
                "作者": " Jiazhan Feng,  Chongyang Tao,  Xiubo Geng,  Tao Shen,  Can Xu,  Guodong Long,  Dongyan Zhao,  Daxin Jiang",
                "发布日期": "2023-12-13",
                "摘要": "  Information retrieval (IR) plays a crucial role in locating relevant\nresources from vast amounts of data, and its applications have evolved from\ntraditional knowledge bases to modern retrieval models (RMs). The emergence of\nlarge language models (LLMs) has further revolutionized the IR field by\nenabling users to interact with search systems in natural languages. In this\npaper, we explore the advantages and disadvantages of LLMs and RMs,\nhighlighting their respective strengths in understanding user-issued queries\nand retrieving up-to-date information. To leverage the benefits of both\nparadigms while circumventing their limitations, we propose InteR, a novel\nframework that facilitates information refinement through synergy between RMs\nand LLMs. InteR allows RMs to expand knowledge in queries using LLM-generated\nknowledge collections and enables LLMs to enhance prompt formulation using\nretrieved documents. This iterative refinement process augments the inputs of\nRMs and LLMs, leading to more accurate retrieval. Experiments on large-scale\nretrieval benchmarks involving web search and low-resource retrieval tasks\ndemonstrate that InteR achieves overall superior zero-shot retrieval\nperformance compared to state-of-the-art methods, even those using relevance\njudgment. Source code is available at https://github.com/Cyril-JZ/InteR\n",
                "链接": "https://arxiv.org/abs/2305.07402"
            },
            {
                "文章ID": "115504",
                "标题": "LLatrieval: LLM-Verified Retrieval for Verifiable Generation",
                "作者": " Xiaonan Li,  Changtai Zhu,  Linyang Li,  Zhangyue Yin,  Tianxiang Sun,  Xipeng Qiu",
                "发布日期": "2023-11-15",
                "摘要": "  Verifiable generation aims to let the large language model (LLM) generate\ntext with corresponding supporting documents, which enables the user to\nflexibly verify the answer and makes it more trustworthy. Its evaluation not\nonly measures the correctness of the answer, but also the answer's\nverifiability, i.e., how well the answer is supported by the corresponding\ndocuments. In typical, verifiable generation adopts the retrieval-read\npipeline, which is divided into two stages: 1) retrieve relevant documents of\nthe question. 2) according to the documents, generate the corresponding answer.\nSince the retrieved documents can supplement knowledge for the LLM to generate\nthe answer and serve as evidence, the retrieval stage is essential for the\ncorrectness and verifiability of the answer. However, the widely used\nretrievers become the bottleneck of the entire pipeline and limit the overall\nperformance. They often have fewer parameters than the large language model and\nhave not been proven to scale well to the size of LLMs. Since the LLM passively\nreceives the retrieval result, if the retriever does not correctly find the\nsupporting documents, the LLM can not generate the correct and verifiable\nanswer, which overshadows the LLM's remarkable abilities. In this paper, we\npropose LLatrieval (Large Language Model Verified Retrieval), where the LLM\nupdates the retrieval result until it verifies that the retrieved documents can\nsupport answering the question. Thus, the LLM can iteratively provide feedback\nto retrieval and facilitate the retrieval result to sufficiently support\nverifiable generation. Experimental results show that our method significantly\noutperforms extensive baselines and achieves new state-of-the-art results.\n",
                "链接": "https://arxiv.org/abs/2311.07838"
            },
            {
                "文章ID": "115558",
                "标题": "Text Retrieval with Multi-Stage Re-Ranking Models",
                "作者": " Yuichi Sasazawa,  Kenichi Yokote,  Osamu Imaichi,  Yasuhiro Sogawa",
                "发布日期": "2023-11-15",
                "摘要": "  The text retrieval is the task of retrieving similar documents to a search\nquery, and it is important to improve retrieval accuracy while maintaining a\ncertain level of retrieval speed. Existing studies have reported accuracy\nimprovements using language models, but many of these do not take into account\nthe reduction in search speed that comes with increased performance. In this\nstudy, we propose three-stage re-ranking model using model ensembles or larger\nlanguage models to improve search accuracy while minimizing the search delay.\nWe ranked the documents by BM25 and language models, and then re-ranks by a\nmodel ensemble or a larger language model for documents with high similarity to\nthe query. In our experiments, we train the MiniLM language model on the\nMS-MARCO dataset and evaluate it in a zero-shot setting. Our proposed method\nachieves higher retrieval accuracy while reducing the retrieval speed decay.\n",
                "链接": "https://arxiv.org/abs/2311.07994"
            },
            {
                "文章ID": "124118",
                "标题": "dIR -- Discrete Information Retrieval: Conversational Search over\n  Unstructured (and Structured) Data with Large Language Models",
                "作者": "Computer Science Department, Stanford University  Pablo M. Rodriguez Bertorello, Computer Science Department, Stanford University  Jean Rodmond Junior Laguerre",
                "发布日期": "2023-12-21",
                "摘要": "  Data is stored in both structured and unstructured form. Querying both, to\npower natural language conversations, is a challenge. This paper introduces\ndIR, Discrete Information Retrieval, providing a unified interface to query\nboth free text and structured knowledge. Specifically, a Large Language Model\n(LLM) transforms text into expressive representation. After the text is\nextracted into columnar form, it can then be queried via a text-to-SQL Semantic\nParser, with an LLM converting natural language into SQL. Where desired, such\nconversation may be effected by a multi-step reasoning conversational agent. We\nvalidate our approach via a proprietary question/answer data set, concluding\nthat dIR makes a whole new class of queries on free text possible when compared\nto traditionally fine-tuned dense-embedding-model-based Information Retrieval\n(IR) and SQL-based Knowledge Bases (KB). For sufficiently complex queries, dIR\ncan succeed where no other method stands a chance.\n",
                "链接": "https://arxiv.org/abs/2312.13264"
            },
            {
                "文章ID": "117340",
                "标题": "Don't forget private retrieval: distributed private similarity search\n  for large language models",
                "作者": " Guy Zyskind,  Tobin South,  Alex Pentland",
                "发布日期": "2023-11-23",
                "摘要": "  While the flexible capabilities of large language models (LLMs) allow them to\nanswer a range of queries based on existing learned knowledge, information\nretrieval to augment generation is an important tool to allow LLMs to answer\nquestions on information not included in pre-training data. Such private\ninformation is increasingly being generated in a wide array of distributed\ncontexts by organizations and individuals. Performing such information\nretrieval using neural embeddings of queries and documents always leaked\ninformation about queries and database content unless both were stored locally.\nWe present Private Retrieval Augmented Generation (PRAG), an approach that uses\nmulti-party computation (MPC) to securely transmit queries to a distributed set\nof servers containing a privately constructed database to return top-k and\napproximate top-k documents. This is a first-of-its-kind approach to dense\ninformation retrieval that ensures no server observes a client's query or can\nsee the database content. The approach introduces a novel MPC friendly protocol\nfor inverted file approximate search (IVF) that allows for fast document search\nover distributed and private data in sublinear communication complexity. This\nwork presents new avenues through which data for use in LLMs can be accessed\nand used without needing to centralize or forgo privacy.\n",
                "链接": "https://arxiv.org/abs/2311.12955"
            }
        ]
    },
    {
        "question": {
            "question": "帮我查找对机器翻译数据集质量进行评估的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "6641",
                "标题": "An Overview on Machine Translation Evaluation",
                "作者": " Lifeng Han",
                "发布日期": "2022-02-23",
                "摘要": "  Since the 1950s, machine translation (MT) has become one of the important\ntasks of AI and development, and has experienced several different periods and\nstages of development, including rule-based methods, statistical methods, and\nrecently proposed neural network-based learning methods. Accompanying these\nstaged leaps is the evaluation research and development of MT, especially the\nimportant role of evaluation methods in statistical translation and neural\ntranslation research. The evaluation task of MT is not only to evaluate the\nquality of machine translation, but also to give timely feedback to machine\ntranslation researchers on the problems existing in machine translation itself,\nhow to improve and how to optimise. In some practical application fields, such\nas in the absence of reference translations, the quality estimation of machine\ntranslation plays an important role as an indicator to reveal the credibility\nof automatically translated target languages. This report mainly includes the\nfollowing contents: a brief history of machine translation evaluation (MTE),\nthe classification of research methods on MTE, and the the cutting-edge\nprogress, including human evaluation, automatic evaluation, and evaluation of\nevaluation methods (meta-evaluation). Manual evaluation and automatic\nevaluation include reference-translation based and reference-translation\nindependent participation; automatic evaluation methods include traditional\nn-gram string matching, models applying syntax and semantics, and deep learning\nmodels; evaluation of evaluation methods includes estimating the credibility of\nhuman evaluations, the reliability of the automatic evaluation, the reliability\nof the test set, etc. Advances in cutting-edge evaluation methods include\ntask-based evaluation, using pre-trained language models based on big data, and\nlightweight optimisation models using distillation techniques.\n",
                "链接": "https://arxiv.org/abs/2202.11027"
            },
            {
                "文章ID": "38541",
                "标题": "Approaching English-Polish Machine Translation Quality Assessment with\n  Neural-based Methods",
                "作者": " Artur Nowakowski",
                "发布日期": "2022-09-23",
                "摘要": "  This paper presents our contribution to the PolEval 2021 Task 2: Evaluation\nof translation quality assessment metrics. We describe experiments with\npre-trained language models and state-of-the-art frameworks for translation\nquality assessment in both nonblind and blind versions of the task. Our\nsolutions ranked second in the nonblind version and third in the blind version.\n",
                "链接": "https://arxiv.org/abs/2209.11016"
            },
            {
                "文章ID": "98603",
                "标题": "Training and Meta-Evaluating Machine Translation Evaluation Metrics at\n  the Paragraph Level",
                "作者": " Daniel Deutsch,  Juraj Juraska,  Mara Finkelstein,  Markus Freitag",
                "发布日期": "2023-08-29",
                "摘要": "  As research on machine translation moves to translating text beyond the\nsentence level, it remains unclear how effective automatic evaluation metrics\nare at scoring longer translations. In this work, we first propose a method for\ncreating paragraph-level data for training and meta-evaluating metrics from\nexisting sentence-level data. Then, we use these new datasets to benchmark\nexisting sentence-level metrics as well as train learned metrics at the\nparagraph level. Interestingly, our experimental results demonstrate that using\nsentence-level metrics to score entire paragraphs is equally as effective as\nusing a metric designed to work at the paragraph level. We speculate this\nresult can be attributed to properties of the task of reference-based\nevaluation as well as limitations of our datasets with respect to capturing all\ntypes of phenomena that occur in paragraph-level translations.\n",
                "链接": "https://arxiv.org/abs/2308.13506"
            },
            {
                "文章ID": "42270",
                "标题": "DATScore: Evaluating Translation with Data Augmented Translations",
                "作者": " Moussa Kamal Eddine,  Guokan Shang,  Michalis Vazirgiannis",
                "发布日期": "2022-10-14",
                "摘要": "  The rapid development of large pretrained language models has revolutionized\nnot only the field of Natural Language Generation (NLG) but also its\nevaluation. Inspired by the recent work of BARTScore: a metric leveraging the\nBART language model to evaluate the quality of generated text from various\naspects, we introduce DATScore. DATScore uses data augmentation techniques to\nimprove the evaluation of machine translation. Our main finding is that\nintroducing data augmented translations of the source and reference texts is\ngreatly helpful in evaluating the quality of the generated translation. We also\npropose two novel score averaging and term weighting strategies to improve the\noriginal score computing process of BARTScore. Experimental results on WMT show\nthat DATScore correlates better with human meta-evaluations than the other\nrecent state-of-the-art metrics, especially for low-resource languages.\nAblation studies demonstrate the value added by our new scoring strategies.\nMoreover, we report in our extended experiments the performance of DATScore on\n3 NLG tasks other than translation.\n",
                "链接": "https://arxiv.org/abs/2210.06576"
            },
            {
                "文章ID": "87010",
                "标题": "Evaluation of Chinese-English Machine Translation of Emotion-Loaded\n  Microblog Texts: A Human Annotated Dataset for the Quality Assessment of\n  Emotion Translation",
                "作者": " Shenbin Qian,  Constantin Orasan,  Felix do Carmo,  Qiuliang Li,  Diptesh Kanojia",
                "发布日期": "2023-06-22",
                "摘要": "  In this paper, we focus on how current Machine Translation (MT) tools perform\non the translation of emotion-loaded texts by evaluating outputs from Google\nTranslate according to a framework proposed in this paper. We propose this\nevaluation framework based on the Multidimensional Quality Metrics (MQM) and\nperform a detailed error analysis of the MT outputs. From our analysis, we\nobserve that about 50% of the MT outputs fail to preserve the original emotion.\nAfter further analysis of the errors, we find that emotion carrying words and\nlinguistic phenomena such as polysemous words, negation, abbreviation etc., are\ncommon causes for these translation errors.\n",
                "链接": "https://arxiv.org/abs/2306.11900"
            },
            {
                "文章ID": "17492",
                "标题": "Quality-Aware Decoding for Neural Machine Translation",
                "作者": " Patrick Fernandes,  António Farinhas,  Ricardo Rei,  José G. C. de Souza,  Perez Ogayo,  Graham Neubig,  André F. T. Martins",
                "发布日期": "2022-05-03",
                "摘要": "  Despite the progress in machine translation quality estimation and evaluation\nin the last years, decoding in neural machine translation (NMT) is mostly\noblivious to this and centers around finding the most probable translation\naccording to the model (MAP decoding), approximated with beam search. In this\npaper, we bring together these two lines of research and propose quality-aware\ndecoding for NMT, by leveraging recent breakthroughs in reference-free and\nreference-based MT evaluation through various inference methods like $N$-best\nreranking and minimum Bayes risk decoding. We perform an extensive comparison\nof various possible candidate generation and ranking methods across four\ndatasets and two model classes and find that quality-aware decoding\nconsistently outperforms MAP-based decoding according both to state-of-the-art\nautomatic metrics (COMET and BLEURT) and to human assessments. Our code is\navailable at https://github.com/deep-spin/qaware-decode.\n",
                "链接": "https://arxiv.org/abs/2205.00978"
            },
            {
                "文章ID": "37434",
                "标题": "Rethinking Round-Trip Translation for Machine Translation Evaluation",
                "作者": " Terry Yue Zhuo,  Qiongkai Xu,  Xuanli He,  Trevor Cohn",
                "发布日期": "2023-05-16",
                "摘要": "  Automatic evaluation on low-resource language translation suffers from a\ndeficiency of parallel corpora. Round-trip translation could be served as a\nclever and straightforward technique to alleviate the requirement of the\nparallel evaluation corpus. However, there was an observation of obscure\ncorrelations between the evaluation scores by forward and round-trip\ntranslations in the era of statistical machine translation (SMT). In this\npaper, we report the surprising finding that round-trip translation can be used\nfor automatic evaluation without the references. Firstly, our revisit on the\nround-trip translation in SMT evaluation unveils that its long-standing\nmisunderstanding is essentially caused by copying mechanism. After removing\ncopying mechanism in SMT, round-trip translation scores can appropriately\nreflect the forward translation performance. Then, we demonstrate the\nrectification is overdue as round-trip translation could benefit multiple\nmachine translation evaluation tasks. To be more specific, round-trip\ntranslation could be used i) to predict corresponding forward translation\nscores; ii) to improve the performance of the recently advanced quality\nestimation model; and iii) to identify adversarial competitors in shared tasks\nvia cross-system verification.\n",
                "链接": "https://arxiv.org/abs/2209.07351"
            },
            {
                "文章ID": "103301",
                "标题": "SignBank+: Multilingual Sign Language Translation Dataset",
                "作者": " Amit Moryossef,  Zifan Jiang",
                "发布日期": "2023-09-22",
                "摘要": "  This work advances the field of sign language machine translation by focusing\non dataset quality and simplification of the translation system. We introduce\nSignBank+, a clean version of the SignBank dataset, optimized for machine\ntranslation. Contrary to previous works that employ complex factorization\ntechniques for translation, we advocate for a simplified text-to-text\ntranslation approach. Our evaluation shows that models trained on SignBank+\nsurpass those on the original dataset, establishing a new benchmark and\nproviding an open resource for future research.\n",
                "链接": "https://arxiv.org/abs/2309.11566"
            },
            {
                "文章ID": "19633",
                "标题": "Consistent Human Evaluation of Machine Translation across Language Pairs",
                "作者": " Daniel Licht,  Cynthia Gao,  Janice Lam,  Francisco Guzman,  Mona Diab,  Philipp Koehn",
                "发布日期": "2022-05-18",
                "摘要": "  Obtaining meaningful quality scores for machine translation systems through\nhuman evaluation remains a challenge given the high variability between human\nevaluators, partly due to subjective expectations for translation quality for\ndifferent language pairs. We propose a new metric called XSTS that is more\nfocused on semantic equivalence and a cross-lingual calibration method that\nenables more consistent assessment. We demonstrate the effectiveness of these\nnovel contributions in large scale evaluation studies across up to 14 language\npairs, with translation both into and out of English.\n",
                "链接": "https://arxiv.org/abs/2205.08533"
            },
            {
                "文章ID": "23081",
                "标题": "MorisienMT: A Dataset for Mauritian Creole Machine Translation",
                "作者": " Raj Dabre,  Aneerav Sukhoo",
                "发布日期": "2022-06-07",
                "摘要": "  In this paper, we describe MorisienMT, a dataset for benchmarking machine\ntranslation quality of Mauritian Creole. Mauritian Creole (Morisien) is the\nlingua franca of the Republic of Mauritius and is a French-based creole\nlanguage. MorisienMT consists of a parallel corpus between English and\nMorisien, French and Morisien and a monolingual corpus for Morisien. We first\ngive an overview of Morisien and then describe the steps taken to create the\ncorpora and, from it, the training and evaluation splits. Thereafter, we\nestablish a variety of baseline models using the created parallel corpora as\nwell as large French--English corpora for transfer learning. We release our\ndatasets publicly for research purposes and hope that this spurs research for\nMorisien machine translation.\n",
                "链接": "https://arxiv.org/abs/2206.02421"
            }
        ]
    },
    {
        "question": {
            "question": "新的大模型结构相关探索的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "91534",
                "标题": "Federated Large Language Model: A Position Paper",
                "作者": " Chaochao Chen,  Xiaohua Feng,  Jun Zhou,  Jianwei Yin,  Xiaolin Zheng",
                "发布日期": "2023-07-19",
                "摘要": "  Large scale language models (LLM) have received significant attention and\nfound diverse applications across various domains, but their development\nencounters challenges in real-world scenarios. These challenges arise due to\nthe scarcity of public domain data availability and the need to maintain\nprivacy with respect to private domain data. To address these issues, federated\nlearning (FL) has emerged as a promising technology that enables collaborative\ntraining of shared models while preserving decentralized data. We propose the\nconcept of federated LLM, which comprises three key components, i.e., federated\nLLM pre-training, federated LLM fine-tuning, and federated LLM prompt\nengineering. For each component, we discuss its advantage over traditional LLM\ntraining methods and propose specific engineering strategies for\nimplementation. Furthermore, we explore the novel challenges introduced by the\nintegration of FL and LLM. We analyze existing solutions and identify potential\nobstacles faced by these solutions within the context of federated LLM.\n",
                "链接": "https://arxiv.org/abs/2307.08925"
            },
            {
                "文章ID": "75063",
                "标题": "Learning Achievement Structure for Structured Exploration in Domains\n  with Sparse Reward",
                "作者": " Zihan Zhou,  Animesh Garg",
                "发布日期": "2023-05-02",
                "摘要": "  We propose Structured Exploration with Achievements (SEA), a multi-stage\nreinforcement learning algorithm designed for achievement-based environments, a\nparticular type of environment with an internal achievement set. SEA first uses\noffline data to learn a representation of the known achievements with a\ndeterminant loss function, then recovers the dependency graph of the learned\nachievements with a heuristic algorithm, and finally interacts with the\nenvironment online to learn policies that master known achievements and explore\nnew ones with a controller built with the recovered dependency graph. We\nempirically demonstrate that SEA can recover the achievement structure\naccurately and improve exploration in hard domains such as Crafter that are\nprocedurally generated with high-dimensional observations like images.\n",
                "链接": "https://arxiv.org/abs/2305.00508"
            },
            {
                "文章ID": "116808",
                "标题": "Meta Prompting for AGI Systems",
                "作者": " Yifan Zhang",
                "发布日期": "2023-11-21",
                "摘要": "  This paper presents an in-depth exploration of Meta Prompting, a novel\ntechnique that revolutionizes the way large language models (LLMs), multi-modal\nfoundation models, and AI systems approach problem-solving and data\ninterpretation. Meta Prompting, rooted in type theory and category theory,\nprioritizes the structure and syntax of information, providing a unique\nframework that transcends traditional content-focused methods. We delve into\nthe formal definitions of Meta Prompting, contrasting it with Few-Shot\nPrompting, and highlight its applicability and superiority in various AI\napplications.\n  Key to this exploration is the expansion of Meta Prompting into the realm of\ncomplex reasoning. Here, we demonstrate how this technique adeptly breaks down\nintricate problems into manageable sub-problems, facilitating a step-by-step,\ndetailed approach to problem-solving. This method proves especially\nadvantageous in terms of token efficiency and offering a fair comparison in\nproblem-solving scenarios, standing out against few-shot example approaches.\n  Furthermore, the paper breaks new ground by extending Meta Prompting into\nmulti-modal foundation model settings. This extension addresses the integration\nof diverse data types, such as images, audio, and video, within the structured\nframework of Meta Prompting, highlighting both the challenges and the vast\npotential of this approach in handling complex, multi-faceted data (The code is\navailable at https://github.com/meta-prompting/meta-prompting).\n",
                "链接": "https://arxiv.org/abs/2311.11482"
            },
            {
                "文章ID": "118092",
                "标题": "A Nearly Optimal and Low-Switching Algorithm for Reinforcement Learning\n  with General Function Approximation",
                "作者": " Heyang Zhao,  Jiafan He,  Quanquan Gu",
                "发布日期": "2023-11-28",
                "摘要": "  The exploration-exploitation dilemma has been a central challenge in\nreinforcement learning (RL) with complex model classes. In this paper, we\npropose a new algorithm, Monotonic Q-Learning with Upper Confidence Bound\n(MQL-UCB) for RL with general function approximation. Our key algorithmic\ndesign includes (1) a general deterministic policy-switching strategy that\nachieves low switching cost, (2) a monotonic value function structure with\ncarefully controlled function class complexity, and (3) a variance-weighted\nregression scheme that exploits historical trajectories with high data\nefficiency. MQL-UCB achieves minimax optimal regret of $\\tilde{O}(d\\sqrt{HK})$\nwhen $K$ is sufficiently large and near-optimal policy switching cost of\n$\\tilde{O}(dH)$, with $d$ being the eluder dimension of the function class, $H$\nbeing the planning horizon, and $K$ being the number of episodes.\n  Our work sheds light on designing provably sample-efficient and\ndeployment-efficient Q-learning with nonlinear function approximation.\n",
                "链接": "https://arxiv.org/abs/2311.15238"
            },
            {
                "文章ID": "123525",
                "标题": "Exploration-Exploitation Model of Moth-Inspired Olfactory Navigation",
                "作者": " Teddy Lazebnik,  Yiftach Golov,  Roi Gurka,  Ally Harari,  Alex Liberzon",
                "发布日期": "2023-12-20",
                "摘要": "  Navigation of male moths toward females during the mating search offers a\nunique perspective on the exploration-exploitation (EE) model in\ndecision-making. This study uses the EE model to explain male moth\npheromone-driven flight paths. We leverage wind tunnel measurements and 3D\ntracking using infrared cameras to gain insights into male moth behavior.\nDuring the experiments in the wind tunnel, we add disturbance to the airflow\nand analyze the effect of increased fluctuations on moth flights in the context\nof the proposed EE model. We separate the exploration and exploitation phases\nby applying a genetic algorithm to the dataset of moth 3D trajectories. First,\nwe demonstrate that the exploration-to-exploitation rate (EER) increases with\ndistance from the source of the female pheromone, which can be explained in the\ncontext of the EE model. Furthermore, our findings reveal a compelling\nrelationship between EER and increased flow fluctuations near the pheromone\nsource. Using the open-source pheromone plume simulation and our moth-inspired\nnavigation model, we explain why male moths exhibit an enhanced EER as\nturbulence levels increase, emphasizing the agent's adaptation to dynamically\nchanging environments. This research extends our understanding of optimal\nnavigation strategies based on general biological EE models and supports the\ndevelopment of advanced, theoretically supported bio-inspired navigation\nalgorithms. We provide important insights into the potential of bio-inspired\nnavigation models for addressing complex decision-making challenges.\n",
                "链接": "https://arxiv.org/abs/2312.11492"
            },
            {
                "文章ID": "75161",
                "标题": "Automated Paper Screening for Clinical Reviews Using Large Language\n  Models",
                "作者": " Eddie Guo,  Mehul Gupta,  Jiawen Deng,  Ye-Jean Park,  Mike Paget,  Christopher Naugler",
                "发布日期": "2023-10-09",
                "摘要": "  Objective: To assess the performance of the OpenAI GPT API in accurately and\nefficiently identifying relevant titles and abstracts from real-world clinical\nreview datasets and compare its performance against ground truth labelling by\ntwo independent human reviewers.\n  Methods: We introduce a novel workflow using the OpenAI GPT API for screening\ntitles and abstracts in clinical reviews. A Python script was created to make\ncalls to the GPT API with the screening criteria in natural language and a\ncorpus of title and abstract datasets that have been filtered by a minimum of\ntwo human reviewers. We compared the performance of our model against\nhuman-reviewed papers across six review papers, screening over 24,000 titles\nand abstracts.\n  Results: Our results show an accuracy of 0.91, a sensitivity of excluded\npapers of 0.91, and a sensitivity of included papers of 0.76. On a randomly\nselected subset of papers, the GPT API demonstrated the ability to provide\nreasoning for its decisions and corrected its initial decision upon being asked\nto explain its reasoning for a subset of incorrect classifications.\n  Conclusion: The GPT API has the potential to streamline the clinical review\nprocess, save valuable time and effort for researchers, and contribute to the\noverall quality of clinical reviews. By prioritizing the workflow and acting as\nan aid rather than a replacement for researchers and reviewers, the GPT API can\nenhance efficiency and lead to more accurate and reliable conclusions in\nmedical research.\n",
                "链接": "https://arxiv.org/abs/2305.00844"
            },
            {
                "文章ID": "110002",
                "标题": "Structured Generation and Exploration of Design Space with Large\n  Language Models for Human-AI Co-Creation",
                "作者": " Sangho Suh,  Meng Chen,  Bryan Min,  Toby Jia-Jun Li,  Haijun Xia",
                "发布日期": "2023-10-24",
                "摘要": "  Thanks to their generative capabilities, large language models (LLMs) have\nbecome an invaluable tool for creative processes. These models have the\ncapacity to produce hundreds and thousands of visual and textual outputs,\noffering abundant inspiration for creative endeavors. But are we harnessing\ntheir full potential? We argue that current interaction paradigms fall short,\nguiding users towards rapid convergence on a limited set of ideas, rather than\nempowering them to explore the vast latent design space in generative models.\nTo address this limitation, we propose a framework that facilitates the\nstructured generation of design space in which users can seamlessly explore,\nevaluate, and synthesize a multitude of responses. We demonstrate the\nfeasibility and usefulness of this framework through the design and development\nof an interactive system, Luminate, and a user study with 8 professional\nwriters. Our work advances how we interact with LLMs for creative tasks,\nintroducing a way to harness the creative potential of LLMs.\n",
                "链接": "https://arxiv.org/abs/2310.12953"
            },
            {
                "文章ID": "63038",
                "标题": "Exploration by self-supervised exploitation",
                "作者": " Matej Pecháč,  Michal Chovanec,  Igor Farkaš",
                "发布日期": "2023-07-04",
                "摘要": "  Reinforcement learning can solve decision-making problems and train an agent\nto behave in an environment according to a predesigned reward function.\nHowever, such an approach becomes very problematic if the reward is too sparse\nand the agent does not come across the reward during the environmental\nexploration. The solution to such a problem may be in equipping the agent with\nan intrinsic motivation, which will provide informed exploration, during which\nthe agent is likely to also encounter external reward. Novelty detection is one\nof the promising branches of intrinsic motivation research. We present\nSelf-supervised Network Distillation (SND), a class of internal motivation\nalgorithms based on the distillation error as a novelty indicator, where the\ntarget model is trained using self-supervised learning. We adapted three\nexisting self-supervised methods for this purpose and experimentally tested\nthem on a set of ten environments that are considered difficult to explore. The\nresults show that our approach achieves faster growth and higher external\nreward for the same training time compared to the baseline models, which\nimplies improved exploration in a very sparse reward environment.\n",
                "链接": "https://arxiv.org/abs/2302.11563"
            },
            {
                "文章ID": "82791",
                "标题": "ReviewerGPT? An Exploratory Study on Using Large Language Models for\n  Paper Reviewing",
                "作者": " Ryan Liu,  Nihar B. Shah",
                "发布日期": "2023-06-02",
                "摘要": "  Given the rapid ascent of large language models (LLMs), we study the\nquestion: (How) can large language models help in reviewing of scientific\npapers or proposals? We first conduct some pilot studies where we find that (i)\nGPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly,\nOpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to\nidentify errors) outperforms prompting to simply write a review. With these\ninsights, we study the use of LLMs (specifically, GPT-4) for three tasks:\n  1. Identifying errors: We construct 13 short computer science papers each\nwith a deliberately inserted error, and ask the LLM to check for the\ncorrectness of these papers. We observe that the LLM finds errors in 7 of them,\nspanning both mathematical and conceptual errors.\n  2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist\nquestions in the respective sections of 15 NeurIPS 2022 papers. We find that\nacross 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy.\n  3. Choosing the \"better\" paper: We generate 10 pairs of abstracts,\ndeliberately designing each pair in such a way that one abstract was clearly\nsuperior than the other. The LLM, however, struggled to discern these\nrelatively straightforward distinctions accurately, committing errors in its\nevaluations for 6 out of the 10 pairs.\n  Based on these experiments, we think that LLMs have a promising use as\nreviewing assistants for specific reviewing tasks, but not (yet) for complete\nevaluations of papers or proposals.\n",
                "链接": "https://arxiv.org/abs/2306.00622"
            },
            {
                "文章ID": "110744",
                "标题": "ALCUNA: Large Language Models Meet New Knowledge",
                "作者": " Xunjian Yin,  Baizhou Huang,  Xiaojun Wan",
                "发布日期": "2023-10-24",
                "摘要": "  With the rapid development of NLP, large-scale language models (LLMs) excel\nin various tasks across multiple domains now. However, existing benchmarks may\nnot adequately measure these models' capabilities, especially when faced with\nnew knowledge. In this paper, we address the lack of benchmarks to evaluate\nLLMs' ability to handle new knowledge, an important and challenging aspect in\nthe rapidly evolving world. We propose an approach called KnowGen that\ngenerates new knowledge by altering existing entity attributes and\nrelationships, resulting in artificial entities that are distinct from\nreal-world entities. With KnowGen, we introduce a benchmark named ALCUNA to\nassess LLMs' abilities in knowledge understanding, differentiation, and\nassociation. We benchmark several LLMs, reveals that their performance in face\nof new knowledge is not satisfactory, particularly in reasoning between new and\ninternal knowledge. We also explore the impact of entity similarity on the\nmodel's understanding of entity knowledge and the influence of contextual\nentities. We appeal to the need for caution when using LLMs in new scenarios or\nwith new knowledge, and hope that our benchmarks can help drive the development\nof LLMs in face of new knowledge.\n",
                "链接": "https://arxiv.org/abs/2310.14820"
            }
        ]
    },
    {
        "question": {
            "question": "帮我查找风格化机器翻译相关的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "1780",
                "标题": "Extending the Vocabulary of Fictional Languages using Neural Networks",
                "作者": " Thomas Zacharias,  Ashutosh Taklikar,  Raja Giryes",
                "发布日期": "2022-01-20",
                "摘要": "  Fictional languages have become increasingly popular over the recent years\nappearing in novels, movies, TV shows, comics, and video games. While some of\nthese fictional languages have a complete vocabulary, most do not. We propose a\ndeep learning solution to the problem. Using style transfer and machine\ntranslation tools, we generate new words for a given target fictional language,\nwhile maintaining the style of its creator, hence extending this language\nvocabulary.\n",
                "链接": "https://arxiv.org/abs/2201.07288"
            },
            {
                "文章ID": "92370",
                "标题": "Incorporating Human Translator Style into English-Turkish Literary\n  Machine Translation",
                "作者": " Zeynep Yirmibeşoğlu,  Olgun Dursun,  Harun Dallı,  Mehmet Şahin,  Ena Hodzik,  Sabri Gürses,  Tunga Güngör",
                "发布日期": "2023-07-24",
                "摘要": "  Although machine translation systems are mostly designed to serve in the\ngeneral domain, there is a growing tendency to adapt these systems to other\ndomains like literary translation. In this paper, we focus on English-Turkish\nliterary translation and develop machine translation models that take into\naccount the stylistic features of translators. We fine-tune a pre-trained\nmachine translation model by the manually-aligned works of a particular\ntranslator. We make a detailed analysis of the effects of manual and automatic\nalignments, data augmentation methods, and corpus size on the translations. We\npropose an approach based on stylistic features to evaluate the style of a\ntranslator in the output translations. We show that the human translator style\ncan be highly recreated in the target machine translations by adapting the\nmodels to the style of the translator.\n",
                "链接": "https://arxiv.org/abs/2307.11457"
            },
            {
                "文章ID": "19713",
                "标题": "Exploiting Social Media Content for Self-Supervised Style Transfer",
                "作者": " Dana Ruiter,  Thomas Kleinbauer,  Cristina España-Bonet,  Josef van Genabith,  Dietrich Klakow",
                "发布日期": "2022-05-19",
                "摘要": "  Recent research on style transfer takes inspiration from unsupervised neural\nmachine translation (UNMT), learning from large amounts of non-parallel data by\nexploiting cycle consistency loss, back-translation, and denoising\nautoencoders. By contrast, the use of self-supervised NMT (SSNMT), which\nleverages (near) parallel instances hidden in non-parallel data more\nefficiently than UNMT, has not yet been explored for style transfer. In this\npaper we present a novel Self-Supervised Style Transfer (3ST) model, which\naugments SSNMT with UNMT methods in order to identify and efficiently exploit\nsupervisory signals in non-parallel social media posts. We compare 3ST with\nstate-of-the-art (SOTA) style transfer models across civil rephrasing,\nformality and polarity tasks. We show that 3ST is able to balance the three\nmajor objectives (fluency, content preservation, attribute transfer accuracy)\nthe best, outperforming SOTA models on averaged performance across their tested\ntasks in automatic and human evaluation.\n",
                "链接": "https://arxiv.org/abs/2205.08814"
            },
            {
                "文章ID": "37030",
                "标题": "Time-of-Day Neural Style Transfer for Architectural Photographs",
                "作者": " Yingshu Chen,  Tuan-Anh Vu,  Ka-Chun Shum,  Binh-Son Hua,  Sai-Kit Yeung",
                "发布日期": "2022-10-31",
                "摘要": "  Architectural photography is a genre of photography that focuses on capturing\na building or structure in the foreground with dramatic lighting in the\nbackground. Inspired by recent successes in image-to-image translation methods,\nwe aim to perform style transfer for architectural photographs. However, the\nspecial composition in architectural photography poses great challenges for\nstyle transfer in this type of photographs. Existing neural style transfer\nmethods treat the architectural images as a single entity, which would generate\nmismatched chrominance and destroy geometric features of the original\narchitecture, yielding unrealistic lighting, wrong color rendition, and visual\nartifacts such as ghosting, appearance distortion, or color mismatching. In\nthis paper, we specialize a neural style transfer method for architectural\nphotography. Our method addresses the composition of the foreground and\nbackground in an architectural photograph in a two-branch neural network that\nseparately considers the style transfer of the foreground and the background,\nrespectively. Our method comprises a segmentation module, a learning-based\nimage-to-image translation module, and an image blending optimization module.\nWe trained our image-to-image translation neural network with a new dataset of\nunconstrained outdoor architectural photographs captured at different magic\ntimes of a day, utilizing additional semantic information for better\nchrominance matching and geometry preservation. Our experiments show that our\nmethod can produce photorealistic lighting and color rendition on both the\nforeground and background, and outperforms general image-to-image translation\nand arbitrary style transfer baselines quantitatively and qualitatively. Our\ncode and data are available at\nhttps://github.com/hkust-vgd/architectural_style_transfer.\n",
                "链接": "https://arxiv.org/abs/2209.05800"
            },
            {
                "文章ID": "83081",
                "标题": "Text Style Transfer Back-Translation",
                "作者": " Daimeng Wei,  Zhanglin Wu,  Hengchao Shang,  Zongyao Li,  Minghan Wang,  Jiaxin Guo,  Xiaoyu Chen,  Zhengzhe Yu,  Hao Yang",
                "发布日期": "2023-06-05",
                "摘要": "  Back Translation (BT) is widely used in the field of machine translation, as\nit has been proved effective for enhancing translation quality. However, BT\nmainly improves the translation of inputs that share a similar style (to be\nmore specific, translation-like inputs), since the source side of BT data is\nmachine-translated. For natural inputs, BT brings only slight improvements and\nsometimes even adverse effects. To address this issue, we propose Text Style\nTransfer Back Translation (TST BT), which uses a style transfer model to modify\nthe source side of BT data. By making the style of source-side text more\nnatural, we aim to improve the translation of natural inputs. Our experiments\non various language pairs, including both high-resource and low-resource ones,\ndemonstrate that TST BT significantly improves translation performance against\npopular BT benchmarks. In addition, TST BT is proved to be effective in domain\nadaptation so this strategy can be regarded as a general data augmentation\nmethod. Our training code and text style transfer model are open-sourced.\n",
                "链接": "https://arxiv.org/abs/2306.01318"
            },
            {
                "文章ID": "11561",
                "标题": "Pastiche Master: Exemplar-Based High-Resolution Portrait Style Transfer",
                "作者": " Shuai Yang,  Liming Jiang,  Ziwei Liu,  Chen Change Loy",
                "发布日期": "2022-03-25",
                "摘要": "  Recent studies on StyleGAN show high performance on artistic portrait\ngeneration by transfer learning with limited data. In this paper, we explore\nmore challenging exemplar-based high-resolution portrait style transfer by\nintroducing a novel DualStyleGAN with flexible control of dual styles of the\noriginal face domain and the extended artistic portrait domain. Different from\nStyleGAN, DualStyleGAN provides a natural way of style transfer by\ncharacterizing the content and style of a portrait with an intrinsic style path\nand a new extrinsic style path, respectively. The delicately designed extrinsic\nstyle path enables our model to modulate both the color and complex structural\nstyles hierarchically to precisely pastiche the style example. Furthermore, a\nnovel progressive fine-tuning scheme is introduced to smoothly transform the\ngenerative space of the model to the target domain, even with the above\nmodifications on the network architecture. Experiments demonstrate the\nsuperiority of DualStyleGAN over state-of-the-art methods in high-quality\nportrait style transfer and flexible style control.\n",
                "链接": "https://arxiv.org/abs/2203.13248"
            },
            {
                "文章ID": "54085",
                "标题": "Controlling Styles in Neural Machine Translation with Activation Prompt",
                "作者": " Yifan Wang,  Zewei Sun,  Shanbo Cheng,  Weiguo Zheng,  Mingxuan Wang",
                "发布日期": "2023-05-30",
                "摘要": "  Controlling styles in neural machine translation (NMT) has attracted wide\nattention, as it is crucial for enhancing user experience. Earlier studies on\nthis topic typically concentrate on regulating the level of formality and\nachieve some progress in this area. However, they still encounter two major\nchallenges. The first is the difficulty in style evaluation. The style\ncomprises various aspects such as lexis, syntax, and others that provide\nabundant information. Nevertheless, only formality has been thoroughly\ninvestigated. The second challenge involves excessive dependence on incremental\nadjustments, particularly when new styles are necessary. To address both\nchallenges, this paper presents a new benchmark and approach. A multiway\nstylized machine translation (MSMT) benchmark is introduced, incorporating\ndiverse categories of styles across four linguistic domains. Then, we propose a\nmethod named style activation prompt (StyleAP) by retrieving prompts from\nstylized monolingual corpus, which does not require extra fine-tuning.\nExperiments show that StyleAP could effectively control the style of\ntranslation and achieve remarkable performance.\n",
                "链接": "https://arxiv.org/abs/2212.08909"
            },
            {
                "文章ID": "95396",
                "标题": "Learning Evaluation Models from Large Language Models for Sequence\n  Generation",
                "作者": " Chenglong Wang,  Hang Zhou,  Kaiyan Chang,  Tongran Liu,  Chunliang Zhang,  Quan Du,  Tong Xiao,  Jingbo Zhu",
                "发布日期": "2023-08-09",
                "摘要": "  Large language models achieve state-of-the-art performance on sequence\ngeneration evaluation, but typically have a large number of parameters. This is\na computational challenge as presented by applying their evaluation capability\nat scale. To overcome the challenge, in this paper, we propose \\textbf{ECT}, an\n\\textbf{e}valuation \\textbf{c}apability \\textbf{t}ransfer method, to transfer\nthe evaluation capability from LLMs to relatively lightweight language models.\nBased on the proposed ECT, we learn various evaluation models from ChatGPT, and\nemploy them as reward models to improve sequence generation models via\nreinforcement learning and reranking approaches. Experimental results on\nmachine translation, text style transfer, and summarization tasks demonstrate\nthe effectiveness of our ECT. Notably, applying the learned evaluation models\nto sequence generation models results in better generated sequences as\nevaluated by commonly used metrics and ChatGPT.\n",
                "链接": "https://arxiv.org/abs/2308.04386"
            },
            {
                "文章ID": "54191",
                "标题": "ColoristaNet for Photorealistic Video Style Transfer",
                "作者": " Xiaowen Qiu,  Ruize Xu,  Boan He,  Yingtao Zhang,  Wenqiang Zhang,  Weifeng Ge",
                "发布日期": "2022-12-22",
                "摘要": "  Photorealistic style transfer aims to transfer the artistic style of an image\nonto an input image or video while keeping photorealism. In this paper, we\nthink it's the summary statistics matching scheme in existing algorithms that\nleads to unrealistic stylization. To avoid employing the popular Gram loss, we\npropose a self-supervised style transfer framework, which contains a style\nremoval part and a style restoration part. The style removal network removes\nthe original image styles, and the style restoration network recovers image\nstyles in a supervised manner. Meanwhile, to address the problems in current\nfeature transformation methods, we propose decoupled instance normalization to\ndecompose feature transformation into style whitening and restylization. It\nworks quite well in ColoristaNet and can transfer image styles efficiently\nwhile keeping photorealism. To ensure temporal coherency, we also incorporate\noptical flow methods and ConvLSTM to embed contextual information. Experiments\ndemonstrates that ColoristaNet can achieve better stylization effects when\ncompared with state-of-the-art algorithms.\n",
                "链接": "https://arxiv.org/abs/2212.09247"
            },
            {
                "文章ID": "84526",
                "标题": "T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text\n  Classification",
                "作者": " Inigo Jauregi Unanue,  Gholamreza Haffari,  Massimo Piccardi",
                "发布日期": "2023-06-09",
                "摘要": "  Cross-lingual text classification leverages text classifiers trained in a\nhigh-resource language to perform text classification in other languages with\nno or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays,\ncross-lingual text classifiers are typically built on large-scale, multilingual\nlanguage models (LMs) pretrained on a variety of languages of interest.\nHowever, the performance of these models vary significantly across languages\nand classification tasks, suggesting that the superposition of the language\nmodelling and classification tasks is not always effective. For this reason, in\nthis paper we propose revisiting the classic \"translate-and-test\" pipeline to\nneatly separate the translation and classification stages. The proposed\napproach couples 1) a neural machine translator translating from the targeted\nlanguage to a high-resource language, with 2) a text classifier trained in the\nhigh-resource language, but the neural machine translator generates \"soft\"\ntranslations to permit end-to-end backpropagation during fine-tuning of the\npipeline. Extensive experiments have been carried out over three cross-lingual\ntext classification datasets (XNLI, MLDoc and MultiEURLEX), with the results\nshowing that the proposed approach has significantly improved performance over\na competitive baseline.\n",
                "链接": "https://arxiv.org/abs/2306.04996"
            }
        ]
    },
    {
        "question": {
            "question": "查找使用GPT4v完成多模态智能体的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "120902",
                "标题": "GPT-4V with Emotion: A Zero-shot Benchmark for Multimodal Emotion\n  Understanding",
                "作者": " Zheng Lian,  Licai Sun,  Haiyang Sun,  Kang Chen,  Zhuofan Wen,  Hao Gu,  Shun Chen,  Bin Liu,  Jianhua Tao",
                "发布日期": "2023-12-08",
                "摘要": "  Recently, GPT-4 with Vision (GPT-4V) has shown remarkable performance across\nvarious multimodal tasks. However, its efficacy in emotion recognition remains\na question. This paper quantitatively evaluates GPT-4V's capabilities in\nmultimodal emotion understanding, encompassing tasks such as facial emotion\nrecognition, visual sentiment analysis, micro-expression recognition, dynamic\nfacial emotion recognition, and multimodal emotion recognition. Our experiments\nshow that GPT-4V exhibits impressive multimodal and temporal understanding\ncapabilities, even surpassing supervised systems in some tasks. Despite these\nachievements, GPT-4V is currently tailored for general domains. It performs\npoorly in micro-expression recognition that requires specialized expertise. The\nmain purpose of this paper is to present quantitative results of GPT-4V on\nemotion understanding and establish a zero-shot benchmark for future research.\nCode and evaluation results are available at:\nhttps://github.com/zeroQiaoba/gpt4v-emotion.\n",
                "链接": "https://arxiv.org/abs/2312.04293"
            },
            {
                "文章ID": "115991",
                "标题": "Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks",
                "作者": " Melanie Mitchell,  Alessandro B. Palmarini,  Arseny Moskvichev",
                "发布日期": "2023-12-25",
                "摘要": "  We explore the abstract reasoning abilities of text-only and multimodal\nversions of GPT-4, using the ConceptARC benchmark [10], which is designed to\nevaluate robust understanding and reasoning with core-knowledge concepts. We\nextend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,\none-shot prompting (rather than simple, zero-shot prompts) with text versions\nof ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,\non zero- and one-shot prompts using image versions of the simplest tasks. Our\nexperimental results support the conclusion that neither version of GPT-4 has\ndeveloped robust abstraction abilities at humanlike levels.\n",
                "链接": "https://arxiv.org/abs/2311.09247"
            },
            {
                "文章ID": "111381",
                "标题": "An Early Evaluation of GPT-4V(ision)",
                "作者": " Yang Wu,  Shilong Wang,  Hao Yang,  Tian Zheng,  Hongbo Zhang,  Yanyan Zhao,  Bing Qin",
                "发布日期": "2023-10-26",
                "摘要": "  In this paper, we evaluate different abilities of GPT-4V including visual\nunderstanding, language understanding, visual puzzle solving, and understanding\nof other modalities such as depth, thermal, video, and audio. To estimate\nGPT-4V's performance, we manually construct 656 test instances and carefully\nevaluate the results of GPT-4V. The highlights of our findings are as follows:\n(1) GPT-4V exhibits impressive performance on English visual-centric benchmarks\nbut fails to recognize simple Chinese texts in the images; (2) GPT-4V shows\ninconsistent refusal behavior when answering questions related to sensitive\ntraits such as gender, race, and age; (3) GPT-4V obtains worse results than\nGPT-4 (API) on language understanding tasks including general language\nunderstanding benchmarks and visual commonsense knowledge evaluation\nbenchmarks; (4) Few-shot prompting can improve GPT-4V's performance on both\nvisual understanding and language understanding; (5) GPT-4V struggles to find\nthe nuances between two similar images and solve the easy math picture puzzles;\n(6) GPT-4V shows non-trivial performance on the tasks of similar modalities to\nimage, such as video and thermal. Our experimental results reveal the ability\nand limitations of GPT-4V and we hope our paper can provide some insights into\nthe application and research of GPT-4V.\n",
                "链接": "https://arxiv.org/abs/2310.16534"
            },
            {
                "文章ID": "113829",
                "标题": "Exploring Grounding Potential of VQA-oriented GPT-4V for Zero-shot\n  Anomaly Detection",
                "作者": " Jiangning Zhang,  Xuhai Chen,  Zhucun Xue,  Yabiao Wang,  Chengjie Wang,  Yong Liu",
                "发布日期": "2023-11-07",
                "摘要": "  Large Multimodal Model (LMM) GPT-4V(ision) endows GPT-4 with visual grounding\ncapabilities, making it possible to handle certain tasks through the Visual\nQuestion Answering (VQA) paradigm. This paper explores the potential of\nVQA-oriented GPT-4V in the recently popular visual Anomaly Detection (AD) and\nis the first to conduct qualitative and quantitative evaluations on the popular\nMVTec AD and VisA datasets. Considering that this task requires both\nimage-/pixel-level evaluations, the proposed GPT-4V-AD framework contains three\ncomponents: 1) Granular Region Division, 2) Prompt Designing, 3)\nText2Segmentation for easy quantitative evaluation, and have made some\ndifferent attempts for comparative analysis. The results show that GPT-4V can\nachieve certain results in the zero-shot AD task through a VQA paradigm, such\nas achieving image-level 77.1/88.0 and pixel-level 68.0/76.6 AU-ROCs on MVTec\nAD and VisA datasets, respectively. However, its performance still has a\ncertain gap compared to the state-of-the-art zero-shot method, e.g., WinCLIP\nann CLIP-AD, and further research is needed. This study provides a baseline\nreference for the research of VQA-oriented LMM in the zero-shot AD task, and we\nalso post several possible future works. Code is available at\n\\url{https://github.com/zhangzjn/GPT-4V-AD}.\n",
                "链接": "https://arxiv.org/abs/2311.02612"
            },
            {
                "文章ID": "114788",
                "标题": "Accuracy of a Vision-Language Model on Challenging Medical Cases",
                "作者": " Thomas Buckley,  James A. Diao,  Adam Rodman,  Arjun K. Manrai",
                "发布日期": "2023-11-10",
                "摘要": "  Background: General-purpose large language models that utilize both text and\nimages have not been evaluated on a diverse array of challenging medical cases.\n  Methods: Using 934 cases from the NEJM Image Challenge published between 2005\nand 2023, we evaluated the accuracy of the recently released Generative\nPre-trained Transformer 4 with Vision model (GPT-4V) compared to human\nrespondents overall and stratified by question difficulty, image type, and skin\ntone. We further conducted a physician evaluation of GPT-4V on 69 NEJM\nclinicopathological conferences (CPCs). Analyses were conducted for models\nutilizing text alone, images alone, and both text and images.\n  Results: GPT-4V achieved an overall accuracy of 61% (95% CI, 58 to 64%)\ncompared to 49% (95% CI, 49 to 50%) for humans. GPT-4V outperformed humans at\nall levels of difficulty and disagreement, skin tones, and image types; the\nexception was radiographic images, where performance was equivalent between\nGPT-4V and human respondents. Longer, more informative captions were associated\nwith improved performance for GPT-4V but similar performance for human\nrespondents. GPT-4V included the correct diagnosis in its differential for 80%\n(95% CI, 68 to 88%) of CPCs when using text alone, compared to 58% (95% CI, 45\nto 70%) of CPCs when using both images and text.\n  Conclusions: GPT-4V outperformed human respondents on challenging medical\ncases and was able to synthesize information from both images and text, but\nperformance deteriorated when images were added to highly informative text.\nOverall, our results suggest that multimodal AI models may be useful in medical\ndiagnostic reasoning but that their accuracy may depend heavily on context.\n",
                "链接": "https://arxiv.org/abs/2311.05591"
            },
            {
                "文章ID": "105257",
                "标题": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)",
                "作者": " Zhengyuan Yang,  Linjie Li,  Kevin Lin,  Jianfeng Wang,  Chung-Ching Lin,  Zicheng Liu,  Lijuan Wang",
                "发布日期": "2023-10-12",
                "摘要": "  Large multimodal models (LMMs) extend large language models (LLMs) with\nmulti-sensory skills, such as visual understanding, to achieve stronger generic\nintelligence. In this paper, we analyze the latest model, GPT-4V(ision), to\ndeepen the understanding of LMMs. The analysis focuses on the intriguing tasks\nthat GPT-4V can perform, containing test samples to probe the quality and\ngenericity of GPT-4V's capabilities, its supported inputs and working modes,\nand the effective ways to prompt the model. In our approach to exploring\nGPT-4V, we curate and organize a collection of carefully designed qualitative\nsamples spanning a variety of domains and tasks. Observations from these\nsamples demonstrate that GPT-4V's unprecedented ability in processing\narbitrarily interleaved multimodal inputs and the genericity of its\ncapabilities together make GPT-4V a powerful multimodal generalist system.\nFurthermore, GPT-4V's unique capability of understanding visual markers drawn\non input images can give rise to new human-computer interaction methods such as\nvisual referring prompting. We conclude the report with in-depth discussions on\nthe emerging application scenarios and the future research directions for\nGPT-4V-based systems. We hope that this preliminary exploration will inspire\nfuture research on the next-generation multimodal task formulation, new ways to\nexploit and enhance LMMs to solve real-world problems, and gaining better\nunderstanding of multimodal foundation models. Finally, we acknowledge that the\nmodel under our study is solely the product of OpenAI's innovative work, and\nthey should be fully credited for its development. Please see the GPT-4V\ncontributions paper for the authorship and credit attribution:\nhttps://cdn.openai.com/contributions/gpt-4v.pdf\n",
                "链接": "https://arxiv.org/abs/2309.17421"
            },
            {
                "文章ID": "112337",
                "标题": "Multimodal ChatGPT for Medical Applications: an Experimental Study of\n  GPT-4V",
                "作者": " Zhiling Yan,  Kai Zhang,  Rong Zhou,  Lifang He,  Xiang Li,  Lichao Sun",
                "发布日期": "2023-10-31",
                "摘要": "  In this paper, we critically evaluate the capabilities of the\nstate-of-the-art multimodal large language model, i.e., GPT-4 with Vision\n(GPT-4V), on Visual Question Answering (VQA) task. Our experiments thoroughly\nassess GPT-4V's proficiency in answering questions paired with images using\nboth pathology and radiology datasets from 11 modalities (e.g. Microscopy,\nDermoscopy, X-ray, CT, etc.) and fifteen objects of interests (brain, liver,\nlung, etc.). Our datasets encompass a comprehensive range of medical inquiries,\nincluding sixteen distinct question types. Throughout our evaluations, we\ndevised textual prompts for GPT-4V, directing it to synergize visual and\ntextual information. The experiments with accuracy score conclude that the\ncurrent version of GPT-4V is not recommended for real-world diagnostics due to\nits unreliable and suboptimal accuracy in responding to diagnostic medical\nquestions. In addition, we delineate seven unique facets of GPT-4V's behavior\nin medical VQA, highlighting its constraints within this complex arena. The\ncomplete details of our evaluation cases are accessible at\nhttps://github.com/ZhilingYan/GPT4V-Medical-Report.\n",
                "链接": "https://arxiv.org/abs/2310.19061"
            },
            {
                "文章ID": "106097",
                "标题": "MathVista: Evaluating Math Reasoning in Visual Contexts with GPT-4V,\n  Bard, and Other Large Multimodal Models",
                "作者": " Pan Lu,  Hritik Bansal,  Tony Xia,  Jiacheng Liu,  Chunyuan Li,  Hannaneh Hajishirzi,  Hao Cheng,  Kai-Wei Chang,  Michel Galley,  Jianfeng Gao",
                "发布日期": "2023-10-27",
                "摘要": "  Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit\nimpressive problem-solving skills in many tasks and domains, but their ability\nin mathematical reasoning in visual contexts has not been systematically\nstudied. To bridge this gap, we present MathVista, a benchmark designed to\ncombine challenges from diverse mathematical and visual tasks. It consists of\n6,141 examples, derived from 28 existing multimodal datasets involving\nmathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and\nPaperQA). Completing these tasks requires fine-grained, deep visual\nunderstanding and compositional reasoning, which all state-of-the-art\nfoundation models find challenging. With MathVista, we have conducted a\ncomprehensive, quantitative evaluation of 12 prominent foundation models. The\nbest-performing GPT-4V model achieves an overall accuracy of 49.9%,\nsubstantially outperforming Bard, the second-best performer, by 15.1%. Our\nin-depth analysis reveals that the superiority of GPT-4V is mainly attributed\nto its enhanced visual perception and mathematical reasoning. However, GPT-4V\nstill falls short of human performance by 10.4%, as it often struggles to\nunderstand complex figures and perform rigorous reasoning. This significant gap\nunderscores the critical role that MathVista will play in the development of\ngeneral-purpose AI agents capable of tackling mathematically intensive and\nvisually rich real-world tasks. We further explore the new ability of\nself-verification, the application of self-consistency, and the interactive\nchatbot capabilities of GPT-4V, highlighting its promising potential for future\nresearch. The project is available at https://mathvista.github.io/.\n",
                "链接": "https://arxiv.org/abs/2310.02255"
            },
            {
                "文章ID": "115400",
                "标题": "GPT-4V(ision) as A Social Media Analysis Engine",
                "作者": " Hanjia Lyu,  Jinfa Huang,  Daoan Zhang,  Yongsheng Yu,  Xinyi Mou,  Jinsheng Pan,  Zhengyuan Yang,  Zhongyu Wei,  Jiebo Luo",
                "发布日期": "2023-11-14",
                "摘要": "  Recent research has offered insights into the extraordinary capabilities of\nLarge Multimodal Models (LMMs) in various general vision and language tasks.\nThere is growing interest in how LMMs perform in more specialized domains.\nSocial media content, inherently multimodal, blends text, images, videos, and\nsometimes audio. Understanding social multimedia content remains a challenging\nproblem for contemporary machine learning frameworks. In this paper, we explore\nGPT-4V(ision)'s capabilities for social multimedia analysis. We select five\nrepresentative tasks, including sentiment analysis, hate speech detection, fake\nnews identification, demographic inference, and political ideology detection,\nto evaluate GPT-4V. Our investigation begins with a preliminary quantitative\nanalysis for each task using existing benchmark datasets, followed by a careful\nreview of the results and a selection of qualitative samples that illustrate\nGPT-4V's potential in understanding multimodal social media content. GPT-4V\ndemonstrates remarkable efficacy in these tasks, showcasing strengths such as\njoint understanding of image-text pairs, contextual and cultural awareness, and\nextensive commonsense knowledge. Despite the overall impressive capacity of\nGPT-4V in the social media domain, there remain notable challenges. GPT-4V\nstruggles with tasks involving multilingual social multimedia comprehension and\nhas difficulties in generalizing to the latest trends in social media.\nAdditionally, it exhibits a tendency to generate erroneous information in the\ncontext of evolving celebrity and politician knowledge, reflecting the known\nhallucination problem. The insights gleaned from our findings underscore a\npromising future for LMMs in enhancing our comprehension of social media\ncontent and its users through the analysis of multimodal information.\n",
                "链接": "https://arxiv.org/abs/2311.07547"
            },
            {
                "文章ID": "121231",
                "标题": "Holistic Evaluation of GPT-4V for Biomedical Imaging",
                "作者": " Zhengliang Liu,  Hanqi Jiang,  Tianyang Zhong,  Zihao Wu,  Chong Ma,  Yiwei Li,  Xiaowei Yu,  Yutong Zhang,  Yi Pan,  Peng Shu,  Yanjun Lyu,  Lu Zhang,  Junjie Yao,  Peixin Dong,  Chao Cao,  Zhenxiang Xiao,  Jiaqi Wang,  Huan Zhao,  Shaochen Xu,  Yaonai Wei,  Jingyuan Chen,  Haixing Dai,  Peilong Wang,  Hao He,  Zewei Wang,  Xinyu Wang,  Xu Zhang,  Lin Zhao,  Yiheng Liu,  Kai Zhang,  Liheng Yan,  Lichao Sun,  Jun Liu,  Ning Qiang,  Bao Ge,  Xiaoyan Cai,  Shijie Zhao,  Xintao Hu,  Yixuan Yuan,  Gang Li,  Shu Zhang,  Xin Zhang,  Xi Jiang,  Tuo Zhang,  Dinggang Shen,  Quanzheng Li,  Wei Liu,  Xiang Li,  Dajiang Zhu,  Tianming Liu",
                "发布日期": "2023-12-12",
                "摘要": "  In this paper, we present a large-scale evaluation probing GPT-4V's\ncapabilities and limitations for biomedical image analysis. GPT-4V represents a\nbreakthrough in artificial general intelligence (AGI) for computer vision, with\napplications in the biomedical domain. We assess GPT-4V's performance across 16\nmedical imaging categories, including radiology, oncology, ophthalmology,\npathology, and more. Tasks include modality recognition, anatomy localization,\ndisease diagnosis, report generation, and lesion detection. The extensive\nexperiments provide insights into GPT-4V's strengths and weaknesses. Results\nshow GPT-4V's proficiency in modality and anatomy recognition but difficulty\nwith disease diagnosis and localization. GPT-4V excels at diagnostic report\ngeneration, indicating strong image captioning skills. While promising for\nbiomedical imaging AI, GPT-4V requires further enhancement and validation\nbefore clinical deployment. We emphasize responsible development and testing\nfor trustworthy integration of biomedical AGI. This rigorous evaluation of\nGPT-4V on diverse medical images advances understanding of multimodal large\nlanguage models (LLMs) and guides future work toward impactful healthcare\napplications.\n",
                "链接": "https://arxiv.org/abs/2312.05256"
            }
        ]
    },
    {
        "question": {
            "question": "查找使用BERT和RoBERTa进行多语言情感分析的最新论文，要求涵盖2022年以来的研究。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "15513",
                "标题": "Mono vs Multilingual BERT for Hate Speech Detection and Text\n  Classification: A Case Study in Marathi",
                "作者": " Abhishek Velankar,  Hrushikesh Patil,  Raviraj Joshi",
                "发布日期": "2022-11-15",
                "摘要": "  Transformers are the most eminent architectures used for a vast range of\nNatural Language Processing tasks. These models are pre-trained over a large\ntext corpus and are meant to serve state-of-the-art results over tasks like\ntext classification. In this work, we conduct a comparative study between\nmonolingual and multilingual BERT models. We focus on the Marathi language and\nevaluate the models on the datasets for hate speech detection, sentiment\nanalysis and simple text classification in Marathi. We use standard\nmultilingual models such as mBERT, indicBERT and xlm-RoBERTa and compare with\nMahaBERT, MahaALBERT and MahaRoBERTa, the monolingual models for Marathi. We\nfurther show that Marathi monolingual models outperform the multilingual BERT\nvariants on five different downstream fine-tuning experiments. We also evaluate\nsentence embeddings from these models by freezing the BERT encoder layers. We\nshow that monolingual MahaBERT based models provide rich representations as\ncompared to sentence embeddings from multi-lingual counterparts. However, we\nobserve that these embeddings are not generic enough and do not work well on\nout of domain social media datasets. We consider two Marathi hate speech\ndatasets L3Cube-MahaHate, HASOC-2021, a Marathi sentiment classification\ndataset L3Cube-MahaSent, and Marathi Headline, Articles classification\ndatasets.\n",
                "链接": "https://arxiv.org/abs/2204.08669"
            },
            {
                "文章ID": "83498",
                "标题": "Leverage Points in Modality Shifts: Comparing Language-only and\n  Multimodal Word Representations",
                "作者": " Aleksey Tikhonov,  Lisa Bylinina,  Denis Paperno",
                "发布日期": "2023-06-06",
                "摘要": "  Multimodal embeddings aim to enrich the semantic information in neural\nrepresentations of language compared to text-only models. While different\nembeddings exhibit different applicability and performance on downstream tasks,\nlittle is known about the systematic representation differences attributed to\nthe visual modality. Our paper compares word embeddings from three\nvision-and-language models (CLIP, OpenCLIP and Multilingual CLIP) and three\ntext-only models, with static (FastText) as well as contextual representations\n(multilingual BERT; XLM-RoBERTa). This is the first large-scale study of the\neffect of visual grounding on language representations, including 46 semantic\nparameters. We identify meaning properties and relations that characterize\nwords whose embeddings are most affected by the inclusion of visual modality in\nthe training data; that is, points where visual grounding turns out most\nimportant. We find that the effect of visual modality correlates most with\ndenotational semantic properties related to concreteness, but is also detected\nfor several specific semantic classes, as well as for valence, a\nsentiment-related connotational property of linguistic expressions.\n",
                "链接": "https://arxiv.org/abs/2306.02348"
            },
            {
                "文章ID": "49478",
                "标题": "L3Cube-MahaSBERT and HindSBERT: Sentence BERT Models and Benchmarking\n  BERT Sentence Representations for Hindi and Marathi",
                "作者": " Ananya Joshi,  Aditi Kajale,  Janhavi Gadre,  Samruddhi Deode,  Raviraj Joshi",
                "发布日期": "2022-11-23",
                "摘要": "  Sentence representation from vanilla BERT models does not work well on\nsentence similarity tasks. Sentence-BERT models specifically trained on STS or\nNLI datasets are shown to provide state-of-the-art performance. However,\nbuilding these models for low-resource languages is not straightforward due to\nthe lack of these specialized datasets. This work focuses on two low-resource\nIndian languages, Hindi and Marathi. We train sentence-BERT models for these\nlanguages using synthetic NLI and STS datasets prepared using machine\ntranslation. We show that the strategy of NLI pre-training followed by STSb\nfine-tuning is effective in generating high-performance sentence-similarity\nmodels for Hindi and Marathi. The vanilla BERT models trained using this simple\nstrategy outperform the multilingual LaBSE trained using a complex training\nstrategy. These models are evaluated on downstream text classification and\nsimilarity tasks. We evaluate these models on real text classification datasets\nto show embeddings obtained from synthetic data training are generalizable to\nreal datasets as well and thus represent an effective training strategy for\nlow-resource languages. We also provide a comparative analysis of sentence\nembeddings from fast text models, multilingual BERT models (mBERT, IndicBERT,\nxlm-RoBERTa, MuRIL), multilingual sentence embedding models (LASER, LaBSE), and\nmonolingual BERT models based on L3Cube-MahaBERT and HindBERT. We release\nL3Cube-MahaSBERT and HindSBERT, the state-of-the-art sentence-BERT models for\nMarathi and Hindi respectively. Our work also serves as a guide to building\nlow-resource sentence embedding models.\n",
                "链接": "https://arxiv.org/abs/2211.11187"
            },
            {
                "文章ID": "15984",
                "标题": "Multimodal Hate Speech Detection from Bengali Memes and Texts",
                "作者": " Md. Rezaul Karim,  Sumon Kanti Dey,  Tanhim Islam,  Md. Shajalal,  Bharathi Raja Chakravarthi",
                "发布日期": "2022-12-22",
                "摘要": "  Numerous machine learning (ML) and deep learning (DL)-based approaches have\nbeen proposed to utilize textual data from social media for anti-social\nbehavior analysis like cyberbullying, fake news detection, and identification\nof hate speech mainly for highly-resourced languages such as English. However,\ndespite having a lot of diversity and millions of native speakers, some\nlanguages like Bengali are under-resourced, which is due to a lack of\ncomputational resources for natural language processing (NLP). Similar to other\nlanguages, Bengali social media contents also include images along with texts\n(e.g., multimodal memes are posted by embedding short texts into images on\nFacebook). Therefore, only the textual data is not enough to judge them since\nimages might give extra context to make a proper judgement. This paper is about\nhate speech detection from multimodal Bengali memes and texts. We prepared the\nonly multimodal hate speech dataset for-a-kind of problem for Bengali, which we\nuse to train state-of-the-art neural architectures (e.g., Bi-LSTM/Conv-LSTM\nwith word embeddings, ConvNets + pre-trained language models, e.g., monolingual\nBangla BERT, multilingual BERT-cased/uncased, and XLM-RoBERTa) to jointly\nanalyze textual and visual information for hate speech detection. Conv-LSTM and\nXLM-RoBERTa models performed best for texts, yielding F1 scores of 0.78 and\n0.82, respectively. As of memes, ResNet-152 and DenseNet-161 models yield F1\nscores of 0.78 and 0.79, respectively. As for multimodal fusion, XLM-RoBERTa +\nDenseNet-161 performed the best, yielding an F1 score of 0.83. Our study\nsuggests that text modality is most useful for hate speech detection, while\nmemes are moderately useful.\n",
                "链接": "https://arxiv.org/abs/2204.10196"
            },
            {
                "文章ID": "79641",
                "标题": "Exploring Large Language Models for Classical Philology",
                "作者": " Frederick Riemenschneider,  Anette Frank",
                "发布日期": "2023-05-24",
                "摘要": "  Recent advances in NLP have led to the creation of powerful language models\nfor many languages including Ancient Greek and Latin. While prior work on\nClassical languages unanimously uses BERT, in this work we create four language\nmodels for Ancient Greek that vary along two dimensions to study their\nversatility for tasks of interest for Classical languages: we explore (i)\nencoder-only and encoder-decoder architectures using RoBERTa and T5 as strong\nmodel types, and create for each of them (ii) a monolingual Ancient Greek and a\nmultilingual instance that includes Latin and English. We evaluate all models\non morphological and syntactic tasks, including lemmatization, which\ndemonstrates the added value of T5's decoding abilities. We further define two\nprobing tasks to investigate the knowledge acquired by models pre-trained on\nClassical texts. Our experiments provide the first benchmarking analysis of\nexisting models of Ancient Greek. Results show that our models provide\nsignificant improvements over the SoTA. The systematic analysis of model types\ncan inform future research in designing language models for Classical\nlanguages, including the development of novel generative tasks. We make all our\nmodels available as community resources, along with a large curated\npre-training corpus for Ancient Greek, to support the creation of a larger,\ncomparable model zoo for Classical Philology. Our models and resources are\navailable at https://github.com/Heidelberg-NLP/ancient-language-models.\n",
                "链接": "https://arxiv.org/abs/2305.13698"
            },
            {
                "文章ID": "52169",
                "标题": "Video Games as a Corpus: Sentiment Analysis using Fallout New Vegas\n  Dialog",
                "作者": " Mika Hämäläinen,  Khalid Alnajjar,  Thierry Poibeau",
                "发布日期": "2022-12-06",
                "摘要": "  We present a method for extracting a multilingual sentiment annotated dialog\ndata set from Fallout New Vegas. The game developers have preannotated every\nline of dialog in the game in one of the 8 different sentiments: \\textit{anger,\ndisgust, fear, happy, neutral, pained, sad } and \\textit{surprised}. The game\nhas been translated into English, Spanish, German, French and Italian. We\nconduct experiments on multilingual, multilabel sentiment analysis on the\nextracted data set using multilingual BERT, XLMRoBERTa and language specific\nBERT models. In our experiments, multilingual BERT outperformed XLMRoBERTa for\nmost of the languages, also language specific models were slightly better than\nmultilingual BERT for most of the languages. The best overall accuracy was 54\\%\nand it was achieved by using multilingual BERT on Spanish data. The extracted\ndata set presents a challenging task for sentiment analysis. We have released\nthe data, including the testing and training splits, openly on Zenodo. The data\nset has been shuffled for copyright reasons.\n",
                "链接": "https://arxiv.org/abs/2212.02168"
            },
            {
                "文章ID": "87606",
                "标题": "L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset\n  and Transformer Models",
                "作者": " Aabha Pingle,  Aditya Vyawahare,  Isha Joshi,  Rahul Tangsali,  Raviraj Joshi",
                "发布日期": "2023-06-27",
                "摘要": "  The exploration of sentiment analysis in low-resource languages, such as\nMarathi, has been limited due to the availability of suitable datasets. In this\nwork, we present L3Cube-MahaSent-MD, a multi-domain Marathi sentiment analysis\ndataset, with four different domains - movie reviews, general tweets, TV show\nsubtitles, and political tweets. The dataset consists of around 60,000 manually\ntagged samples covering 3 distinct sentiments - positive, negative, and\nneutral. We create a sub-dataset for each domain comprising 15k samples. The\nMahaSent-MD is the first comprehensive multi-domain sentiment analysis dataset\nwithin the Indic sentiment landscape. We fine-tune different monolingual and\nmultilingual BERT models on these datasets and report the best accuracy with\nthe MahaBERT model. We also present an extensive in-domain and cross-domain\nanalysis thus highlighting the need for low-resource multi-domain datasets. The\ndata and models are available at https://github.com/l3cube-pune/MarathiNLP .\n",
                "链接": "https://arxiv.org/abs/2306.13888"
            },
            {
                "文章ID": "31048",
                "标题": "Enhancing Collaborative Filtering Recommender with Prompt-Based\n  Sentiment Analysis",
                "作者": " Elliot Dang,  Zheyuan Hu,  Tong Li",
                "发布日期": "2022-07-27",
                "摘要": "  Collaborative Filtering(CF) recommender is a crucial application in the\nonline market and ecommerce. However, CF recommender has been proven to suffer\nfrom persistent problems related to sparsity of the user rating that will\nfurther lead to a cold-start issue. Existing methods address the data sparsity\nissue by applying token-level sentiment analysis that translate text review\ninto sentiment scores as a complement of the user rating. In this paper, we\nattempt to optimize the sentiment analysis with advanced NLP models including\nBERT and RoBERTa, and experiment on whether the CF recommender has been further\nenhanced. We build the recommenders on the Amazon US Reviews dataset, and tune\nthe pretrained BERT and RoBERTa with the traditional fine-tuned paradigm as\nwell as the new prompt-based learning paradigm. Experimental result shows that\nthe recommender enhanced with the sentiment ratings predicted by the fine-tuned\nRoBERTa has the best performance, and achieved 30.7% overall gain by comparing\nMAP, NDCG and precision at K to the baseline recommender. Prompt-based learning\nparadigm, although superior to traditional fine-tune paradigm in pure sentiment\nanalysis, fail to further improve the CF recommender.\n",
                "链接": "https://arxiv.org/abs/2207.12883"
            },
            {
                "文章ID": "78993",
                "标题": "SEntFiN 1.0: Entity-Aware Sentiment Analysis for Financial News",
                "作者": " Ankur Sinha,  Satishwar Kedas,  Rishu Kumar,  Pekka Malo",
                "发布日期": "2023-05-23",
                "摘要": "  Fine-grained financial sentiment analysis on news headlines is a challenging\ntask requiring human-annotated datasets to achieve high performance. Limited\nstudies have tried to address the sentiment extraction task in a setting where\nmultiple entities are present in a news headline. In an effort to further\nresearch in this area, we make publicly available SEntFiN 1.0, a\nhuman-annotated dataset of 10,753 news headlines with entity-sentiment\nannotations, of which 2,847 headlines contain multiple entities, often with\nconflicting sentiments. We augment our dataset with a database of over 1,000\nfinancial entities and their various representations in news media amounting to\nover 5,000 phrases. We propose a framework that enables the extraction of\nentity-relevant sentiments using a feature-based approach rather than an\nexpression-based approach. For sentiment extraction, we utilize 12 different\nlearning schemes utilizing lexicon-based and pre-trained sentence\nrepresentations and five classification approaches. Our experiments indicate\nthat lexicon-based n-gram ensembles are above par with pre-trained word\nembedding schemes such as GloVe. Overall, RoBERTa and finBERT (domain-specific\nBERT) achieve the highest average accuracy of 94.29% and F1-score of 93.27%.\nFurther, using over 210,000 entity-sentiment predictions, we validate the\neconomic effect of sentiments on aggregate market movements over a long\nduration.\n",
                "链接": "https://arxiv.org/abs/2305.12257"
            },
            {
                "文章ID": "114320",
                "标题": "Modelling Sentiment Analysis: LLMs and data augmentation techniques",
                "作者": " Guillem Senabre Prades",
                "发布日期": "2023-11-08",
                "摘要": "  This paper provides different approaches for a binary sentiment\nclassification on a small training dataset. LLMs that provided state-of-the-art\nresults in sentiment analysis and similar domains are being used, such as BERT,\nRoBERTa and XLNet.\n",
                "链接": "https://arxiv.org/abs/2311.04139"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下关于大模型使用工具的安全性的论文。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "109885",
                "标题": "Safety-Gymnasium: A Unified Safe Reinforcement Learning Benchmark",
                "作者": " Jiaming Ji,  Borong Zhang,  Jiayi Zhou,  Xuehai Pan,  Weidong Huang,  Ruiyang Sun,  Yiran Geng,  Yifan Zhong,  Juntao Dai,  Yaodong Yang",
                "发布日期": "2023-11-08",
                "摘要": "  Artificial intelligence (AI) systems possess significant potential to drive\nsocietal progress. However, their deployment often faces obstacles due to\nsubstantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a\nsolution to optimize policies while simultaneously adhering to multiple\nconstraints, thereby addressing the challenge of integrating reinforcement\nlearning in safety-critical scenarios. In this paper, we present an environment\nsuite called Safety-Gymnasium, which encompasses safety-critical tasks in both\nsingle and multi-agent scenarios, accepting vector and vision-only input.\nAdditionally, we offer a library of algorithms named Safe Policy Optimization\n(SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive\nlibrary can serve as a validation tool for the research community. By\nintroducing this benchmark, we aim to facilitate the evaluation and comparison\nof safety performance, thus fostering the development of reinforcement learning\nfor safer, more reliable, and responsible real-world applications. The website\nof this project can be accessed at\nhttps://sites.google.com/view/safety-gymnasium.\n",
                "链接": "https://arxiv.org/abs/2310.12567"
            },
            {
                "文章ID": "105613",
                "标题": "All Languages Matter: On the Multilingual Safety of Large Language\n  Models",
                "作者": " Wenxuan Wang,  Zhaopeng Tu,  Chang Chen,  Youliang Yuan,  Jen-tse Huang,  Wenxiang Jiao,  Michael R. Lyu",
                "发布日期": "2023-10-03",
                "摘要": "  Safety lies at the core of developing and deploying large language models\n(LLMs). However, previous safety benchmarks only concern the safety in one\nlanguage, e.g. the majority language in the pretraining data such as English.\nIn this work, we build the first multilingual safety benchmark for LLMs,\nXSafety, in response to the global deployment of LLMs in practice. XSafety\ncovers 14 kinds of commonly used safety issues across 10 languages that span\nseveral language families. We utilize XSafety to empirically study the\nmultilingual safety for 4 widely-used LLMs, including both close-API and\nopen-source models. Experimental results show that all LLMs produce\nsignificantly more unsafe responses for non-English queries than English ones,\nindicating the necessity of developing safety alignment for non-English\nlanguages. In addition, we propose several simple and effective prompting\nmethods to improve the multilingual safety of ChatGPT by evoking safety\nknowledge and improving cross-lingual generalization of safety alignment. Our\nprompting method can significantly reduce the ratio of unsafe responses from\n19.1% to 9.7% for non-English queries. We release our data at\nhttps://github.com/Jarviswang94/Multilingual_safety_benchmark.\n",
                "链接": "https://arxiv.org/abs/2310.00905"
            },
            {
                "文章ID": "104712",
                "标题": "Identifying the Risks of LM Agents with an LM-Emulated Sandbox",
                "作者": " Yangjun Ruan,  Honghua Dong,  Andrew Wang,  Silviu Pitis,  Yongchao Zhou,  Jimmy Ba,  Yann Dubois,  Chris J. Maddison,  Tatsunori Hashimoto",
                "发布日期": "2023-09-28",
                "摘要": "  Recent advances in Language Model (LM) agents and tool use, exemplified by\napplications like ChatGPT Plugins, enable a rich set of capabilities but also\namplify potential risks - such as leaking private data or causing financial\nlosses. Identifying these risks is labor-intensive, necessitating implementing\nthe tools, manually setting up the environment for each test scenario, and\nfinding risky cases. As tools and agents become more complex, the high cost of\ntesting these agents will make it increasingly difficult to find high-stakes,\nlong-tailed risks. To address these challenges, we introduce ToolEmu: a\nframework that uses an LM to emulate tool execution and enables the testing of\nLM agents against a diverse range of tools and scenarios, without manual\ninstantiation. Alongside the emulator, we develop an LM-based automatic safety\nevaluator that examines agent failures and quantifies associated risks. We test\nboth the tool emulator and evaluator through human evaluation and find that\n68.8% of failures identified with ToolEmu would be valid real-world agent\nfailures. Using our curated initial benchmark consisting of 36 high-stakes\ntools and 144 test cases, we provide a quantitative risk analysis of current LM\nagents and identify numerous failures with potentially severe outcomes.\nNotably, even the safest LM agent exhibits such failures 23.9% of the time\naccording to our evaluator, underscoring the need to develop safer LM agents\nfor real-world deployment.\n",
                "链接": "https://arxiv.org/abs/2309.15817"
            },
            {
                "文章ID": "4874",
                "标题": "SAFER: Data-Efficient and Safe Reinforcement Learning via Skill\n  Acquisition",
                "作者": " Dylan Slack,  Yinlam Chow,  Bo Dai,  Nevan Wichers",
                "发布日期": "2022-07-04",
                "摘要": "  Methods that extract policy primitives from offline demonstrations using deep\ngenerative models have shown promise at accelerating reinforcement learning(RL)\nfor new tasks. Intuitively, these methods should also help to trainsafeRLagents\nbecause they enforce useful skills. However, we identify these techniques are\nnot well equipped for safe policy learning because they ignore negative\nexperiences(e.g., unsafe or unsuccessful), focusing only on positive\nexperiences, which harms their ability to generalize to new tasks safely.\nRather, we model the latentsafetycontextusing principled contrastive training\non an offline dataset of demonstrations from many tasks, including both\nnegative and positive experiences. Using this late variable, our RL framework,\nSAFEty skill pRiors (SAFER) extracts task-specific safe primitive skills to\nsafely and successfully generalize to new tasks. In the inference stage,\npolicies trained with SAFER learn to compose safe skills into successful\npolicies. We theoretically characterize why SAFER can enforce safe policy\nlearning and demonstrate its effectiveness on several complex safety-critical\nrobotic grasping tasks inspired by the game Operation, in which\nSAFERoutperforms state-of-the-art primitive learning methods in success and\nsafety.\n",
                "链接": "https://arxiv.org/abs/2202.04849"
            },
            {
                "文章ID": "102063",
                "标题": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language\n  Models that Follow Instructions",
                "作者": " Federico Bianchi,  Mirac Suzgun,  Giuseppe Attanasio,  Paul Röttger,  Dan Jurafsky,  Tatsunori Hashimoto,  James Zou",
                "发布日期": "2023-09-26",
                "摘要": "  Training large language models to follow instructions makes them perform\nbetter on a wide range of tasks, generally becoming more helpful. However, a\nperfectly helpful model will follow even the most malicious instructions and\nreadily generate harmful content. In this paper, we raise concerns over the\nsafety of models that only emphasize helpfulness, not safety, in their\ninstruction-tuning. We show that several popular instruction-tuned models are\nhighly unsafe. Moreover, we show that adding just 3% safety examples (a few\nhundred demonstrations) in the training set when fine-tuning a model like LLaMA\ncan substantially improve their safety. Our safety-tuning does not make models\nsignificantly less capable or helpful as measured by standard benchmarks.\nHowever, we do find a behavior of exaggerated safety, where too much\nsafety-tuning makes models refuse to respond to reasonable prompts that\nsuperficially resemble unsafe ones. Our study sheds light on trade-offs in\ntraining LLMs to follow instructions and exhibit safe behavior.\n",
                "链接": "https://arxiv.org/abs/2309.07875"
            },
            {
                "文章ID": "73266",
                "标题": "Supporting Human-AI Collaboration in Auditing LLMs with LLMs",
                "作者": " Charvi Rastogi,  Marco Tulio Ribeiro,  Nicholas King,  Harsha Nori,  Saleema Amershi",
                "发布日期": "2023-12-01",
                "摘要": "  Large language models are becoming increasingly pervasive and ubiquitous in\nsociety via deployment in sociotechnical systems. Yet these language models, be\nit for classification or generation, have been shown to be biased and behave\nirresponsibly, causing harm to people at scale. It is crucial to audit these\nlanguage models rigorously. Existing auditing tools leverage either or both\nhumans and AI to find failures. In this work, we draw upon literature in\nhuman-AI collaboration and sensemaking, and conduct interviews with research\nexperts in safe and fair AI, to build upon the auditing tool: AdaTest (Ribeiro\nand Lundberg, 2022), which is powered by a generative large language model\n(LLM). Through the design process we highlight the importance of sensemaking\nand human-AI communication to leverage complementary strengths of humans and\ngenerative models in collaborative auditing. To evaluate the effectiveness of\nthe augmented tool, AdaTest++, we conduct user studies with participants\nauditing two commercial language models: OpenAI's GPT-3 and Azure's sentiment\nanalysis model. Qualitative analysis shows that AdaTest++ effectively leverages\nhuman strengths such as schematization, hypothesis formation and testing.\nFurther, with our tool, participants identified a variety of failures modes,\ncovering 26 different topics over 2 tasks, that have been shown before in\nformal audits and also those previously under-reported.\n",
                "链接": "https://arxiv.org/abs/2304.09991"
            },
            {
                "文章ID": "106345",
                "标题": "Shadow Alignment: The Ease of Subverting Safely-Aligned Language Models",
                "作者": " Xianjun Yang,  Xiao Wang,  Qi Zhang,  Linda Petzold,  William Yang Wang,  Xun Zhao,  Dahua Lin",
                "发布日期": "2023-10-05",
                "摘要": "  Warning: This paper contains examples of harmful language, and reader\ndiscretion is recommended. The increasing open release of powerful large\nlanguage models (LLMs) has facilitated the development of downstream\napplications by reducing the essential cost of data annotation and computation.\nTo ensure AI safety, extensive safety-alignment measures have been conducted to\narmor these models against malicious use (primarily hard prompt attack).\nHowever, beneath the seemingly resilient facade of the armor, there might lurk\na shadow. By simply tuning on 100 malicious examples with 1 GPU hour, these\nsafely aligned LLMs can be easily subverted to generate harmful content.\nFormally, we term a new attack as Shadow Alignment: utilizing a tiny amount of\ndata can elicit safely-aligned models to adapt to harmful tasks without\nsacrificing model helpfulness. Remarkably, the subverted models retain their\ncapability to respond appropriately to regular inquiries. Experiments across 8\nmodels released by 5 different organizations (LLaMa-2, Falcon, InternLM,\nBaiChuan2, Vicuna) demonstrate the effectiveness of shadow alignment attack.\nBesides, the single-turn English-only attack successfully transfers to\nmulti-turn dialogue and other languages. This study serves as a clarion call\nfor a collective effort to overhaul and fortify the safety of open-source LLMs\nagainst malicious attackers.\n",
                "链接": "https://arxiv.org/abs/2310.02949"
            },
            {
                "文章ID": "84645",
                "标题": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000\n  Simulated Cases",
                "作者": " Qiaoyu Tang,  Ziliang Deng,  Hongyu Lin,  Xianpei Han,  Qiao Liang,  Boxi Cao,  Le Sun",
                "发布日期": "2023-09-08",
                "摘要": "  Enabling large language models to utilize real-world tools effectively is\ncrucial for achieving embodied intelligence. Existing approaches to tool\nlearning have either primarily relied on extremely large language models, such\nas GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or\nutilized supervised learning to train limited scopes of tools on compact\nmodels. However, it remains uncertain whether smaller language models can\nachieve generalized tool-use abilities without tool-specific training. To\naddress this question, this paper introduces ToolAlpaca, a novel framework\ndesigned to automatically generate a diverse tool-use corpus and learn\ngeneralized tool-use abilities on compact language models with minimal human\nintervention. Specifically, ToolAlpaca first automatically creates a highly\ndiversified tool-use corpus by building a multi-agent simulation environment.\nThe corpus contains 3938 tool-use instances from more than 400 real-world tool\nAPIs spanning 50 distinct categories. Subsequently, the constructed corpus is\nemployed to fine-tune compact language models, resulting in two models, namely\nToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the\nability of these models to utilize previously unseen tools without specific\ntraining. Experimental results demonstrate that ToolAlpaca achieves effective\ngeneralized tool-use capabilities comparable to those of extremely large\nlanguage models like GPT-3.5, demonstrating that learning generalized tool-use\nability is feasible for compact language models.\n",
                "链接": "https://arxiv.org/abs/2306.05301"
            },
            {
                "文章ID": "106425",
                "标题": "MetaTool Benchmark for Large Language Models: Deciding Whether to Use\n  Tools and Which to Use",
                "作者": " Yue Huang,  Jiawen Shi,  Yuan Li,  Chenrui Fan,  Siyuan Wu,  Qihui Zhang,  Yixin Liu,  Pan Zhou,  Yao Wan,  Neil Zhenqiang Gong,  Lichao Sun",
                "发布日期": "2023-10-25",
                "摘要": "  Large language models (LLMs) have garnered significant attention due to their\nimpressive natural language processing (NLP) capabilities. Recently, many\nstudies have focused on the tool utilization ability of LLMs. They primarily\ninvestigated how LLMs effectively collaborate with given specific tools.\nHowever, in scenarios where LLMs serve as intelligent agents, as seen in\napplications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate\ndecision-making processes that involve deciding whether to employ a tool and\nselecting the most suitable tool(s) from a collection of available tools to\nfulfill user requests. Therefore, in this paper, we introduce MetaTool, a\nbenchmark designed to evaluate whether LLMs have tool usage awareness and can\ncorrectly choose tools. Specifically, we create a dataset called ToolE within\nthe benchmark. This dataset contains various types of user queries in the form\nof prompts that trigger LLMs to use tools, including both single-tool and\nmulti-tool scenarios. Subsequently, we set the tasks for both tool usage\nawareness and tool selection. We define four subtasks from different\nperspectives in tool selection, including tool selection with similar choices,\ntool selection in specific scenarios, tool selection with possible reliability\nissues, and multi-tool selection. We conduct experiments involving nine popular\nLLMs and find that the majority of them still struggle to effectively select\ntools, highlighting the existing gaps between LLMs and genuine intelligent\nagents. However, through the error analysis, we found there is still\nsignificant room for improvement. Finally, we conclude with insights for tool\ndevelopers that follow ChatGPT to provide detailed descriptions that can\nenhance the tool selection performance of LLMs.\n",
                "链接": "https://arxiv.org/abs/2310.03128"
            },
            {
                "文章ID": "59564",
                "标题": "Conditioning Predictive Models: Risks and Strategies",
                "作者": " Evan Hubinger,  Adam Jermyn,  Johannes Treutlein,  Rubi Hudson,  Kate Woolverton",
                "发布日期": "2023-02-07",
                "摘要": "  Our intention is to provide a definitive reference on what it would take to\nsafely make use of generative/predictive models in the absence of a solution to\nthe Eliciting Latent Knowledge problem. Furthermore, we believe that large\nlanguage models can be understood as such predictive models of the world, and\nthat such a conceptualization raises significant opportunities for their safe\nyet powerful use via carefully conditioning them to predict desirable outputs.\nUnfortunately, such approaches also raise a variety of potentially fatal safety\nproblems, particularly surrounding situations where predictive models predict\nthe output of other AI systems, potentially unbeknownst to us. There are\nnumerous potential solutions to such problems, however, primarily via carefully\nconditioning models to predict the things we want (e.g. humans) rather than the\nthings we don't (e.g. malign AIs). Furthermore, due to the simplicity of the\nprediction objective, we believe that predictive models present the easiest\ninner alignment problem that we are aware of. As a result, we think that\nconditioning approaches for predictive models represent the safest known way of\neliciting human-level and slightly superhuman capabilities from large language\nmodels and other similar future models.\n",
                "链接": "https://arxiv.org/abs/2302.00805"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下对使用GPT生成数据集的训练步骤进行改进的论文。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "69918",
                "标题": "Self-Refine: Iterative Refinement with Self-Feedback",
                "作者": " Aman Madaan,  Niket Tandon,  Prakhar Gupta,  Skyler Hallinan,  Luyu Gao,  Sarah Wiegreffe,  Uri Alon,  Nouha Dziri,  Shrimai Prabhumoye,  Yiming Yang,  Shashank Gupta,  Bodhisattwa Prasad Majumder,  Katherine Hermann,  Sean Welleck,  Amir Yazdanbakhsh,  Peter Clark",
                "发布日期": "2023-05-29",
                "摘要": "  Like humans, large language models (LLMs) do not always generate the best\noutput on their first try. Motivated by how humans refine their written text,\nwe introduce Self-Refine, an approach for improving initial outputs from LLMs\nthrough iterative feedback and refinement. The main idea is to generate an\ninitial output using an LLMs; then, the same LLMs provides feedback for its\noutput and uses it to refine itself, iteratively. Self-Refine does not require\nany supervised training data, additional training, or reinforcement learning,\nand instead uses a single LLM as the generator, refiner, and feedback provider.\nWe evaluate Self-Refine across 7 diverse tasks, ranging from dialog response\ngeneration to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT,\nand GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine\nare preferred by humans and automatic metrics over those generated with the\nsame LLM using conventional one-step generation, improving by ~20% absolute on\naverage in task performance. Our work demonstrates that even state-of-the-art\nLLMs like GPT-4 can be further improved at test time using our simple,\nstandalone approach.\n",
                "链接": "https://arxiv.org/abs/2303.17651"
            },
            {
                "文章ID": "1797",
                "标题": "Improving Neural Machine Translation by Denoising Training",
                "作者": " Liang Ding,  Keqin Peng,  Dacheng Tao",
                "发布日期": "2022-01-21",
                "摘要": "  We present a simple and effective pretraining strategy {D}en{o}ising\n{T}raining DoT for neural machine translation. Specifically, we update the\nmodel parameters with source- and target-side denoising tasks at the early\nstage and then tune the model normally. Notably, our approach does not increase\nany parameters or training steps, requiring the parallel data merely.\nExperiments show that DoT consistently improves the neural machine translation\nperformance across 12 bilingual and 16 multilingual directions (data size\nranges from 80K to 20M). In addition, we show that DoT can complement existing\ndata manipulation strategies, i.e. curriculum learning, knowledge distillation,\ndata diversification, bidirectional training, and back-translation.\nEncouragingly, we found that DoT outperforms costly pretrained model mBART in\nhigh-resource settings. Analyses show DoT is a novel in-domain cross-lingual\npretraining strategy and could offer further improvements with task-relevant\nself-supervisions.\n",
                "链接": "https://arxiv.org/abs/2201.07365"
            },
            {
                "文章ID": "2429",
                "标题": "Generative Planning for Temporally Coordinated Exploration in\n  Reinforcement Learning",
                "作者": " Haichao Zhang,  Wei Xu,  Haonan Yu",
                "发布日期": "2022-02-07",
                "摘要": "  Standard model-free reinforcement learning algorithms optimize a policy that\ngenerates the action to be taken in the current time step in order to maximize\nexpected future return. While flexible, it faces difficulties arising from the\ninefficient exploration due to its single step nature. In this work, we present\nGenerative Planning method (GPM), which can generate actions not only for the\ncurrent step, but also for a number of future steps (thus termed as generative\nplanning). This brings several benefits to GPM. Firstly, since GPM is trained\nby maximizing value, the plans generated from it can be regarded as intentional\naction sequences for reaching high value regions. GPM can therefore leverage\nits generated multi-step plans for temporally coordinated exploration towards\nhigh value regions, which is potentially more effective than a sequence of\nactions generated by perturbing each action at single step level, whose\nconsistent movement decays exponentially with the number of exploration steps.\nSecondly, starting from a crude initial plan generator, GPM can refine it to be\nadaptive to the task, which, in return, benefits future explorations. This is\npotentially more effective than commonly used action-repeat strategy, which is\nnon-adaptive in its form of plans. Additionally, since the multi-step plan can\nbe interpreted as the intent of the agent from now to a span of time period\ninto the future, it offers a more informative and intuitive signal for\ninterpretation. Experiments are conducted on several benchmark environments and\nthe results demonstrated its effectiveness compared with several baseline\nmethods.\n",
                "链接": "https://arxiv.org/abs/2201.09765"
            },
            {
                "文章ID": "45055",
                "标题": "On Robust Incremental Learning over Many Multilingual Steps",
                "作者": " Karan Praharaj,  Irina Matveeva",
                "发布日期": "2022-10-27",
                "摘要": "  Recent work in incremental learning has introduced diverse approaches to\ntackle catastrophic forgetting from data augmentation to optimized training\nregimes. However, most of them focus on very few training steps. We propose a\nmethod for robust incremental learning over dozens of fine-tuning steps using\ndata from a variety of languages. We show that a combination of\ndata-augmentation and an optimized training regime allows us to continue\nimproving the model even for as many as fifty training steps. Crucially, our\naugmentation strategy does not require retaining access to previous training\ndata and is suitable in scenarios with privacy constraints.\n",
                "链接": "https://arxiv.org/abs/2210.14307"
            },
            {
                "文章ID": "75334",
                "标题": "MTrainS: Improving DLRM training efficiency using heterogeneous memories",
                "作者": " Hiwot Tadese Kassa,  Paul Johnson,  Jason Akers,  Mrinmoy Ghosh,  Andrew Tulloch,  Dheevatsa Mudigere,  Jongsoo Park,  Xing Liu,  Ronald Dreslinski,  Ehsan K. Ardestani",
                "发布日期": "2023-05-03",
                "摘要": "  Recommendation models are very large, requiring terabytes (TB) of memory\nduring training. In pursuit of better quality, the model size and complexity\ngrow over time, which requires additional training data to avoid overfitting.\nThis model growth demands a large number of resources in data centers. Hence,\ntraining efficiency is becoming considerably more important to keep the data\ncenter power demand manageable. In Deep Learning Recommendation Models (DLRM),\nsparse features capturing categorical inputs through embedding tables are the\nmajor contributors to model size and require high memory bandwidth. In this\npaper, we study the bandwidth requirement and locality of embedding tables in\nreal-world deployed models. We observe that the bandwidth requirement is not\nuniform across different tables and that embedding tables show high temporal\nlocality. We then design MTrainS, which leverages heterogeneous memory,\nincluding byte and block addressable Storage Class Memory for DLRM\nhierarchically. MTrainS allows for higher memory capacity per node and\nincreases training efficiency by lowering the need to scale out to multiple\nhosts in memory capacity bound use cases. By optimizing the platform memory\nhierarchy, we reduce the number of nodes for training by 4-8X, saving power and\ncost of training while meeting our target training performance.\n",
                "链接": "https://arxiv.org/abs/2305.01515"
            },
            {
                "文章ID": "56440",
                "标题": "Automatic Generation of German Drama Texts Using Fine Tuned GPT-2 Models",
                "作者": " Mariam Bangura,  Kristina Barabashova,  Anna Karnysheva,  Sarah Semczuk,  Yifan Wang",
                "发布日期": "2023-01-11",
                "摘要": "  This study is devoted to the automatic generation of German drama texts. We\nsuggest an approach consisting of two key steps: fine-tuning a GPT-2 model (the\noutline model) to generate outlines of scenes based on keywords and fine-tuning\na second model (the generation model) to generate scenes from the scene\noutline. The input for the neural model comprises two datasets: the German\nDrama Corpus (GerDraCor) and German Text Archive (Deutsches Textarchiv or DTA).\nIn order to estimate the effectiveness of the proposed method, our models are\ncompared with baseline GPT-2 models. Our models perform well according to\nautomatic quantitative evaluation, but, conversely, manual qualitative analysis\nreveals a poor quality of generated texts. This may be due to the quality of\nthe dataset or training inputs.\n",
                "链接": "https://arxiv.org/abs/2301.03119"
            },
            {
                "文章ID": "35504",
                "标题": "Generating Intermediate Steps for NLI with Next-Step Supervision",
                "作者": " Deepanway Ghosal,  Somak Aditya,  Monojit Choudhury",
                "发布日期": "2022-09-01",
                "摘要": "  The Natural Language Inference (NLI) task often requires reasoning over\nmultiple steps to reach the conclusion. While the necessity of generating such\nintermediate steps (instead of a summary explanation) has gained popular\nsupport, it is unclear how to generate such steps without complete end-to-end\nsupervision and how such generated steps can be further utilized. In this work,\nwe train a sequence-to-sequence model to generate only the next step given an\nNLI premise and hypothesis pair (and previous steps); then enhance it with\nexternal knowledge and symbolic search to generate intermediate steps with only\nnext-step supervision. We show the correctness of such generated steps through\nautomated and human verification. Furthermore, we show that such generated\nsteps can help improve end-to-end NLI task performance using simple data\naugmentation strategies, across multiple public NLI datasets.\n",
                "链接": "https://arxiv.org/abs/2208.14641"
            },
            {
                "文章ID": "26262",
                "标题": "InfoAT: Improving Adversarial Training Using the Information Bottleneck\n  Principle",
                "作者": " Mengting Xu,  Tao Zhang,  Zhongnian Li,  Daoqiang Zhang",
                "发布日期": "2022-06-27",
                "摘要": "  Adversarial training (AT) has shown excellent high performance in defending\nagainst adversarial examples. Recent studies demonstrate that examples are not\nequally important to the final robustness of models during AT, that is, the\nso-called hard examples that can be attacked easily exhibit more influence than\nrobust examples on the final robustness. Therefore, guaranteeing the robustness\nof hard examples is crucial for improving the final robustness of the model.\nHowever, defining effective heuristics to search for hard examples is still\ndifficult. In this article, inspired by the information bottleneck (IB)\nprinciple, we uncover that an example with high mutual information of the input\nand its associated latent representation is more likely to be attacked. Based\non this observation, we propose a novel and effective adversarial training\nmethod (InfoAT). InfoAT is encouraged to find examples with high mutual\ninformation and exploit them efficiently to improve the final robustness of\nmodels. Experimental results show that InfoAT achieves the best robustness\namong different datasets and models in comparison with several state-of-the-art\nmethods.\n",
                "链接": "https://arxiv.org/abs/2206.12292"
            },
            {
                "文章ID": "79980",
                "标题": "Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with\n  Customized Exercise Generation",
                "作者": " Zhenwen Liang,  Wenhao Yu,  Tanmay Rajpurohit,  Peter Clark,  Xiangliang Zhang,  Ashwin Kaylan",
                "发布日期": "2023-05-25",
                "摘要": "  In this paper, we present a novel approach for distilling math word problem\nsolving capabilities from large language models (LLMs) into smaller, more\nefficient student models. Our approach is designed to consider the student\nmodel's weaknesses and foster a tailored learning experience by generating\ntargeted exercises aligned with educational science principles, such as\nknowledge tracing and personalized learning. Concretely, we let GPT-3 be a math\ntutor and run two steps iteratively: 1) assessing the student model's current\nlearning status on a GPT-generated exercise book, and 2) improving the student\nmodel by training it with tailored exercise samples generated by GPT-3.\nExperimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and\nPaLM) in accuracy across three distinct benchmarks while employing\nsignificantly fewer parameters. Furthermore, we provide a comprehensive\nanalysis of the various components within our methodology to substantiate their\nefficacy.\n",
                "链接": "https://arxiv.org/abs/2305.14386"
            },
            {
                "文章ID": "70174",
                "标题": "Improving Fast Adversarial Training with Prior-Guided Knowledge",
                "作者": " Xiaojun Jia,  Yong Zhang,  Xingxing Wei,  Baoyuan Wu,  Ke Ma,  Jue Wang,  Xiaochun Cao",
                "发布日期": "2023-04-07",
                "摘要": "  Fast adversarial training (FAT) is an efficient method to improve robustness.\nHowever, the original FAT suffers from catastrophic overfitting, which\ndramatically and suddenly reduces robustness after a few training epochs.\nAlthough various FAT variants have been proposed to prevent overfitting, they\nrequire high training costs. In this paper, we investigate the relationship\nbetween adversarial example quality and catastrophic overfitting by comparing\nthe training processes of standard adversarial training and FAT. We find that\ncatastrophic overfitting occurs when the attack success rate of adversarial\nexamples becomes worse. Based on this observation, we propose a positive\nprior-guided adversarial initialization to prevent overfitting by improving\nadversarial example quality without extra training costs. This initialization\nis generated by using high-quality adversarial perturbations from the\nhistorical training process. We provide theoretical analysis for the proposed\ninitialization and propose a prior-guided regularization method that boosts the\nsmoothness of the loss function. Additionally, we design a prior-guided\nensemble FAT method that averages the different model weights of historical\nmodels using different decay rates. Our proposed method, called FGSM-PGK,\nassembles the prior-guided knowledge, i.e., the prior-guided initialization and\nmodel weights, acquired during the historical training process. Evaluations of\nfour datasets demonstrate the superiority of the proposed method.\n",
                "链接": "https://arxiv.org/abs/2304.00202"
            }
        ]
    },
    {
        "question": {
            "question": "请找到使用自蒸馏加强目标检测性能的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "89041",
                "标题": "Efficient Visual Fault Detection for Freight Train Braking System via\n  Heterogeneous Self Distillation in the Wild",
                "作者": " Yang Zhang,  Huilin Pan,  Yang Zhou,  Mingying Li,  Guodong Sun",
                "发布日期": "2023-07-04",
                "摘要": "  Efficient visual fault detection of freight trains is a critical part of\nensuring the safe operation of railways under the restricted hardware\nenvironment. Although deep learning-based approaches have excelled in object\ndetection, the efficiency of freight train fault detection is still\ninsufficient to apply in real-world engineering. This paper proposes a\nheterogeneous self-distillation framework to ensure detection accuracy and\nspeed while satisfying low resource requirements. The privileged information in\nthe output feature knowledge can be transferred from the teacher to the student\nmodel through distillation to boost performance. We first adopt a lightweight\nbackbone to extract features and generate a new heterogeneous knowledge neck.\nSuch neck models positional information and long-range dependencies among\nchannels through parallel encoding to optimize feature extraction capabilities.\nThen, we utilize the general distribution to obtain more credible and accurate\nbounding box estimates. Finally, we employ a novel loss function that makes the\nnetwork easily concentrate on values near the label to improve learning\nefficiency. Experiments on four fault datasets reveal that our framework can\nachieve over 37 frames per second and maintain the highest accuracy in\ncomparison with traditional distillation approaches. Moreover, compared to\nstate-of-the-art methods, our framework demonstrates more competitive\nperformance with lower memory usage and the smallest model size.\n",
                "链接": "https://arxiv.org/abs/2307.00701"
            },
            {
                "文章ID": "107741",
                "标题": "V2X-AHD:Vehicle-to-Everything Cooperation Perception via Asymmetric\n  Heterogenous Distillation Network",
                "作者": " Caizhen He,  Hai Wang,  Long Chen,  Tong Luo,  Yingfeng Cai",
                "发布日期": "2023-10-11",
                "摘要": "  Object detection is the central issue of intelligent traffic systems, and\nrecent advancements in single-vehicle lidar-based 3D detection indicate that it\ncan provide accurate position information for intelligent agents to make\ndecisions and plan. Compared with single-vehicle perception, multi-view\nvehicle-road cooperation perception has fundamental advantages, such as the\nelimination of blind spots and a broader range of perception, and has become a\nresearch hotspot. However, the current perception of cooperation focuses on\nimproving the complexity of fusion while ignoring the fundamental problems\ncaused by the absence of single-view outlines. We propose a multi-view\nvehicle-road cooperation perception system, vehicle-to-everything cooperative\nperception (V2X-AHD), in order to enhance the identification capability,\nparticularly for predicting the vehicle's shape. At first, we propose an\nasymmetric heterogeneous distillation network fed with different training data\nto improve the accuracy of contour recognition, with multi-view teacher\nfeatures transferring to single-view student features. While the point cloud\ndata are sparse, we propose Spara Pillar, a spare convolutional-based plug-in\nfeature extraction backbone, to reduce the number of parameters and improve and\nenhance feature extraction capabilities. Moreover, we leverage the multi-head\nself-attention (MSA) to fuse the single-view feature, and the lightweight\ndesign makes the fusion feature a smooth expression. The results of applying\nour algorithm to the massive open dataset V2Xset demonstrate that our method\nachieves the state-of-the-art result. The V2X-AHD can effectively improve the\naccuracy of 3D object detection and reduce the number of network parameters,\naccording to this study, which serves as a benchmark for cooperative\nperception. The code for this article is available at\nhttps://github.com/feeling0414-lab/V2X-AHD.\n",
                "链接": "https://arxiv.org/abs/2310.06603"
            },
            {
                "文章ID": "65588",
                "标题": "Smooth and Stepwise Self-Distillation for Object Detection",
                "作者": " Jieren Deng,  Xin Zhou,  Hao Tian,  Zhihong Pan,  Derek Aguiar",
                "发布日期": "2023-03-10",
                "摘要": "  Distilling the structured information captured in feature maps has\ncontributed to improved results for object detection tasks, but requires\ncareful selection of baseline architectures and substantial pre-training.\nSelf-distillation addresses these limitations and has recently achieved\nstate-of-the-art performance for object detection despite making several\nsimplifying architectural assumptions. Building on this work, we propose Smooth\nand Stepwise Self-Distillation (SSSD) for object detection. Our SSSD\narchitecture forms an implicit teacher from object labels and a feature pyramid\nnetwork backbone to distill label-annotated feature maps using Jensen-Shannon\ndistance, which is smoother than distillation losses used in prior work. We\nadditionally add a distillation coefficient that is adaptively configured based\non the learning rate. We extensively benchmark SSSD against a baseline and two\nstate-of-the-art object detector architectures on the COCO dataset by varying\nthe coefficients and backbone and detector networks. We demonstrate that SSSD\nachieves higher average precision in most experimental settings, is robust to a\nwide range of coefficients, and benefits from our stepwise distillation\nprocedure.\n",
                "链接": "https://arxiv.org/abs/2303.05015"
            },
            {
                "文章ID": "68022",
                "标题": "Efficient Feature Distillation for Zero-shot Annotation Object Detection",
                "作者": " Zhuoming Liu,  Xuefeng Hu,  Ram Nevatia",
                "发布日期": "2023-11-03",
                "摘要": "  We propose a new setting for detecting unseen objects called Zero-shot\nAnnotation object Detection (ZAD). It expands the zero-shot object detection\nsetting by allowing the novel objects to exist in the training images and\nrestricts the additional information the detector uses to novel category names.\nRecently, to detect unseen objects, large-scale vision-language models (e.g.,\nCLIP) are leveraged by different methods. The distillation-based methods have\ngood overall performance but suffer from a long training schedule caused by two\nfactors. First, existing work creates distillation regions biased to the base\ncategories, which limits the distillation of novel category information.\nSecond, directly using the raw feature from CLIP for distillation neglects the\ndomain gap between the training data of CLIP and the detection datasets, which\nmakes it difficult to learn the mapping from the image region to the\nvision-language feature space. To solve these problems, we propose Efficient\nfeature distillation for Zero-shot Annotation object Detection (EZAD). Firstly,\nEZAD adapts the CLIP's feature space to the target detection domain by\nre-normalizing CLIP; Secondly, EZAD uses CLIP to generate distillation\nproposals with potential novel category names to avoid the distillation being\noverly biased toward the base categories. Finally, EZAD takes advantage of\nsemantic meaning for regression to further improve the model performance. As a\nresult, EZAD outperforms the previous distillation-based methods in COCO by 4%\nwith a much shorter training schedule and achieves a 3% improvement on the LVIS\ndataset. Our code is available at https://github.com/dragonlzm/EZAD\n",
                "链接": "https://arxiv.org/abs/2303.12145"
            },
            {
                "文章ID": "17577",
                "标题": "Cross Domain Object Detection by Target-Perceived Dual Branch\n  Distillation",
                "作者": " Mengzhe He,  Yali Wang,  Jiaxi Wu,  Yiru Wang,  Hanqing Li,  Bo Li,  Weihao Gan,  Wei Wu,  Yu Qiao",
                "发布日期": "2022-05-04",
                "摘要": "  Cross domain object detection is a realistic and challenging task in the\nwild. It suffers from performance degradation due to large shift of data\ndistributions and lack of instance-level annotations in the target domain.\nExisting approaches mainly focus on either of these two difficulties, even\nthough they are closely coupled in cross domain object detection. To solve this\nproblem, we propose a novel Target-perceived Dual-branch Distillation (TDD)\nframework. By integrating detection branches of both source and target domains\nin a unified teacher-student learning scheme, it can reduce domain shift and\ngenerate reliable supervision effectively. In particular, we first introduce a\ndistinct Target Proposal Perceiver between two domains. It can adaptively\nenhance source detector to perceive objects in a target image, by leveraging\ntarget proposal contexts from iterative cross-attention. Afterwards, we design\na concise Dual Branch Self Distillation strategy for model training, which can\nprogressively integrate complementary object knowledge from different domains\nvia self-distillation in two branches. Finally, we conduct extensive\nexperiments on a number of widely-used scenarios in cross domain object\ndetection. The results show that our TDD significantly outperforms the\nstate-of-the-art methods on all the benchmarks. Our code and model will be\navailable at https://github.com/Feobi1999/TDD.\n",
                "链接": "https://arxiv.org/abs/2205.01291"
            },
            {
                "文章ID": "92560",
                "标题": "Spatial Self-Distillation for Object Detection with Inaccurate Bounding\n  Boxes",
                "作者": " Di Wu,  Pengfei Chen,  Xuehui Yu,  Guorong Li,  Zhenjun Han,  Jianbin Jiao",
                "发布日期": "2023-08-16",
                "摘要": "  Object detection via inaccurate bounding boxes supervision has boosted a\nbroad interest due to the expensive high-quality annotation data or the\noccasional inevitability of low annotation quality (\\eg tiny objects). The\nprevious works usually utilize multiple instance learning (MIL), which highly\ndepends on category information, to select and refine a low-quality box. Those\nmethods suffer from object drift, group prediction and part domination problems\nwithout exploring spatial information. In this paper, we heuristically propose\na \\textbf{Spatial Self-Distillation based Object Detector (SSD-Det)} to mine\nspatial information to refine the inaccurate box in a self-distillation\nfashion. SSD-Det utilizes a Spatial Position Self-Distillation \\textbf{(SPSD)}\nmodule to exploit spatial information and an interactive structure to combine\nspatial information and category information, thus constructing a high-quality\nproposal bag. To further improve the selection procedure, a Spatial Identity\nSelf-Distillation \\textbf{(SISD)} module is introduced in SSD-Det to obtain\nspatial confidence to help select the best proposals. Experiments on MS-COCO\nand VOC datasets with noisy box annotation verify our method's effectiveness\nand achieve state-of-the-art performance. The code is available at\nhttps://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det.\n",
                "链接": "https://arxiv.org/abs/2307.12101"
            },
            {
                "文章ID": "6788",
                "标题": "Self-Supervised Transformers for Unsupervised Object Discovery using\n  Normalized Cut",
                "作者": "M-PSI  Yangtao Wang, LIGM  Xi Shen, MIT CSAIL  Shell Hu, MIT CSAIL  Yuan Yuan, M-PSI  James Crowley, M-PSI  Dominique Vaufreydaz",
                "发布日期": "2022-03-25",
                "摘要": "  Transformers trained with self-supervised learning using self-distillation\nloss (DINO) have been shown to produce attention maps that highlight salient\nforeground objects. In this paper, we demonstrate a graph-based approach that\nuses the self-supervised transformer features to discover an object from an\nimage. Visual tokens are viewed as nodes in a weighted graph with edges\nrepresenting a connectivity score based on the similarity of tokens. Foreground\nobjects can then be segmented using a normalized graph-cut to group\nself-similar regions. We solve the graph-cut problem using spectral clustering\nwith generalized eigen-decomposition and show that the second smallest\neigenvector provides a cutting solution since its absolute value indicates the\nlikelihood that a token belongs to a foreground object. Despite its simplicity,\nthis approach significantly boosts the performance of unsupervised object\ndiscovery: we improve over the recent state of the art LOST by a margin of\n6.9%, 8.1%, and 8.1% respectively on the VOC07, VOC12, and COCO20K. The\nperformance can be further improved by adding a second stage class-agnostic\ndetector (CAD). Our proposed method can be easily extended to unsupervised\nsaliency detection and weakly supervised object detection. For unsupervised\nsaliency detection, we improve IoU for 4.9%, 5.2%, 12.9% on ECSSD, DUTS,\nDUT-OMRON respectively compared to previous state of the art. For weakly\nsupervised object detection, we achieve competitive performance on CUB and\nImageNet.\n",
                "链接": "https://arxiv.org/abs/2202.11539"
            },
            {
                "文章ID": "122446",
                "标题": "A Simple Knowledge Distillation Framework for Open-world Object\n  Detection",
                "作者": " Shuailei Ma,  Yuefeng Wang,  Ying Wei,  Jiaqi Fan,  Xinyu Sun,  Peihao Chen,  Enming Zhang",
                "发布日期": "2023-12-15",
                "摘要": "  Open World Object Detection (OWOD) is a novel computer vision task with a\nconsiderable challenge, bridging the gap between classic object detection (OD)\nbenchmarks and real-world object detection. In addition to detecting and\nclassifying seen/known objects, OWOD algorithms are expected to localize all\npotential unseen/unknown objects and incrementally learn them. The large\npre-trained vision-language grounding models (VLM,eg, GLIP) have rich knowledge\nabout the open world, but are limited by text prompts and cannot localize\nindescribable objects. However, there are many detection scenarios which\npre-defined language descriptions are unavailable during inference. In this\npaper, we attempt to specialize the VLM model for OWOD task by distilling its\nopen-world knowledge into a language-agnostic detector. Surprisingly, we\nobserve that the combination of a simple knowledge distillation approach and\nthe automatic pseudo-labeling mechanism in OWOD can achieve better performance\nfor unknown object detection, even with a small amount of data. Unfortunately,\nknowledge distillation for unknown objects severely affects the learning of\ndetectors with conventional structures for known objects, leading to\ncatastrophic forgetting. To alleviate these problems, we propose the\ndown-weight loss function for knowledge distillation from vision-language to\nsingle vision modality. Meanwhile, we decouple the learning of localization and\nrecognition to reduce the impact of category interactions of known and unknown\nobjects on the localization learning process. Comprehensive experiments\nperformed on MS-COCO and PASCAL VOC demonstrate the effectiveness of our\nmethods.\n",
                "链接": "https://arxiv.org/abs/2312.08653"
            },
            {
                "文章ID": "10746",
                "标题": "Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language\n  Knowledge Distillation",
                "作者": " Zongyang Ma,  Guan Luo,  Jin Gao,  Liang Li,  Yuxin Chen,  Shaoru Wang,  Congxuan Zhang,  Weiming Hu",
                "发布日期": "2022-03-22",
                "摘要": "  Open-vocabulary object detection aims to detect novel object categories\nbeyond the training set.\n  The advanced open-vocabulary two-stage detectors employ instance-level\nvisual-to-visual knowledge distillation to align the visual space of the\ndetector with the semantic space of the Pre-trained Visual-Language Model\n(PVLM).\n  However, in the more efficient one-stage detector, the absence of\nclass-agnostic object proposals hinders the knowledge distillation on unseen\nobjects, leading to severe performance degradation.\n  In this paper, we propose a hierarchical visual-language knowledge\ndistillation method, i.e., HierKD, for open-vocabulary one-stage detection.\n  Specifically, a global-level knowledge distillation is explored to transfer\nthe knowledge of unseen categories from the PVLM to the detector.\n  Moreover, we combine the proposed global-level knowledge distillation and the\ncommon instance-level knowledge distillation to learn the knowledge of seen and\nunseen categories simultaneously.\n  Extensive experiments on MS-COCO show that our method significantly surpasses\nthe previous best one-stage detector with 11.9\\% and 6.7\\% $AP_{50}$ gains\nunder the zero-shot detection and generalized zero-shot detection settings, and\nreduces the $AP_{50}$ performance gap from 14\\% to 7.3\\% compared to the best\ntwo-stage detector.\n",
                "链接": "https://arxiv.org/abs/2203.10593"
            },
            {
                "文章ID": "27512",
                "标题": "Boosting Single-Frame 3D Object Detection by Simulating Multi-Frame\n  Point Clouds",
                "作者": " Wu Zheng,  Li Jiang,  Fanbin Lu,  Yangyang Ye,  Chi-Wing Fu",
                "发布日期": "2022-07-13",
                "摘要": "  To boost a detector for single-frame 3D object detection, we present a new\napproach to train it to simulate features and responses following a detector\ntrained on multi-frame point clouds. Our approach needs multi-frame point\nclouds only when training the single-frame detector, and once trained, it can\ndetect objects with only single-frame point clouds as inputs during the\ninference. We design a novel Simulated Multi-Frame Single-Stage object Detector\n(SMF-SSD) framework to realize the approach: multi-view dense object fusion to\ndensify ground-truth objects to generate a multi-frame point cloud;\nself-attention voxel distillation to facilitate one-to-many knowledge transfer\nfrom multi- to single-frame voxels; multi-scale BEV feature distillation to\ntransfer knowledge in low-level spatial and high-level semantic BEV features;\nand adaptive response distillation to activate single-frame responses of high\nconfidence and accurate localization. Experimental results on the Waymo test\nset show that our SMF-SSD consistently outperforms all state-of-the-art\nsingle-frame 3D object detectors for all object classes of difficulty levels 1\nand 2 in terms of both mAP and mAPH.\n",
                "链接": "https://arxiv.org/abs/2207.01030"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下使用2020年以后CONLL 2004数据集进行NER评测的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "46217",
                "标题": "Recognizing Nested Entities from Flat Supervision: A New NER Subtask,\n  Feasibility and Challenges",
                "作者": " Enwei Zhu,  Yiyang Liu,  Ming Jin,  Jinpeng Li",
                "发布日期": "2022-11-02",
                "摘要": "  Many recent named entity recognition (NER) studies criticize flat NER for its\nnon-overlapping assumption, and switch to investigating nested NER. However,\nexisting nested NER models heavily rely on training data annotated with nested\nentities, while labeling such data is costly. This study proposes a new\nsubtask, nested-from-flat NER, which corresponds to a realistic application\nscenario: given data annotated with flat entities only, one may still desire\nthe trained model capable of recognizing nested entities. To address this task,\nwe train span-based models and deliberately ignore the spans nested inside\nlabeled entities, since these spans are possibly unlabeled entities. With\nnested entities removed from the training data, our model achieves 54.8%, 54.2%\nand 41.1% F1 scores on the subset of spans within entities on ACE 2004, ACE\n2005 and GENIA, respectively. This suggests the effectiveness of our approach\nand the feasibility of the task. In addition, the model's performance on flat\nentities is entirely unaffected. We further manually annotate the nested\nentities in the test set of CoNLL 2003, creating a nested-from-flat NER\nbenchmark. Analysis results show that the main challenges stem from the data\nand annotation inconsistencies between the flat and nested entities.\n",
                "链接": "https://arxiv.org/abs/2211.00301"
            },
            {
                "文章ID": "50504",
                "标题": "Finetuning BERT on Partially Annotated NER Corpora",
                "作者": " Viktor Scherbakov,  Vladimir Mayorov",
                "发布日期": "2022-11-29",
                "摘要": "  Most Named Entity Recognition (NER) models operate under the assumption that\ntraining datasets are fully labelled. While it is valid for established\ndatasets like CoNLL 2003 and OntoNotes, sometimes it is not feasible to obtain\nthe complete dataset annotation. These situations may occur, for instance,\nafter selective annotation of entities for cost reduction. This work presents\nan approach to finetuning BERT on such partially labelled datasets using\nself-supervision and label preprocessing. Our approach outperforms the previous\nLSTM-based label preprocessing baseline, significantly improving the\nperformance on poorly labelled datasets. We demonstrate that following our\napproach while finetuning RoBERTa on CoNLL 2003 dataset with only 10% of total\nentities labelled is enough to reach the performance of the baseline trained on\nthe same dataset with 50% of the entities labelled.\n",
                "链接": "https://arxiv.org/abs/2211.14360"
            },
            {
                "文章ID": "54351",
                "标题": "Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?",
                "作者": " Shuheng Liu,  Alan Ritter",
                "发布日期": "2023-07-13",
                "摘要": "  The CoNLL-2003 English named entity recognition (NER) dataset has been widely\nused to train and evaluate NER models for almost 20 years. However, it is\nunclear how well models that are trained on this 20-year-old data and developed\nover a period of decades using the same test set will perform when applied on\nmodern data. In this paper, we evaluate the generalization of over 20 different\nmodels trained on CoNLL-2003, and show that NER models have very different\ngeneralization. Surprisingly, we find no evidence of performance degradation in\npre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using\ndecades-old data. We investigate why some models generalize well to new data\nwhile others do not, and attempt to disentangle the effects of temporal drift\nand overfitting due to test reuse. Our analysis suggests that most\ndeterioration is due to temporal mismatch between the pre-training corpora and\nthe downstream test sets. We found that four factors are important for good\ngeneralization: model architecture, number of parameters, time period of the\npre-training corpus, in addition to the amount of fine-tuning data. We suggest\ncurrent evaluation methods have, in some sense, underestimated progress on NER\nover the past 20 years, as NER models have not only improved on the original\nCoNLL-2003 test set, but improved even more on modern data. Our datasets can be\nfound at https://github.com/ShuhengL/acl2023_conllpp.\n",
                "链接": "https://arxiv.org/abs/2212.09747"
            },
            {
                "文章ID": "54218",
                "标题": "E-NER -- An Annotated Named Entity Recognition Corpus of Legal Text",
                "作者": " Ting Wai Terence Au,  Ingemar J. Cox,  Vasileios Lampos",
                "发布日期": "2022-12-20",
                "摘要": "  Identifying named entities such as a person, location or organization, in\ndocuments can highlight key information to readers. Training Named Entity\nRecognition (NER) models requires an annotated data set, which can be a\ntime-consuming labour-intensive task. Nevertheless, there are publicly\navailable NER data sets for general English. Recently there has been interest\nin developing NER for legal text. However, prior work and experimental results\nreported here indicate that there is a significant degradation in performance\nwhen NER methods trained on a general English data set are applied to legal\ntext. We describe a publicly available legal NER data set, called E-NER, based\non legal company filings available from the US Securities and Exchange\nCommission's EDGAR data set. Training a number of different NER algorithms on\nthe general English CoNLL-2003 corpus but testing on our test collection\nconfirmed significant degradations in accuracy, as measured by the F1-score, of\nbetween 29.4\\% and 60.4\\%, compared to training and testing on the E-NER\ncollection.\n",
                "链接": "https://arxiv.org/abs/2212.09306"
            },
            {
                "文章ID": "101950",
                "标题": "SIB-200: A Simple, Inclusive, and Big Evaluation Dataset for Topic\n  Classification in 200+ Languages and Dialects",
                "作者": " David Ifeoluwa Adelani,  Hannah Liu,  Xiaoyu Shen,  Nikita Vassilyev,  Jesujoba O. Alabi,  Yanke Mao,  Haonan Gao,  Annie En-Shiun Lee",
                "发布日期": "2023-09-15",
                "摘要": "  Despite the progress we have recorded in the last few years in multilingual\nnatural language processing, evaluation is typically limited to a small set of\nlanguages with available datasets which excludes a large number of low-resource\nlanguages. In this paper, we created SIB-200 -- a large-scale open-sourced\nbenchmark dataset for topic classification in 200 languages and dialects to\naddress the lack of evaluation dataset for Natural Language Understanding\n(NLU). For many of the languages covered in SIB-200, this is the first publicly\navailable evaluation dataset for NLU. The dataset is based on Flores-200\nmachine translation corpus. We annotated the English portion of the dataset and\nextended the sentence-level annotation to the remaining 203 languages covered\nin the corpus. Despite the simplicity of this task, our evaluation in\nfull-supervised setting, cross-lingual transfer setting and prompting of large\nlanguage model setting show that there is still a large gap between the\nperformance of high-resource and low-resource languages when multilingual\nevaluation is scaled to numerous world languages. We found that languages\nunseen during the pre-training of multilingual language models,\nunder-represented language families (like Nilotic and Altantic-Congo), and\nlanguages from the regions of Africa, Americas, Oceania and South East Asia,\noften have the lowest performance on our topic classification dataset. We hope\nour dataset will encourage a more inclusive evaluation of multilingual language\nmodels on a more diverse set of languages. https://github.com/dadelani/sib-200\n",
                "链接": "https://arxiv.org/abs/2309.07445"
            },
            {
                "文章ID": "12159",
                "标题": "Federated Named Entity Recognition",
                "作者": " Joel Mathew,  Dimitris Stripelis,  José Luis Ambite",
                "发布日期": "2022-03-30",
                "摘要": "  We present an analysis of the performance of Federated Learning in a\nparadigmatic natural-language processing task: Named-Entity Recognition (NER).\nFor our evaluation, we use the language-independent CoNLL-2003 dataset as our\nbenchmark dataset and a Bi-LSTM-CRF model as our benchmark NER model. We show\nthat federated training reaches almost the same performance as the centralized\nmodel, though with some performance degradation as the learning environments\nbecome more heterogeneous. We also show the convergence rate of federated\nmodels for NER. Finally, we discuss existing challenges of Federated Learning\nfor NLP applications that can foster future research directions.\n",
                "链接": "https://arxiv.org/abs/2203.15101"
            },
            {
                "文章ID": "111254",
                "标题": "CleanCoNLL: A Nearly Noise-Free Named Entity Recognition Dataset",
                "作者": " Susanna Rücker,  Alan Akbik",
                "发布日期": "2023-10-26",
                "摘要": "  The CoNLL-03 corpus is arguably the most well-known and utilized benchmark\ndataset for named entity recognition (NER). However, prior works found\nsignificant numbers of annotation errors, incompleteness, and inconsistencies\nin the data. This poses challenges to objectively comparing NER approaches and\nanalyzing their errors, as current state-of-the-art models achieve F1-scores\nthat are comparable to or even exceed the estimated noise level in CoNLL-03. To\naddress this issue, we present a comprehensive relabeling effort assisted by\nautomatic consistency checking that corrects 7.0% of all labels in the English\nCoNLL-03. Our effort adds a layer of entity linking annotation both for better\nexplainability of NER labels and as additional safeguard of annotation quality.\nOur experimental evaluation finds not only that state-of-the-art approaches\nreach significantly higher F1-scores (97.1%) on our data, but crucially that\nthe share of correct predictions falsely counted as errors due to annotation\nnoise drops from 47% to 6%. This indicates that our resource is well suited to\nanalyze the remaining errors made by state-of-the-art models, and that the\ntheoretical upper bound even on high resource, coarse-grained NER is not yet\nreached. To facilitate such analysis, we make CleanCoNLL publicly available to\nthe research community.\n",
                "链接": "https://arxiv.org/abs/2310.16225"
            },
            {
                "文章ID": "10782",
                "标题": "Leveraging Expert Guided Adversarial Augmentation For Improving\n  Generalization in Named Entity Recognition",
                "作者": " Aaron Reich,  Jiaao Chen,  Aastha Agrawal,  Yanzhe Zhang,  Diyi Yang",
                "发布日期": "2022-03-22",
                "摘要": "  Named Entity Recognition (NER) systems often demonstrate great performance on\nin-distribution data, but perform poorly on examples drawn from a shifted\ndistribution. One way to evaluate the generalization ability of NER models is\nto use adversarial examples, on which the specific variations associated with\nnamed entities are rarely considered. To this end, we propose leveraging\nexpert-guided heuristics to change the entity tokens and their surrounding\ncontexts thereby altering their entity types as adversarial attacks. Using\nexpert-guided heuristics, we augmented the CoNLL 2003 test set and manually\nannotated it to construct a high-quality challenging set. We found that\nstate-of-the-art NER systems trained on CoNLL 2003 training data drop\nperformance dramatically on our challenging set. By training on adversarial\naugmented training examples and using mixup for regularization, we were able to\nsignificantly improve the performance on the challenging set as well as improve\nout-of-domain generalization which we evaluated by using OntoNotes data. We\nhave publicly released our dataset and code at\nhttps://github.com/GT-SALT/Guided-Adversarial-Augmentation.\n",
                "链接": "https://arxiv.org/abs/2203.10693"
            },
            {
                "文章ID": "39331",
                "标题": "mRobust04: A Multilingual Version of the TREC Robust 2004 Benchmark",
                "作者": " Vitor Jeronymo,  Mauricio Nascimento,  Roberto Lotufo,  Rodrigo Nogueira",
                "发布日期": "2022-09-29",
                "摘要": "  Robust 2004 is an information retrieval benchmark whose large number of\njudgments per query make it a reliable evaluation dataset. In this paper, we\npresent mRobust04, a multilingual version of Robust04 that was translated to 8\nlanguages using Google Translate. We also provide results of three different\nmultilingual retrievers on this dataset. The dataset is available at\nhttps://huggingface.co/datasets/unicamp-dl/mrobust\n",
                "链接": "https://arxiv.org/abs/2209.13738"
            },
            {
                "文章ID": "54203",
                "标题": "Statistical Dataset Evaluation: Reliability, Difficulty, and Validity",
                "作者": " Chengwen Wang,  Qingxiu Dong,  Xiaochen Wang,  Haitao Wang,  Zhifang Sui",
                "发布日期": "2022-12-20",
                "摘要": "  Datasets serve as crucial training resources and model performance trackers.\nHowever, existing datasets have exposed a plethora of problems, inducing biased\nmodels and unreliable evaluation results. In this paper, we propose a\nmodel-agnostic dataset evaluation framework for automatic dataset quality\nevaluation. We seek the statistical properties of the datasets and address\nthree fundamental dimensions: reliability, difficulty, and validity, following\na classical testing theory. Taking the Named Entity Recognition (NER) datasets\nas a case study, we introduce $9$ statistical metrics for a statistical dataset\nevaluation framework. Experimental results and human evaluation validate that\nour evaluation framework effectively assesses various aspects of the dataset\nquality. Furthermore, we study how the dataset scores on our statistical\nmetrics affect the model performance, and appeal for dataset quality evaluation\nor targeted dataset improvement before training or testing models.\n",
                "链接": "https://arxiv.org/abs/2212.09272"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下使用CrossWoz或MultiWoz数据集进行DST评测的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "7222",
                "标题": "ASSIST: Towards Label Noise-Robust Dialogue State Tracking",
                "作者": " Fanghua Ye,  Yue Feng,  Emine Yilmaz",
                "发布日期": "2022-03-15",
                "摘要": "  The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state\ntracking (DST). However, substantial noise has been discovered in its state\nannotations. Such noise brings about huge challenges for training DST models\nrobustly. Although several refined versions, including MultiWOZ 2.1-2.4, have\nbeen published recently, there are still lots of noisy labels, especially in\nthe training set. Besides, it is costly to rectify all the problematic\nannotations. In this paper, instead of improving the annotation quality\nfurther, we propose a general framework, named ASSIST (lAbel noiSe-robuSt\ndIalogue State Tracking), to train DST models robustly from noisy labels.\nASSIST first generates pseudo labels for each sample in the training set by\nusing an auxiliary model trained on a small clean dataset, then puts the\ngenerated pseudo labels and vanilla noisy labels together to train the primary\nmodel. We show the validity of ASSIST theoretically. Experimental results also\ndemonstrate that ASSIST improves the joint goal accuracy of DST by up to\n$28.16\\%$ on MultiWOZ 2.0 and $8.41\\%$ on MultiWOZ 2.4, compared to using only\nthe vanilla noisy labels.\n",
                "链接": "https://arxiv.org/abs/2202.13024"
            },
            {
                "文章ID": "48889",
                "标题": "Self-Training with Purpose Preserving Augmentation Improves Few-shot\n  Generative Dialogue State Tracking",
                "作者": " Jihyun Lee,  Chaebin Lee,  Yunsu Kim,  Gary Geunbae Lee",
                "发布日期": "2022-11-18",
                "摘要": "  In dialogue state tracking (DST), labeling the dataset involves considerable\nhuman labor. We propose a new self-training framework for few-shot generative\nDST that utilize unlabeled data. Our self-training method iteratively improves\nthe model by pseudo labeling and employs Purpose Preserving Augmentation\n(PPAug) to prevent overfitting. We increaese the few-shot 10% performance by\napproximately 4% on MultiWOZ 2.1 and enhances the slot-recall 8.34% for unseen\nvalues compared to baseline.\n",
                "链接": "https://arxiv.org/abs/2211.09379"
            },
            {
                "文章ID": "61900",
                "标题": "Dialogue State Distillation Network with Inter-slot Contrastive Learning\n  for Dialogue State Tracking",
                "作者": " Jing Xu,  Dandan Song,  Chong Liu,  Siu Cheung Hui,  Fei Li,  Qiang Ju,  Xiaonan He,  Jian Xie",
                "发布日期": "2023-03-08",
                "摘要": "  In task-oriented dialogue systems, Dialogue State Tracking (DST) aims to\nextract users' intentions from the dialogue history. Currently, most existing\napproaches suffer from error propagation and are unable to dynamically select\nrelevant information when utilizing previous dialogue states. Moreover, the\nrelations between the updates of different slots provide vital clues for DST.\nHowever, the existing approaches rely only on predefined graphs to indirectly\ncapture the relations. In this paper, we propose a Dialogue State Distillation\nNetwork (DSDN) to utilize relevant information of previous dialogue states and\nmigrate the gap of utilization between training and testing. Thus, it can\ndynamically exploit previous dialogue states and avoid introducing error\npropagation simultaneously. Further, we propose an inter-slot contrastive\nlearning loss to effectively capture the slot co-update relations from dialogue\ncontext. Experiments are conducted on the widely used MultiWOZ 2.0 and MultiWOZ\n2.1 datasets. The experimental results show that our proposed model achieves\nthe state-of-the-art performance for DST.\n",
                "链接": "https://arxiv.org/abs/2302.08220"
            },
            {
                "文章ID": "40726",
                "标题": "Schema Encoding for Transferable Dialogue State Tracking",
                "作者": " Hyunmin Jeon,  Gary Geunbae Lee",
                "发布日期": "2022-10-06",
                "摘要": "  Dialogue state tracking (DST) is an essential sub-task for task-oriented\ndialogue systems. Recent work has focused on deep neural models for DST.\nHowever, the neural models require a large dataset for training. Furthermore,\napplying them to another domain needs a new dataset because the neural models\nare generally trained to imitate the given dataset. In this paper, we propose\nSchema Encoding for Transferable Dialogue State Tracking (SETDST), which is a\nneural DST method for effective transfer to new domains. Transferable DST could\nassist developments of dialogue systems even with few dataset on target\ndomains. We use a schema encoder not just to imitate the dataset but to\ncomprehend the schema of the dataset. We aim to transfer the model to new\ndomains by encoding new schemas and using them for DST on multi-domain\nsettings. As a result, SET-DST improved the joint accuracy by 1.46 points on\nMultiWOZ 2.1.\n",
                "链接": "https://arxiv.org/abs/2210.02351"
            },
            {
                "文章ID": "46848",
                "标题": "MultiWOZ-DF -- A Dataflow implementation of the MultiWOZ dataset",
                "作者": " Joram Meron,  Victor Guimarães",
                "发布日期": "2022-11-07",
                "摘要": "  Semantic Machines (SM) have introduced the use of the dataflow (DF) paradigm\nto dialogue modelling, using computational graphs to hierarchically represent\nuser requests, data, and the dialogue history [Semantic Machines et al. 2020].\nAlthough the main focus of that paper was the SMCalFlow dataset (to date, the\nonly dataset with \"native\" DF annotations), they also reported some results of\nan experiment using a transformed version of the commonly used MultiWOZ dataset\n[Budzianowski et al. 2018] into a DF format. In this paper, we expand the\nexperiments using DF for the MultiWOZ dataset, exploring some additional\nexperimental set-ups. The code and instructions to reproduce the experiments\nreported here have been released. The contributions of this paper are: 1.) A DF\nimplementation capable of executing MultiWOZ dialogues; 2.) Several versions of\nconversion of MultiWOZ into a DF format are presented; 3.) Experimental results\non state match and translation accuracy.\n",
                "链接": "https://arxiv.org/abs/2211.02303"
            },
            {
                "文章ID": "17975",
                "标题": "LUNA: Learning Slot-Turn Alignment for Dialogue State Tracking",
                "作者": " Yifan Wang,  Jing Zhao,  Junwei Bao,  Chaoqun Duan,  Youzheng Wu,  Xiaodong He",
                "发布日期": "2022-05-06",
                "摘要": "  Dialogue state tracking (DST) aims to predict the current dialogue state\ngiven the dialogue history. Existing methods generally exploit the utterances\nof all dialogue turns to assign value for each slot. This could lead to\nsuboptimal results due to the information introduced from irrelevant utterances\nin the dialogue history, which may be useless and can even cause confusion. To\naddress this problem, we propose LUNA, a sLot-tUrN Alignment enhanced approach.\nIt first explicitly aligns each slot with its most relevant utterance, then\nfurther predicts the corresponding value based on this aligned utterance\ninstead of all dialogue utterances. Furthermore, we design a slot ranking\nauxiliary task to learn the temporal correlation among slots which could\nfacilitate the alignment. Comprehensive experiments are conducted on\nmulti-domain task-oriented dialogue datasets, i.e., MultiWOZ 2.0, MultiWOZ 2.1,\nand MultiWOZ 2.2. The results show that LUNA achieves new state-of-the-art\nresults on these datasets.\n",
                "链接": "https://arxiv.org/abs/2205.02550"
            },
            {
                "文章ID": "37569",
                "标题": "SF-DST: Few-Shot Self-Feeding Reading Comprehension Dialogue State\n  Tracking with Auxiliary Task",
                "作者": " Jihyun Lee,  Gary Geunbae Lee",
                "发布日期": "2022-09-19",
                "摘要": "  Few-shot dialogue state tracking (DST) model tracks user requests in dialogue\nwith reliable accuracy even with a small amount of data. In this paper, we\nintroduce an ontology-free few-shot DST with self-feeding belief state input.\nThe self-feeding belief state input increases the accuracy in multi-turn\ndialogue by summarizing previous dialogue. Also, we newly developed a slot-gate\nauxiliary task. This new auxiliary task helps classify whether a slot is\nmentioned in the dialogue. Our model achieved the best score in a few-shot\nsetting for four domains on multiWOZ 2.0.\n",
                "链接": "https://arxiv.org/abs/2209.07742"
            },
            {
                "文章ID": "43163",
                "标题": "Mars: Modeling Context & State Representations with Contrastive Learning\n  for End-to-End Task-Oriented Dialog",
                "作者": " Haipeng Sun,  Junwei Bao,  Youzheng Wu,  Xiaodong He",
                "发布日期": "2023-07-11",
                "摘要": "  Traditional end-to-end task-oriented dialog systems first convert dialog\ncontext into belief state and action state before generating the system\nresponse. The system response performance is significantly affected by the\nquality of the belief state and action state. We first explore what dialog\ncontext representation is beneficial to improving the quality of the belief\nstate and action state, which further enhances the generated response quality.\nTo tackle our exploration, we propose Mars, an end-to-end task-oriented dialog\nsystem with two contrastive learning strategies to model the relationship\nbetween dialog context and belief/action state representations. Empirical\nresults show dialog context representations, which are more different from\nsemantic state representations, are more conducive to multi-turn task-oriented\ndialog. Moreover, our proposed Mars achieves state-of-the-art performance on\nthe MultiWOZ 2.0, CamRest676, and CrossWOZ.\n",
                "链接": "https://arxiv.org/abs/2210.08917"
            },
            {
                "文章ID": "109152",
                "标题": "UNO-DST: Leveraging Unlabelled Data in Zero-Shot Dialogue State Tracking",
                "作者": " Chuang Li,  Yan Zhang,  Min-Yen Kan,  Haizhou Li",
                "发布日期": "2023-10-17",
                "摘要": "  Previous zero-shot dialogue state tracking (DST) methods only apply transfer\nlearning, but ignore unlabelled data in the target domain. We transform\nzero-shot DST into few-shot DST by utilising such unlabelled data via joint and\nself-training methods. Our method incorporates auxiliary tasks that generate\nslot types as inverse prompts for main tasks, creating slot values during joint\ntraining. Cycle consistency between these two tasks enables the generation and\nselection of quality samples in unknown target domains for subsequent\nfine-tuning. This approach also facilitates automatic label creation, thereby\noptimizing the training and fine-tuning of DST models. We demonstrate this\nmethod's effectiveness on large language models in zero-shot scenarios,\nimproving average joint goal accuracy by $8\\%$ across all domains in MultiWOZ.\n",
                "链接": "https://arxiv.org/abs/2310.10492"
            },
            {
                "文章ID": "112862",
                "标题": "Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users",
                "作者": " Yohan Jo,  Xinyan Zhao,  Arijit Biswas,  Nikoletta Basiou,  Vincent Auvray,  Nikolaos Malandrakis,  Angeliki Metallinou,  Alexandros Potamianos",
                "发布日期": "2023-11-01",
                "摘要": "  While most task-oriented dialogues assume conversations between the agent and\none user at a time, dialogue systems are increasingly expected to communicate\nwith multiple users simultaneously who make decisions collaboratively. To\nfacilitate development of such systems, we release the Multi-User MultiWOZ\ndataset: task-oriented dialogues among two users and one agent. To collect this\ndataset, each user utterance from MultiWOZ 2.2 was replaced with a small chat\nbetween two users that is semantically and pragmatically consistent with the\noriginal user utterance, thus resulting in the same dialogue state and system\nresponse. These dialogues reflect interesting dynamics of collaborative\ndecision-making in task-oriented scenarios, e.g., social chatter and\ndeliberation. Supported by this data, we propose the novel task of multi-user\ncontextual query rewriting: to rewrite a task-oriented chat between two users\nas a concise task-oriented query that retains only task-relevant information\nand that is directly consumable by the dialogue system. We demonstrate that in\nmulti-user dialogues, using predicted rewrites substantially improves dialogue\nstate tracking without modifying existing dialogue systems that are trained for\nsingle-user dialogues. Further, this method surpasses training a medium-sized\nmodel directly on multi-user dialogues and generalizes to unseen domains.\n",
                "链接": "https://arxiv.org/abs/2310.20479"
            }
        ]
    },
    {
        "question": {
            "question": "2023年后利用hotpotqa数据集做问题生成任务的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "10231",
                "标题": "Ask to Understand: Question Generation for Multi-hop Question Answering",
                "作者": " Jiawei Li,  Mucheng Ren,  Yang Gao,  Yizhe Yang",
                "发布日期": "2022-03-18",
                "摘要": "  Multi-hop Question Answering (QA) requires the machine to answer complex\nquestions by finding scattering clues and reasoning from multiple documents.\nGraph Network (GN) and Question Decomposition (QD) are two common approaches at\npresent. The former uses the \"black-box\" reasoning process to capture the\npotential relationship between entities and sentences, thus achieving good\nperformance. At the same time, the latter provides a clear reasoning logical\nroute by decomposing multi-hop questions into simple single-hop sub-questions.\nIn this paper, we propose a novel method to complete multi-hop QA from the\nperspective of Question Generation (QG). Specifically, we carefully design an\nend-to-end QG module on the basis of a classical QA module, which could help\nthe model understand the context by asking inherently logical sub-questions,\nthus inheriting interpretability from the QD-based method and showing superior\nperformance. Experiments on the HotpotQA dataset demonstrate that the\neffectiveness of our proposed QG module, human evaluation further clarifies its\ninterpretability quantitatively, and thorough analysis shows that the QG module\ncould generate better sub-questions than QD methods in terms of fluency,\nconsistency, and diversity.\n",
                "链接": "https://arxiv.org/abs/2203.09073"
            },
            {
                "文章ID": "5376",
                "标题": "QA4QG: Using Question Answering to Constrain Multi-Hop Question\n  Generation",
                "作者": " Dan Su,  Peng Xu,  Pascale Fung",
                "发布日期": "2022-02-15",
                "摘要": "  Multi-hop question generation (MQG) aims to generate complex questions which\nrequire reasoning over multiple pieces of information of the input passage.\nMost existing work on MQG has focused on exploring graph-based networks to\nequip the traditional Sequence-to-sequence framework with reasoning ability.\nHowever, these models do not take full advantage of the constraint between\nquestions and answers. Furthermore, studies on multi-hop question answering\n(QA) suggest that Transformers can replace the graph structure for multi-hop\nreasoning. Therefore, in this work, we propose a novel framework, QA4QG, a\nQA-augmented BART-based framework for MQG. It augments the standard BART model\nwith an additional multi-hop QA module to further constrain the generated\nquestion. Our results on the HotpotQA dataset show that QA4QG outperforms all\nstate-of-the-art models, with an increase of 8 BLEU-4 and 8 ROUGE points\ncompared to the best results previously reported. Our work suggests the\nadvantage of introducing pre-trained language models and QA module for the MQG\ntask.\n",
                "链接": "https://arxiv.org/abs/2202.06538"
            },
            {
                "文章ID": "87964",
                "标题": "Towards Enriched Controllability for Educational Question Generation",
                "作者": " Bernardo Leite,  Henrique Lopes Cardoso",
                "发布日期": "2023-06-28",
                "摘要": "  Question Generation (QG) is a task within Natural Language Processing (NLP)\nthat involves automatically generating questions given an input, typically\ncomposed of a text and a target answer. Recent work on QG aims to control the\ntype of generated questions so that they meet educational needs. A remarkable\nexample of controllability in educational QG is the generation of questions\nunderlying certain narrative elements, e.g., causal relationship, outcome\nresolution, or prediction. This study aims to enrich controllability in QG by\nintroducing a new guidance attribute: question explicitness. We propose to\ncontrol the generation of explicit and implicit wh-questions from\nchildren-friendly stories. We show preliminary evidence of controlling QG via\nquestion explicitness alone and simultaneously with another target attribute:\nthe question's narrative element. The code is publicly available at\ngithub.com/bernardoleite/question-generation-control.\n",
                "链接": "https://arxiv.org/abs/2306.14917"
            },
            {
                "文章ID": "109543",
                "标题": "What is a good question? Task-oriented asking with fact-level masking",
                "作者": " Matthew Toles,  Yukun Huang,  Zhou Yu,  Luis Gravano",
                "发布日期": "2023-10-19",
                "摘要": "  Asking questions is an important element of real-life collaboration on\nreasoning tasks like question answering. For example, a legal assistant chatbot\nmay be unable to make accurate recommendations without specific information on\nthe user's circumstances. However, large language models are usually deployed\nto solve reasoning tasks directly without asking follow-up questions to the\nuser or third parties. We term this problem task-oriented asking (TOA).\nZero-shot chat models can perform TOA, but their training is primarily based on\nnext-token prediction rather than whether questions contribute to successful\ncollaboration. To enable the training and evaluation of TOA models, we present\na definition and framework for natural language task-oriented asking, the\nproblem of generating questions that result in answers useful for a reasoning\ntask. We also present fact-level masking (FLM), a procedure for converting\nnatural language datasets into self-supervised TOA datasets by omitting\nparticular critical facts. Finally, we generate a TOA dataset from the HotpotQA\ndataset using FLM and evaluate several zero-shot language models on it. Our\nexperiments show that current zero-shot models struggle to ask questions that\nretrieve useful information, as compared to human annotators. These results\ndemonstrate an opportunity to use FLM datasets and the TOA framework to train\nand evaluate better TOA models.\n",
                "链接": "https://arxiv.org/abs/2310.11571"
            },
            {
                "文章ID": "113943",
                "标题": "In-Context Learning for Knowledge Base Question Answering for Unmanned\n  Systems based on Large Language Models",
                "作者": " Yunlong Chen,  Yaming Zhang,  Jianfei Yu,  Li Yang,  Rui Xia",
                "发布日期": "2023-11-07",
                "摘要": "  Knowledge Base Question Answering (KBQA) aims to answer factoid questions\nbased on knowledge bases. However, generating the most appropriate knowledge\nbase query code based on Natural Language Questions (NLQ) poses a significant\nchallenge in KBQA. In this work, we focus on the CCKS2023 Competition of\nQuestion Answering with Knowledge Graph Inference for Unmanned Systems.\nInspired by the recent success of large language models (LLMs) like ChatGPT and\nGPT-3 in many QA tasks, we propose a ChatGPT-based Cypher Query Language (CQL)\ngeneration framework to generate the most appropriate CQL based on the given\nNLQ. Our generative framework contains six parts: an auxiliary model predicting\nthe syntax-related information of CQL based on the given NLQ, a proper noun\nmatcher extracting proper nouns from the given NLQ, a demonstration example\nselector retrieving similar examples of the input sample, a prompt constructor\ndesigning the input template of ChatGPT, a ChatGPT-based generation model\ngenerating the CQL, and an ensemble model to obtain the final answers from\ndiversified outputs. With our ChatGPT-based CQL generation framework, we\nachieved the second place in the CCKS 2023 Question Answering with Knowledge\nGraph Inference for Unmanned Systems competition, achieving an F1-score of\n0.92676.\n",
                "链接": "https://arxiv.org/abs/2311.02956"
            },
            {
                "文章ID": "76360",
                "标题": "SkillQG: Learning to Generate Question for Reading Comprehension\n  Assessment",
                "作者": " Xiaoqiang Wang,  Bang Liu,  Siliang Tang,  Lingfei Wu",
                "发布日期": "2023-05-09",
                "摘要": "  We present $\\textbf{$\\texttt{SkillQG}$}$: a question generation framework\nwith controllable comprehension types for assessing and improving machine\nreading comprehension models. Existing question generation systems widely\ndifferentiate questions by $\\textit{literal}$ information such as question\nwords and answer types to generate semantically relevant questions for a given\ncontext. However, they rarely consider the $\\textit{comprehension}$ nature of\nquestions, i.e. the different comprehension capabilities embodied by different\nquestions. In comparison, our $\\texttt{SkillQG}$ is able to tailor a\nfine-grained assessment and improvement to the capabilities of question\nanswering models built on it. Specifically, we first frame the comprehension\ntype of questions based on a hierarchical skill-based schema, then formulate\n$\\texttt{SkillQG}$ as a skill-conditioned question generator. Furthermore, to\nimprove the controllability of generation, we augment the input text with\nquestion focus and skill-specific knowledge, which are constructed by\niteratively prompting the pre-trained language models. Empirical results\ndemonstrate that $\\texttt{SkillQG}$ outperforms baselines in terms of quality,\nrelevance, and skill-controllability while showing a promising performance\nboost in downstream question answering task.\n",
                "链接": "https://arxiv.org/abs/2305.04737"
            },
            {
                "文章ID": "101214",
                "标题": "FOLLOWUPQG: Towards Information-Seeking Follow-up Question Generation",
                "作者": " Yan Meng,  Liangming Pan,  Yixin Cao,  Min-Yen Kan",
                "发布日期": "2023-09-20",
                "摘要": "  Humans ask follow-up questions driven by curiosity, which reflects a creative\nhuman cognitive process. We introduce the task of real-world\ninformation-seeking follow-up question generation (FQG), which aims to generate\nfollow-up questions seeking a more in-depth understanding of an initial\nquestion and answer. We construct FOLLOWUPQG, a dataset of over 3K real-world\n(initial question, answer, follow-up question) tuples collected from a Reddit\nforum providing layman-friendly explanations for open-ended questions. In\ncontrast to existing datasets, questions in FOLLOWUPQG use more diverse\npragmatic strategies to seek information, and they also show higher-order\ncognitive skills (such as applying and relating). We evaluate current question\ngeneration models on their efficacy for generating follow-up questions,\nexploring how to generate specific types of follow-up questions based on\nstep-by-step demonstrations. Our results validate FOLLOWUPQG as a challenging\nbenchmark, as model-generated questions are adequate but far from human-raised\nquestions in terms of informativeness and complexity.\n",
                "链接": "https://arxiv.org/abs/2309.05007"
            },
            {
                "文章ID": "73844",
                "标题": "IslamicPCQA: A Dataset for Persian Multi-hop Complex Question Answering\n  in Islamic Text Resources",
                "作者": " Arash Ghafouri,  Hasan Naderi,  Mohammad Aghajani asl,  Mahdi Firouzmandi",
                "发布日期": "2023-04-25",
                "摘要": "  Nowadays, one of the main challenges for Question Answering Systems is to\nanswer complex questions using various sources of information. Multi-hop\nquestions are a type of complex questions that require multi-step reasoning to\nanswer. In this article, the IslamicPCQA dataset is introduced. This is the\nfirst Persian dataset for answering complex questions based on non-structured\ninformation sources and consists of 12,282 question-answer pairs extracted from\n9 Islamic encyclopedias. This dataset has been created inspired by the HotpotQA\nEnglish dataset approach, which was customized to suit the complexities of the\nPersian language. Answering questions in this dataset requires more than one\nparagraph and reasoning. The questions are not limited to any prior knowledge\nbase or ontology, and to provide robust reasoning ability, the dataset also\nincludes supporting facts and key sentences. The prepared dataset covers a wide\nrange of Islamic topics and aims to facilitate answering complex Persian\nquestions within this subject matter\n",
                "链接": "https://arxiv.org/abs/2304.11664"
            },
            {
                "文章ID": "111349",
                "标题": "Diversity Enhanced Narrative Question Generation for Storybooks",
                "作者": " Hokeun Yoon,  JinYeong Bak",
                "发布日期": "2023-10-26",
                "摘要": "  Question generation (QG) from a given context can enhance comprehension,\nengagement, assessment, and overall efficacy in learning or conversational\nenvironments. Despite recent advancements in QG, the challenge of enhancing or\nmeasuring the diversity of generated questions often remains unaddressed. In\nthis paper, we introduce a multi-question generation model (mQG), which is\ncapable of generating multiple, diverse, and answerable questions by focusing\non context and questions. To validate the answerability of the generated\nquestions, we employ a SQuAD2.0 fine-tuned question answering model,\nclassifying the questions as answerable or not. We train and evaluate mQG on\nthe FairytaleQA dataset, a well-structured QA dataset based on storybooks, with\nnarrative questions. We further apply a zero-shot adaptation on the TellMeWhy\nand SQuAD1.1 datasets. mQG shows promising results across various evaluation\nmetrics, among strong baselines.\n",
                "链接": "https://arxiv.org/abs/2310.16446"
            },
            {
                "文章ID": "68929",
                "标题": "Automatic Generation of Multiple-Choice Questions",
                "作者": " Cheng Zhang",
                "发布日期": "2023-03-28",
                "摘要": "  Creating multiple-choice questions to assess reading comprehension of a given\narticle involves generating question-answer pairs (QAPs) and adequate\ndistractors. We present two methods to tackle the challenge of QAP generations:\n(1) A deep-learning-based end-to-end question generation system based on T5\nTransformer with Preprocessing and Postprocessing Pipelines (TP3). We use the\nfinetuned T5 model for our downstream task of question generation and improve\naccuracy using a combination of various NLP tools and algorithms in\npreprocessing and postprocessing to select appropriate answers and filter\nundesirable questions. (2) A sequence-learning-based scheme to generate\nadequate QAPs via meta-sequence representations of sentences. A meta-sequence\nis a sequence of vectors comprising semantic and syntactic tags. we devise a\nscheme called MetaQA to learn meta sequences from training data to form pairs\nof a meta sequence for a declarative sentence and a corresponding interrogative\nsentence. The TP3 works well on unseen data, which is complemented by MetaQA.\nBoth methods can generate well-formed and grammatically correct questions.\nMoreover, we present a novel approach to automatically generate adequate\ndistractors for a given QAP. The method is a combination of part-of-speech\ntagging, named-entity tagging, semantic-role labeling, regular expressions,\ndomain knowledge bases, word embeddings, word edit distance, WordNet, and other\nalgorithms.\n",
                "链接": "https://arxiv.org/abs/2303.14576"
            }
        ]
    },
    {
        "question": {
            "question": "2023年以后关于大语言模型和人文学科交叉的研究",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "98860",
                "标题": "Symbolic and Language Agnostic Large Language Models",
                "作者": " Walid S. Saba",
                "发布日期": "2023-08-29",
                "摘要": "  We argue that the relative success of large language models (LLMs) is not a\nreflection on the symbolic vs. subsymbolic debate but a reflection on employing\nan appropriate strategy of bottom-up reverse engineering of language at scale.\nHowever, due to the subsymbolic nature of these models whatever knowledge these\nsystems acquire about language will always be buried in millions of\nmicrofeatures (weights) none of which is meaningful on its own. Moreover, and\ndue to their stochastic nature, these models will often fail in capturing\nvarious inferential aspects that are prevalent in natural language. What we\nsuggest here is employing the successful bottom-up strategy in a symbolic\nsetting, producing symbolic, language agnostic and ontologically grounded large\nlanguage models.\n",
                "链接": "https://arxiv.org/abs/2308.14199"
            },
            {
                "文章ID": "90555",
                "标题": "Large Language Models",
                "作者": " Michael R. Douglas",
                "发布日期": "2023-10-09",
                "摘要": "  Artificial intelligence is making spectacular progress, and one of the best\nexamples is the development of large language models (LLMs) such as OpenAI's\nGPT series. In these lectures, written for readers with a background in\nmathematics or physics, we give a brief history and survey of the state of the\nart, and describe the underlying transformer architecture in detail. We then\nexplore some current ideas on how LLMs work and how models trained to predict\nthe next word in a text are able to perform other tasks displaying\nintelligence.\n",
                "链接": "https://arxiv.org/abs/2307.05782"
            },
            {
                "文章ID": "123394",
                "标题": "Split and Rephrase with Large Language Models",
                "作者": " David Ponce,  Thierry Etchegoyhen,  Jesús Calleja Pérez,  Harritxu Gete",
                "发布日期": "2023-12-20",
                "摘要": "  The Split and Rephrase task, which consists in splitting complex sentences\ninto a sequence of shorter grammatical sentences, while preserving the original\nmeaning, can facilitate the processing of complex texts for humans and machines\nalike. In this work, we describe an approach based on large language models,\nwhich improves over the state of the art by large margins on all the major\nmetrics for the task, on publicly available datasets. We also describe results\nfrom two human evaluations that further establish the significant improvements\nobtained with large language models and the viability of the approach. We\nevaluate different strategies, including fine-tuning pretrained language models\nof varying parameter size, and applying both zero-shot and few-shot in-context\nlearning on instruction-tuned language models. Although the latter were\nmarkedly outperformed by fine-tuned models, they still achieved promising\nresults overall. Our results thus demonstrate the strong potential of different\nvariants of large language models for the Split and Rephrase task, using\nrelatively small amounts of training samples and model parameters overall.\n",
                "链接": "https://arxiv.org/abs/2312.11075"
            },
            {
                "文章ID": "116050",
                "标题": "When Large Language Models contradict humans? Large Language Models'\n  Sycophantic Behaviour",
                "作者": " Leonardo Ranaldi,  Giulia Pucci",
                "发布日期": "2023-11-17",
                "摘要": "  Large Language Models (LLMs) have been demonstrating the ability to solve\ncomplex tasks by delivering answers that are positively evaluated by humans due\nin part to the intensive use of human feedback that refines responses. However,\nthe suggestibility transmitted through human feedback increases the inclination\nto produce responses that correspond to the user's beliefs or misleading\nprompts as opposed to true facts, a behaviour known as sycophancy. This\nphenomenon decreases the bias, robustness, and, consequently, their\nreliability.\n  In this paper, we shed light on the suggestibility of LLMs to sycophantic\nbehaviour, demonstrating these tendencies via human-influenced prompts over\ndifferent tasks. Our investigation reveals that LLMs show sycophantic\ntendencies when responding to queries involving subjective opinions and\nstatements that should elicit a contrary response based on facts, demonstrating\na lack of robustness.\n",
                "链接": "https://arxiv.org/abs/2311.09410"
            },
            {
                "文章ID": "110339",
                "标题": "Copyright Violations and Large Language Models",
                "作者": " Antonia Karamolegkou,  Jiaang Li,  Li Zhou,  Anders Søgaard",
                "发布日期": "2023-10-24",
                "摘要": "  Language models may memorize more than just facts, including entire chunks of\ntexts seen during training. Fair use exemptions to copyright laws typically\nallow for limited use of copyrighted material without permission from the\ncopyright holder, but typically for extraction of information from copyrighted\nmaterials, rather than {\\em verbatim} reproduction. This work explores the\nissue of copyright violations and large language models through the lens of\nverbatim memorization, focusing on possible redistribution of copyrighted text.\nWe present experiments with a range of language models over a collection of\npopular books and coding problems, providing a conservative characterization of\nthe extent to which language models can redistribute these materials. Overall,\nthis research highlights the need for further examination and the potential\nimpact on future developments in natural language processing to ensure\nadherence to copyright regulations. Code is at\n\\url{https://github.com/coastalcph/CopyrightLLMs}.\n",
                "链接": "https://arxiv.org/abs/2310.13771"
            },
            {
                "文章ID": "91937",
                "标题": "Challenges and Applications of Large Language Models",
                "作者": " Jean Kaddour,  Joshua Harris,  Maximilian Mozes,  Herbie Bradley,  Roberta Raileanu,  Robert McHardy",
                "发布日期": "2023-07-20",
                "摘要": "  Large Language Models (LLMs) went from non-existent to ubiquitous in the\nmachine learning discourse within a few years. Due to the fast pace of the\nfield, it is difficult to identify the remaining challenges and already\nfruitful application areas. In this paper, we aim to establish a systematic set\nof open problems and application successes so that ML researchers can\ncomprehend the field's current state more quickly and become productive.\n",
                "链接": "https://arxiv.org/abs/2307.10169"
            },
            {
                "文章ID": "57243",
                "标题": "Dissociating language and thought in large language models",
                "作者": " Kyle Mahowald,  Anna A. Ivanova,  Idan A. Blank,  Nancy Kanwisher,  Joshua B. Tenenbaum,  Evelina Fedorenko",
                "发布日期": "2023-11-07",
                "摘要": "  Large language models (LLMs) have come closest among all models to date to\nmastering human language, yet opinions about their linguistic and cognitive\ncapabilities remain split. Here, we evaluate LLMs using a distinction between\nformal linguistic competence--knowledge of linguistic rules and patterns--and\nfunctional linguistic competence--understanding and using language in the\nworld. We ground this distinction in human neuroscience, showing that formal\nand functional competence rely on different neural mechanisms. Although LLMs\nare surprisingly good at formal competence, their performance on functional\ncompetence tasks remains spotty and often requires specialized fine-tuning\nand/or coupling with external modules. In short, LLMs are good models of\nlanguage but incomplete models of human thought.\n",
                "链接": "https://arxiv.org/abs/2301.06627"
            },
            {
                "文章ID": "92072",
                "标题": "Dynamic Large Language Models on Blockchains",
                "作者": " Yuanhao Gong",
                "发布日期": "2023-07-21",
                "摘要": "  Training and deploying the large language models requires a large mount of\ncomputational resource because the language models contain billions of\nparameters and the text has thousands of tokens. Another problem is that the\nlarge language models are static. They are fixed after the training process. To\ntackle these issues, in this paper, we propose to train and deploy the dynamic\nlarge language model on blockchains, which have high computation performance\nand are distributed across a network of computers. A blockchain is a secure,\ndecentralized, and transparent system that allows for the creation of a\ntamper-proof ledger for transactions without the need for intermediaries. The\ndynamic large language models can continuously learn from the user input after\nthe training process. Our method provides a new way to develop the large\nlanguage models and also sheds a light on the next generation artificial\nintelligence systems.\n",
                "链接": "https://arxiv.org/abs/2307.10549"
            },
            {
                "文章ID": "84099",
                "标题": "Turning large language models into cognitive models",
                "作者": " Marcel Binz,  Eric Schulz",
                "发布日期": "2023-06-08",
                "摘要": "  Large language models are powerful systems that excel at many tasks, ranging\nfrom translation to mathematical reasoning. Yet, at the same time, these models\noften show unhuman-like characteristics. In the present paper, we address this\ngap and ask whether large language models can be turned into cognitive models.\nWe find that -- after finetuning them on data from psychological experiments --\nthese models offer accurate representations of human behavior, even\noutperforming traditional cognitive models in two decision-making domains. In\naddition, we show that their representations contain the information necessary\nto model behavior on the level of individual subjects. Finally, we demonstrate\nthat finetuning on multiple tasks enables large language models to predict\nhuman behavior in a previously unseen task. Taken together, these results\nsuggest that large, pre-trained models can be adapted to become generalist\ncognitive models, thereby opening up new research directions that could\ntransform cognitive psychology and the behavioral sciences as a whole.\n",
                "链接": "https://arxiv.org/abs/2306.03917"
            },
            {
                "文章ID": "92881",
                "标题": "Multilevel Large Language Models for Everyone",
                "作者": " Yuanhao Gong",
                "发布日期": "2023-07-26",
                "摘要": "  Large language models have made significant progress in the past few years.\nHowever, they are either generic {\\it or} field specific, splitting the\ncommunity into different groups. In this paper, we unify these large language\nmodels into a larger map, where the generic {\\it and} specific models are\nlinked together and can improve each other, based on the user personal input\nand information from the internet. The idea of linking several large language\nmodels together is inspired by the functionality of human brain. The specific\nregions on the brain cortex are specific for certain low level functionality.\nAnd these regions can jointly work together to achieve more complex high level\nfunctionality. Such behavior on human brain cortex sheds the light to design\nthe multilevel large language models that contain global level, field level and\nuser level models. The user level models run on local machines to achieve\nefficient response and protect the user's privacy. Such multilevel models\nreduce some redundancy and perform better than the single level models. The\nproposed multilevel idea can be applied in various applications, such as\nnatural language processing, computer vision tasks, professional assistant,\nbusiness and healthcare.\n",
                "链接": "https://arxiv.org/abs/2307.13221"
            }
        ]
    },
    {
        "question": {
            "question": "2022年后与AI for Science相关的综述论文",
            "type": "6"
        },
        "results": [
            {
                "文章ID": "94694",
                "标题": "AI Literature Review Suite",
                "作者": " David A. Tovar",
                "发布日期": "2023-08-07",
                "摘要": "  The process of conducting literature reviews is often time-consuming and\nlabor-intensive. To streamline this process, I present an AI Literature Review\nSuite that integrates several functionalities to provide a comprehensive\nliterature review. This tool leverages the power of open access science, large\nlanguage models (LLMs) and natural language processing to enable the searching,\ndownloading, and organizing of PDF files, as well as extracting content from\narticles. Semantic search queries are used for data retrieval, while text\nembeddings and summarization using LLMs present succinct literature reviews.\nInteraction with PDFs is enhanced through a user-friendly graphical user\ninterface (GUI). The suite also features integrated programs for bibliographic\norganization, interaction and query, and literature review summaries. This tool\npresents a robust solution to automate and optimize the process of literature\nreview in academic and industrial research.\n",
                "链接": "https://arxiv.org/abs/2308.02443"
            },
            {
                "文章ID": "80483",
                "标题": "Science in the Era of ChatGPT, Large Language Models and Generative AI:\n  Challenges for Research Ethics and How to Respond",
                "作者": " Evangelos Pournaras",
                "发布日期": "2023-08-01",
                "摘要": "  Large language models of artificial intelligence (AI), such as ChatGPT, find\nremarkable but controversial applicability in science and research. This paper\nreviews epistemological challenges, ethical and integrity risks in science\nconduct in the advent of generative AI. This is with the aim to lay new timely\nfoundations for a high-quality research ethics review. The role of AI language\nmodels as a research instrument and subject is scrutinized along with ethical\nimplications for scientists, participants and reviewers. New emerging practices\nfor research ethics review are discussed, concluding with ten recommendations\nthat shape a response for a more responsible research conduct in the era of AI.\n",
                "链接": "https://arxiv.org/abs/2305.15299"
            },
            {
                "文章ID": "59467",
                "标题": "Serious Games and AI: Challenges and Opportunities for Computational\n  Social Science",
                "作者": " Jaime Pérez,  Mario Castro,  Gregorio López",
                "发布日期": "2023-07-06",
                "摘要": "  The video game industry plays an essential role in the entertainment sphere\nof our society. However, from Monopoly to Flight Simulators, serious games have\nalso been appealing tools for learning a new language, conveying values, or\ntraining skills. Furthermore, the resurgence of Artificial Intelligence (AI)\nand data science in the last decade has created a unique opportunity since the\namount of data collected through a game is immense, as is the amount of data\nneeded to feed such AI algorithms. This paper aims to identify relevant\nresearch lines using Serious Games as a novel research tool, especially in\nComputational Social Sciences. To contextualize, we also conduct a\n(non-systematic) literature review of this field. We conclude that the synergy\nbetween games and data can foster the use of AI for good and open up new\nstrategies to empower humanity and support social research with novel\ncomputational tools. We also discuss the challenges and new opportunities that\narise from aspiring to such lofty goals.\n",
                "链接": "https://arxiv.org/abs/2302.00500"
            },
            {
                "文章ID": "65341",
                "标题": "AI for Science: An Emerging Agenda",
                "作者": " Philipp Berens,  Kyle Cranmer,  Neil D. Lawrence,  Ulrike von Luxburg,  Jessica Montgomery",
                "发布日期": "2023-03-09",
                "摘要": "  This report documents the programme and the outcomes of Dagstuhl Seminar\n22382 \"Machine Learning for Science: Bridging Data-Driven and Mechanistic\nModelling\". Today's scientific challenges are characterised by complexity.\nInterconnected natural, technological, and human systems are influenced by\nforces acting across time- and spatial-scales, resulting in complex\ninteractions and emergent behaviours. Understanding these phenomena -- and\nleveraging scientific advances to deliver innovative solutions to improve\nsociety's health, wealth, and well-being -- requires new ways of analysing\ncomplex systems. The transformative potential of AI stems from its widespread\napplicability across disciplines, and will only be achieved through integration\nacross research domains. AI for science is a rendezvous point. It brings\ntogether expertise from $\\mathrm{AI}$ and application domains; combines\nmodelling knowledge with engineering know-how; and relies on collaboration\nacross disciplines and between humans and machines. Alongside technical\nadvances, the next wave of progress in the field will come from building a\ncommunity of machine learning researchers, domain experts, citizen scientists,\nand engineers working together to design and deploy effective AI tools. This\nreport summarises the discussions from the seminar and provides a roadmap to\nsuggest how different communities can collaborate to deliver a new wave of\nprogress in AI and its application for scientific discovery.\n",
                "链接": "https://arxiv.org/abs/2303.04217"
            },
            {
                "文章ID": "91376",
                "标题": "Towards eXplainable AI for Mobility Data Science",
                "作者": " Anahid Jalali,  Anita Graser,  Clemens Heistracher",
                "发布日期": "2023-09-08",
                "摘要": "  This paper presents our ongoing work towards XAI for Mobility Data Science\napplications, focusing on explainable models that can learn from dense\ntrajectory data, such as GPS tracks of vehicles and vessels using temporal\ngraph neural networks (GNNs) and counterfactuals. We review the existing GeoXAI\nstudies, argue the need for comprehensible explanations with human-centered\napproaches, and outline a research path toward XAI for Mobility Data Science.\n",
                "链接": "https://arxiv.org/abs/2307.08461"
            },
            {
                "文章ID": "116688",
                "标题": "Best uses of ChatGPT and Generative AI for computer science research",
                "作者": " Eduardo C. Garrido-Merchan",
                "发布日期": "2023-11-21",
                "摘要": "  Generative Artificial Intelligence (AI), particularly tools like OpenAI's\npopular ChatGPT, is reshaping the landscape of computer science research. Used\nwisely, these tools can boost the productivity of a computer research\nscientist. This paper provides an exploration of the diverse applications of\nChatGPT and other generative AI technologies in computer science academic\nresearch, making recommendations about the use of Generative AI to make more\nproductive the role of the computer research scientist, with the focus of\nwriting new research papers. We highlight innovative uses such as brainstorming\nresearch ideas, aiding in the drafting and styling of academic papers and\nassisting in the synthesis of state-of-the-art section. Further, we delve into\nusing these technologies in understanding interdisciplinary approaches, making\ncomplex texts simpler, and recommending suitable academic journals for\npublication. Significant focus is placed on generative AI's contributions to\nsynthetic data creation, research methodology, and mentorship, as well as in\ntask organization and article quality assessment. The paper also addresses the\nutility of AI in article review, adapting texts to length constraints,\nconstructing counterarguments, and survey development. Moreover, we explore the\ncapabilities of these tools in disseminating ideas, generating images and\naudio, text transcription, and engaging with editors. We also describe some\nnon-recommended uses of generative AI for computer science research, mainly\nbecause of the limitations of this technology.\n",
                "链接": "https://arxiv.org/abs/2311.11175"
            },
            {
                "文章ID": "110050",
                "标题": "AI for Mathematics: A Cognitive Science Perspective",
                "作者": " Cedegao E. Zhang,  Katherine M. Collins,  Adrian Weller,  Joshua B. Tenenbaum",
                "发布日期": "2023-10-23",
                "摘要": "  Mathematics is one of the most powerful conceptual systems developed and used\nby the human species. Dreams of automated mathematicians have a storied history\nin artificial intelligence (AI). Rapid progress in AI, particularly propelled\nby advances in large language models (LLMs), has sparked renewed, widespread\ninterest in building such systems. In this work, we reflect on these goals from\na \\textit{cognitive science} perspective. We call attention to several\nclassical and ongoing research directions from cognitive science, which we\nbelieve are valuable for AI practitioners to consider when seeking to build\ntruly human (or superhuman)-level mathematical systems. We close with open\ndiscussions and questions that we believe necessitate a multi-disciplinary\nperspective -- cognitive scientists working in tandem with AI researchers and\nmathematicians -- as we move toward better mathematical AI systems which not\nonly help us push the frontier of the mathematics, but also offer glimpses into\nhow we as humans are even capable of such great cognitive feats.\n",
                "链接": "https://arxiv.org/abs/2310.13021"
            },
            {
                "文章ID": "116561",
                "标题": "Generative AI has lowered the barriers to computational social sciences",
                "作者": " Yongjun Zhang",
                "发布日期": "2023-11-21",
                "摘要": "  Generative artificial intelligence (AI) has revolutionized the field of\ncomputational social science, unleashing new possibilities for analyzing\nmultimodal data, especially for scholars who may not have extensive programming\nexpertise. This breakthrough carries profound implications for the realm of\nsocial sciences. Firstly, generative AI can significantly enhance the\nproductivity of social scientists by automating the generation, annotation, and\ndebugging of code. Secondly, it empowers researchers to delve into\nsophisticated data analysis through the innovative use of prompt engineering.\nLastly, the educational sphere of computational social science stands to\nbenefit immensely from these tools, given their exceptional ability to annotate\nand elucidate complex codes for learners, thereby simplifying the learning\nprocess and making the technology more accessible.\n",
                "链接": "https://arxiv.org/abs/2311.10833"
            },
            {
                "文章ID": "55075",
                "标题": "On How AI Needs to Change to Advance the Science of Drug Discovery",
                "作者": " Kieran Didi,  Matej Zečević",
                "发布日期": "2022-12-27",
                "摘要": "  Research around AI for Science has seen significant success since the rise of\ndeep learning models over the past decade, even with longstanding challenges\nsuch as protein structure prediction. However, this fast development inevitably\nmade their flaws apparent -- especially in domains of reasoning where\nunderstanding the cause-effect relationship is important. One such domain is\ndrug discovery, in which such understanding is required to make sense of data\notherwise plagued by spurious correlations. Said spuriousness only becomes\nworse with the ongoing trend of ever-increasing amounts of data in the life\nsciences and thereby restricts researchers in their ability to understand\ndisease biology and create better therapeutics. Therefore, to advance the\nscience of drug discovery with AI it is becoming necessary to formulate the key\nproblems in the language of causality, which allows the explication of\nmodelling assumptions needed for identifying true cause-effect relationships.\n  In this attention paper, we present causal drug discovery as the craft of\ncreating models that ground the process of drug discovery in causal reasoning.\n",
                "链接": "https://arxiv.org/abs/2212.12560"
            },
            {
                "文章ID": "91991",
                "标题": "AI empowering research: 10 ways how science can benefit from AI",
                "作者": " César França",
                "发布日期": "2023-07-21",
                "摘要": "  This article explores the transformative impact of artificial intelligence\n(AI) on scientific research. It highlights ten ways in which AI is\nrevolutionizing the work of scientists, including powerful referencing tools,\nimproved understanding of research problems, enhanced research question\ngeneration, optimized research design, stub data generation, data\ntransformation, advanced data analysis, and AI-assisted reporting. While AI\noffers numerous benefits, challenges such as bias, privacy concerns, and the\nneed for human-AI collaboration must be considered. The article emphasizes that\nAI can augment human creativity in science but not replace it.\n",
                "链接": "https://arxiv.org/abs/2307.10265"
            }
        ]
    },
    {
        "question": {
            "question": "对比解码综述",
            "type": "6"
        },
        "results": [
            {
                "文章ID": "50069",
                "标题": "How do Authors' Perceptions of their Papers Compare with Co-authors'\n  Perceptions and Peer-review Decisions?",
                "作者": " Charvi Rastogi,  Ivan Stelmakh,  Alina Beygelzimer,  Yann N. Dauphin,  Percy Liang,  Jennifer Wortman Vaughan,  Zhenyu Xue, III Hal Daumé,  Emma Pierson,  Nihar B. Shah",
                "发布日期": "2022-11-24",
                "摘要": "  How do author perceptions match up to the outcomes of the peer-review process\nand perceptions of others? In a top-tier computer science conference (NeurIPS\n2021) with more than 23,000 submitting authors and 9,000 submitted papers, we\nsurvey the authors on three questions: (i) their predicted probability of\nacceptance for each of their papers, (ii) their perceived ranking of their own\npapers based on scientific contribution, and (iii) the change in their\nperception about their own papers after seeing the reviews. The salient results\nare: (1) Authors have roughly a three-fold overestimate of the acceptance\nprobability of their papers: The median prediction is 70% for an approximately\n25% acceptance rate. (2) Female authors exhibit a marginally higher\n(statistically significant) miscalibration than male authors; predictions of\nauthors invited to serve as meta-reviewers or reviewers are similarly\ncalibrated, but better than authors who were not invited to review. (3)\nAuthors' relative ranking of scientific contribution of two submissions they\nmade generally agree (93%) with their predicted acceptance probabilities, but\nthere is a notable 7% responses where authors think their better paper will\nface a worse outcome. (4) The author-provided rankings disagreed with the\npeer-review decisions about a third of the time; when co-authors ranked their\njointly authored papers, co-authors disagreed at a similar rate -- about a\nthird of the time. (5) At least 30% of respondents of both accepted and\nrejected papers said that their perception of their own paper improved after\nthe review process. The stakeholders in peer review should take these findings\ninto account in setting their expectations from peer review.\n",
                "链接": "https://arxiv.org/abs/2211.12966"
            },
            {
                "文章ID": "107470",
                "标题": "Learning to Decode the Surface Code with a Recurrent, Transformer-Based\n  Neural Network",
                "作者": " Johannes Bausch,  Andrew W Senior,  Francisco J H Heras,  Thomas Edlich,  Alex Davies,  Michael Newman,  Cody Jones,  Kevin Satzinger,  Murphy Yuezhen Niu,  Sam Blackwell,  George Holland,  Dvir Kafri,  Juan Atalaya,  Craig Gidney,  Demis Hassabis,  Sergio Boixo,  Hartmut Neven,  Pushmeet Kohli",
                "发布日期": "2023-10-10",
                "摘要": "  Quantum error-correction is a prerequisite for reliable quantum computation.\nTowards this goal, we present a recurrent, transformer-based neural network\nwhich learns to decode the surface code, the leading quantum error-correction\ncode. Our decoder outperforms state-of-the-art algorithmic decoders on\nreal-world data from Google's Sycamore quantum processor for distance 3 and 5\nsurface codes. On distances up to 11, the decoder maintains its advantage on\nsimulated data with realistic noise including cross-talk, leakage, and analog\nreadout signals, and sustains its accuracy far beyond the 25 cycles it was\ntrained on. Our work illustrates the ability of machine learning to go beyond\nhuman-designed algorithms by learning from data directly, highlighting machine\nlearning as a strong contender for decoding in quantum computers.\n",
                "链接": "https://arxiv.org/abs/2310.05900"
            },
            {
                "文章ID": "109422",
                "标题": "Sparse Multi-Object Render-and-Compare",
                "作者": " Florian Langer,  Ignas Budvytis,  Roberto Cipolla",
                "发布日期": "2023-10-18",
                "摘要": "  Reconstructing 3D shape and pose of static objects from a single image is an\nessential task for various industries, including robotics, augmented reality,\nand digital content creation. This can be done by directly predicting 3D shape\nin various representations or by retrieving CAD models from a database and\npredicting their alignments. Directly predicting 3D shapes often produces\nunrealistic, overly smoothed or tessellated shapes. Retrieving CAD models\nensures realistic shapes but requires robust and accurate alignment. Learning\nto directly predict CAD model poses from image features is challenging and\ninaccurate. Works, such as ROCA, compute poses from predicted normalised object\ncoordinates which can be more accurate but are susceptible to systematic\nfailure. SPARC demonstrates that following a ''render-and-compare'' approach\nwhere a network iteratively improves upon its own predictions achieves accurate\nalignments. Nevertheless, it performs individual CAD alignment for every object\ndetected in an image. This approach is slow when applied to many objects as the\ntime complexity increases linearly with the number of objects and can not learn\ninter-object relations. Introducing a new network architecture Multi-SPARC we\nlearn to perform CAD model alignments for multiple detected objects jointly.\nCompared to other single-view methods we achieve state-of-the-art performance\non the challenging real-world dataset ScanNet. By improving the instance\nalignment accuracy from 31.8% to 40.3% we perform similar to state-of-the-art\nmulti-view methods.\n",
                "链接": "https://arxiv.org/abs/2310.11184"
            },
            {
                "文章ID": "93773",
                "标题": "Count, Decode and Fetch: A New Approach to Handwritten Chinese Character\n  Error Correction",
                "作者": " Pengfei Hu,  Jiefeng Ma,  Zhenrong Zhang,  Jun Du,  Jianshu Zhang",
                "发布日期": "2023-08-01",
                "摘要": "  Recently, handwritten Chinese character error correction has been greatly\nimproved by employing encoder-decoder methods to decompose a Chinese character\ninto an ideographic description sequence (IDS). However, existing methods\nimplicitly capture and encode linguistic information inherent in IDS sequences,\nleading to a tendency to generate IDS sequences that match seen characters.\nThis poses a challenge when dealing with an unseen misspelled character, as the\ndecoder may generate an IDS sequence that matches a seen character instead.\nTherefore, we introduce Count, Decode and Fetch (CDF), a novel approach that\nexhibits better generalization towards unseen misspelled characters. CDF is\nmainly composed of three parts: the counter, the decoder, and the fetcher. In\nthe first stage, the counter predicts the number of each radical class without\nthe symbol-level position annotations. In the second stage, the decoder employs\nthe counting information and generates the IDS sequence step by step. Moreover,\nby updating the counting information at each time step, the decoder becomes\naware of the existence of each radical. With the decomposed IDS sequence, we\ncan determine whether the given character is misspelled. If it is misspelled,\nthe fetcher under the transductive transfer learning strategy predicts the\nideal character that the user originally intended to write. We integrate our\nmethod into existing encoder-decoder models and significantly enhance their\nperformance.\n",
                "链接": "https://arxiv.org/abs/2307.16253"
            },
            {
                "文章ID": "99517",
                "标题": "DECODE: DilatEd COnvolutional neural network for Detecting\n  Extreme-mass-ratio inspirals",
                "作者": " Tianyu Zhao,  Yue Zhou,  Ruijun Shi,  Zhoujian Cao,  Zhixiang Ren",
                "发布日期": "2023-10-17",
                "摘要": "  The detection of Extreme Mass Ratio Inspirals (EMRIs) is intricate due to\ntheir complex waveforms, extended duration, and low signal-to-noise ratio\n(SNR), making them more challenging to be identified compared to compact binary\ncoalescences. While matched filtering-based techniques are known for their\ncomputational demands, existing deep learning-based methods primarily handle\ntime-domain data and are often constrained by data duration and SNR. In\naddition, most existing work ignores time-delay interferometry (TDI) and\napplies the long-wavelength approximation in detector response calculations,\nthus limiting their ability to handle laser frequency noise. In this study, we\nintroduce DECODE, an end-to-end model focusing on EMRI signal detection by\nsequence modeling in the frequency domain. Centered around a dilated causal\nconvolutional neural network, trained on synthetic data considering TDI-1.5\ndetector response, DECODE can efficiently process a year's worth of\nmultichannel TDI data with an SNR of around 50. We evaluate our model on 1-year\ndata with accumulated SNR ranging from 50 to 120 and achieve a true positive\nrate of 96.3% at a false positive rate of 1%, keeping an inference time of less\nthan 0.01 seconds. With the visualization of three showcased EMRI signals for\ninterpretability and generalization, DECODE exhibits strong potential for\nfuture space-based gravitational wave data analyses.\n",
                "链接": "https://arxiv.org/abs/2308.16422"
            },
            {
                "文章ID": "99502",
                "标题": "SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked\n  Prefills",
                "作者": " Amey Agrawal,  Ashish Panwar,  Jayashree Mohan,  Nipun Kwatra,  Bhargav S. Gulavani,  Ramachandran Ramjee",
                "发布日期": "2023-09-01",
                "摘要": "  Large Language Model (LLM) inference consists of two distinct phases -\nprefill phase which processes the input prompt and decode phase which generates\noutput tokens autoregressively. While the prefill phase effectively saturates\nGPU compute at small batch sizes, the decode phase results in low compute\nutilization as it generates one token at a time per request. The varying\nprefill and decode times also lead to imbalance across micro-batches when using\npipeline parallelism, resulting in further inefficiency due to bubbles.\n  We present SARATHI to address these challenges. SARATHI employs\nchunked-prefills, which splits a prefill request into equal sized chunks, and\ndecode-maximal batching, which constructs a batch using a single prefill chunk\nand populates the remaining slots with decodes. During inference, the prefill\nchunk saturates GPU compute, while the decode requests 'piggyback' and cost up\nto an order of magnitude less compared to a decode-only batch. Chunked-prefills\nallows constructing multiple decode-maximal batches from a single prefill\nrequest, maximizing coverage of decodes that can piggyback. Furthermore, the\nuniform compute design of these batches ameliorates the imbalance between\nmicro-batches, significantly reducing pipeline bubbles.\n  Our techniques yield significant improvements in inference performance across\nmodels and hardware. For the LLaMA-13B model on A6000 GPU, SARATHI improves\ndecode throughput by up to 10x, and accelerates end-to-end throughput by up to\n1.33x. For LLaMa-33B on A100 GPU, we achieve 1.25x higher end-to-end-throughput\nand up to 4.25x higher decode throughput. When used with pipeline parallelism\non GPT-3, SARATHI reduces bubbles by 6.29x, resulting in an end-to-end\nthroughput improvement of 1.91x.\n",
                "链接": "https://arxiv.org/abs/2308.16369"
            },
            {
                "文章ID": "81377",
                "标题": "Counterfactual Evaluation of Peer-Review Assignment Policies",
                "作者": " Martin Saveski,  Steven Jecmen,  Nihar B. Shah,  Johan Ugander",
                "发布日期": "2023-05-30",
                "摘要": "  Peer review assignment algorithms aim to match research papers to suitable\nexpert reviewers, working to maximize the quality of the resulting reviews. A\nkey challenge in designing effective assignment policies is evaluating how\nchanges to the assignment algorithm map to changes in review quality. In this\nwork, we leverage recently proposed policies that introduce randomness in\npeer-review assignment--in order to mitigate fraud--as a valuable opportunity\nto evaluate counterfactual assignment policies. Specifically, we exploit how\nsuch randomized assignments provide a positive probability of observing the\nreviews of many assignment policies of interest. To address challenges in\napplying standard off-policy evaluation methods, such as violations of\npositivity, we introduce novel methods for partial identification based on\nmonotonicity and Lipschitz smoothness assumptions for the mapping between\nreviewer-paper covariates and outcomes. We apply our methods to peer-review\ndata from two computer science venues: the TPDP'21 workshop (95 papers and 35\nreviewers) and the AAAI'22 conference (8,450 papers and 3,145 reviewers). We\nconsider estimates of (i) the effect on review quality when changing weights in\nthe assignment algorithm, e.g., weighting reviewers' bids vs. textual\nsimilarity (between the review's past papers and the submission), and (ii) the\n\"cost of randomization\", capturing the difference in expected quality between\nthe perturbed and unperturbed optimal match. We find that placing higher weight\non text similarity results in higher review quality and that introducing\nrandomization in the reviewer-paper assignment only marginally reduces the\nreview quality. Our methods for partial identification may be of independent\ninterest, while our off-policy approach can likely find use evaluating a broad\nclass of algorithmic matching systems.\n",
                "链接": "https://arxiv.org/abs/2305.17339"
            },
            {
                "文章ID": "70884",
                "标题": "Learning to Compare Longitudinal Images",
                "作者": " Heejong Kim,  Mert R. Sabuncu",
                "发布日期": "2023-04-18",
                "摘要": "  Longitudinal studies, where a series of images from the same set of\nindividuals are acquired at different time-points, represent a popular\ntechnique for studying and characterizing temporal dynamics in biomedical\napplications. The classical approach for longitudinal comparison involves\nnormalizing for nuisance variations, such as image orientation or contrast\ndifferences, via pre-processing. Statistical analysis is, in turn, conducted to\ndetect changes of interest, either at the individual or population level. This\nclassical approach can suffer from pre-processing issues and limitations of the\nstatistical modeling. For example, normalizing for nuisance variation might be\nhard in settings where there are a lot of idiosyncratic changes. In this paper,\nwe present a simple machine learning-based approach that can alleviate these\nissues. In our approach, we train a deep learning model (called PaIRNet, for\nPairwise Image Ranking Network) to compare pairs of longitudinal images, with\nor without supervision. In the self-supervised setup, for instance, the model\nis trained to temporally order the images, which requires learning to recognize\ntime-irreversible changes. Our results from four datasets demonstrate that\nPaIRNet can be very effective in localizing and quantifying meaningful\nlongitudinal changes while discounting nuisance variation. Our code is\navailable at\n\\url{https://github.com/heejong-kim/learning-to-compare-longitudinal-images.git}\n",
                "链接": "https://arxiv.org/abs/2304.02531"
            },
            {
                "文章ID": "80431",
                "标题": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review\n  Generation",
                "作者": " Tetsu Kasanishi,  Masaru Isonuma,  Junichiro Mori,  Ichiro Sakata",
                "发布日期": "2023-05-25",
                "摘要": "  Automatic literature review generation is one of the most challenging tasks\nin natural language processing. Although large language models have tackled\nliterature review generation, the absence of large-scale datasets has been a\nstumbling block to the progress. We release SciReviewGen, consisting of over\n10,000 literature reviews and 690,000 papers cited in the reviews. Based on the\ndataset, we evaluate recent transformer-based summarization models on the\nliterature review generation task, including Fusion-in-Decoder extended for\nliterature review generation. Human evaluation results show that some\nmachine-generated summaries are comparable to human-written reviews, while\nrevealing the challenges of automatic literature review generation such as\nhallucinations and a lack of detailed information. Our dataset and code are\navailable at https://github.com/tetsu9923/SciReviewGen.\n",
                "链接": "https://arxiv.org/abs/2305.15186"
            },
            {
                "文章ID": "70216",
                "标题": "Reviewer Assignment Problem: A Systematic Review of the Literature",
                "作者": " Meltem Aksoy,  Seda Yanik,  Mehmet Fatih Amasyali",
                "发布日期": "2023-04-05",
                "摘要": "  Appropriate reviewer assignment significantly impacts the quality of proposal\nevaluation, as accurate and fair reviews are contingent on their assignment to\nrelevant reviewers. The crucial task of assigning reviewers to submitted\nproposals is the starting point of the review process and is also known as the\nreviewer assignment problem (RAP). Due to the obvious restrictions of manual\nassignment, journal editors, conference organizers, and grant managers demand\nautomatic reviewer assignment approaches. Many studies have proposed assignment\nsolutions in response to the demand for automated procedures since 1992. The\nprimary objective of this survey paper is to provide scholars and practitioners\nwith a comprehensive overview of available research on the RAP. To achieve this\ngoal, this article presents an in-depth systematic review of 103 publications\nin the field of reviewer assignment published in the past three decades and\navailable in the Web of Science, Scopus, ScienceDirect, Google Scholar, and\nSemantic Scholar databases. This review paper classified and discussed the RAP\napproaches into two broad categories and numerous subcategories based on their\nunderlying techniques. Furthermore, potential future research directions for\neach category are presented. This survey shows that the research on the RAP is\nbecoming more significant and that more effort is required to develop new\napproaches and a framework.\n",
                "链接": "https://arxiv.org/abs/2304.00353"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下近三个月有关语言模型rlhf的arxiv上的全部文章。",
            "type": "5"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找多模态大模型理解和生成统一建模、端到端训练相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "104271",
                "标题": "LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language\n  Models",
                "作者": " Ahmad Faiz,  Sotaro Kaneda,  Ruhan Wang,  Rita Osi,  Parteek Sharma,  Fan Chen,  Lei Jiang",
                "发布日期": "2023-09-27",
                "摘要": "  The carbon footprint associated with large language models (LLMs) is a\nsignificant concern, encompassing emissions from their training, inference,\nexperimentation, and storage processes, including operational and embodied\ncarbon emissions. An essential aspect is accurately estimating the carbon\nimpact of emerging LLMs even before their training, which heavily relies on GPU\nusage. Existing studies have reported the carbon footprint of LLM training, but\nonly one tool, mlco2, can predict the carbon footprint of new neural networks\nprior to physical training. However, mlco2 has several serious limitations. It\ncannot extend its estimation to dense or mixture-of-experts (MoE) LLMs,\ndisregards critical architectural parameters, focuses solely on GPUs, and\ncannot model embodied carbon footprints. Addressing these gaps, we introduce\n\\textit{LLMCarbon}, an end-to-end carbon footprint projection model designed\nfor both dense and MoE LLMs. Compared to mlco2, LLMCarbon significantly\nenhances the accuracy of carbon footprint estimations for various LLMs.\n",
                "链接": "https://arxiv.org/abs/2309.14393"
            },
            {
                "文章ID": "79233",
                "标题": "Zero-Shot End-to-End Spoken Language Understanding via Cross-Modal\n  Selective Self-Training",
                "作者": " Jianfeng He,  Julian Salazar,  Kaisheng Yao,  Haoqi Li,  Jinglun Cai",
                "发布日期": "2023-05-23",
                "摘要": "  End-to-end (E2E) spoken language understanding (SLU) is constrained by the\ncost of collecting speech-semantics pairs, especially when label domains\nchange. Hence, we explore \\textit{zero-shot} E2E SLU, which learns E2E SLU\nwithout speech-semantics pairs, instead using only speech-text and\ntext-semantics pairs. Previous work achieved zero-shot by pseudolabeling all\nspeech-text transcripts with a natural language understanding (NLU) model\nlearned on text-semantics corpora. However, this method requires the domains of\nspeech-text and text-semantics to match, which often mismatch due to separate\ncollections. Furthermore, using the entire speech-text corpus from any domains\nleads to \\textit{imbalance} and \\textit{noise} issues. To address these, we\npropose \\textit{cross-modal selective self-training} (CMSST). CMSST tackles\nimbalance by clustering in a joint space of the three modalities (speech, text,\nand semantics) and handles label noise with a selection network. We also\nintroduce two benchmarks for zero-shot E2E SLU, covering matched and found\nspeech (mismatched) settings. Experiments show that CMSST improves performance\nin both two settings, with significantly reduced sample sizes and training\ntime.\n",
                "链接": "https://arxiv.org/abs/2305.12793"
            },
            {
                "文章ID": "45814",
                "标题": "End-to-end Spoken Language Understanding with Tree-constrained Pointer\n  Generator",
                "作者": " Guangzhi Sun,  Chao Zhang,  Philip C. Woodland",
                "发布日期": "2023-03-16",
                "摘要": "  End-to-end spoken language understanding (SLU) suffers from the long-tail\nword problem. This paper exploits contextual biasing, a technique to improve\nthe speech recognition of rare words, in end-to-end SLU systems. Specifically,\na tree-constrained pointer generator (TCPGen), a powerful and efficient biasing\nmodel component, is studied, which leverages a slot shortlist with\ncorresponding entities to extract biasing lists. Meanwhile, to bias the SLU\nmodel output slot distribution, a slot probability biasing (SPB) mechanism is\nproposed to calculate a slot distribution from TCPGen. Experiments on the SLURP\ndataset showed consistent SLU-F1 improvements using TCPGen and SPB, especially\non unseen entities. On a new split by holding out 5 slot types for the test,\nTCPGen with SPB achieved zero-shot learning with an SLU-F1 score over 50%\ncompared to baselines which can not deal with it. In addition to slot filling,\nthe intent classification accuracy was also improved.\n",
                "链接": "https://arxiv.org/abs/2210.16554"
            },
            {
                "文章ID": "92569",
                "标题": "Modality Confidence Aware Training for Robust End-to-End Spoken Language\n  Understanding",
                "作者": " Suyoun Kim,  Akshat Shrivastava,  Duc Le,  Ju Lin,  Ozlem Kalinli,  Michael L. Seltzer",
                "发布日期": "2023-07-25",
                "摘要": "  End-to-end (E2E) spoken language understanding (SLU) systems that generate a\nsemantic parse from speech have become more promising recently. This approach\nuses a single model that utilizes audio and text representations from\npre-trained speech recognition models (ASR), and outperforms traditional\npipeline SLU systems in on-device streaming scenarios. However, E2E SLU systems\nstill show weakness when text representation quality is low due to ASR\ntranscription errors. To overcome this issue, we propose a novel E2E SLU system\nthat enhances robustness to ASR errors by fusing audio and text representations\nbased on the estimated modality confidence of ASR hypotheses. We introduce two\nnovel techniques: 1) an effective method to encode the quality of ASR\nhypotheses and 2) an effective approach to integrate them into E2E SLU models.\nWe show accuracy improvements on STOP dataset and share the analysis to\ndemonstrate the effectiveness of our approach.\n",
                "链接": "https://arxiv.org/abs/2307.12134"
            },
            {
                "文章ID": "106028",
                "标题": "Tuning Large language model for End-to-end Speech Translation",
                "作者": " Hao Zhang,  Nianwen Si,  Yaqi Chen,  Wenlin Zhang,  Xukui Yang,  Dan Qu,  Xiaolin Jiao",
                "发布日期": "2023-10-04",
                "摘要": "  With the emergence of large language models (LLMs), multimodal models based\non LLMs have demonstrated significant potential. Models such as LLaSM, X-LLM,\nand SpeechGPT exhibit an impressive ability to comprehend and generate human\ninstructions. However, their performance often falters when faced with complex\ntasks like end-to-end speech translation (E2E-ST), a cross-language and\ncross-modal translation task. In comparison to single-modal models, multimodal\nmodels lag behind in these scenarios. This paper introduces LST, a Large\nmultimodal model designed to excel at the E2E-ST task. LST consists of a speech\nfrontend, an adapter, and a LLM backend. The training of LST consists of two\nstages: (1) Modality adjustment, where the adapter is tuned to align speech\nrepresentation with text embedding space, and (2) Downstream task fine-tuning,\nwhere both the adapter and LLM model are trained to optimize performance on the\nE2EST task. Experimental results on the MuST-C speech translation benchmark\ndemonstrate that LST-13B achieves BLEU scores of 30.39/41.55/35.33 on\nEn-De/En-Fr/En-Es language pairs, surpassing previous models and establishing a\nnew state-of-the-art. Additionally, we conduct an in-depth analysis of\nsingle-modal model selection and the impact of training strategies, which lays\nthe foundation for future research. We will open up our code and models after\nreview.\n",
                "链接": "https://arxiv.org/abs/2310.02050"
            },
            {
                "文章ID": "108687",
                "标题": "QUIK: Towards End-to-End 4-Bit Inference on Generative Large Language\n  Models",
                "作者": " Saleh Ashkboos,  Ilia Markov,  Elias Frantar,  Tingxuan Zhong,  Xincheng Wang,  Jie Ren,  Torsten Hoefler,  Dan Alistarh",
                "发布日期": "2023-11-03",
                "摘要": "  Large Language Models (LLMs) from the GPT family have become extremely\npopular, leading to a race towards reducing their inference costs to allow for\nefficient local computation. Yet, the vast majority of existing work focuses on\nweight-only quantization, which can reduce runtime costs in the memory-bound\none-token-at-a-time generative setting, but does not address them in\ncompute-bound scenarios, such as batched inference or prompt processing. In\nthis paper, we address the general quantization problem, where both weights and\nactivations should be quantized. We show, for the first time, that the majority\nof inference computations for large generative models such as LLaMA, OPT, and\nFalcon can be performed with both weights and activations being cast to 4 bits,\nin a way that leads to practical speedups, while at the same time maintaining\ngood accuracy. We achieve this via a hybrid quantization strategy called QUIK,\nwhich compresses most of the weights and activations to 4-bit, while keeping\nsome outlier weights and activations in higher-precision. The key feature of\nour scheme is that it is designed with computational efficiency in mind: we\nprovide GPU kernels matching the QUIK format with highly-efficient layer-wise\nruntimes, which lead to practical end-to-end throughput improvements of up to\n3.4x relative to FP16 execution. We provide detailed studies for models from\nthe OPT, LLaMA-2 and Falcon families, as well as a first instance of accurate\ninference using quantization plus 2:4 sparsity. Code is available at:\nhttps://github.com/IST-DASLab/QUIK.\n",
                "链接": "https://arxiv.org/abs/2310.09259"
            },
            {
                "文章ID": "103083",
                "标题": "End-to-End Speech Recognition Contextualization with Large Language\n  Models",
                "作者": " Egor Lakomkin,  Chunyang Wu,  Yassir Fathullah,  Ozlem Kalinli,  Michael L. Seltzer,  Christian Fuegen",
                "发布日期": "2023-09-21",
                "摘要": "  In recent years, Large Language Models (LLMs) have garnered significant\nattention from the research community due to their exceptional performance and\ngeneralization capabilities. In this paper, we introduce a novel method for\ncontextualizing speech recognition models incorporating LLMs. Our approach\ncasts speech recognition as a mixed-modal language modeling task based on a\npretrained LLM. We provide audio features, along with optional text tokens for\ncontext, to train the system to complete transcriptions in a decoder-only\nfashion. As a result, the system is implicitly incentivized to learn how to\nleverage unstructured contextual information during training. Our empirical\nresults demonstrate a significant improvement in performance, with a 6% WER\nreduction when additional textual context is provided. Moreover, we find that\nour method performs competitively and improve by 7.5% WER overall and 17% WER\non rare words against a baseline contextualized RNN-T system that has been\ntrained on more than twenty five times larger speech dataset. Overall, we\ndemonstrate that by only adding a handful number of trainable parameters via\nadapters, we can unlock contextualized speech recognition capability for the\npretrained LLM while keeping the same text-only input functionality.\n",
                "链接": "https://arxiv.org/abs/2309.10917"
            },
            {
                "文章ID": "2020",
                "标题": "End-to-end Generative Pretraining for Multimodal Video Captioning",
                "作者": " Paul Hongsuck Seo,  Arsha Nagrani,  Anurag Arnab,  Cordelia Schmid",
                "发布日期": "2022-05-11",
                "摘要": "  Recent video and language pretraining frameworks lack the ability to generate\nsentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new\npretraining framework for learning from unlabelled videos which can be\neffectively used for generative tasks such as multimodal video captioning.\nUnlike recent video-language pretraining frameworks, our framework trains both\na multimodal video encoder and a sentence decoder jointly. To overcome the lack\nof captions in unlabelled videos, we leverage the future utterance as an\nadditional text source and propose a bidirectional generation objective -- we\ngenerate future utterances given the present mulitmodal context, and also the\npresent utterance given future observations. With this objective, we train an\nencoder-decoder model end-to-end to generate a caption from raw pixels and\ntranscribed speech directly. Our model achieves state-of-the-art performance\nfor multimodal video captioning on four standard benchmarks, as well as for\nother video understanding tasks such as VideoQA, video retrieval and action\nclassification.\n",
                "链接": "https://arxiv.org/abs/2201.08264"
            },
            {
                "文章ID": "27280",
                "标题": "Toward Low-Cost End-to-End Spoken Language Understanding",
                "作者": " Marco Dinarelli,  Marco Naguib,  François Portet",
                "发布日期": "2022-07-04",
                "摘要": "  Recent advances in spoken language understanding benefited from\nSelf-Supervised models trained on large speech corpora. For French, the\nLeBenchmark project has made such models available and has led to impressive\nprogress on several tasks including spoken language understanding. These\nadvances have a non-negligible cost in terms of computation time and energy\nconsumption. In this paper, we compare several learning strategies trying to\nreduce such cost while keeping competitive performance. At the same time we\npropose an extensive analysis where we measure the cost of our models in terms\nof training time and electric energy consumption, hopefully promoting a\ncomprehensive evaluation procedure. The experiments are performed on the FSC\nand MEDIA corpora, and show that it is possible to reduce the learning cost\nwhile maintaining state-of-the-art performance and using SSL models.\n",
                "链接": "https://arxiv.org/abs/2207.00352"
            },
            {
                "文章ID": "105786",
                "标题": "DriveGPT4: Interpretable End-to-end Autonomous Driving via Large\n  Language Model",
                "作者": " Zhenhua Xu,  Yujia Zhang,  Enze Xie,  Zhen Zhao,  Yong Guo,  Kwan-Yee. K. Wong,  Zhenguo Li,  Hengshuang Zhao",
                "发布日期": "2023-10-10",
                "摘要": "  In the past decade, autonomous driving has experienced rapid development in\nboth academia and industry. However, its limited interpretability remains a\nsignificant unsolved problem, severely hindering autonomous vehicle\ncommercialization and further development. Previous approaches utilizing small\nlanguage models have failed to address this issue due to their lack of\nflexibility, generalization ability, and robustness. Recently, multimodal large\nlanguage models (LLMs) have gained considerable attention from the research\ncommunity for their capability to process and reason non-text data (e.g.,\nimages and videos) by text. In this paper, we present DriveGPT4, an\ninterpretable end-to-end autonomous driving system utilizing LLMs. DriveGPT4 is\ncapable of interpreting vehicle actions and providing corresponding reasoning,\nas well as answering diverse questions posed by human users for enhanced\ninteraction. Additionally, DriveGPT4 predicts vehicle low-level control signals\nin an end-to-end fashion. These capabilities stem from a customized visual\ninstruction tuning dataset specifically designed for autonomous driving. To the\nbest of our knowledge, DriveGPT4 is the first work focusing on interpretable\nend-to-end autonomous driving. When evaluated on multiple tasks alongside\nconventional methods and video understanding LLMs, DriveGPT4 demonstrates\nsuperior qualitative and quantitative performance. Additionally, DriveGPT4 can\nbe generalized in a zero-shot fashion to accommodate more unseen scenarios. The\nproject page is available at https://tonyxuqaq.github.io/projects/DriveGPT4/ .\n",
                "链接": "https://arxiv.org/abs/2310.01412"
            }
        ]
    },
    {
        "question": {
            "question": "查找可以用于验证模型推理能力的数据集论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "12157",
                "标题": "LogicInference: A New Dataset for Teaching Logical Inference to seq2seq\n  Models",
                "作者": " Santiago Ontanon,  Joshua Ainslie,  Vaclav Cvicek,  Zachary Fisher",
                "发布日期": "2022-04-12",
                "摘要": "  Machine learning models such as Transformers or LSTMs struggle with tasks\nthat are compositional in nature such as those involving reasoning/inference.\nAlthough many datasets exist to evaluate compositional generalization, when it\ncomes to evaluating inference abilities, options are more limited. This paper\npresents LogicInference, a new dataset to evaluate the ability of models to\nperform logical inference. The dataset focuses on inference using propositional\nlogic and a small subset of first-order logic, represented both in semi-formal\nlogical notation, as well as in natural language. We also report initial\nresults using a collection of machine learning models to establish an initial\nbaseline in this dataset.\n",
                "链接": "https://arxiv.org/abs/2203.15099"
            },
            {
                "文章ID": "109326",
                "标题": "UNK-VQA: A Dataset and A Probe into Multi-modal Large Models' Abstention\n  Ability",
                "作者": " Yanyang Guo,  Fangkai Jiao,  Zhiqi Shen,  Liqiang Nie,  Mohan Kankanhalli",
                "发布日期": "2023-10-31",
                "摘要": "  Teaching Visual Question Answering (VQA) models to refrain from answering\nunanswerable questions is necessary for building a trustworthy AI system.\nExisting studies, though have explored various aspects of VQA but somewhat\nignored this particular attribute. This paper aims to bridge the research gap\nby contributing a comprehensive dataset, called UNK-VQA. The dataset is\nspecifically designed to address the challenge of questions that models do not\nknow. To this end, we first augment the existing data via deliberate\nperturbations on either the image or question. In specific, we carefully ensure\nthat the question-image semantics remain close to the original unperturbed\ndistribution. By this means, the identification of unanswerable questions\nbecomes challenging, setting our dataset apart from others that involve mere\nimage replacement. We then extensively evaluate the zero- and few-shot\nperformance of several emerging multi-modal large models and discover their\nsignificant limitations when applied to our dataset. Additionally, we also\npropose a straightforward method to tackle these unanswerable questions. This\ndataset, we believe, will serve as a valuable benchmark for enhancing the\nabstention capability of VQA models, thereby leading to increased\ntrustworthiness of AI systems. We have made the\n\\href{https://github.com/guoyang9/UNK-VQA}{dataset} available to facilitate\nfurther exploration in this area.\n",
                "链接": "https://arxiv.org/abs/2310.10942"
            },
            {
                "文章ID": "37962",
                "标题": "Dataset Inference for Self-Supervised Models",
                "作者": " Adam Dziedzic,  Haonan Duan,  Muhammad Ahmad Kaleem,  Nikita Dhawan,  Jonas Guan,  Yannis Cattan,  Franziska Boenisch,  Nicolas Papernot",
                "发布日期": "2023-01-18",
                "摘要": "  Self-supervised models are increasingly prevalent in machine learning (ML)\nsince they reduce the need for expensively labeled data. Because of their\nversatility in downstream applications, they are increasingly used as a service\nexposed via public APIs. At the same time, these encoder models are\nparticularly vulnerable to model stealing attacks due to the high\ndimensionality of vector representations they output. Yet, encoders remain\nundefended: existing mitigation strategies for stealing attacks focus on\nsupervised learning. We introduce a new dataset inference defense, which uses\nthe private training set of the victim encoder model to attribute its ownership\nin the event of stealing. The intuition is that the log-likelihood of an\nencoder's output representations is higher on the victim's training data than\non test data if it is stolen from the victim, but not if it is independently\ntrained. We compute this log-likelihood using density estimation models. As\npart of our evaluation, we also propose measuring the fidelity of stolen\nencoders and quantifying the effectiveness of the theft detection without\ninvolving downstream tasks; instead, we leverage mutual information and\ndistance measurements. Our extensive empirical results in the vision domain\ndemonstrate that dataset inference is a promising direction for defending\nself-supervised models against model stealing.\n",
                "链接": "https://arxiv.org/abs/2209.09024"
            },
            {
                "文章ID": "65767",
                "标题": "Position Paper on Dataset Engineering to Accelerate Science",
                "作者": " Emilio Vital Brazil,  Eduardo Soares,  Lucas Villa Real,  Leonardo Azevedo,  Vinicius Segura,  Luiz Zerkowski,  Renato Cerqueira",
                "发布日期": "2023-03-13",
                "摘要": "  Data is a critical element in any discovery process. In the last decades, we\nobserved exponential growth in the volume of available data and the technology\nto manipulate it. However, data is only practical when one can structure it for\na well-defined task. For instance, we need a corpus of text broken into\nsentences to train a natural language machine-learning model. In this work, we\nwill use the token \\textit{dataset} to designate a structured set of data built\nto perform a well-defined task. Moreover, the dataset will be used in most\ncases as a blueprint of an entity that at any moment can be stored as a table.\nSpecifically, in science, each area has unique forms to organize, gather and\nhandle its datasets. We believe that datasets must be a first-class entity in\nany knowledge-intensive process, and all workflows should have exceptional\nattention to datasets' lifecycle, from their gathering to uses and evolution.\nWe advocate that science and engineering discovery processes are extreme\ninstances of the need for such organization on datasets, claiming for new\napproaches and tooling. Furthermore, these requirements are more evident when\nthe discovery workflow uses artificial intelligence methods to empower the\nsubject-matter expert. In this work, we discuss an approach to bringing\ndatasets as a critical entity in the discovery process in science. We\nillustrate some concepts using material discovery as a use case. We chose this\ndomain because it leverages many significant problems that can be generalized\nto other science fields.\n",
                "链接": "https://arxiv.org/abs/2303.05545"
            },
            {
                "文章ID": "11787",
                "标题": "CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues",
                "作者": " Deepanway Ghosal,  Siqi Shen,  Navonil Majumder,  Rada Mihalcea,  Soujanya Poria",
                "发布日期": "2022-04-08",
                "摘要": "  This paper addresses the problem of dialogue reasoning with contextualized\ncommonsense inference. We curate CICERO, a dataset of dyadic conversations with\nfive types of utterance-level reasoning-based inferences: cause, subsequent\nevent, prerequisite, motivation, and emotional reaction. The dataset contains\n53,105 of such inferences from 5,672 dialogues. We use this dataset to solve\nrelevant generative and discriminative tasks: generation of cause and\nsubsequent event; generation of prerequisite, motivation, and listener's\nemotional reaction; and selection of plausible alternatives. Our results\nascertain the value of such dialogue-centric commonsense knowledge datasets. It\nis our hope that CICERO will open new research avenues into commonsense-based\ndialogue reasoning.\n",
                "链接": "https://arxiv.org/abs/2203.13926"
            },
            {
                "文章ID": "32390",
                "标题": "A Case for Dataset Specific Profiling",
                "作者": " Seth Ockerman,  John Wu,  Christopher Stewart",
                "发布日期": "2022-08-09",
                "摘要": "  Data-driven science is an emerging paradigm where scientific discoveries\ndepend on the execution of computational AI models against rich,\ndiscipline-specific datasets. With modern machine learning frameworks, anyone\ncan develop and execute computational models that reveal concepts hidden in the\ndata that could enable scientific applications. For important and widely used\ndatasets, computing the performance of every computational model that can run\nagainst a dataset is cost prohibitive in terms of cloud resources. Benchmarking\napproaches used in practice use representative datasets to infer performance\nwithout actually executing models. While practicable, these approaches limit\nextensive dataset profiling to a few datasets and introduce bias that favors\nmodels suited for representative datasets. As a result, each dataset's unique\ncharacteristics are left unexplored and subpar models are selected based on\ninference from generalized datasets. This necessitates a new paradigm that\nintroduces dataset profiling into the model selection process. To demonstrate\nthe need for dataset-specific profiling, we answer two questions:(1) Can\nscientific datasets significantly permute the rank order of computational\nmodels compared to widely used representative datasets? (2) If so, could\nlightweight model execution improve benchmarking accuracy? Taken together, the\nanswers to these questions lay the foundation for a new dataset-aware\nbenchmarking paradigm.\n",
                "链接": "https://arxiv.org/abs/2208.03315"
            },
            {
                "文章ID": "27917",
                "标题": "A Dataset on Malicious Paper Bidding in Peer Review",
                "作者": " Steven Jecmen,  Minji Yoon,  Vincent Conitzer,  Nihar B. Shah,  Fei Fang",
                "发布日期": "2023-03-14",
                "摘要": "  In conference peer review, reviewers are often asked to provide \"bids\" on\neach submitted paper that express their interest in reviewing that paper. A\npaper assignment algorithm then uses these bids (along with other data) to\ncompute a high-quality assignment of reviewers to papers. However, this process\nhas been exploited by malicious reviewers who strategically bid in order to\nunethically manipulate the paper assignment, crucially undermining the peer\nreview process. For example, these reviewers may aim to get assigned to a\nfriend's paper as part of a quid-pro-quo deal. A critical impediment towards\ncreating and evaluating methods to mitigate this issue is the lack of any\npublicly-available data on malicious paper bidding. In this work, we collect\nand publicly release a novel dataset to fill this gap, collected from a mock\nconference activity where participants were instructed to bid either honestly\nor maliciously. We further provide a descriptive analysis of the bidding\nbehavior, including our categorization of different strategies employed by\nparticipants. Finally, we evaluate the ability of each strategy to manipulate\nthe assignment, and also evaluate the performance of some simple algorithms\nmeant to detect malicious bidding. The performance of these detection\nalgorithms can be taken as a baseline for future research on detecting\nmalicious bidding.\n",
                "链接": "https://arxiv.org/abs/2207.02303"
            },
            {
                "文章ID": "91795",
                "标题": "Improved Distribution Matching for Dataset Condensation",
                "作者": " Ganlong Zhao,  Guanbin Li,  Yipeng Qin,  Yizhou Yu",
                "发布日期": "2023-07-20",
                "摘要": "  Dataset Condensation aims to condense a large dataset into a smaller one\nwhile maintaining its ability to train a well-performing model, thus reducing\nthe storage cost and training effort in deep learning applications. However,\nconventional dataset condensation methods are optimization-oriented and\ncondense the dataset by performing gradient or parameter matching during model\noptimization, which is computationally intensive even on small datasets and\nmodels. In this paper, we propose a novel dataset condensation method based on\ndistribution matching, which is more efficient and promising. Specifically, we\nidentify two important shortcomings of naive distribution matching (i.e.,\nimbalanced feature numbers and unvalidated embeddings for distance computation)\nand address them with three novel techniques (i.e., partitioning and expansion\naugmentation, efficient and enriched model sampling, and class-aware\ndistribution regularization). Our simple yet effective method outperforms most\nprevious optimization-oriented methods with much fewer computational resources,\nthereby scaling data condensation to larger datasets and models. Extensive\nexperiments demonstrate the effectiveness of our method. Codes are available at\nhttps://github.com/uitrbn/IDM\n",
                "链接": "https://arxiv.org/abs/2307.09742"
            },
            {
                "文章ID": "81273",
                "标题": "Characterizing and Measuring Linguistic Dataset Drift",
                "作者": " Tyler A. Chang,  Kishaloy Halder,  Neha Anna John,  Yogarshi Vyas,  Yassine Benajiba,  Miguel Ballesteros,  Dan Roth",
                "发布日期": "2023-05-29",
                "摘要": "  NLP models often degrade in performance when real world data distributions\ndiffer markedly from training data. However, existing dataset drift metrics in\nNLP have generally not considered specific dimensions of linguistic drift that\naffect model performance, and they have not been validated in their ability to\npredict model performance at the individual example level, where such metrics\nare often used in practice. In this paper, we propose three dimensions of\nlinguistic dataset drift: vocabulary, structural, and semantic drift. These\ndimensions correspond to content word frequency divergences, syntactic\ndivergences, and meaning changes not captured by word frequencies (e.g. lexical\nsemantic change). We propose interpretable metrics for all three drift\ndimensions, and we modify past performance prediction methods to predict model\nperformance at both the example and dataset level for English sentiment\nclassification and natural language inference. We find that our drift metrics\nare more effective than previous metrics at predicting out-of-domain model\naccuracies (mean 16.8% root mean square error decrease), particularly when\ncompared to popular fine-tuned embedding distances (mean 47.7% error decrease).\nFine-tuned embedding distances are much more effective at ranking individual\nexamples by expected performance, but decomposing into vocabulary, structural,\nand semantic drift produces the best example rankings of all considered\nmodel-agnostic drift metrics (mean 6.7% ROC AUC increase).\n",
                "链接": "https://arxiv.org/abs/2305.17127"
            },
            {
                "文章ID": "5419",
                "标题": "ArgSciChat: A Dataset for Argumentative Dialogues on Scientific Papers",
                "作者": " Federico Ruggeri,  Mohsen Mesgar,  Iryna Gurevych",
                "发布日期": "2022-10-14",
                "摘要": "  The applications of conversational agents for scientific disciplines (as\nexpert domains) are understudied due to the lack of dialogue data to train such\nagents. While most data collection frameworks, such as Amazon Mechanical Turk,\nfoster data collection for generic domains by connecting crowd workers and task\ndesigners, these frameworks are not much optimized for data collection in\nexpert domains. Scientists are rarely present in these frameworks due to their\nlimited time budget. Therefore, we introduce a novel framework to collect\ndialogues between scientists as domain experts on scientific papers. Our\nframework lets scientists present their scientific papers as groundings for\ndialogues and participate in dialogue they like its paper title. We use our\nframework to collect a novel argumentative dialogue dataset, ArgSciChat. It\nconsists of 498 messages collected from 41 dialogues on 20 scientific papers.\nAlongside extensive analysis on ArgSciChat, we evaluate a recent conversational\nagent on our dataset. Experimental results show that this agent poorly performs\non ArgSciChat, motivating further research on argumentative scientific agents.\nWe release our framework and the dataset.\n",
                "链接": "https://arxiv.org/abs/2202.06690"
            }
        ]
    },
    {
        "question": {
            "question": "帮我找一下用大模型进行论文查找的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "94694",
                "标题": "AI Literature Review Suite",
                "作者": " David A. Tovar",
                "发布日期": "2023-08-07",
                "摘要": "  The process of conducting literature reviews is often time-consuming and\nlabor-intensive. To streamline this process, I present an AI Literature Review\nSuite that integrates several functionalities to provide a comprehensive\nliterature review. This tool leverages the power of open access science, large\nlanguage models (LLMs) and natural language processing to enable the searching,\ndownloading, and organizing of PDF files, as well as extracting content from\narticles. Semantic search queries are used for data retrieval, while text\nembeddings and summarization using LLMs present succinct literature reviews.\nInteraction with PDFs is enhanced through a user-friendly graphical user\ninterface (GUI). The suite also features integrated programs for bibliographic\norganization, interaction and query, and literature review summaries. This tool\npresents a robust solution to automate and optimize the process of literature\nreview in academic and industrial research.\n",
                "链接": "https://arxiv.org/abs/2308.02443"
            },
            {
                "文章ID": "97077",
                "标题": "Document Automation Architectures: Updated Survey in Light of Large\n  Language Models",
                "作者": " Mohammad Ahmadi Achachlouei,  Omkar Patil,  Tarun Joshi,  Vijayan N. Nair",
                "发布日期": "2023-08-21",
                "摘要": "  This paper surveys the current state of the art in document automation (DA).\nThe objective of DA is to reduce the manual effort during the generation of\ndocuments by automatically creating and integrating input from different\nsources and assembling documents conforming to defined templates. There have\nbeen reviews of commercial solutions of DA, particularly in the legal domain,\nbut to date there has been no comprehensive review of the academic research on\nDA architectures and technologies. The current survey of DA reviews the\nacademic literature and provides a clearer definition and characterization of\nDA and its features, identifies state-of-the-art DA architectures and\ntechnologies in academic research, and provides ideas that can lead to new\nresearch opportunities within the DA field in light of recent advances in\ngenerative AI and large language models.\n",
                "链接": "https://arxiv.org/abs/2308.09341"
            },
            {
                "文章ID": "114387",
                "标题": "KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI\n  Integration using Retrieval-Augmented Generation",
                "作者": " Suad Alshammari,  Lama Basalelah,  Walaa Abu Rukbah,  Ali Alsuhibani,  Dayanjan S. Wijesinghe",
                "发布日期": "2023-11-09",
                "摘要": "  Academic researchers face challenges keeping up with exponentially growing\npublished findings in their field. Performing comprehensive literature reviews\nto synthesize knowledge is time-consuming and labor-intensive using manual\napproaches. Recent advances in artificial intelligence provide promising\nsolutions, yet many require coding expertise, limiting accessibility.\nKNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the\nKNIME visual programming platform to automate literature review tasks for users\nwith no coding experience. By leveraging KNIME's intuitive graphical interface,\nresearchers can create workflows to search their Zotero libraries and utilize\nOpenAI models to extract key information without coding. Users simply provide\nAPI keys and configure settings through a user-friendly interface in a locally\nstored copy of the workflow. KNIMEZoBot then allows asking natural language\nquestions via a chatbot and retrieves relevant passages from papers to generate\nsynthesized answers. This system has significant potential to expedite\nliterature reviews for researchers unfamiliar with coding by automating\nretrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot\ndemonstrates how thoughtfully designed AI tools can expand accessibility and\naccelerate knowledge building across diverse research domains.\n",
                "链接": "https://arxiv.org/abs/2311.04310"
            },
            {
                "文章ID": "66370",
                "标题": "Algorithmic Ghost in the Research Shell: Large Language Models and\n  Academic Knowledge Creation in Management Research",
                "作者": " Nigel Williams,  Stanislav Ivanov,  Dimitrios Buhalis",
                "发布日期": "2023-03-14",
                "摘要": "  The paper looks at the role of large language models in academic knowledge\ncreation based on a scoping review (2018 to January 2023) of how researchers\nhave previously used the language model GPT to assist in the performance of\nacademic knowledge creation tasks beyond data analysis. These tasks include\nwriting, editing, reviewing, dataset creation and curation, which have been\ndifficult to perform using earlier ML tools. Based on a synthesis of these\npapers, this study identifies pathways for a future academic research landscape\nthat incorporates wider usage of large language models based on the current\nmodes of adoption in published articles as a Co-Writer, Research Assistant and\nRespondent.\n",
                "链接": "https://arxiv.org/abs/2303.07304"
            },
            {
                "文章ID": "18247",
                "标题": "CORWA: A Citation-Oriented Related Work Annotation Dataset",
                "作者": " Xiangci Li,  Biswadip Mandal,  Jessica Ouyang",
                "发布日期": "2022-05-10",
                "摘要": "  Academic research is an exploratory activity to discover new solutions to\nproblems. By this nature, academic research works perform literature reviews to\ndistinguish their novelties from prior work. In natural language processing,\nthis literature review is usually conducted under the \"Related Work\" section.\nThe task of related work generation aims to automatically generate the related\nwork section given the rest of the research paper and a list of papers to cite.\nPrior work on this task has focused on the sentence as the basic unit of\ngeneration, neglecting the fact that related work sections consist of variable\nlength text fragments derived from different information sources. As a first\nstep toward a linguistically-motivated related work generation framework, we\npresent a Citation Oriented Related Work Annotation (CORWA) dataset that labels\ndifferent types of citation text fragments from different information sources.\nWe train a strong baseline model that automatically tags the CORWA labels on\nmassive unlabeled related work section texts. We further suggest a novel\nframework for human-in-the-loop, iterative, abstractive related work\ngeneration.\n",
                "链接": "https://arxiv.org/abs/2205.03512"
            },
            {
                "文章ID": "71217",
                "标题": "Hierarchical Catalogue Generation for Literature Review: A Benchmark",
                "作者": " Kun Zhu,  Xiaocheng Feng,  Xiachong Feng,  Yingsheng Wu,  Bing Qin",
                "发布日期": "2023-11-20",
                "摘要": "  Scientific literature review generation aims to extract and organize\nimportant information from an abundant collection of reference papers and\nproduces corresponding reviews while lacking a clear and logical hierarchy. We\nobserve that a high-quality catalogue-guided generation process can effectively\nalleviate this problem. Therefore, we present an atomic and challenging task\nnamed Hierarchical Catalogue Generation for Literature Review as the first step\nfor review generation, which aims to produce a hierarchical catalogue of a\nreview paper given various references. We construct a novel English\nHierarchical Catalogues of Literature Reviews Dataset with 7.6k literature\nreview catalogues and 389k reference papers. To accurately assess the model\nperformance, we design two evaluation metrics for informativeness and\nsimilarity to ground truth from semantics and structure.Our extensive analyses\nverify the high quality of our dataset and the effectiveness of our evaluation\nmetrics. We further benchmark diverse experiments on state-of-the-art\nsummarization models like BART and large language models like ChatGPT to\nevaluate their capabilities. We further discuss potential directions for this\ntask to motivate future research.\n",
                "链接": "https://arxiv.org/abs/2304.03512"
            },
            {
                "文章ID": "64172",
                "标题": "Authorship Conflicts in Academia: an International Cross-Discipline\n  Survey",
                "作者": " Elizaveta Savchenko,  Ariel Rosenfeld",
                "发布日期": "2023-03-16",
                "摘要": "  Collaboration among scholars has emerged as a significant characteristic of\ncontemporary science. As a result, the number of authors listed in publications\ncontinues to rise steadily. Unfortunately, determining the authors to be\nincluded in the byline and their respective order entails multiple difficulties\nwhich often lead to conflicts. Despite the large volume of literature about\nconflicts in academia, it remains unclear how exactly it is distributed over\nthe main socio-demographic properties, as well as the different types of\ninteractions academics experience. To address this gap, we conducted an\ninternational and cross-disciplinary survey answered by 752 academics from 41\nfields of research and 93 countries that statistically well-represent the\noverall academic workforce. Our findings are concerning and suggest that\nauthorship credit conflicts arise very early in one's academic career, even at\nthe level of Master and Ph.D., and become increasingly common over time.\n",
                "链接": "https://arxiv.org/abs/2303.00386"
            },
            {
                "文章ID": "100219",
                "标题": "CRUISE-Screening: Living Literature Reviews Toolbox",
                "作者": " Wojciech Kusa,  Petr Knoth,  Allan Hanbury",
                "发布日期": "2023-09-06",
                "摘要": "  Keeping up with research and finding related work is still a time-consuming\ntask for academics. Researchers sift through thousands of studies to identify a\nfew relevant ones. Automation techniques can help by increasing the efficiency\nand effectiveness of this task. To this end, we developed CRUISE-Screening, a\nweb-based application for conducting living literature reviews - a type of\nliterature review that is continuously updated to reflect the latest research\nin a particular field. CRUISE-Screening is connected to several search engines\nvia an API, which allows for updating the search results periodically.\nMoreover, it can facilitate the process of screening for relevant publications\nby using text classification and question answering models. CRUISE-Screening\ncan be used both by researchers conducting literature reviews and by those\nworking on automating the citation screening process to validate their\nalgorithms. The application is open-source:\nhttps://github.com/ProjectDoSSIER/cruise-screening, and a demo is available\nunder this URL: https://citation-screening.ec.tuwien.ac.at. We discuss the\nlimitations of our tool in Appendix A.\n",
                "链接": "https://arxiv.org/abs/2309.01684"
            },
            {
                "文章ID": "80431",
                "标题": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review\n  Generation",
                "作者": " Tetsu Kasanishi,  Masaru Isonuma,  Junichiro Mori,  Ichiro Sakata",
                "发布日期": "2023-05-25",
                "摘要": "  Automatic literature review generation is one of the most challenging tasks\nin natural language processing. Although large language models have tackled\nliterature review generation, the absence of large-scale datasets has been a\nstumbling block to the progress. We release SciReviewGen, consisting of over\n10,000 literature reviews and 690,000 papers cited in the reviews. Based on the\ndataset, we evaluate recent transformer-based summarization models on the\nliterature review generation task, including Fusion-in-Decoder extended for\nliterature review generation. Human evaluation results show that some\nmachine-generated summaries are comparable to human-written reviews, while\nrevealing the challenges of automatic literature review generation such as\nhallucinations and a lack of detailed information. Our dataset and code are\navailable at https://github.com/tetsu9923/SciReviewGen.\n",
                "链接": "https://arxiv.org/abs/2305.15186"
            },
            {
                "文章ID": "60466",
                "标题": "Can ChatGPT Write a Good Boolean Query for Systematic Review Literature\n  Search?",
                "作者": " Shuai Wang,  Harrisen Scells,  Bevan Koopman,  Guido Zuccon",
                "发布日期": "2023-02-10",
                "摘要": "  Systematic reviews are comprehensive reviews of the literature for a highly\nfocused research question. These reviews are often treated as the highest form\nof evidence in evidence-based medicine, and are the key strategy to answer\nresearch questions in the medical field. To create a high-quality systematic\nreview, complex Boolean queries are often constructed to retrieve studies for\nthe review topic. However, it often takes a long time for systematic review\nresearchers to construct a high quality systematic review Boolean query, and\noften the resulting queries are far from effective. Poor queries may lead to\nbiased or invalid reviews, because they missed to retrieve key evidence, or to\nextensive increase in review costs, because they retrieved too many irrelevant\nstudies. Recent advances in Transformer-based generative models have shown\ngreat potential to effectively follow instructions from users and generate\nanswers based on the instructions being made. In this paper, we investigate the\neffectiveness of the latest of such models, ChatGPT, in generating effective\nBoolean queries for systematic review literature search. Through a number of\nextensive experiments on standard test collections for the task, we find that\nChatGPT is capable of generating queries that lead to high search precision,\nalthough trading-off this for recall. Overall, our study demonstrates the\npotential of ChatGPT in generating effective Boolean queries for systematic\nreview literature search. The ability of ChatGPT to follow complex instructions\nand generate queries with high precision makes it a valuable tool for\nresearchers conducting systematic reviews, particularly for rapid reviews where\ntime is a constraint and often trading-off higher precision for lower recall is\nacceptable.\n",
                "链接": "https://arxiv.org/abs/2302.03495"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下nips 2023 paper list",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "99360",
                "标题": "SharpSAT-TD in Model Counting Competitions 2021-2023",
                "作者": " Tuukka Korhonen,  Matti Järvisalo",
                "发布日期": "2023-08-31",
                "摘要": "  We describe SharpSAT-TD, our submission to the unweighted and weighted tracks\nof the Model Counting Competition in 2021-2023, which has won in total $6$\nfirst places in different tracks of the competition. SharpSAT-TD is based on\nSharpSAT [Thurley, SAT 2006], with the primary novel modification being the use\nof tree decompositions in the variable selection heuristic as introduced by the\nauthors in [CP 2021]. Unlike the version of SharpSAT-TD evaluated in [CP 2021],\nthe current version that is available in https://github.com/Laakeri/sharpsat-td\nfeatures also other significant modifications compared to the original\nSharpSAT, for example, a new preprocessor.\n",
                "链接": "https://arxiv.org/abs/2308.15819"
            },
            {
                "文章ID": "34179",
                "标题": "Near-Optimal $\\Phi$-Regret Learning in Extensive-Form Games",
                "作者": " Ioannis Anagnostides,  Gabriele Farina,  Tuomas Sandholm",
                "发布日期": "2023-09-20",
                "摘要": "  In this paper, we establish efficient and uncoupled learning dynamics so\nthat, when employed by all players in multiplayer perfect-recall\nimperfect-information extensive-form games, the trigger regret of each player\ngrows as $O(\\log T)$ after $T$ repetitions of play. This improves exponentially\nover the prior best known trigger-regret bound of $O(T^{1/4})$, and settles a\nrecent open question by Bai et al. (2022). As an immediate consequence, we\nguarantee convergence to the set of extensive-form correlated equilibria and\ncoarse correlated equilibria at a near-optimal rate of $\\frac{\\log T}{T}$.\n  Building on prior work, at the heart of our construction lies a more general\nresult regarding fixed points deriving from rational functions with polynomial\ndegree, a property that we establish for the fixed points of (coarse) trigger\ndeviation functions. Moreover, our construction leverages a refined regret\ncircuit for the convex hull, which -- unlike prior guarantees -- preserves the\nRVU property introduced by Syrgkanis et al. (NIPS, 2015); this observation has\nan independent interest in establishing near-optimal regret under learning\ndynamics based on a CFR-type decomposition of the regret.\n",
                "链接": "https://arxiv.org/abs/2208.09747"
            },
            {
                "文章ID": "113325",
                "标题": "ACES: Translation Accuracy Challenge Sets at WMT 2023",
                "作者": " Chantal Amrhein,  Nikita Moghe,  Liane Guillou",
                "发布日期": "2023-11-03",
                "摘要": "  We benchmark the performance of segmentlevel metrics submitted to WMT 2023\nusing the ACES Challenge Set (Amrhein et al., 2022). The challenge set consists\nof 36K examples representing challenges from 68 phenomena and covering 146\nlanguage pairs. The phenomena range from simple perturbations at the\nword/character level to more complex errors based on discourse and real-world\nknowledge. For each metric, we provide a detailed profile of performance over a\nrange of error categories as well as an overall ACES-Score for quick\ncomparison. We also measure the incremental performance of the metrics\nsubmitted to both WMT 2023 and 2022. We find that 1) there is no clear winner\namong the metrics submitted to WMT 2023, and 2) performance change between the\n2023 and 2022 versions of the metrics is highly variable. Our recommendations\nare similar to those from WMT 2022. Metric developers should focus on: building\nensembles of metrics from different design families, developing metrics that\npay more attention to the source and rely less on surface-level overlap, and\ncarefully determining the influence of multilingual embeddings on MT\nevaluation.\n",
                "链接": "https://arxiv.org/abs/2311.01153"
            },
            {
                "文章ID": "102298",
                "标题": "Monitoring Urban Changes in Mariupol/Ukraine in 2022/23",
                "作者": " Georg Zitzlsberger,  Michal Podhoranyi",
                "发布日期": "2023-09-19",
                "摘要": "  The ability to constantly monitor urban changes is of large socio-economic\ninterest. Previous works have already shown approaches in this field with the\nuse of Deep Neural Networks (DNNs) and transfer learning. However, they fell\nshort in demonstrating temporal scale outside of either the training or\ntransfer domain.\n  This work builds on existing research and proves that transfer learning with\nthe use of historic data is a feasible solution, which still allows the urban\nchange monitoring of later years. We considered a case with limited access to\npublic and free Very High Resolution (VHR) imagery to guide the transfer. To\nprovide a high temporal resolution, the core data of our monitoring method\ncomprised multi-modal Synthetic Aperture Radar (SAR) and optical multispectral\nobservations from Sentinel 1 and Sentinel 2, respectively.\n  We chose a practical application of our methods for monitoring urban-related\nchanges in the city of Mariupol in Ukraine during the beginning of the\nRusso-Ukrainian War in 2022/23. During this conflict, availability of VHR data\nwas limited and hence an inexpensive direct transfer to the years 2022/23 was\nrendered impossible. Instead, a transfer was made for the years 2017-2020 that\nprovided sufficient public and free VHR data with an application of the\ntransferred model in the years late 2021 to mid-2023. It was shown that\ntransferring for the years 2017-2020 with this inexpensive historical VHR data\nenabled monitoring during times of war in 2022/23.\n  An ablation study on the impact of the frequency of observations showed our\nmethod as resilient to even a large loss of observations. However, it also\nindicated that our method, despite the multi-modal input, was more dependent on\noptical observations than SAR observations. Neither the indirect transfer, nor\nthe malfunction of Sentinel 1B had a significant impact on the monitoring\ncapabilities of our method.\n",
                "链接": "https://arxiv.org/abs/2309.08607"
            },
            {
                "文章ID": "49542",
                "标题": "Extended Multilingual Protest News Detection -- Shared Task 1, CASE 2021\n  and 2022",
                "作者": " Ali Hürriyetoğlu,  Osman Mutlu,  Fırat Duruşan,  Onur Uca,  Alaeddin Selçuk Gürel,  Benjamin Radford,  Yaoyao Dai,  Hansi Hettiarachchi,  Niklas Stoehr,  Tadashi Nomoto,  Milena Slavcheva,  Francielle Vargas,  Aaqib Javid,  Fatih Beyhan,  Erdem Yörük",
                "发布日期": "2022-11-22",
                "摘要": "  We report results of the CASE 2022 Shared Task 1 on Multilingual Protest\nEvent Detection. This task is a continuation of CASE 2021 that consists of four\nsubtasks that are i) document classification, ii) sentence classification, iii)\nevent sentence coreference identification, and iv) event extraction. The CASE\n2022 extension consists of expanding the test data with more data in previously\navailable languages, namely, English, Hindi, Portuguese, and Spanish, and\nadding new test data in Mandarin, Turkish, and Urdu for Sub-task 1, document\nclassification. The training data from CASE 2021 in English, Portuguese and\nSpanish were utilized. Therefore, predicting document labels in Hindi,\nMandarin, Turkish, and Urdu occurs in a zero-shot setting. The CASE 2022\nworkshop accepts reports on systems developed for predicting test data of CASE\n2021 as well. We observe that the best systems submitted by CASE 2022\nparticipants achieve between 79.71 and 84.06 F1-macro for new languages in a\nzero-shot setting. The winning approaches are mainly ensembling models and\nmerging data in multiple languages. The best two submissions on CASE 2021 data\noutperform submissions from last year for Subtask 1 and Subtask 2 in all\nlanguages. Only the following scenarios were not outperformed by new\nsubmissions on CASE 2021: Subtask 3 Portuguese \\& Subtask 4 English.\n",
                "链接": "https://arxiv.org/abs/2211.11360"
            },
            {
                "文章ID": "83364",
                "标题": "Context-PIPs: Persistent Independent Particles Demands Spatial Context\n  Features",
                "作者": " Weikang Bian,  Zhaoyang Huang,  Xiaoyu Shi,  Yitong Dong,  Yijin Li,  Hongsheng Li",
                "发布日期": "2023-12-07",
                "摘要": "  We tackle the problem of Persistent Independent Particles (PIPs), also called\nTracking Any Point (TAP), in videos, which specifically aims at estimating\npersistent long-term trajectories of query points in videos. Previous methods\nattempted to estimate these trajectories independently to incorporate longer\nimage sequences, therefore, ignoring the potential benefits of incorporating\nspatial context features. We argue that independent video point tracking also\ndemands spatial context features. To this end, we propose a novel framework\nContext-PIPs, which effectively improves point trajectory accuracy by\naggregating spatial context features in videos. Context-PIPs contains two main\nmodules: 1) a SOurse Feature Enhancement (SOFE) module, and 2) a TArget Feature\nAggregation (TAFA) module. Context-PIPs significantly improves PIPs all-sided,\nreducing 11.4% Average Trajectory Error of Occluded Points (ATE-Occ) on CroHD\nand increasing 11.8% Average Percentage of Correct Keypoint (A-PCK) on\nTAP-Vid-Kinectics. Demos are available at\nhttps://wkbian.github.io/Projects/Context-PIPs/.\n",
                "链接": "https://arxiv.org/abs/2306.02000"
            },
            {
                "文章ID": "76098",
                "标题": "Learning Action Embeddings for Off-Policy Evaluation",
                "作者": " Matej Cief,  Jacek Golebiowski,  Philipp Schmidt,  Ziawasch Abedjan,  Artur Bekasov",
                "发布日期": "2023-05-09",
                "摘要": "  Off-policy evaluation (OPE) methods allow us to compute the expected reward\nof a policy by using the logged data collected by a different policy. OPE is a\nviable alternative to running expensive online A/B tests: it can speed up the\ndevelopment of new policies, and reduces the risk of exposing customers to\nsuboptimal treatments. However, when the number of actions is large, or certain\nactions are under-explored by the logging policy, existing estimators based on\ninverse-propensity scoring (IPS) can have a high or even infinite variance.\nSaito and Joachims (arXiv:2202.06317v2 [cs.LG]) propose marginalized IPS (MIPS)\nthat uses action embeddings instead, which reduces the variance of IPS in large\naction spaces. MIPS assumes that good action embeddings can be defined by the\npractitioner, which is difficult to do in many real-world applications. In this\nwork, we explore learning action embeddings from logged data. In particular, we\nuse intermediate outputs of a trained reward model to define action embeddings\nfor MIPS. This approach extends MIPS to more applications, and in our\nexperiments improves upon MIPS with pre-defined embeddings, as well as standard\nbaselines, both on synthetic and real-world data. Our method does not make\nassumptions about the reward model class, and supports using additional action\ninformation to further improve the estimates. The proposed approach presents an\nappealing alternative to DR for combining the low variance of DM with the low\nbias of IPS.\n",
                "链接": "https://arxiv.org/abs/2305.03954"
            },
            {
                "文章ID": "36791",
                "标题": "An Improved Algorithm For Online Min-Sum Set Cover",
                "作者": " Marcin Bienkowski,  Marcin Mucha",
                "发布日期": "2023-03-28",
                "摘要": "  We study a fundamental model of online preference aggregation, where an\nalgorithm maintains an ordered list of $n$ elements. An input is a stream of\npreferred sets $R_1, R_2, \\dots, R_t, \\dots$. Upon seeing $R_t$ and without\nknowledge of any future sets, an algorithm has to rerank elements (change the\nlist ordering), so that at least one element of $R_t$ is found near the list\nfront. The incurred cost is a sum of the list update costs (the number of swaps\nof neighboring list elements) and access costs (position of the first element\nof $R_t$ on the list). This scenario occurs naturally in applications such as\nordering items in an online shop using aggregated preferences of shop\ncustomers. The theoretical underpinning of this problem is known as Min-Sum Set\nCover.\n  Unlike previous work (Fotakis et al., ICALP 2020, NIPS 2020) that mostly\nstudied the performance of an online algorithm ALG against the static optimal\nsolution (a single optimal list ordering), in this paper, we study an arguably\nharder variant where the benchmark is the provably stronger optimal dynamic\nsolution OPT (that may also modify the list ordering). In terms of an online\nshop, this means that the aggregated preferences of its user base evolve with\ntime. We construct a computationally efficient randomized algorithm whose\ncompetitive ratio (ALG-to-OPT cost ratio) is $O(r^2)$ and prove the existence\nof a deterministic $O(r^4)$-competitive algorithm. Here, $r$ is the maximum\ncardinality of sets $R_t$. This is the first algorithm whose ratio does not\ndepend on $n$: the previously best algorithm for this problem was $O(r^{3/2}\n\\cdot \\sqrt{n})$-competitive and $\\Omega(r)$ is a lower bound on the\nperformance of any deterministic online algorithm.\n",
                "链接": "https://arxiv.org/abs/2209.04870"
            },
            {
                "文章ID": "22537",
                "标题": "NIPQ: Noise proxy-based Integrated Pseudo-Quantization",
                "作者": " Juncheol Shin,  Junhyuk So,  Sein Park,  Seungyeop Kang,  Sungjoo Yoo,  Eunhyeok Park",
                "发布日期": "2023-07-04",
                "摘要": "  Straight-through estimator (STE), which enables the gradient flow over the\nnon-differentiable function via approximation, has been favored in studies\nrelated to quantization-aware training (QAT). However, STE incurs unstable\nconvergence during QAT, resulting in notable quality degradation in low\nprecision. Recently, pseudoquantization training has been proposed as an\nalternative approach to updating the learnable parameters using the\npseudo-quantization noise instead of STE. In this study, we propose a novel\nnoise proxy-based integrated pseudoquantization (NIPQ) that enables unified\nsupport of pseudoquantization for both activation and weight by integrating the\nidea of truncation on the pseudo-quantization framework. NIPQ updates all of\nthe quantization parameters (e.g., bit-width and truncation boundary) as well\nas the network parameters via gradient descent without STE instability.\nAccording to our extensive experiments, NIPQ outperforms existing quantization\nalgorithms in various vision and language applications by a large margin.\n",
                "链接": "https://arxiv.org/abs/2206.00820"
            },
            {
                "文章ID": "85541",
                "标题": "NAVER LABS Europe's Multilingual Speech Translation Systems for the\n  IWSLT 2023 Low-Resource Track",
                "作者": " Edward Gow-Smith,  Alexandre Berard,  Marcely Zanon Boito,  Ioan Calapodescu",
                "发布日期": "2023-06-14",
                "摘要": "  This paper presents NAVER LABS Europe's systems for Tamasheq-French and\nQuechua-Spanish speech translation in the IWSLT 2023 Low-Resource track. Our\nwork attempts to maximize translation quality in low-resource settings using\nmultilingual parameter-efficient solutions that leverage strong pre-trained\nmodels. Our primary submission for Tamasheq outperforms the previous state of\nthe art by 7.5 BLEU points on the IWSLT 2022 test set, and achieves 23.6 BLEU\non this year's test set, outperforming the second best participant by 7.7\npoints. For Quechua, we also rank first and achieve 17.7 BLEU, despite having\nonly two hours of translation data. Finally, we show that our proposed\nmultilingual architecture is also competitive for high-resource languages,\noutperforming the best unconstrained submission to the IWSLT 2021 Multilingual\ntrack, despite using much less training data and compute.\n",
                "链接": "https://arxiv.org/abs/2306.07763"
            }
        ]
    },
    {
        "question": {
            "question": "推荐与AutoGPT相似的10篇文献",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "78552",
                "标题": "Investigating and Designing for Trust in AI-powered Code Generation\n  Tools",
                "作者": " Ruotong Wang,  Ruijia Cheng,  Denae Ford,  Thomas Zimmermann",
                "发布日期": "2023-05-22",
                "摘要": "  As AI-powered code generation tools such as GitHub Copilot become popular, it\nis crucial to understand software developers' trust in AI tools -- a key factor\nfor tool adoption and responsible usage. However, we know little about how\ndevelopers build trust with AI, nor do we understand how to design the\ninterface of generative AI systems to facilitate their appropriate levels of\ntrust. In this paper, we describe findings from a two-stage qualitative\ninvestigation. We first interviewed 17 developers to contextualize their\nnotions of trust and understand their challenges in building appropriate trust\nin AI code generation tools. We surfaced three main challenges -- including\nbuilding appropriate expectations, configuring AI tools, and validating AI\nsuggestions. To address these challenges, we conducted a design probe study in\nthe second stage to explore design concepts that support developers'\ntrust-building process by 1) communicating AI performance to help users set\nproper expectations, 2) allowing users to configure AI by setting and adjusting\npreferences, and 3) offering indicators of model mechanism to support\nevaluation of AI suggestions. We gathered developers' feedback on how these\ndesign concepts can help them build appropriate trust in AI-powered code\ngeneration tools, as well as potential risks in design. These findings inform\nour proposed design recommendations on how to design for trust in AI-powered\ncode generation tools.\n",
                "链接": "https://arxiv.org/abs/2305.11248"
            },
            {
                "文章ID": "86417",
                "标题": "AD-AutoGPT: An Autonomous GPT for Alzheimer's Disease Infodemiology",
                "作者": " Haixing Dai,  Yiwei Li,  Zhengliang Liu,  Lin Zhao,  Zihao Wu,  Suhang Song,  Ye Shen,  Dajiang Zhu,  Xiang Li,  Sheng Li,  Xiaobai Yao,  Lu Shi,  Quanzheng Li,  Zhuo Chen,  Donglan Zhang,  Gengchen Mai,  Tianming Liu",
                "发布日期": "2023-06-21",
                "摘要": "  In this pioneering study, inspired by AutoGPT, the state-of-the-art\nopen-source application based on the GPT-4 large language model, we develop a\nnovel tool called AD-AutoGPT which can conduct data collection, processing, and\nanalysis about complex health narratives of Alzheimer's Disease in an\nautonomous manner via users' textual prompts. We collated comprehensive data\nfrom a variety of news sources, including the Alzheimer's Association, BBC,\nMayo Clinic, and the National Institute on Aging since June 2022, leading to\nthe autonomous execution of robust trend analyses, intertopic distance maps\nvisualization, and identification of salient terms pertinent to Alzheimer's\nDisease. This approach has yielded not only a quantifiable metric of relevant\ndiscourse but also valuable insights into public focus on Alzheimer's Disease.\nThis application of AD-AutoGPT in public health signifies the transformative\npotential of AI in facilitating a data-rich understanding of complex health\nnarratives like Alzheimer's Disease in an autonomous manner, setting the\ngroundwork for future AI-driven investigations in global health landscapes.\n",
                "链接": "https://arxiv.org/abs/2306.10095"
            },
            {
                "文章ID": "52555",
                "标题": "\"It would work for me too\": How Online Communities Shape Software\n  Developers' Trust in AI-Powered Code Generation Tools",
                "作者": " Ruijia Cheng,  Ruotong Wang,  Thomas Zimmermann,  Denae Ford",
                "发布日期": "2023-03-30",
                "摘要": "  While revolutionary AI-powered code generation tools have been rising\nrapidly, we know little about how and how to help software developers form\nappropriate trust in those AI tools. Through a two-phase formative study, we\ninvestigate how online communities shape developers' trust in AI tools and how\nwe can leverage community features to facilitate appropriate user trust.\nThrough interviewing 17 developers, we find that developers collectively make\nsense of AI tools using the experiences shared by community members and\nleverage community signals to evaluate AI suggestions. We then surface design\nopportunities and conduct 11 design probe sessions to explore the design space\nof using community features to support user trust in AI code generation\nsystems. We synthesize our findings and extend an existing model of user trust\nin AI technologies with sociotechnical factors. We map out the design\nconsiderations for integrating user community into the AI code generation\nexperience.\n",
                "链接": "https://arxiv.org/abs/2212.03491"
            },
            {
                "文章ID": "96648",
                "标题": "AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation",
                "作者": " Qingyun Wu,  Gagan Bansal,  Jieyu Zhang,  Yiran Wu,  Beibin Li,  Erkang Zhu,  Li Jiang,  Xiaoyun Zhang,  Shaokun Zhang,  Jiale Liu,  Ahmed Hassan Awadallah,  Ryen W White,  Doug Burger,  Chi Wang",
                "发布日期": "2023-10-05",
                "摘要": "  AutoGen is an open-source framework that allows developers to build LLM\napplications via multiple agents that can converse with each other to\naccomplish tasks. AutoGen agents are customizable, conversable, and can operate\nin various modes that employ combinations of LLMs, human inputs, and tools.\nUsing AutoGen, developers can also flexibly define agent interaction behaviors.\nBoth natural language and computer code can be used to program flexible\nconversation patterns for different applications. AutoGen serves as a generic\ninfrastructure to build diverse applications of various complexities and LLM\ncapacities. Empirical studies demonstrate the effectiveness of the framework in\nmany example applications, with domains ranging from mathematics, coding,\nquestion answering, operations research, online decision-making, entertainment,\netc.\n",
                "链接": "https://arxiv.org/abs/2308.08155"
            },
            {
                "文章ID": "74264",
                "标题": "AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking\n  Head",
                "作者": " Rongjie Huang,  Mingze Li,  Dongchao Yang,  Jiatong Shi,  Xuankai Chang,  Zhenhui Ye,  Yuning Wu,  Zhiqing Hong,  Jiawei Huang,  Jinglin Liu,  Yi Ren,  Zhou Zhao,  Shinji Watanabe",
                "发布日期": "2023-04-26",
                "摘要": "  Large language models (LLMs) have exhibited remarkable capabilities across a\nvariety of domains and tasks, challenging our understanding of learning and\ncognition. Despite the recent success, current LLMs are not capable of\nprocessing complex audio information or conducting spoken conversations (like\nSiri or Alexa). In this work, we propose a multi-modal AI system named\nAudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to\nprocess complex audio information and solve numerous understanding and\ngeneration tasks; and 2) the input/output interface (ASR, TTS) to support\nspoken dialogue. With an increasing demand to evaluate multi-modal LLMs of\nhuman intention understanding and cooperation with foundation models, we\noutline the principles and processes and test AudioGPT in terms of consistency,\ncapability, and robustness. Experimental results demonstrate the capabilities\nof AudioGPT in solving AI tasks with speech, music, sound, and talking head\nunderstanding and generation in multi-round dialogues, which empower humans to\ncreate rich and diverse audio content with unprecedented ease. Our system is\npublicly available at \\url{https://github.com/AIGC-Audio/AudioGPT}.\n",
                "链接": "https://arxiv.org/abs/2304.12995"
            },
            {
                "文章ID": "13182",
                "标题": "AutoOpt: A General Framework for Automatically Designing Metaheuristic\n  Optimization Algorithms with Diverse Structures",
                "作者": " Qi Zhao,  Bai Yan,  Xianglong Chen,  Taiwei Hu,  Shi Cheng,  Yuhui Shi",
                "发布日期": "2023-05-05",
                "摘要": "  Metaheuristics are widely recognized gradient-free solvers to hard problems\nthat do not meet the rigorous mathematical assumptions of conventional solvers.\nThe automated design of metaheuristic algorithms provides an attractive path to\nrelieve manual design effort and gain enhanced performance beyond human-made\nalgorithms. However, the specific algorithm prototype and linear algorithm\nrepresentation in the current automated design pipeline restrict the design\nwithin a fixed algorithm structure, which hinders discovering novelties and\ndiversity across the metaheuristic family. To address this challenge, this\npaper proposes a general framework, AutoOpt, for automatically designing\nmetaheuristic algorithms with diverse structures. AutoOpt contains three\ninnovations: (i) A general algorithm prototype dedicated to covering the\nmetaheuristic family as widely as possible. It promotes high-quality automated\ndesign on different problems by fully discovering potentials and novelties\nacross the family. (ii) A directed acyclic graph algorithm representation to\nfit the proposed prototype. Its flexibility and evolvability enable discovering\nvarious algorithm structures in a single run of design, thus boosting the\npossibility of finding high-performance algorithms. (iii) A graph\nrepresentation embedding method offering an alternative compact form of the\ngraph to be manipulated, which ensures AutoOpt's generality. Experiments on\nnumeral functions and real applications validate AutoOpt's efficiency and\npracticability.\n",
                "链接": "https://arxiv.org/abs/2204.00998"
            },
            {
                "文章ID": "53479",
                "标题": "AutoPV: Automated photovoltaic forecasts with limited information using\n  an ensemble of pre-trained models",
                "作者": " Stefan Meisenbacher,  Benedikt Heidrich,  Tim Martin,  Ralf Mikut,  Veit Hagenmeyer",
                "发布日期": "2023-06-21",
                "摘要": "  Accurate PhotoVoltaic (PV) power generation forecasting is vital for the\nefficient operation of Smart Grids. The automated design of such accurate\nforecasting models for individual PV plants includes two challenges: First,\ninformation about the PV mounting configuration (i.e. inclination and azimuth\nangles) is often missing. Second, for new PV plants, the amount of historical\ndata available to train a forecasting model is limited (cold-start problem). We\naddress these two challenges by proposing a new method for day-ahead PV power\ngeneration forecasts called AutoPV. AutoPV is a weighted ensemble of\nforecasting models that represent different PV mounting configurations. This\nrepresentation is achieved by pre-training each forecasting model on a separate\nPV plant and by scaling the model's output with the peak power rating of the\ncorresponding PV plant. To tackle the cold-start problem, we initially weight\neach forecasting model in the ensemble equally. To tackle the problem of\nmissing information about the PV mounting configuration, we use new data that\nbecome available during operation to adapt the ensemble weights to minimize the\nforecasting error. AutoPV is advantageous as the unknown PV mounting\nconfiguration is implicitly reflected in the ensemble weights, and only the PV\nplant's peak power rating is required to re-scale the ensemble's output. AutoPV\nalso allows to represent PV plants with panels distributed on different roofs\nwith varying alignments, as these mounting configurations can be reflected\nproportionally in the weighting. Additionally, the required computing memory is\ndecoupled when scaling AutoPV to hundreds of PV plants, which is beneficial in\nSmart Grids with limited computing capabilities. For a real-world data set with\n11 PV plants, the accuracy of AutoPV is comparable to a model trained on two\nyears of data and outperforms an incrementally trained model.\n",
                "链接": "https://arxiv.org/abs/2212.06797"
            },
            {
                "文章ID": "9495",
                "标题": "AutoGPart: Intermediate Supervision Search for Generalizable 3D Part\n  Segmentation",
                "作者": " Xueyi Liu,  Xiaomeng Xu,  Anyi Rao,  Chuang Gan,  Li Yi",
                "发布日期": "2022-04-18",
                "摘要": "  Training a generalizable 3D part segmentation network is quite challenging\nbut of great importance in real-world applications. To tackle this problem,\nsome works design task-specific solutions by translating human understanding of\nthe task to machine's learning process, which faces the risk of missing the\noptimal strategy since machines do not necessarily understand in the exact\nhuman way. Others try to use conventional task-agnostic approaches designed for\ndomain generalization problems with no task prior knowledge considered. To\nsolve the above issues, we propose AutoGPart, a generic method enabling\ntraining generalizable 3D part segmentation networks with the task prior\nconsidered. AutoGPart builds a supervision space with geometric prior knowledge\nencoded, and lets the machine to search for the optimal supervisions from the\nspace for a specific segmentation task automatically. Extensive experiments on\nthree generalizable 3D part segmentation tasks are conducted to demonstrate the\neffectiveness and versatility of AutoGPart. We demonstrate that the performance\nof segmentation networks using simple backbones can be significantly improved\nwhen trained with supervisions searched by our method.\n",
                "链接": "https://arxiv.org/abs/2203.06558"
            },
            {
                "文章ID": "108389",
                "标题": "AutoVP: An Automated Visual Prompting Framework and Benchmark",
                "作者": " Hsi-Ai Tsao,  Lei Hsiung,  Pin-Yu Chen,  Sijia Liu,  Tsung-Yi Ho",
                "发布日期": "2023-10-13",
                "摘要": "  Visual prompting (VP) is an emerging parameter-efficient fine-tuning approach\nto adapting pre-trained vision models to solve various downstream\nimage-classification tasks. However, there has hitherto been little systematic\nstudy of the design space of VP and no clear benchmark for evaluating its\nperformance. To bridge this gap, we propose AutoVP, an end-to-end expandable\nframework for automating VP design choices, along with 12 downstream\nimage-classification tasks that can serve as a holistic VP-performance\nbenchmark. Our design space covers 1) the joint optimization of the prompts; 2)\nthe selection of pre-trained models, including image classifiers and text-image\nencoders; and 3) model output mapping strategies, including nonparametric and\ntrainable label mapping. Our extensive experimental results show that AutoVP\noutperforms the best-known current VP methods by a substantial margin, having\nup to 6.7% improvement in accuracy; and attains a maximum performance increase\nof 27.5% compared to linear-probing (LP) baseline. AutoVP thus makes a two-fold\ncontribution: serving both as an efficient tool for hyperparameter tuning on VP\ndesign choices, and as a comprehensive benchmark that can reasonably be\nexpected to accelerate VP's development. The source code is available at\nhttps://github.com/IBM/AutoVP.\n",
                "链接": "https://arxiv.org/abs/2310.08381"
            },
            {
                "文章ID": "65490",
                "标题": "Cost-Effective Hyperparameter Optimization for Large Language Model\n  Generation Inference",
                "作者": " Chi Wang,  Susan Xueqing Liu,  Ahmed H. Awadallah",
                "发布日期": "2023-08-10",
                "摘要": "  Large Language Models (LLMs) have sparked significant interest in their\ngenerative capabilities, leading to the development of various commercial\napplications. The high cost of using the models drives application builders to\nmaximize the value of generation under a limited inference budget. This paper\npresents a study of optimizing inference hyperparameters such as the number of\nresponses, temperature and max tokens, which significantly affects the\nutility/cost of text generation. We design a framework named EcoOptiGen which\nleverages economical hyperparameter optimization and cost-based pruning.\nExperiments with the GPT-3.5/GPT-4 models on a variety of tasks verify its\neffectiveness. EcoOptiGen is implemented in the `autogen' package of the FLAML\nlibrary: \\url{https://aka.ms/autogen}.\n",
                "链接": "https://arxiv.org/abs/2303.04673"
            }
        ]
    },
    {
        "question": {
            "question": "请搜索近一年发表的有关多模态大模型与产业相结合、与具体应用场景相结合的论文",
            "type": "2"
        },
        "results": [
            {
                "文章ID": "97795",
                "标题": "Federated Learning in Big Model Era: Domain-Specific Multimodal Large\n  Models",
                "作者": " Zengxiang Li,  Zhaoxiang Hou,  Hui Liu,  Ying Wang,  Tongzhi Li,  Longfei Xie,  Chao Shi,  Chengyi Yang,  Weishan Zhang,  Zelei Liu,  Liang Xu",
                "发布日期": "2023-08-25",
                "摘要": "  Multimodal data, which can comprehensively perceive and recognize the\nphysical world, has become an essential path towards general artificial\nintelligence. However, multimodal large models trained on public datasets often\nunderperform in specific industrial domains. This paper proposes a multimodal\nfederated learning framework that enables multiple enterprises to utilize\nprivate domain data to collaboratively train large models for vertical domains,\nachieving intelligent services across scenarios. The authors discuss in-depth\nthe strategic transformation of federated learning in terms of intelligence\nfoundation and objectives in the era of big model, as well as the new\nchallenges faced in heterogeneous data, model aggregation, performance and cost\ntrade-off, data privacy, and incentive mechanism. The paper elaborates a case\nstudy of leading enterprises contributing multimodal data and expert knowledge\nto city safety operation management , including distributed deployment and\nefficient coordination of the federated learning platform, technical\ninnovations on data quality improvement based on large model capabilities and\nefficient joint fine-tuning approaches. Preliminary experiments show that\nenterprises can enhance and accumulate intelligent capabilities through\nmultimodal model federated learning, thereby jointly creating an smart city\nmodel that provides high-quality intelligent services covering energy\ninfrastructure safety, residential community security, and urban operation\nmanagement. The established federated learning cooperation ecosystem is\nexpected to further aggregate industry, academia, and research resources,\nrealize large models in multiple vertical domains, and promote the large-scale\nindustrial application of artificial intelligence and cutting-edge research on\nmultimodal federated learning.\n",
                "链接": "https://arxiv.org/abs/2308.11217"
            },
            {
                "文章ID": "86687",
                "标题": "MotionGPT: Finetuned LLMs are General-Purpose Motion Generators",
                "作者": " Yaqi Zhang,  Di Huang,  Bin Liu,  Shixiang Tang,  Yan Lu,  Lu Chen,  Lei Bai,  Qi Chu,  Nenghai Yu,  Wanli Ouyang",
                "发布日期": "2023-06-21",
                "摘要": "  Generating realistic human motion from given action descriptions has\nexperienced significant advancements because of the emerging requirement of\ndigital humans. While recent works have achieved impressive results in\ngenerating motion directly from textual action descriptions, they often support\nonly a single modality of the control signal, which limits their application in\nthe real digital human industry. This paper presents a Motion General-Purpose\ngeneraTor (MotionGPT) that can use multimodal control signals, e.g., text and\nsingle-frame poses, for generating consecutive human motions by treating\nmultimodal signals as special input tokens in large language models (LLMs).\nSpecifically, we first quantize multimodal control signals into discrete codes\nand then formulate them in a unified prompt instruction to ask the LLMs to\ngenerate the motion answer. Our MotionGPT demonstrates a unified human motion\ngeneration model with multimodal control signals by tuning a mere 0.4% of LLM\nparameters. To the best of our knowledge, MotionGPT is the first method to\ngenerate human motion by multimodal control signals, which we hope can shed\nlight on this new direction. Codes shall be released upon acceptance.\n",
                "链接": "https://arxiv.org/abs/2306.10900"
            },
            {
                "文章ID": "91047",
                "标题": "CephGPT-4: An Interactive Multimodal Cephalometric Measurement and\n  Diagnostic System with Visual Large Language Model",
                "作者": " Lei Ma,  Jincong Han,  Zhaoxin Wang,  Dian Zhang",
                "发布日期": "2023-07-18",
                "摘要": "  Large-scale multimodal language models (LMMs) have achieved remarkable\nsuccess in general domains. However, the exploration of diagnostic language\nmodels based on multimodal cephalometric medical data remains limited. In this\npaper, we propose a novel multimodal cephalometric analysis and diagnostic\ndialogue model. Firstly, a multimodal orthodontic medical dataset is\nconstructed, comprising cephalometric images and doctor-patient dialogue data,\nwith automatic analysis of cephalometric landmarks using U-net and generation\nof diagnostic reports. Then, the cephalometric dataset and generated diagnostic\nreports are separately fine-tuned on Minigpt-4 and VisualGLM. Results\ndemonstrate that the CephGPT-4 model exhibits excellent performance and has the\npotential to revolutionize orthodontic measurement and diagnostic applications.\nThese innovations hold revolutionary application potential in the field of\northodontics.\n",
                "链接": "https://arxiv.org/abs/2307.07518"
            },
            {
                "文章ID": "117088",
                "标题": "A Survey on Multimodal Large Language Models for Autonomous Driving",
                "作者": " Can Cui,  Yunsheng Ma,  Xu Cao,  Wenqian Ye,  Yang Zhou,  Kaizhao Liang,  Jintai Chen,  Juanwu Lu,  Zichong Yang,  Kuei-Da Liao,  Tianren Gao,  Erlong Li,  Kun Tang,  Zhipeng Cao,  Tong Zhou,  Ao Liu,  Xinrui Yan,  Shuqi Mei,  Jianguo Cao,  Ziran Wang,  Chao Zheng",
                "发布日期": "2023-11-22",
                "摘要": "  With the emergence of Large Language Models (LLMs) and Vision Foundation\nModels (VFMs), multimodal AI systems benefiting from large models have the\npotential to equally perceive the real world, make decisions, and control tools\nas humans. In recent months, LLMs have shown widespread attention in autonomous\ndriving and map systems. Despite its immense potential, there is still a lack\nof a comprehensive understanding of key challenges, opportunities, and future\nendeavors to apply in LLM driving systems. In this paper, we present a\nsystematic investigation in this field. We first introduce the background of\nMultimodal Large Language Models (MLLMs), the multimodal models development\nusing LLMs, and the history of autonomous driving. Then, we overview existing\nMLLM tools for driving, transportation, and map systems together with existing\ndatasets and benchmarks. Moreover, we summarized the works in The 1st WACV\nWorkshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD),\nwhich is the first workshop of its kind regarding LLMs in autonomous driving.\nTo further promote the development of this field, we also discuss several\nimportant problems regarding using MLLMs in autonomous driving systems that\nneed to be solved by both academia and industry.\n",
                "链接": "https://arxiv.org/abs/2311.12320"
            },
            {
                "文章ID": "63245",
                "标题": "Quantifying & Modeling Multimodal Interactions: An Information\n  Decomposition Framework",
                "作者": " Paul Pu Liang,  Yun Cheng,  Xiang Fan,  Chun Kai Ling,  Suzanne Nie,  Richard Chen,  Zihao Deng,  Nicholas Allen,  Randy Auerbach,  Faisal Mahmood,  Ruslan Salakhutdinov,  Louis-Philippe Morency",
                "发布日期": "2023-12-12",
                "摘要": "  The recent explosion of interest in multimodal applications has resulted in a\nwide selection of datasets and methods for representing and integrating\ninformation from different modalities. Despite these empirical advances, there\nremain fundamental research questions: How can we quantify the interactions\nthat are necessary to solve a multimodal task? Subsequently, what are the most\nsuitable multimodal models to capture these interactions? To answer these\nquestions, we propose an information-theoretic approach to quantify the degree\nof redundancy, uniqueness, and synergy relating input modalities with an output\ntask. We term these three measures as the PID statistics of a multimodal\ndistribution (or PID for short), and introduce two new estimators for these PID\nstatistics that scale to high-dimensional distributions. To validate PID\nestimation, we conduct extensive experiments on both synthetic datasets where\nthe PID is known and on large-scale multimodal benchmarks where PID estimations\nare compared with human annotations. Finally, we demonstrate their usefulness\nin (1) quantifying interactions within multimodal datasets, (2) quantifying\ninteractions captured by multimodal models, (3) principled approaches for model\nselection, and (4) three real-world case studies engaging with domain experts\nin pathology, mood prediction, and robotic perception where our framework helps\nto recommend strong multimodal models for each application.\n",
                "链接": "https://arxiv.org/abs/2302.12247"
            },
            {
                "文章ID": "91678",
                "标题": "Company2Vec -- German Company Embeddings based on Corporate Websites",
                "作者": " Christopher Gerling",
                "发布日期": "2023-07-19",
                "摘要": "  With Company2Vec, the paper proposes a novel application in representation\nlearning. The model analyzes business activities from unstructured company\nwebsite data using Word2Vec and dimensionality reduction. Company2Vec maintains\nsemantic language structures and thus creates efficient company embeddings in\nfine-granular industries. These semantic embeddings can be used for various\napplications in banking. Direct relations between companies and words allow\nsemantic business analytics (e.g. top-n words for a company). Furthermore,\nindustry prediction is presented as a supervised learning application and\nevaluation method. The vectorized structure of the embeddings allows measuring\ncompanies similarities with the cosine distance. Company2Vec hence offers a\nmore fine-grained comparison of companies than the standard industry labels\n(NACE). This property is relevant for unsupervised learning tasks, such as\nclustering. An alternative industry segmentation is shown with k-means\nclustering on the company embeddings. Finally, this paper proposes three\nalgorithms for (1) firm-centric, (2) industry-centric and (3) portfolio-centric\npeer-firm identification.\n",
                "链接": "https://arxiv.org/abs/2307.09332"
            },
            {
                "文章ID": "103447",
                "标题": "CAMERA: A Multimodal Dataset and Benchmark for Ad Text Generation",
                "作者": " Masato Mita,  Soichiro Murakami,  Akihiko Kato,  Peinan Zhang",
                "发布日期": "2023-09-22",
                "摘要": "  In response to the limitations of manual online ad production, significant\nresearch has been conducted in the field of automatic ad text generation (ATG).\nHowever, comparing different methods has been challenging because of the lack\nof benchmarks encompassing the entire field and the absence of well-defined\nproblem sets with clear model inputs and outputs. To address these challenges,\nthis paper aims to advance the field of ATG by introducing a redesigned task\nand constructing a benchmark. Specifically, we defined ATG as a\ncross-application task encompassing various aspects of the Internet\nadvertising. As part of our contribution, we propose a first benchmark dataset,\nCA Multimodal Evaluation for Ad Text GeneRAtion (CAMERA), carefully designed\nfor ATG to be able to leverage multi-modal information and conduct an\nindustry-wise evaluation. Furthermore, we demonstrate the usefulness of our\nproposed benchmark through evaluation experiments using multiple baseline\nmodels, which vary in terms of the type of pre-trained language model used and\nthe incorporation of multi-modal information. We also discuss the current state\nof the task and the future challenges.\n",
                "链接": "https://arxiv.org/abs/2309.12030"
            },
            {
                "文章ID": "81277",
                "标题": "Integrating Generative Artificial Intelligence in Intelligent Vehicle\n  Systems",
                "作者": " Lukas Stappen,  Jeremy Dillmann,  Serena Striegel,  Hans-Jörg Vögel,  Nicolas Flores-Herr,  Björn W. Schuller",
                "发布日期": "2023-05-30",
                "摘要": "  This paper aims to serve as a comprehensive guide for researchers and\npractitioners, offering insights into the current state, potential\napplications, and future research directions for generative artificial\nintelligence and foundation models within the context of intelligent vehicles.\nAs the automotive industry progressively integrates AI, generative artificial\nintelligence technologies hold the potential to revolutionize user\ninteractions, delivering more immersive, intuitive, and personalised in-car\nexperiences. We provide an overview of current applications of generative\nartificial intelligence in the automotive domain, emphasizing speech, audio,\nvision, and multimodal interactions. We subsequently outline critical future\nresearch areas, including domain adaptability, alignment, multimodal\nintegration and others, as well as, address the challenges and risks associated\nwith ethics. By fostering collaboration and addressing these research areas,\ngenerative artificial intelligence can unlock its full potential, transforming\nthe driving experience and shaping the future of intelligent vehicles.\n",
                "链接": "https://arxiv.org/abs/2305.17137"
            },
            {
                "文章ID": "113108",
                "标题": "CROMA: Remote Sensing Representations with Contrastive Radar-Optical\n  Masked Autoencoders",
                "作者": " Anthony Fuller,  Koreen Millard,  James R. Green",
                "发布日期": "2023-11-02",
                "摘要": "  A vital and rapidly growing application, remote sensing offers vast yet\nsparsely labeled, spatially aligned multimodal data; this makes self-supervised\nlearning algorithms invaluable. We present CROMA: a framework that combines\ncontrastive and reconstruction self-supervised objectives to learn rich\nunimodal and multimodal representations. Our method separately encodes\nmasked-out multispectral optical and synthetic aperture radar samples --\naligned in space and time -- and performs cross-modal contrastive learning.\nAnother encoder fuses these sensors, producing joint multimodal encodings that\nare used to predict the masked patches via a lightweight decoder. We show that\nthese objectives are complementary when leveraged on spatially aligned\nmultimodal data. We also introduce X- and 2D-ALiBi, which spatially biases our\ncross- and self-attention matrices. These strategies improve representations\nand allow our models to effectively extrapolate to images up to 17.6x larger at\ntest-time. CROMA outperforms the current SoTA multispectral model, evaluated\non: four classification benchmarks -- finetuning (avg. 1.8%), linear (avg.\n2.4%) and nonlinear (avg. 1.4%) probing, kNN classification (avg. 3.5%), and\nK-means clustering (avg. 8.4%); and three segmentation benchmarks (avg. 6.4%).\nCROMA's rich, optionally multimodal representations can be widely leveraged\nacross remote sensing applications.\n",
                "链接": "https://arxiv.org/abs/2311.00566"
            },
            {
                "文章ID": "105257",
                "标题": "The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)",
                "作者": " Zhengyuan Yang,  Linjie Li,  Kevin Lin,  Jianfeng Wang,  Chung-Ching Lin,  Zicheng Liu,  Lijuan Wang",
                "发布日期": "2023-10-12",
                "摘要": "  Large multimodal models (LMMs) extend large language models (LLMs) with\nmulti-sensory skills, such as visual understanding, to achieve stronger generic\nintelligence. In this paper, we analyze the latest model, GPT-4V(ision), to\ndeepen the understanding of LMMs. The analysis focuses on the intriguing tasks\nthat GPT-4V can perform, containing test samples to probe the quality and\ngenericity of GPT-4V's capabilities, its supported inputs and working modes,\nand the effective ways to prompt the model. In our approach to exploring\nGPT-4V, we curate and organize a collection of carefully designed qualitative\nsamples spanning a variety of domains and tasks. Observations from these\nsamples demonstrate that GPT-4V's unprecedented ability in processing\narbitrarily interleaved multimodal inputs and the genericity of its\ncapabilities together make GPT-4V a powerful multimodal generalist system.\nFurthermore, GPT-4V's unique capability of understanding visual markers drawn\non input images can give rise to new human-computer interaction methods such as\nvisual referring prompting. We conclude the report with in-depth discussions on\nthe emerging application scenarios and the future research directions for\nGPT-4V-based systems. We hope that this preliminary exploration will inspire\nfuture research on the next-generation multimodal task formulation, new ways to\nexploit and enhance LMMs to solve real-world problems, and gaining better\nunderstanding of multimodal foundation models. Finally, we acknowledge that the\nmodel under our study is solely the product of OpenAI's innovative work, and\nthey should be fully credited for its development. Please see the GPT-4V\ncontributions paper for the authorship and credit attribution:\nhttps://cdn.openai.com/contributions/gpt-4v.pdf\n",
                "链接": "https://arxiv.org/abs/2309.17421"
            }
        ]
    },
    {
        "question": {
            "question": "请找出最近一年内发表的关于自然语言处理领域中，使用Transformer模型并在大规模数据集上进行预训练的论文。特别关注模型结构和性能指标。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "82467",
                "标题": "Structure-Aware Language Model Pretraining Improves Dense Retrieval on\n  Structured Data",
                "作者": " Xinze Li,  Zhenghao Liu,  Chenyan Xiong,  Shi Yu,  Yu Gu,  Zhiyuan Liu,  Ge Yu",
                "发布日期": "2023-06-01",
                "摘要": "  This paper presents Structure Aware Dense Retrieval (SANTA) model, which\nencodes user queries and structured data in one universal embedding space for\nretrieving structured data. SANTA proposes two pretraining methods to make\nlanguage models structure-aware and learn effective representations for\nstructured data: 1) Structured Data Alignment, which utilizes the natural\nalignment relations between structured data and unstructured data for\nstructure-aware pretraining. It contrastively trains language models to\nrepresent multi-modal text data and teaches models to distinguish matched\nstructured data for unstructured texts. 2) Masked Entity Prediction, which\ndesigns an entity-oriented mask strategy and asks language models to fill in\nthe masked entities. Our experiments show that SANTA achieves state-of-the-art\non code search and product search and conducts convincing results in the\nzero-shot setting. SANTA learns tailored representations for multi-modal text\ndata by aligning structured and unstructured data pairs and capturing\nstructural semantics by masking and predicting entities in the structured data.\nAll codes are available at https://github.com/OpenMatch/OpenMatch.\n",
                "链接": "https://arxiv.org/abs/2305.19912"
            },
            {
                "文章ID": "20298",
                "标题": "DeepStruct: Pretraining of Language Models for Structure Prediction",
                "作者": " Chenguang Wang,  Xiao Liu,  Zui Chen,  Haoyun Hong,  Jie Tang,  Dawn Song",
                "发布日期": "2023-03-07",
                "摘要": "  We introduce a method for improving the structural understanding abilities of\nlanguage models. Unlike previous approaches that finetune the models with\ntask-specific augmentation, we pretrain language models on a collection of\ntask-agnostic corpora to generate structures from text. Our structure\npretraining enables zero-shot transfer of the learned knowledge that models\nhave about the structure tasks. We study the performance of this approach on 28\ndatasets, spanning 10 structure prediction tasks including open information\nextraction, joint entity and relation extraction, named entity recognition,\nrelation classification, semantic role labeling, event extraction, coreference\nresolution, factual probe, intent detection, and dialogue state tracking. We\nfurther enhance the pretraining with the task-specific training sets. We show\nthat a 10B parameter language model transfers non-trivially to most tasks and\nobtains state-of-the-art performance on 21 of 28 datasets that we evaluate.\n",
                "链接": "https://arxiv.org/abs/2205.10475"
            },
            {
                "文章ID": "109322",
                "标题": "Enhanced Transformer Architecture for Natural Language Processing",
                "作者": " Woohyeon Moon,  Taeyoung Kim,  Bumgeun Park,  Dongsoo Har",
                "发布日期": "2023-10-18",
                "摘要": "  Transformer is a state-of-the-art model in the field of natural language\nprocessing (NLP). Current NLP models primarily increase the number of\ntransformers to improve processing performance. However, this technique\nrequires a lot of training resources such as computing capacity. In this paper,\na novel structure of Transformer is proposed. It is featured by full layer\nnormalization, weighted residual connection, positional encoding exploiting\nreinforcement learning, and zero masked self-attention. The proposed\nTransformer model, which is called Enhanced Transformer, is validated by the\nbilingual evaluation understudy (BLEU) score obtained with the Multi30k\ntranslation dataset. As a result, the Enhanced Transformer achieves 202.96%\nhigher BLEU score as compared to the original transformer with the translation\ndataset.\n",
                "链接": "https://arxiv.org/abs/2310.10930"
            },
            {
                "文章ID": "56812",
                "标题": "NarrowBERT: Accelerating Masked Language Model Pretraining and Inference",
                "作者": " Haoxin Li,  Phillip Keung,  Daniel Cheng,  Jungo Kasai,  Noah A. Smith",
                "发布日期": "2023-06-07",
                "摘要": "  Large-scale language model pretraining is a very successful form of\nself-supervised learning in natural language processing, but it is increasingly\nexpensive to perform as the models and pretraining corpora have become larger\nover time. We propose NarrowBERT, a modified transformer encoder that increases\nthe throughput for masked language model pretraining by more than $2\\times$.\nNarrowBERT sparsifies the transformer model such that the self-attention\nqueries and feedforward layers only operate on the masked tokens of each\nsentence during pretraining, rather than all of the tokens as with the usual\ntransformer encoder. We also show that NarrowBERT increases the throughput at\ninference time by as much as $3.5\\times$ with minimal (or no) performance\ndegradation on sentence encoding tasks like MNLI. Finally, we examine the\nperformance of NarrowBERT on the IMDB and Amazon reviews classification and\nCoNLL NER tasks and show that it is also comparable to standard BERT\nperformance.\n",
                "链接": "https://arxiv.org/abs/2301.04761"
            },
            {
                "文章ID": "64313",
                "标题": "Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language\n  Processing",
                "作者": " Sheng Zhang,  Yanbo Xu,  Naoto Usuyama,  Jaspreet Bagga,  Robert Tinn,  Sam Preston,  Rajesh Rao,  Mu Wei,  Naveen Valluri,  Cliff Wong,  Matthew P. Lungren,  Tristan Naumann,  Hoifung Poon",
                "发布日期": "2023-03-03",
                "摘要": "  Contrastive pretraining on parallel image-text data has attained great\nsuccess in vision-language processing (VLP), as exemplified by CLIP and related\nmethods. However, prior explorations tend to focus on general domains in the\nweb. Biomedical images and text are rather different, but publicly available\ndatasets are small and skew toward chest X-ray, thus severely limiting\nprogress. In this paper, we conducted by far the largest study on biomedical\nVLP, using 15 million figure-caption pairs extracted from biomedical research\narticles in PubMed Central. Our dataset (PMC-15M) is two orders of magnitude\nlarger than existing biomedical image-text datasets such as MIMIC-CXR, and\nspans a diverse range of biomedical images. The standard CLIP method is\nsuboptimal for the biomedical domain. We propose BiomedCLIP with\ndomain-specific adaptations tailored to biomedical VLP. We conducted extensive\nexperiments and ablation studies on standard biomedical imaging tasks from\nretrieval to classification to visual question-answering (VQA). BiomedCLIP\nestablished new state of the art in a wide range of standard datasets,\nsubstantially outperformed prior VLP approaches. Surprisingly, BiomedCLIP even\noutperformed radiology-specific state-of-the-art models such as BioViL on\nradiology-specific tasks such as RSNA pneumonia detection, thus highlighting\nthe utility in large-scale pretraining across all biomedical image types. We\nwill release our models at https://aka.ms/biomedclip to facilitate future\nresearch in biomedical VLP.\n",
                "链接": "https://arxiv.org/abs/2303.00915"
            },
            {
                "文章ID": "108023",
                "标题": "NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series\n  Pretraining",
                "作者": " Chenguo Lin,  Xumeng Wen,  Wei Cao,  Congrui Huang,  Jiang Bian,  Stephen Lin,  Zhirong Wu",
                "发布日期": "2023-10-13",
                "摘要": "  Recent research on time-series self-supervised models shows great promise in\nlearning semantic representations. However, it has been limited to small-scale\ndatasets, e.g., thousands of temporal sequences. In this work, we make key\ntechnical contributions that are tailored to the numerical properties of\ntime-series data and allow the model to scale to large datasets, e.g., millions\nof temporal sequences. We adopt the Transformer architecture by first\npartitioning the input into non-overlapping windows. Each window is then\ncharacterized by its normalized shape and two scalar values denoting the mean\nand standard deviation within each window. To embed scalar values that may\npossess arbitrary numerical scales to high-dimensional vectors, we propose a\nnumerically multi-scaled embedding module enumerating all possible scales for\nthe scalar values. The model undergoes pretraining using the proposed\nnumerically multi-scaled embedding with a simple contrastive objective on a\nlarge-scale dataset containing over a million sequences. We study its transfer\nperformance on a number of univariate and multivariate classification\nbenchmarks. Our method exhibits remarkable improvement against previous\nrepresentation learning approaches and establishes the new state of the art,\neven compared with domain-specific non-learning-based methods.\n",
                "链接": "https://arxiv.org/abs/2310.07402"
            },
            {
                "文章ID": "39521",
                "标题": "Downstream Datasets Make Surprisingly Good Pretraining Corpora",
                "作者": " Kundan Krishna,  Saurabh Garg,  Jeffrey P. Bigham,  Zachary C. Lipton",
                "发布日期": "2023-05-29",
                "摘要": "  For most natural language processing tasks, the dominant practice is to\nfinetune large pretrained transformer models (e.g., BERT) using smaller\ndownstream datasets. Despite the success of this approach, it remains unclear\nto what extent these gains are attributable to the massive background corpora\nemployed for pretraining versus to the pretraining objectives themselves. This\npaper introduces a large-scale study of self-pretraining, where the same\n(downstream) training data is used for both pretraining and finetuning. In\nexperiments addressing both ELECTRA and RoBERTa models and 10 distinct\ndownstream classification datasets, we observe that self-pretraining rivals\nstandard pretraining on the BookWiki corpus (despite using around\n$10\\times$--$500\\times$ less data), outperforming the latter on $7$ and $5$\ndatasets, respectively. Surprisingly, these task-specific pretrained models\noften perform well on other tasks, including the GLUE benchmark. Besides\nclassification tasks, self-pretraining also provides benefits on structured\noutput prediction tasks such as span based question answering and commonsense\ninference, often providing more than $50\\%$ of the performance boosts provided\nby pretraining on the BookWiki corpus. Our results hint that in many scenarios,\nperformance gains attributable to pretraining are driven primarily by the\npretraining objective itself and are not always attributable to the use of\nexternal pretraining data in massive amounts. These findings are especially\nrelevant in light of concerns about intellectual property and offensive content\nin web-scale pretraining data.\n",
                "链接": "https://arxiv.org/abs/2209.14389"
            },
            {
                "文章ID": "82012",
                "标题": "Grokking of Hierarchical Structure in Vanilla Transformers",
                "作者": " Shikhar Murty,  Pratyusha Sharma,  Jacob Andreas,  Christopher D. Manning",
                "发布日期": "2023-05-31",
                "摘要": "  For humans, language production and comprehension is sensitive to the\nhierarchical structure of sentences. In natural language processing, past work\nhas questioned how effectively neural sequence models like transformers capture\nthis hierarchical structure when generalizing to structurally novel inputs. We\nshow that transformer language models can learn to generalize hierarchically\nafter training for extremely long periods -- far beyond the point when\nin-domain accuracy has saturated. We call this phenomenon \\emph{structural\ngrokking}. On multiple datasets, structural grokking exhibits inverted U-shaped\nscaling in model depth: intermediate-depth models generalize better than both\nvery deep and very shallow transformers. When analyzing the relationship\nbetween model-internal properties and grokking, we find that optimal depth for\ngrokking can be identified using the tree-structuredness metric of\n\\citet{murty2023projections}. Overall, our work provides strong evidence that,\nwith extended training, vanilla transformers discover and use hierarchical\nstructure.\n",
                "链接": "https://arxiv.org/abs/2305.18741"
            },
            {
                "文章ID": "79244",
                "标题": "Farewell to Aimless Large-scale Pretraining: Influential Subset\n  Selection for Language Model",
                "作者": " Xiao Wang,  Weikang Zhou,  Qi Zhang,  Jie Zhou,  Songyang Gao,  Junzhe Wang,  Menghan Zhang,  Xiang Gao,  Yunwen Chen,  Tao Gui",
                "发布日期": "2023-05-23",
                "摘要": "  Pretrained language models have achieved remarkable success in various\nnatural language processing tasks. However, pretraining has recently shifted\ntoward larger models and larger data, and this has resulted in significant\ncomputational and energy costs. In this paper, we propose Influence Subset\nSelection (ISS) for language model, which explicitly utilizes end-task\nknowledge to select a tiny subset of the pretraining corpus. Specifically, the\nISS selects the samples that will provide the most positive influence on the\nperformance of the end-task. Furthermore, we design a gradient matching based\ninfluence estimation method, which can drastically reduce the computation time\nof influence. With only 0.45% of the data and a three-orders-of-magnitude lower\ncomputational cost, ISS outperformed pretrained models (e.g., RoBERTa) on eight\ndatasets covering four domains.\n",
                "链接": "https://arxiv.org/abs/2305.12816"
            },
            {
                "文章ID": "4273",
                "标题": "Evaluating natural language processing models with generalization\n  metrics that do not need access to any training or testing data",
                "作者": " Yaoqing Yang,  Ryan Theisen,  Liam Hodgkinson,  Joseph E. Gonzalez,  Kannan Ramchandran,  Charles H. Martin,  Michael W. Mahoney",
                "发布日期": "2023-06-06",
                "摘要": "  Selecting suitable architecture parameters and training hyperparameters is\nessential for enhancing machine learning (ML) model performance. Several recent\nempirical studies conduct large-scale correlational analysis on neural networks\n(NNs) to search for effective \\emph{generalization metrics} that can guide this\ntype of model selection. Effective metrics are typically expected to correlate\nstrongly with test performance. In this paper, we expand on prior analyses by\nexamining generalization-metric-based model selection with the following\nobjectives: (i) focusing on natural language processing (NLP) tasks, as prior\nwork primarily concentrates on computer vision (CV) tasks; (ii) considering\nmetrics that directly predict \\emph{test error} instead of the\n\\emph{generalization gap}; (iii) exploring metrics that do not need access to\ndata to compute. From these objectives, we are able to provide the first model\nselection results on large pretrained Transformers from Huggingface using\ngeneralization metrics. Our analyses consider (I) hundreds of Transformers\ntrained in different settings, in which we systematically vary the amount of\ndata, the model size and the optimization hyperparameters, (II) a total of 51\npretrained Transformers from eight families of Huggingface NLP models,\nincluding GPT2, BERT, etc., and (III) a total of 28 existing and novel\ngeneralization metrics. Despite their niche status, we find that metrics\nderived from the heavy-tail (HT) perspective are particularly useful in NLP\ntasks, exhibiting stronger correlations than other, more popular metrics. To\nfurther examine these metrics, we extend prior formulations relying on power\nlaw (PL) spectral distributions to exponential (EXP) and\nexponentially-truncated power law (E-TPL) families.\n",
                "链接": "https://arxiv.org/abs/2202.02842"
            }
        ]
    }
]