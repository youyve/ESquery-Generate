[
    {
        "question": {
            "question": "与大模型工具学习相关论文",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "查询近一年模型推理加速相关的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "105649",
                "标题": "Subtractor-Based CNN Inference Accelerator",
                "作者": " Victor Gao,  Issam Hammad,  Kamal El-Sankary,  Jason Gu",
                "发布日期": "2023-10-03",
                "摘要": "  This paper presents a novel method to boost the performance of CNN inference\naccelerators by utilizing subtractors. The proposed CNN preprocessing\naccelerator relies on sorting, grouping, and rounding the weights to create\ncombinations that allow for the replacement of one multiplication operation and\naddition operation by a single subtraction operation when applying convolution\nduring inference. Given the high cost of multiplication in terms of power and\narea, replacing it with subtraction allows for a performance boost by reducing\npower and area. The proposed method allows for controlling the trade-off\nbetween performance gains and accuracy loss through increasing or decreasing\nthe usage of subtractors. With a rounding size of 0.05 and by utilizing LeNet-5\nwith the MNIST dataset, the proposed design can achieve 32.03% power savings\nand a 24.59% reduction in area at the cost of only 0.1% in terms of accuracy\nloss.\n",
                "链接": "https://arxiv.org/abs/2310.01022"
            },
            {
                "文章ID": "88446",
                "标题": "An Efficient Sparse Inference Software Accelerator for Transformer-based\n  Language Models on CPUs",
                "作者": " Haihao Shen,  Hengyu Meng,  Bo Dong,  Zhe Wang,  Ofir Zafrir,  Yi Ding,  Yu Luo,  Hanwen Chang,  Qun Gao,  Ziheng Wang,  Guy Boudoukh,  Moshe Wasserblat",
                "发布日期": "2023-06-30",
                "摘要": "  In recent years, Transformer-based language models have become the standard\napproach for natural language processing tasks. However, stringent throughput\nand latency requirements in industrial applications are limiting their\nadoption. To mitigate the gap, model compression techniques such as structured\npruning are being used to improve inference efficiency. However, most existing\nneural network inference runtimes lack adequate support for structured\nsparsity. In this paper, we propose an efficient sparse deep learning inference\nsoftware stack for Transformer-based language models where the weights are\npruned with constant block size. Our sparse software accelerator leverages\nIntel Deep Learning Boost to maximize the performance of sparse matrix - dense\nmatrix multiplication (commonly abbreviated as SpMM) on CPUs. Our SpMM kernel\noutperforms the existing sparse libraries (oneMKL, TVM, and LIBXSMM) by an\norder of magnitude on a wide range of GEMM shapes under 5 representative\nsparsity ratios (70%, 75%, 80%, 85%, 90%). Moreover, our SpMM kernel shows up\nto 5x speedup over dense GEMM kernel of oneDNN, a well-optimized dense library\nwidely used in industry. We apply our sparse accelerator on widely-used\nTransformer-based language models including Bert-Mini, DistilBERT, Bert-Base,\nand BERT-Large. Our sparse inference software shows up to 1.5x speedup over\nNeural Magic's Deepsparse under same configurations on Xeon on Amazon Web\nServices under proxy production latency constraints. We also compare our\nsolution with two framework-based inference solutions, ONNX Runtime and\nPyTorch, and demonstrate up to 37x speedup over ONNX Runtime and 345x over\nPyTorch on Xeon under the latency constraints. All the source code is publicly\navailable on Github: https://github.com/intel/intel-extension-for-transformers.\n",
                "链接": "https://arxiv.org/abs/2306.16601"
            },
            {
                "文章ID": "70160",
                "标题": "Practical Conformer: Optimizing size, speed and flops of Conformer for\n  on-Device and cloud ASR",
                "作者": " Rami Botros,  Anmol Gulati,  Tara N. Sainath,  Krzysztof Choromanski,  Ruoming Pang,  Trevor Strohman,  Weiran Wang,  Jiahui Yu",
                "发布日期": "2023-04-04",
                "摘要": "  Conformer models maintain a large number of internal states, the vast\nmajority of which are associated with self-attention layers. With limited\nmemory bandwidth, reading these from memory at each inference step can slow\ndown inference. In this paper, we design an optimized conformer that is small\nenough to meet on-device restrictions and has fast inference on TPUs. We\nexplore various ideas to improve the execution speed, including replacing lower\nconformer blocks with convolution-only blocks, strategically downsizing the\narchitecture, and utilizing an RNNAttention-Performer. Our optimized conformer\ncan be readily incorporated into a cascaded-encoder setting, allowing a\nsecond-pass decoder to operate on its output and improve the accuracy whenever\nmore resources are available. Altogether, we find that these optimizations can\nreduce latency by a factor of 6.8x, and come at a reasonable trade-off in\nquality. With the cascaded second-pass, we show that the recognition accuracy\nis completely recoverable. Thus, our proposed encoder can double as a strong\nstandalone encoder in on device, and as the first part of a high-performance\nASR pipeline.\n",
                "链接": "https://arxiv.org/abs/2304.00171"
            },
            {
                "文章ID": "7420",
                "标题": "Did AI get more negative recently?",
                "作者": " Dominik Beese,  Begüm Altunbaş,  Görkem Güzeler,  Steffen Eger",
                "发布日期": "2023-06-30",
                "摘要": "  In this paper, we classify scientific articles in the domain of natural\nlanguage processing (NLP) and machine learning (ML), as core subfields of\nartificial intelligence (AI), into whether (i) they extend the current\nstate-of-the-art by the introduction of novel techniques which beat existing\nmodels or whether (ii) they mainly criticize the existing state-of-the-art,\ni.e. that it is deficient with respect to some property (e.g. wrong evaluation,\nwrong datasets, misleading task specification). We refer to contributions under\n(i) as having a 'positive stance' and contributions under (ii) as having a\n'negative stance' (to related work). We annotate over 1.5 k papers from NLP and\nML to train a SciBERT-based model to automatically predict the stance of a\npaper based on its title and abstract. We then analyse large-scale trends on\nover 41 k papers from the last approximately 35 years in NLP and ML, finding\nthat papers have become substantially more positive over time, but negative\npapers also got more negative and we observe considerably more negative papers\nin recent years. Negative papers are also more influential in terms of\ncitations they receive.\n",
                "链接": "https://arxiv.org/abs/2202.13610"
            },
            {
                "文章ID": "120482",
                "标题": "A Hardware Evaluation Framework for Large Language Model Inference",
                "作者": " Hengrui Zhang,  August Ning,  Rohan Prabhakar,  David Wentzlaff",
                "发布日期": "2023-12-07",
                "摘要": "  The past year has witnessed the increasing popularity of Large Language\nModels (LLMs). Their unprecedented scale and associated high hardware cost have\nimpeded their broader adoption, calling for efficient hardware designs. With\nthe large hardware needed to simply run LLM inference, evaluating different\nhardware designs becomes a new bottleneck.\n  This work introduces LLMCompass, a hardware evaluation framework for LLM\ninference workloads. LLMCompass is fast, accurate, versatile, and able to\ndescribe and evaluate different hardware designs. LLMCompass includes a mapper\nto automatically find performance-optimal mapping and scheduling. It also\nincorporates an area-based cost model to help architects reason about their\ndesign choices. Compared to real-world hardware, LLMCompass' estimated latency\nachieves an average 10.4% error rate across various operators with various\ninput sizes and an average 4.1% error rate for LLM inference. With LLMCompass,\nsimulating a 4-NVIDIA A100 GPU node running GPT-3 175B inference can be done\nwithin 16 minutes on commodity hardware, including 26,400 rounds of the\nmapper's parameter search.\n  With the aid of LLMCompass, this work draws architectural implications and\nexplores new cost-effective hardware designs. By reducing the compute\ncapability or replacing High Bandwidth Memory (HBM) with traditional DRAM,\nthese new designs can achieve as much as 3.41x improvement in performance/cost\ncompared to an NVIDIA A100, making them promising choices for democratizing\nLLMs.\n  LLMCompass is planned to be fully open-source.\n",
                "链接": "https://arxiv.org/abs/2312.03134"
            },
            {
                "文章ID": "76938",
                "标题": "Inter-frame Accelerate Attack against Video Interpolation Models",
                "作者": " Junpei Liao,  Zhikai Chen,  Liang Yi,  Wenyuan Yang,  Baoyuan Wu,  Xiaochun Cao",
                "发布日期": "2023-05-12",
                "摘要": "  Deep learning based video frame interpolation (VIF) method, aiming to\nsynthesis the intermediate frames to enhance video quality, have been highly\ndeveloped in the past few years. This paper investigates the adversarial\nrobustness of VIF models. We apply adversarial attacks to VIF models and find\nthat the VIF models are very vulnerable to adversarial examples. To improve\nattack efficiency, we suggest to make full use of the property of video frame\ninterpolation task. The intuition is that the gap between adjacent frames would\nbe small, leading to the corresponding adversarial perturbations being similar\nas well. Then we propose a novel attack method named Inter-frame Accelerate\nAttack (IAA) that initializes the perturbation as the perturbation for the\nprevious adjacent frame and reduces the number of attack iterations. It is\nshown that our method can improve attack efficiency greatly while achieving\ncomparable attack performance with traditional methods. Besides, we also extend\nour method to video recognition models which are higher level vision tasks and\nachieves great attack efficiency.\n",
                "链接": "https://arxiv.org/abs/2305.06540"
            },
            {
                "文章ID": "77137",
                "标题": "Machine-learning-accelerated simulations to enable automatic surface\n  reconstruction",
                "作者": " Xiaochen Du,  James K. Damewood,  Jaclyn R. Lunger,  Reisel Millan,  Bilge Yildiz,  Lin Li,  Rafael Gómez-Bombarelli",
                "发布日期": "2023-12-22",
                "摘要": "  Understanding material surfaces and interfaces is vital in applications like\ncatalysis or electronics. By combining energies from electronic structure with\nstatistical mechanics, ab initio simulations can in principle predict the\nstructure of material surfaces as a function of thermodynamic variables.\nHowever, accurate energy simulations are prohibitive when coupled to the vast\nphase space that must be statistically sampled. Here, we present a bi-faceted\ncomputational loop to predict surface phase diagrams of multi-component\nmaterials that accelerates both the energy scoring and statistical sampling\nmethods. Fast, scalable, and data-efficient machine learning interatomic\npotentials are trained on high-throughput density-functional theory\ncalculations through closed-loop active learning. Markov-chain Monte Carlo\nsampling in the semi-grand canonical ensemble is enabled by using virtual\nsurface sites. The predicted surfaces for GaN(0001), Si(111), and SrTiO3(001)\nare in agreement with past work and suggest that the proposed strategy can\nmodel complex material surfaces and discover previously unreported surface\nterminations.\n",
                "链接": "https://arxiv.org/abs/2305.07251"
            },
            {
                "文章ID": "36901",
                "标题": "A Nonparametric Contextual Bandit with Arm-level Eligibility Control for\n  Customer Service Routing",
                "作者": " Ruofeng Wen,  Wenjun Zeng,  Yi Liu",
                "发布日期": "2022-09-13",
                "摘要": "  Amazon Customer Service provides real-time support for millions of customer\ncontacts every year. While bot-resolver helps automate some traffic, we still\nsee high demand for human agents, also called subject matter experts (SMEs).\nCustomers outreach with questions in different domains (return policy, device\ntroubleshooting, etc.). Depending on their training, not all SMEs are eligible\nto handle all contacts. Routing contacts to eligible SMEs turns out to be a\nnon-trivial problem because SMEs' domain eligibility is subject to training\nquality and can change over time. To optimally recommend SMEs while\nsimultaneously learning the true eligibility status, we propose to formulate\nthe routing problem with a nonparametric contextual bandit algorithm (K-Boot)\nplus an eligibility control (EC) algorithm. K-Boot models reward with a kernel\nsmoother on similar past samples selected by $k$-NN, and Bootstrap Thompson\nSampling for exploration. EC filters arms (SMEs) by the initially\nsystem-claimed eligibility and dynamically validates the reliability of this\ninformation. The proposed K-Boot is a general bandit algorithm, and EC is\napplicable to other bandits. Our simulation studies show that K-Boot performs\non par with state-of-the-art Bandit models, and EC boosts K-Boot performance\nwhen stochastic eligibility signal exists.\n",
                "链接": "https://arxiv.org/abs/2209.05278"
            },
            {
                "文章ID": "53823",
                "标题": "UnitY: Two-pass Direct Speech-to-speech Translation with Discrete Units",
                "作者": " Hirofumi Inaguma,  Sravya Popuri,  Ilia Kulikov,  Peng-Jen Chen,  Changhan Wang,  Yu-An Chung,  Yun Tang,  Ann Lee,  Shinji Watanabe,  Juan Pino",
                "发布日期": "2023-05-29",
                "摘要": "  Direct speech-to-speech translation (S2ST), in which all components can be\noptimized jointly, is advantageous over cascaded approaches to achieve fast\ninference with a simplified pipeline. We present a novel two-pass direct S2ST\narchitecture, UnitY, which first generates textual representations and predicts\ndiscrete acoustic units subsequently. We enhance the model performance by\nsubword prediction in the first-pass decoder, advanced two-pass decoder\narchitecture design and search strategy, and better training regularization. To\nleverage large amounts of unlabeled text data, we pre-train the first-pass text\ndecoder based on the self-supervised denoising auto-encoding task. Experimental\nevaluations on benchmark datasets at various data scales demonstrate that UnitY\noutperforms a single-pass speech-to-unit translation model by 2.5-4.2 ASR-BLEU\nwith 2.83x decoding speed-up. We show that the proposed methods boost the\nperformance even when predicting spectrogram in the second pass. However,\npredicting discrete units achieves 2.51x decoding speed-up compared to that\ncase.\n",
                "链接": "https://arxiv.org/abs/2212.08055"
            },
            {
                "文章ID": "75091",
                "标题": "How does GPT-2 compute greater-than?: Interpreting mathematical\n  abilities in a pre-trained language model",
                "作者": " Michael Hanna,  Ollie Liu,  Alexandre Variengien",
                "发布日期": "2023-11-03",
                "摘要": "  Pre-trained language models can be surprisingly adept at tasks they were not\nexplicitly trained on, but how they implement these capabilities is poorly\nunderstood. In this paper, we investigate the basic mathematical abilities\noften acquired by pre-trained language models. Concretely, we use mechanistic\ninterpretability techniques to explain the (limited) mathematical abilities of\nGPT-2 small. As a case study, we examine its ability to take in sentences such\nas \"The war lasted from the year 1732 to the year 17\", and predict valid\ntwo-digit end years (years > 32). We first identify a circuit, a small subset\nof GPT-2 small's computational graph that computes this task's output. Then, we\nexplain the role of each circuit component, showing that GPT-2 small's final\nmulti-layer perceptrons boost the probability of end years greater than the\nstart year. Finally, we find related tasks that activate our circuit. Our\nresults suggest that GPT-2 small computes greater-than using a complex but\ngeneral mechanism that activates across diverse contexts.\n",
                "链接": "https://arxiv.org/abs/2305.00586"
            }
        ]
    },
    {
        "question": {
            "question": "查找OCR文本检测最新进展",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "42779",
                "标题": "Text Detection Forgot About Document OCR",
                "作者": " Krzysztof Olejniczak,  Milan Šulc",
                "发布日期": "2023-01-24",
                "摘要": "  Detection and recognition of text from scans and other images, commonly\ndenoted as Optical Character Recognition (OCR), is a widely used form of\nautomated document processing with a number of methods available. Yet OCR\nsystems still do not achieve 100% accuracy, requiring human corrections in\napplications where correct readout is essential. Advances in machine learning\nenabled even more challenging scenarios of text detection and recognition\n\"in-the-wild\" - such as detecting text on objects from photographs of complex\nscenes. While the state-of-the-art methods for in-the-wild text recognition are\ntypically evaluated on complex scenes, their performance in the domain of\ndocuments is typically not published, and a comprehensive comparison with\nmethods for document OCR is missing. This paper compares several methods\ndesigned for in-the-wild text recognition and for document text recognition,\nand provides their evaluation on the domain of structured documents. The\nresults suggest that state-of-the-art methods originally proposed for\nin-the-wild text detection also achieve competitive results on document text\ndetection, outperforming available OCR methods. We argue that the application\nof document OCR should not be omitted in evaluation of text detection and\nrecognition methods.\n",
                "链接": "https://arxiv.org/abs/2210.07903"
            },
            {
                "文章ID": "43946",
                "标题": "OCR-VQGAN: Taming Text-within-Image Generation",
                "作者": " Juan A. Rodriguez,  David Vazquez,  Issam Laradji,  Marco Pedersoli,  Pau Rodriguez",
                "发布日期": "2022-10-26",
                "摘要": "  Synthetic image generation has recently experienced significant improvements\nin domains such as natural image or art generation. However, the problem of\nfigure and diagram generation remains unexplored. A challenging aspect of\ngenerating figures and diagrams is effectively rendering readable texts within\nthe images. To alleviate this problem, we present OCR-VQGAN, an image encoder,\nand decoder that leverages OCR pre-trained features to optimize a text\nperceptual loss, encouraging the architecture to preserve high-fidelity text\nand diagram structure. To explore our approach, we introduce the Paper2Fig100k\ndataset, with over 100k images of figures and texts from research papers. The\nfigures show architecture diagrams and methodologies of articles available at\narXiv.org from fields like artificial intelligence and computer vision. Figures\nusually include text and discrete objects, e.g., boxes in a diagram, with lines\nand arrows that connect them. We demonstrate the effectiveness of OCR-VQGAN by\nconducting several experiments on the task of figure reconstruction.\nAdditionally, we explore the qualitative and quantitative impact of weighting\ndifferent perceptual metrics in the overall loss function. We release code,\nmodels, and dataset at https://github.com/joanrod/ocr-vqgan.\n",
                "链接": "https://arxiv.org/abs/2210.11248"
            },
            {
                "文章ID": "53136",
                "标题": "Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned\n  Receipt Images",
                "作者": " Hongkuan Zhang,  Edward Whittaker,  Ikuo Kitagishi",
                "发布日期": "2023-10-17",
                "摘要": "  Digitization of scanned receipts aims to extract text from receipt images and\nsave it into structured documents. This is usually split into two sub-tasks:\ntext localization and optical character recognition (OCR). Most existing OCR\nmodels only focus on the cropped text instance images, which require the\nbounding box information provided by a text region detection model. Introducing\nan additional detector to identify the text instance images in advance adds\ncomplexity, however instance-level OCR models have very low accuracy when\nprocessing the whole image for the document-level OCR, such as receipt images\ncontaining multiple text lines arranged in various layouts. To this end, we\npropose a localization-free document-level OCR model for transcribing all the\ncharacters in a receipt image into an ordered sequence end-to-end.\nSpecifically, we finetune the pretrained instance-level model TrOCR with\nrandomly cropped image chunks, and gradually increase the image chunk size to\ngeneralize the recognition ability from instance images to full-page images. In\nour experiments on the SROIE receipt OCR dataset, the model finetuned with our\nstrategy achieved 64.4 F1-score and a 22.8% character error rate (CER),\nrespectively, which outperforms the baseline results with 48.5 F1-score and\n50.6% CER. The best model, which splits the full image into 15 equally sized\nchunks, gives 87.8 F1-score and 4.98% CER with minimal additional pre or\npost-processing of the output. Moreover, the characters in the generated\ndocument-level sequences are arranged in the reading order, which is practical\nfor real-world applications.\n",
                "链接": "https://arxiv.org/abs/2212.05525"
            },
            {
                "文章ID": "19555",
                "标题": "Detection Masking for Improved OCR on Noisy Documents",
                "作者": " Daniel Rotman,  Ophir Azulai,  Inbar Shapira,  Yevgeny Burshtein,  Udi Barzelay",
                "发布日期": "2022-05-18",
                "摘要": "  Optical Character Recognition (OCR), the task of extracting textual\ninformation from scanned documents is a vital and broadly used technology for\ndigitizing and indexing physical documents. Existing technologies perform well\nfor clean documents, but when the document is visually degraded, or when there\nare non-textual elements, OCR quality can be greatly impacted, specifically due\nto erroneous detections. In this paper we present an improved detection network\nwith a masking system to improve the quality of OCR performed on documents. By\nfiltering non-textual elements from the image we can utilize document-level OCR\nto incorporate contextual information to improve OCR results. We perform a\nunified evaluation on a publicly available dataset demonstrating the usefulness\nand broad applicability of our method. Additionally, we present and make\npublicly available our synthetic dataset with a unique hard-negative component\nspecifically tuned to improve detection results, and evaluate the benefits that\ncan be gained from its usage\n",
                "链接": "https://arxiv.org/abs/2205.08257"
            },
            {
                "文章ID": "98478",
                "标题": "DISGO: Automatic End-to-End Evaluation for Scene Text OCR",
                "作者": " Mei-Yuh Hwang,  Yangyang Shi,  Ankit Ramchandani,  Guan Pang,  Praveen Krishnan,  Lucas Kabela,  Frank Seide,  Samyak Datta,  Jun Liu",
                "发布日期": "2023-08-28",
                "摘要": "  This paper discusses the challenges of optical character recognition (OCR) on\nnatural scenes, which is harder than OCR on documents due to the wild content\nand various image backgrounds. We propose to uniformly use word error rates\n(WER) as a new measurement for evaluating scene-text OCR, both end-to-end (e2e)\nperformance and individual system component performances. Particularly for the\ne2e metric, we name it DISGO WER as it considers Deletion, Insertion,\nSubstitution, and Grouping/Ordering errors. Finally we propose to utilize the\nconcept of super blocks to automatically compute BLEU scores for e2e OCR\nmachine translation. The small SCUT public test set is used to demonstrate WER\nperformance by a modularized OCR system.\n",
                "链接": "https://arxiv.org/abs/2308.13173"
            },
            {
                "文章ID": "120332",
                "标题": "UPOCR: Towards Unified Pixel-Level OCR Interface",
                "作者": " Dezhi Peng,  Zhenhua Yang,  Jiaxin Zhang,  Chongyu Liu,  Yongxin Shi,  Kai Ding,  Fengjun Guo,  Lianwen Jin",
                "发布日期": "2023-12-06",
                "摘要": "  In recent years, the optical character recognition (OCR) field has been\nproliferating with plentiful cutting-edge approaches for a wide spectrum of\ntasks. However, these approaches are task-specifically designed with divergent\nparadigms, architectures, and training strategies, which significantly\nincreases the complexity of research and maintenance and hinders the fast\ndeployment in applications. To this end, we propose UPOCR, a\nsimple-yet-effective generalist model for Unified Pixel-level OCR interface.\nSpecifically, the UPOCR unifies the paradigm of diverse OCR tasks as\nimage-to-image transformation and the architecture as a vision Transformer\n(ViT)-based encoder-decoder. Learnable task prompts are introduced to push the\ngeneral feature representations extracted by the encoder toward task-specific\nspaces, endowing the decoder with task awareness. Moreover, the model training\nis uniformly aimed at minimizing the discrepancy between the generated and\nground-truth images regardless of the inhomogeneity among tasks. Experiments\nare conducted on three pixel-level OCR tasks including text removal, text\nsegmentation, and tampered text detection. Without bells and whistles, the\nexperimental results showcase that the proposed method can simultaneously\nachieve state-of-the-art performance on three tasks with a unified single\nmodel, which provides valuable strategies and insights for future research on\ngeneralist OCR models. Code will be publicly available.\n",
                "链接": "https://arxiv.org/abs/2312.02694"
            },
            {
                "文章ID": "108352",
                "标题": "Invisible Threats: Backdoor Attack in OCR Systems",
                "作者": " Mauro Conti,  Nicola Farronato,  Stefanos Koffas,  Luca Pajola,  Stjepan Picek",
                "发布日期": "2023-10-13",
                "摘要": "  Optical Character Recognition (OCR) is a widely used tool to extract text\nfrom scanned documents. Today, the state-of-the-art is achieved by exploiting\ndeep neural networks. However, the cost of this performance is paid at the\nprice of system vulnerability. For instance, in backdoor attacks, attackers\ncompromise the training phase by inserting a backdoor in the victim's model\nthat will be activated at testing time by specific patterns while leaving the\noverall model performance intact. This work proposes a backdoor attack for OCR\nresulting in the injection of non-readable characters from malicious input\nimages. This simple but effective attack exposes the state-of-the-art OCR\nweakness, making the extracted text correct to human eyes but simultaneously\nunusable for the NLP application that uses OCR as a preprocessing step.\nExperimental results show that the attacked models successfully output\nnon-readable characters for around 90% of the poisoned instances without\nharming their performance for the remaining instances.\n",
                "链接": "https://arxiv.org/abs/2310.08259"
            },
            {
                "文章ID": "48455",
                "标题": "A Benchmark and Dataset for Post-OCR text correction in Sanskrit",
                "作者": " Ayush Maheshwari,  Nikhil Singh,  Amrith Krishna,  Ganesh Ramakrishnan",
                "发布日期": "2022-11-16",
                "摘要": "  Sanskrit is a classical language with about 30 million extant manuscripts fit\nfor digitisation, available in written, printed or scannedimage forms. However,\nit is still considered to be a low-resource language when it comes to available\ndigital resources. In this work, we release a post-OCR text correction dataset\ncontaining around 218,000 sentences, with 1.5 million words, from 30 different\nbooks. Texts in Sanskrit are known to be diverse in terms of their linguistic\nand stylistic usage since Sanskrit was the 'lingua franca' for discourse in the\nIndian subcontinent for about 3 millennia. Keeping this in mind, we release a\nmulti-domain dataset, from areas as diverse as astronomy, medicine and\nmathematics, with some of them as old as 18 centuries. Further, we release\nmultiple strong baselines as benchmarks for the task, based on pre-trained\nSeq2Seq language models. We find that our best-performing model, consisting of\nbyte level tokenization in conjunction with phonetic encoding (Byt5+SLP1),\nyields a 23% point increase over the OCR output in terms of word and character\nerror rates. Moreover, we perform extensive experiments in evaluating these\nmodels on their performance and analyse common causes of mispredictions both at\nthe graphemic and lexical levels. Our code and dataset is publicly available at\nhttps://github.com/ayushbits/pe-ocr-sanskrit.\n",
                "链接": "https://arxiv.org/abs/2211.07980"
            },
            {
                "文章ID": "24093",
                "标题": "An Evaluation of OCR on Egocentric Data",
                "作者": " Valentin Popescu,  Dima Damen,  Toby Perrett",
                "发布日期": "2022-06-14",
                "摘要": "  In this paper, we evaluate state-of-the-art OCR methods on Egocentric data.\nWe annotate text in EPIC-KITCHENS images, and demonstrate that existing OCR\nmethods struggle with rotated text, which is frequently observed on objects\nbeing handled. We introduce a simple rotate-and-merge procedure which can be\napplied to pre-trained OCR models that halves the normalized edit distance\nerror. This suggests that future OCR attempts should incorporate rotation into\nmodel design and training procedures.\n",
                "链接": "https://arxiv.org/abs/2206.05496"
            },
            {
                "文章ID": "28385",
                "标题": "Detection of Furigana Text in Images",
                "作者": " Nikolaj Kjøller Bjerregaard,  Veronika Cheplygina,  Stefan Heinrich",
                "发布日期": "2022-07-11",
                "摘要": "  Furigana are pronunciation notes used in Japanese writing. Being able to\ndetect these can help improve optical character recognition (OCR) performance\nor make more accurate digital copies of Japanese written media by correctly\ndisplaying furigana. This project focuses on detecting furigana in Japanese\nbooks and comics. While there has been research into the detection of Japanese\ntext in general, there are currently no proposed methods for detecting\nfurigana.\n  We construct a new dataset containing Japanese written media and annotations\nof furigana. We propose an evaluation metric for such data which is similar to\nthe evaluation protocols used in object detection except that it allows groups\nof objects to be labeled by one annotation. We propose a method for detection\nof furigana that is based on mathematical morphology and connected component\nanalysis. We evaluate the detections of the dataset and compare different\nmethods for text extraction. We also evaluate different types of images such as\nbooks and comics individually and discuss the challenges of each type of image.\n  The proposed method reaches an F1-score of 76\\% on the dataset. The method\nperforms well on regular books, but less so on comics, and books of irregular\nformat. Finally, we show that the proposed method can improve the performance\nof OCR by 5\\% on the manga109 dataset.\n  Source code is available via\n\\texttt{\\url{https://github.com/nikolajkb/FuriganaDetection}}\n",
                "链接": "https://arxiv.org/abs/2207.03960"
            }
        ]
    },
    {
        "question": {
            "question": "查找OCR文本识别最新进展。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "42779",
                "标题": "Text Detection Forgot About Document OCR",
                "作者": " Krzysztof Olejniczak,  Milan Šulc",
                "发布日期": "2023-01-24",
                "摘要": "  Detection and recognition of text from scans and other images, commonly\ndenoted as Optical Character Recognition (OCR), is a widely used form of\nautomated document processing with a number of methods available. Yet OCR\nsystems still do not achieve 100% accuracy, requiring human corrections in\napplications where correct readout is essential. Advances in machine learning\nenabled even more challenging scenarios of text detection and recognition\n\"in-the-wild\" - such as detecting text on objects from photographs of complex\nscenes. While the state-of-the-art methods for in-the-wild text recognition are\ntypically evaluated on complex scenes, their performance in the domain of\ndocuments is typically not published, and a comprehensive comparison with\nmethods for document OCR is missing. This paper compares several methods\ndesigned for in-the-wild text recognition and for document text recognition,\nand provides their evaluation on the domain of structured documents. The\nresults suggest that state-of-the-art methods originally proposed for\nin-the-wild text detection also achieve competitive results on document text\ndetection, outperforming available OCR methods. We argue that the application\nof document OCR should not be omitted in evaluation of text detection and\nrecognition methods.\n",
                "链接": "https://arxiv.org/abs/2210.07903"
            },
            {
                "文章ID": "43946",
                "标题": "OCR-VQGAN: Taming Text-within-Image Generation",
                "作者": " Juan A. Rodriguez,  David Vazquez,  Issam Laradji,  Marco Pedersoli,  Pau Rodriguez",
                "发布日期": "2022-10-26",
                "摘要": "  Synthetic image generation has recently experienced significant improvements\nin domains such as natural image or art generation. However, the problem of\nfigure and diagram generation remains unexplored. A challenging aspect of\ngenerating figures and diagrams is effectively rendering readable texts within\nthe images. To alleviate this problem, we present OCR-VQGAN, an image encoder,\nand decoder that leverages OCR pre-trained features to optimize a text\nperceptual loss, encouraging the architecture to preserve high-fidelity text\nand diagram structure. To explore our approach, we introduce the Paper2Fig100k\ndataset, with over 100k images of figures and texts from research papers. The\nfigures show architecture diagrams and methodologies of articles available at\narXiv.org from fields like artificial intelligence and computer vision. Figures\nusually include text and discrete objects, e.g., boxes in a diagram, with lines\nand arrows that connect them. We demonstrate the effectiveness of OCR-VQGAN by\nconducting several experiments on the task of figure reconstruction.\nAdditionally, we explore the qualitative and quantitative impact of weighting\ndifferent perceptual metrics in the overall loss function. We release code,\nmodels, and dataset at https://github.com/joanrod/ocr-vqgan.\n",
                "链接": "https://arxiv.org/abs/2210.11248"
            },
            {
                "文章ID": "48455",
                "标题": "A Benchmark and Dataset for Post-OCR text correction in Sanskrit",
                "作者": " Ayush Maheshwari,  Nikhil Singh,  Amrith Krishna,  Ganesh Ramakrishnan",
                "发布日期": "2022-11-16",
                "摘要": "  Sanskrit is a classical language with about 30 million extant manuscripts fit\nfor digitisation, available in written, printed or scannedimage forms. However,\nit is still considered to be a low-resource language when it comes to available\ndigital resources. In this work, we release a post-OCR text correction dataset\ncontaining around 218,000 sentences, with 1.5 million words, from 30 different\nbooks. Texts in Sanskrit are known to be diverse in terms of their linguistic\nand stylistic usage since Sanskrit was the 'lingua franca' for discourse in the\nIndian subcontinent for about 3 millennia. Keeping this in mind, we release a\nmulti-domain dataset, from areas as diverse as astronomy, medicine and\nmathematics, with some of them as old as 18 centuries. Further, we release\nmultiple strong baselines as benchmarks for the task, based on pre-trained\nSeq2Seq language models. We find that our best-performing model, consisting of\nbyte level tokenization in conjunction with phonetic encoding (Byt5+SLP1),\nyields a 23% point increase over the OCR output in terms of word and character\nerror rates. Moreover, we perform extensive experiments in evaluating these\nmodels on their performance and analyse common causes of mispredictions both at\nthe graphemic and lexical levels. Our code and dataset is publicly available at\nhttps://github.com/ayushbits/pe-ocr-sanskrit.\n",
                "链接": "https://arxiv.org/abs/2211.07980"
            },
            {
                "文章ID": "98478",
                "标题": "DISGO: Automatic End-to-End Evaluation for Scene Text OCR",
                "作者": " Mei-Yuh Hwang,  Yangyang Shi,  Ankit Ramchandani,  Guan Pang,  Praveen Krishnan,  Lucas Kabela,  Frank Seide,  Samyak Datta,  Jun Liu",
                "发布日期": "2023-08-28",
                "摘要": "  This paper discusses the challenges of optical character recognition (OCR) on\nnatural scenes, which is harder than OCR on documents due to the wild content\nand various image backgrounds. We propose to uniformly use word error rates\n(WER) as a new measurement for evaluating scene-text OCR, both end-to-end (e2e)\nperformance and individual system component performances. Particularly for the\ne2e metric, we name it DISGO WER as it considers Deletion, Insertion,\nSubstitution, and Grouping/Ordering errors. Finally we propose to utilize the\nconcept of super blocks to automatically compute BLEU scores for e2e OCR\nmachine translation. The small SCUT public test set is used to demonstrate WER\nperformance by a modularized OCR system.\n",
                "链接": "https://arxiv.org/abs/2308.13173"
            },
            {
                "文章ID": "36484",
                "标题": "Levenshtein OCR",
                "作者": " Cheng Da,  Peng Wang,  Cong Yao",
                "发布日期": "2022-11-15",
                "摘要": "  A novel scene text recognizer based on Vision-Language Transformer (VLT) is\npresented. Inspired by Levenshtein Transformer in the area of NLP, the proposed\nmethod (named Levenshtein OCR, and LevOCR for short) explores an alternative\nway for automatically transcribing textual content from cropped natural images.\nSpecifically, we cast the problem of scene text recognition as an iterative\nsequence refinement process. The initial prediction sequence produced by a pure\nvision model is encoded and fed into a cross-modal transformer to interact and\nfuse with the visual features, to progressively approximate the ground truth.\nThe refinement process is accomplished via two basic character-level\noperations: deletion and insertion, which are learned with imitation learning\nand allow for parallel decoding, dynamic length change and good\ninterpretability. The quantitative experiments clearly demonstrate that LevOCR\nachieves state-of-the-art performances on standard benchmarks and the\nqualitative analyses verify the effectiveness and advantage of the proposed\nLevOCR algorithm. Code is available at\nhttps://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/LevOCR.\n",
                "链接": "https://arxiv.org/abs/2209.03594"
            },
            {
                "文章ID": "108352",
                "标题": "Invisible Threats: Backdoor Attack in OCR Systems",
                "作者": " Mauro Conti,  Nicola Farronato,  Stefanos Koffas,  Luca Pajola,  Stjepan Picek",
                "发布日期": "2023-10-13",
                "摘要": "  Optical Character Recognition (OCR) is a widely used tool to extract text\nfrom scanned documents. Today, the state-of-the-art is achieved by exploiting\ndeep neural networks. However, the cost of this performance is paid at the\nprice of system vulnerability. For instance, in backdoor attacks, attackers\ncompromise the training phase by inserting a backdoor in the victim's model\nthat will be activated at testing time by specific patterns while leaving the\noverall model performance intact. This work proposes a backdoor attack for OCR\nresulting in the injection of non-readable characters from malicious input\nimages. This simple but effective attack exposes the state-of-the-art OCR\nweakness, making the extracted text correct to human eyes but simultaneously\nunusable for the NLP application that uses OCR as a preprocessing step.\nExperimental results show that the attacked models successfully output\nnon-readable characters for around 90% of the poisoned instances without\nharming their performance for the remaining instances.\n",
                "链接": "https://arxiv.org/abs/2310.08259"
            },
            {
                "文章ID": "53136",
                "标题": "Extending TrOCR for Text Localization-Free OCR of Full-Page Scanned\n  Receipt Images",
                "作者": " Hongkuan Zhang,  Edward Whittaker,  Ikuo Kitagishi",
                "发布日期": "2023-10-17",
                "摘要": "  Digitization of scanned receipts aims to extract text from receipt images and\nsave it into structured documents. This is usually split into two sub-tasks:\ntext localization and optical character recognition (OCR). Most existing OCR\nmodels only focus on the cropped text instance images, which require the\nbounding box information provided by a text region detection model. Introducing\nan additional detector to identify the text instance images in advance adds\ncomplexity, however instance-level OCR models have very low accuracy when\nprocessing the whole image for the document-level OCR, such as receipt images\ncontaining multiple text lines arranged in various layouts. To this end, we\npropose a localization-free document-level OCR model for transcribing all the\ncharacters in a receipt image into an ordered sequence end-to-end.\nSpecifically, we finetune the pretrained instance-level model TrOCR with\nrandomly cropped image chunks, and gradually increase the image chunk size to\ngeneralize the recognition ability from instance images to full-page images. In\nour experiments on the SROIE receipt OCR dataset, the model finetuned with our\nstrategy achieved 64.4 F1-score and a 22.8% character error rate (CER),\nrespectively, which outperforms the baseline results with 48.5 F1-score and\n50.6% CER. The best model, which splits the full image into 15 equally sized\nchunks, gives 87.8 F1-score and 4.98% CER with minimal additional pre or\npost-processing of the output. Moreover, the characters in the generated\ndocument-level sequences are arranged in the reading order, which is practical\nfor real-world applications.\n",
                "链接": "https://arxiv.org/abs/2212.05525"
            },
            {
                "文章ID": "95779",
                "标题": "Is there progress in activity progress prediction?",
                "作者": " Frans de Boer,  Jan C. van Gemert,  Jouke Dijkstra,  Silvia L. Pintea",
                "发布日期": "2023-08-11",
                "摘要": "  Activity progress prediction aims to estimate what percentage of an activity\nhas been completed. Currently this is done with machine learning approaches,\ntrained and evaluated on complicated and realistic video datasets. The videos\nin these datasets vary drastically in length and appearance. And some of the\nactivities have unanticipated developments, making activity progression\ndifficult to estimate. In this work, we examine the results obtained by\nexisting progress prediction methods on these datasets. We find that current\nprogress prediction methods seem not to extract useful visual information for\nthe progress prediction task. Therefore, these methods fail to exceed simple\nframe-counting baselines. We design a precisely controlled dataset for activity\nprogress prediction and on this synthetic dataset we show that the considered\nmethods can make use of the visual information, when this directly relates to\nthe progress prediction. We conclude that the progress prediction task is\nill-posed on the currently used real-world datasets. Moreover, to fairly\nmeasure activity progression we advise to consider a, simple but effective,\nframe-counting baseline.\n",
                "链接": "https://arxiv.org/abs/2308.05533"
            },
            {
                "文章ID": "1390",
                "标题": "Recent Progress in the CUHK Dysarthric Speech Recognition System",
                "作者": " Shansong Liu,  Mengzhe Geng,  Shoukang Hu,  Xurong Xie,  Mingyu Cui,  Jianwei Yu,  Xunying Liu,  Helen Meng",
                "发布日期": "2022-03-01",
                "摘要": "  Despite the rapid progress of automatic speech recognition (ASR) technologies\nin the past few decades, recognition of disordered speech remains a highly\nchallenging task to date. Disordered speech presents a wide spectrum of\nchallenges to current data intensive deep neural networks (DNNs) based ASR\ntechnologies that predominantly target normal speech. This paper presents\nrecent research efforts at the Chinese University of Hong Kong (CUHK) to\nimprove the performance of disordered speech recognition systems on the largest\npublicly available UASpeech dysarthric speech corpus. A set of novel modelling\ntechniques including neural architectural search, data augmentation using\nspectra-temporal perturbation, model based speaker adaptation and cross-domain\ngeneration of visual features within an audio-visual speech recognition (AVSR)\nsystem framework were employed to address the above challenges. The combination\nof these techniques produced the lowest published word error rate (WER) of\n25.21% on the UASpeech test set 16 dysarthric speakers, and an overall WER\nreduction of 5.4% absolute (17.6% relative) over the CUHK 2018 dysarthric\nspeech recognition system featuring a 6-way DNN system combination and cross\nadaptation of out-of-domain normal speech data trained systems. Bayesian model\nadaptation further allows rapid adaptation to individual dysarthric speakers to\nbe performed using as little as 3.06 seconds of speech. The efficacy of these\ntechniques were further demonstrated on a CUDYS Cantonese dysarthric speech\nrecognition task.\n",
                "链接": "https://arxiv.org/abs/2201.05845"
            },
            {
                "文章ID": "25669",
                "标题": "Towards Optimizing OCR for Accessibility",
                "作者": " Peya Mowar,  Tanuja Ganu,  Saikat Guha",
                "发布日期": "2022-06-27",
                "摘要": "  Visual cues such as structure, emphasis, and icons play an important role in\nefficient information foraging by sighted individuals and make for a\npleasurable reading experience. Blind, low-vision and other print-disabled\nindividuals miss out on these cues since current OCR and text-to-speech\nsoftware ignore them, resulting in a tedious reading experience. We identify\nfour semantic goals for an enjoyable listening experience, and identify\nsyntactic visual cues that help make progress towards these goals. Empirically,\nwe find that preserving even one or two visual cues in aural form significantly\nenhances the experience for listening to print content.\n",
                "链接": "https://arxiv.org/abs/2206.10254"
            }
        ]
    },
    {
        "question": {
            "question": "近几个月agent系列决策文章。",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "488",
                "标题": "Sales Time Series Analytics Using Deep Q-Learning",
                "作者": " Bohdan M. Pavlyshenko",
                "发布日期": "2022-01-07",
                "摘要": "  The article describes the use of deep Q-learning models in the problems of\nsales time series analytics. In contrast to supervised machine learning which\nis a kind of passive learning using historical data, Q-learning is a kind of\nactive learning with goal to maximize a reward by optimal sequence of actions.\nModel free Q-learning approach for optimal pricing strategies and supply-demand\nproblems was considered in the work. The main idea of the study is to show that\nusing deep Q-learning approach in time series analytics, the sequence of\nactions can be optimized by maximizing the reward function when the environment\nfor learning agent interaction can be modeled using the parametric model and in\nthe case of using the model which is based on the historical data. In the\npricing optimizing case study environment was modeled using sales dependence on\nextras price and randomly simulated demand. In the pricing optimizing case\nstudy, the environment was modeled using sales dependence on extra price and\nrandomly simulated demand. In the supply-demand case study, it was proposed to\nuse historical demand time series for environment modeling, agent states were\nrepresented by promo actions, previous demand values and weekly seasonality\nfeatures. Obtained results show that using deep Q-learning, we can optimize the\ndecision making process for price optimization and supply-demand problems.\nEnvironment modeling using parametric models and historical data can be used\nfor the cold start of learning agent. On the next steps, after the cold start,\nthe trained agent can be used in real business environment.\n",
                "链接": "https://arxiv.org/abs/2201.02058"
            },
            {
                "文章ID": "49179",
                "标题": "Distributed Average Consensus Over Noisy Communication Links in Directed\n  Graphs",
                "作者": " Vivek Khatana,  Murti V. Salapaka",
                "发布日期": "2022-11-21",
                "摘要": "  Motivated by the needs of resiliency, scalability, and plug-and-play\noperation, distributed decision-making is becoming increasingly prevalent. The\nproblem of achieving consensus in a multi-agent system is at the core of\ndistributed decision-making. In this article, we study the problem of achieving\naverage consensus over a directed multi-agent network when the communication\nlinks are corrupted with noise. We propose an algorithm where each agent\nupdates its estimates based on the local mixing of information and adds its\nweighted noise-free initial information to its updates during every iteration.\nWe demonstrate that with appropriately designed weights the agents achieve\nconsensus under additive communication noise. We establish that when the\ncommunication links are noiseless the proposed algorithm moves towards\nconsensus at a geometric rate. Under communication noise, we prove that the\nagent estimates reach a consensus value almost surely. We present numerical\nexperiments to corroborate the efficacy of the proposed algorithm under\ndifferent noise realizations and various algorithm parameters.\n",
                "链接": "https://arxiv.org/abs/2211.10334"
            },
            {
                "文章ID": "86597",
                "标题": "Evolving Strategies for Competitive Multi-Agent Search",
                "作者": " Erkin Bahceci,  Riitta Katila,  Risto Miikkulainen",
                "发布日期": "2023-07-04",
                "摘要": "  While evolutionary computation is well suited for automatic discovery in\nengineering, it can also be used to gain insight into how humans and\norganizations could perform more effectively. Using a real-world problem of\ninnovation search in organizations as the motivating example, this article\nfirst formalizes human creative problem solving as competitive multi-agent\nsearch (CMAS). CMAS is different from existing single-agent and team search\nproblems in that the agents interact through knowledge of other agents'\nsearches and through the dynamic changes in the search landscape that result\nfrom these searches. The main hypothesis is that evolutionary computation can\nbe used to discover effective strategies for CMAS; this hypothesis is verified\nin a series of experiments on the NK model, i.e.\\ partially correlated and\ntunably rugged fitness landscapes. Different specialized strategies are evolved\nfor each different competitive environment, and also general strategies that\nperform well across environments. These strategies are more effective and more\ncomplex than hand-designed strategies and a strategy based on traditional tree\nsearch. Using a novel spherical visualization of such landscapes, insight is\ngained about how successful strategies work, e.g.\\ by tracking positive changes\nin the landscape. The article thus provides a possible framework for studying\nvarious human creative activities as competitive multi-agent search in the\nfuture.\n",
                "链接": "https://arxiv.org/abs/2306.10640"
            },
            {
                "文章ID": "34295",
                "标题": "Exploring Task-oriented Communication in Multi-agent System: A Deep\n  Reinforcement Learning Approach",
                "作者": " Guojun He",
                "发布日期": "2022-09-16",
                "摘要": "  The multi-agent system (MAS) enables the sharing of capabilities among\nagents, such that collaborative tasks can be accomplished with high scalability\nand efficiency. MAS is increasingly widely applied in various fields.\nMeanwhile, the large-scale and time-sensitive data transmission between agents\nbrings challenges to the communication system. The traditional wireless\ncommunication ignores the content of the data and its impact on the task\nexecution at the receiver, which makes it difficult to guarantee the timeliness\nand relevance of the information. This limitation leads to that traditional\nwireless communication struggles to effectively support emerging multi-agent\ncollaborative applications. Faced with this dilemma, task-oriented\ncommunication is a potential solution, which aims to transmit task-relevant\ninformation to improve task execution performance. However, multi-agent\ncollaboration itself is a complex class of sequential decision problems. It is\nchallenging to explore efficient information flow in this context. In this\narticle, we use deep reinforcement learning (DRL) to explore task-oriented\ncommunication in MAS. We begin with a discussion on the application of DRL to\ntask-oriented communication. We then envision a task-oriented communication\narchitecture for MAS, and discuss the designs based on DRL. Finally, we discuss\nopen problems for future research and conclude this article.\n",
                "链接": "https://arxiv.org/abs/2208.10165"
            },
            {
                "文章ID": "33233",
                "标题": "RLang: A Declarative Language for Describing Partial World Knowledge to\n  Reinforcement Learning Agents",
                "作者": " Rafael Rodriguez-Sanchez,  Benjamin A. Spiegel,  Jennifer Wang,  Roma Patel,  Stefanie Tellex,  George Konidaris",
                "发布日期": "2023-05-31",
                "摘要": "  We introduce RLang, a domain-specific language (DSL) for communicating domain\nknowledge to an RL agent. Unlike existing RL DSLs that ground to\n\\textit{single} elements of a decision-making formalism (e.g., the reward\nfunction or policy), RLang can specify information about every element of a\nMarkov decision process. We define precise syntax and grounding semantics for\nRLang, and provide a parser that grounds RLang programs to an\nalgorithm-agnostic \\textit{partial} world model and policy that can be\nexploited by an RL agent. We provide a series of example RLang programs\ndemonstrating how different RL methods can exploit the resulting knowledge,\nencompassing model-free and model-based tabular algorithms, policy gradient and\nvalue-based methods, hierarchical approaches, and deep methods.\n",
                "链接": "https://arxiv.org/abs/2208.06448"
            },
            {
                "文章ID": "60944",
                "标题": "Scalability Bottlenecks in Multi-Agent Reinforcement Learning Systems",
                "作者": " Kailash Gogineni,  Peng Wei,  Tian Lan,  Guru Venkataramani",
                "发布日期": "2023-02-13",
                "摘要": "  Multi-Agent Reinforcement Learning (MARL) is a promising area of research\nthat can model and control multiple, autonomous decision-making agents. During\nonline training, MARL algorithms involve performance-intensive computations\nsuch as exploration and exploitation phases originating from large\nobservation-action space belonging to multiple agents. In this article, we seek\nto characterize the scalability bottlenecks in several popular classes of MARL\nalgorithms during their training phases. Our experimental results reveal new\ninsights into the key modules of MARL algorithms that limit the scalability,\nand outline potential strategies that may help address these performance\nissues.\n",
                "链接": "https://arxiv.org/abs/2302.05007"
            },
            {
                "文章ID": "47057",
                "标题": "Graph Reinforcement Learning Application to Co-operative Decision-Making\n  in Mixed Autonomy Traffic: Framework, Survey, and Challenges",
                "作者": " Qi Liu,  Xueyuan Li,  Zirui Li,  Jingda Wu,  Guodong Du,  Xin Gao,  Fan Yang,  Shihua Yuan",
                "发布日期": "2022-11-08",
                "摘要": "  Proper functioning of connected and automated vehicles (CAVs) is crucial for\nthe safety and efficiency of future intelligent transport systems. Meanwhile,\ntransitioning to fully autonomous driving requires a long period of mixed\nautonomy traffic, including both CAVs and human-driven vehicles. Thus,\ncollaboration decision-making for CAVs is essential to generate appropriate\ndriving behaviors to enhance the safety and efficiency of mixed autonomy\ntraffic. In recent years, deep reinforcement learning (DRL) has been widely\nused in solving decision-making problems. However, the existing DRL-based\nmethods have been mainly focused on solving the decision-making of a single\nCAV. Using the existing DRL-based methods in mixed autonomy traffic cannot\naccurately represent the mutual effects of vehicles and model dynamic traffic\nenvironments. To address these shortcomings, this article proposes a graph\nreinforcement learning (GRL) approach for multi-agent decision-making of CAVs\nin mixed autonomy traffic. First, a generic and modular GRL framework is\ndesigned. Then, a systematic review of DRL and GRL methods is presented,\nfocusing on the problems addressed in recent research. Moreover, a comparative\nstudy on different GRL methods is further proposed based on the designed\nframework to verify the effectiveness of GRL methods. Results show that the GRL\nmethods can well optimize the performance of multi-agent decision-making for\nCAVs in mixed autonomy traffic compared to the DRL methods. Finally, challenges\nand future research directions are summarized. This study can provide a\nvaluable research reference for solving the multi-agent decision-making\nproblems of CAVs in mixed autonomy traffic and can promote the implementation\nof GRL-based methods into intelligent transportation systems. The source code\nof our work can be found at https://github.com/Jacklinkk/Graph_CAVs.\n",
                "链接": "https://arxiv.org/abs/2211.03005"
            },
            {
                "文章ID": "90666",
                "标题": "Maneuver Decision-Making Through Automatic Curriculum Reinforcement\n  Learning Without Handcrafted Reward functions",
                "作者": " Zhang Hong-Peng",
                "发布日期": "2023-07-13",
                "摘要": "  Maneuver decision-making is the core of unmanned combat aerial vehicle for\nautonomous air combat. To solve this problem, we propose an automatic\ncurriculum reinforcement learning method, which enables agents to learn\neffective decisions in air combat from scratch. The range of initial states are\nused for distinguishing curricula of different difficulty levels, thereby\nmaneuver decision is divided into a series of sub-tasks from easy to difficult,\nand test results are used to change sub-tasks. As sub-tasks change, agents\ngradually learn to complete a series of sub-tasks from easy to difficult,\nenabling them to make effective maneuvering decisions to cope with various\nstates without the need to spend effort designing reward functions. The\nablation studied show that the automatic curriculum learning proposed in this\narticle is an essential component for training through reinforcement learning,\nnamely, agents cannot complete effective decisions without curriculum learning.\nSimulation experiments show that, after training, agents are able to make\neffective decisions given different states, including tracking, attacking and\nescaping, which are both rational and interpretable.\n",
                "链接": "https://arxiv.org/abs/2307.06152"
            },
            {
                "文章ID": "8476",
                "标题": "Diversifying Agent's Behaviors in Interactive Decision Models",
                "作者": " Yinghui Pan,  Hanyi Zhang,  Yifeng Zeng,  Biyang Ma,  Jing Tang,  Zhong Ming",
                "发布日期": "2022-03-08",
                "摘要": "  Modelling other agents' behaviors plays an important role in decision models\nfor interactions among multiple agents. To optimise its own decisions, a\nsubject agent needs to model what other agents act simultaneously in an\nuncertain environment. However, modelling insufficiency occurs when the agents\nare competitive and the subject agent can not get full knowledge about other\nagents. Even when the agents are collaborative, they may not share their true\nbehaviors due to their privacy concerns. In this article, we investigate into\ndiversifying behaviors of other agents in the subject agent's decision model\nprior to their interactions. Starting with prior knowledge about other agents'\nbehaviors, we use a linear reduction technique to extract representative\nbehavioral features from the known behaviors. We subsequently generate their\nnew behaviors by expanding the features and propose two diversity measurements\nto select top-K behaviors. We demonstrate the performance of the new techniques\nin two well-studied problem domains. This research will contribute to\nintelligent systems dealing with unknown unknowns in an open artificial\nintelligence world.\n",
                "链接": "https://arxiv.org/abs/2203.03068"
            },
            {
                "文章ID": "21914",
                "标题": "Multi-Agent Reinforcement Learning is a Sequence Modeling Problem",
                "作者": " Muning Wen,  Jakub Grudzien Kuba,  Runji Lin,  Weinan Zhang,  Ying Wen,  Jun Wang,  Yaodong Yang",
                "发布日期": "2022-10-31",
                "摘要": "  Large sequence model (SM) such as GPT series and BERT has displayed\noutstanding performance and generalization capabilities on vision, language,\nand recently reinforcement learning tasks. A natural follow-up question is how\nto abstract multi-agent decision making into an SM problem and benefit from the\nprosperous development of SMs. In this paper, we introduce a novel architecture\nnamed Multi-Agent Transformer (MAT) that effectively casts cooperative\nmulti-agent reinforcement learning (MARL) into SM problems wherein the task is\nto map agents' observation sequence to agents' optimal action sequence. Our\ngoal is to build the bridge between MARL and SMs so that the modeling power of\nmodern sequence models can be unleashed for MARL. Central to our MAT is an\nencoder-decoder architecture which leverages the multi-agent advantage\ndecomposition theorem to transform the joint policy search problem into a\nsequential decision making process; this renders only linear time complexity\nfor multi-agent problems and, most importantly, endows MAT with monotonic\nperformance improvement guarantee. Unlike prior arts such as Decision\nTransformer fit only pre-collected offline data, MAT is trained by online\ntrials and errors from the environment in an on-policy fashion. To validate\nMAT, we conduct extensive experiments on StarCraftII, Multi-Agent MuJoCo,\nDexterous Hands Manipulation, and Google Research Football benchmarks. Results\ndemonstrate that MAT achieves superior performance and data efficiency compared\nto strong baselines including MAPPO and HAPPO. Furthermore, we demonstrate that\nMAT is an excellent few-short learner on unseen tasks regardless of changes in\nthe number of agents. See our project page at\nhttps://sites.google.com/view/multi-agent-transformer.\n",
                "链接": "https://arxiv.org/abs/2205.14953"
            }
        ]
    },
    {
        "question": {
            "question": "查找大模型推理的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "15923",
                "标题": "Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and\n  Adaptive Inference Approach",
                "作者": " Chen Tang,  Haoyu Zhai,  Kai Ouyang,  Zhi Wang,  Yifei Zhu,  Wenwu Zhu",
                "发布日期": "2022-04-22",
                "摘要": "  Conventional model quantization methods use a fixed quantization scheme to\ndifferent data samples, which ignores the inherent \"recognition difficulty\"\ndifferences between various samples. We propose to feed different data samples\nwith varying quantization schemes to achieve a data-dependent dynamic\ninference, at a fine-grained layer level. However, enabling this adaptive\ninference with changeable layer-wise quantization schemes is challenging\nbecause the combination of bit-widths and layers is growing exponentially,\nmaking it extremely difficult to train a single model in such a vast searching\nspace and use it in practice. To solve this problem, we present the Arbitrary\nBit-width Network (ABN), where the bit-widths of a single deep network can\nchange at runtime for different data samples, with a layer-wise granularity.\nSpecifically, first we build a weight-shared layer-wise quantizable\n\"super-network\" in which each layer can be allocated with multiple bit-widths\nand thus quantized differently on demand. The super-network provides a\nconsiderably large number of combinations of bit-widths and layers, each of\nwhich can be used during inference without retraining or storing myriad models.\nSecond, based on the well-trained super-network, each layer's runtime bit-width\nselection decision is modeled as a Markov Decision Process (MDP) and solved by\nan adaptive inference strategy accordingly. Experiments show that the\nsuper-network can be built without accuracy degradation, and the bit-widths\nallocation of each layer can be adjusted to deal with various inputs on the\nfly. On ImageNet classification, we achieve 1.1% top1 accuracy improvement\nwhile saving 36.2% BitOps.\n",
                "链接": "https://arxiv.org/abs/2204.09992"
            },
            {
                "文章ID": "108687",
                "标题": "QUIK: Towards End-to-End 4-Bit Inference on Generative Large Language\n  Models",
                "作者": " Saleh Ashkboos,  Ilia Markov,  Elias Frantar,  Tingxuan Zhong,  Xincheng Wang,  Jie Ren,  Torsten Hoefler,  Dan Alistarh",
                "发布日期": "2023-11-03",
                "摘要": "  Large Language Models (LLMs) from the GPT family have become extremely\npopular, leading to a race towards reducing their inference costs to allow for\nefficient local computation. Yet, the vast majority of existing work focuses on\nweight-only quantization, which can reduce runtime costs in the memory-bound\none-token-at-a-time generative setting, but does not address them in\ncompute-bound scenarios, such as batched inference or prompt processing. In\nthis paper, we address the general quantization problem, where both weights and\nactivations should be quantized. We show, for the first time, that the majority\nof inference computations for large generative models such as LLaMA, OPT, and\nFalcon can be performed with both weights and activations being cast to 4 bits,\nin a way that leads to practical speedups, while at the same time maintaining\ngood accuracy. We achieve this via a hybrid quantization strategy called QUIK,\nwhich compresses most of the weights and activations to 4-bit, while keeping\nsome outlier weights and activations in higher-precision. The key feature of\nour scheme is that it is designed with computational efficiency in mind: we\nprovide GPU kernels matching the QUIK format with highly-efficient layer-wise\nruntimes, which lead to practical end-to-end throughput improvements of up to\n3.4x relative to FP16 execution. We provide detailed studies for models from\nthe OPT, LLaMA-2 and Falcon families, as well as a first instance of accurate\ninference using quantization plus 2:4 sparsity. Code is available at:\nhttps://github.com/IST-DASLab/QUIK.\n",
                "链接": "https://arxiv.org/abs/2310.09259"
            },
            {
                "文章ID": "114875",
                "标题": "Automated Heterogeneous Low-Bit Quantization of Multi-Model Deep\n  Learning Inference Pipeline",
                "作者": " Jayeeta Mondal,  Swarnava Dey,  Arijit Mukherjee",
                "发布日期": "2023-11-13",
                "摘要": "  Multiple Deep Neural Networks (DNNs) integrated into single Deep Learning\n(DL) inference pipelines e.g. Multi-Task Learning (MTL) or Ensemble Learning\n(EL), etc., albeit very accurate, pose challenges for edge deployment. In these\nsystems, models vary in their quantization tolerance and resource demands,\nrequiring meticulous tuning for accuracy-latency balance. This paper introduces\nan automated heterogeneous quantization approach for DL inference pipelines\nwith multiple DNNs.\n",
                "链接": "https://arxiv.org/abs/2311.05870"
            },
            {
                "文章ID": "24870",
                "标题": "Accelerating Inference and Language Model Fusion of Recurrent Neural\n  Network Transducers via End-to-End 4-bit Quantization",
                "作者": " Andrea Fasoli,  Chia-Yu Chen,  Mauricio Serrano,  Swagath Venkataramani,  George Saon,  Xiaodong Cui,  Brian Kingsbury,  Kailash Gopalakrishnan",
                "发布日期": "2022-06-17",
                "摘要": "  We report on aggressive quantization strategies that greatly accelerate\ninference of Recurrent Neural Network Transducers (RNN-T). We use a 4 bit\ninteger representation for both weights and activations and apply Quantization\nAware Training (QAT) to retrain the full model (acoustic encoder and language\nmodel) and achieve near-iso-accuracy. We show that customized quantization\nschemes that are tailored to the local properties of the network are essential\nto achieve good performance while limiting the computational overhead of QAT.\n  Density ratio Language Model fusion has shown remarkable accuracy gains on\nRNN-T workloads but it severely increases the computational cost of inference.\nWe show that our quantization strategies enable using large beam widths for\nhypothesis search while achieving streaming-compatible runtimes and a full\nmodel compression ratio of 7.6$\\times$ compared to the full precision model.\n  Via hardware simulations, we estimate a 3.4$\\times$ acceleration from FP16 to\nINT4 for the end-to-end quantized RNN-T inclusive of LM fusion, resulting in a\nReal Time Factor (RTF) of 0.06. On the NIST Hub5 2000, Hub5 2001, and RT-03\ntest sets, we retain most of the gains associated with LM fusion, improving the\naverage WER by $>$1.5%.\n",
                "链接": "https://arxiv.org/abs/2206.07882"
            },
            {
                "文章ID": "105189",
                "标题": "Training and inference of large language models using 8-bit floating\n  point",
                "作者": " Sergio P. Perez,  Yan Zhang,  James Briggs,  Charlie Blake,  Josh Levy-Kramer,  Paul Balanca,  Carlo Luschi,  Stephen Barlow,  Andrew William Fitzgibbon",
                "发布日期": "2023-10-02",
                "摘要": "  FP8 formats are gaining popularity to boost the computational efficiency for\ntraining and inference of large deep learning models. Their main challenge is\nthat a careful choice of scaling is needed to prevent degradation due to the\nreduced dynamic range compared to higher-precision formats. Although there\nexists ample literature about selecting such scalings for INT formats, this\ncritical aspect has yet to be addressed for FP8. This paper presents a\nmethodology to select the scalings for FP8 linear layers, based on dynamically\nupdating per-tensor scales for the weights, gradients and activations. We apply\nthis methodology to train and validate large language models of the type of GPT\nand Llama 2 using FP8, for model sizes ranging from 111M to 70B. To facilitate\nthe understanding of the FP8 dynamics, our results are accompanied by plots of\nthe per-tensor scale distribution for weights, activations and gradients during\nboth training and inference.\n",
                "链接": "https://arxiv.org/abs/2309.17224"
            },
            {
                "文章ID": "29166",
                "标题": "Large-scale Knowledge Distillation with Elastic Heterogeneous Computing\n  Resources",
                "作者": " Ji Liu,  Daxiang Dong,  Xi Wang,  An Qin,  Xingjian Li,  Patrick Valduriez,  Dejing Dou,  Dianhai Yu",
                "发布日期": "2022-07-15",
                "摘要": "  Although more layers and more parameters generally improve the accuracy of\nthe models, such big models generally have high computational complexity and\nrequire big memory, which exceed the capacity of small devices for inference\nand incurs long training time. In addition, it is difficult to afford long\ntraining time and inference time of big models even in high performance\nservers, as well. As an efficient approach to compress a large deep model (a\nteacher model) to a compact model (a student model), knowledge distillation\nemerges as a promising approach to deal with the big models. Existing knowledge\ndistillation methods cannot exploit the elastic available computing resources\nand correspond to low efficiency. In this paper, we propose an Elastic Deep\nLearning framework for knowledge Distillation, i.e., EDL-Dist. The advantages\nof EDL-Dist are three-fold. First, the inference and the training process is\nseparated. Second, elastic available computing resources can be utilized to\nimprove the efficiency. Third, fault-tolerance of the training and inference\nprocesses is supported. We take extensive experimentation to show that the\nthroughput of EDL-Dist is up to 3.125 times faster than the baseline method\n(online knowledge distillation) while the accuracy is similar or higher.\n",
                "链接": "https://arxiv.org/abs/2207.06667"
            },
            {
                "文章ID": "97684",
                "标题": "Analyzing Quantization in TVM",
                "作者": " Mingfei Guo",
                "发布日期": "2023-08-23",
                "摘要": "  There has been many papers in academic literature on quantizing weight\ntensors in deep learning models to reduce inference latency and memory\nfootprint. TVM also has the ability to quantize weights and support low-bit\ncomputations. Although quantization is typically expected to improve inference\ntime, in TVM, the performance of 8-bit quantization does not meet the\nexpectations. Typically, when applying 8-bit quantization to a deep learning\nmodel, it is usually expected to achieve around 50% of the full-precision\ninference time. However, in this particular case, not only does the quantized\nversion fail to achieve the desired performance boost, but it actually performs\nworse, resulting in an inference time that is about 2 times as slow as the\nnon-quantized version. In this project, we thoroughly investigate the reasons\nbehind the underperformance and assess the compatibility and optimization\nopportunities of 8-bit quantization in TVM. We discuss the optimization of two\ndifferent types of tasks: computation-bound and memory-bound, and provide a\ndetailed comparison of various optimization techniques in TVM. Through the\nidentification of performance issues, we have successfully improved\nquantization by addressing a bug in graph building. Furthermore, we analyze\nmultiple optimization strategies to achieve the optimal quantization result.\nThe best experiment achieves 163.88% improvement compared with the TVM compiled\nbaseline in inference time for the compute-bound task and 194.98% for the\nmemory-bound task.\n",
                "链接": "https://arxiv.org/abs/2308.10905"
            },
            {
                "文章ID": "124027",
                "标题": "Big Tech influence over AI research revisited: memetic analysis of\n  attribution of ideas to affiliation",
                "作者": " Stanisław Giziński,  Paulina Kaczyńska,  Hubert Ruczyński,  Emilia Wiśnios,  Bartosz Pieliński,  Przemysław Biecek,  Julian Sienkiewicz",
                "发布日期": "2023-12-21",
                "摘要": "  There exists a growing discourse around the domination of Big Tech on the\nlandscape of artificial intelligence (AI) research, yet our comprehension of\nthis phenomenon remains cursory. This paper aims to broaden and deepen our\nunderstanding of Big Tech's reach and power within AI research. It highlights\nthe dominance not merely in terms of sheer publication volume but rather in the\npropagation of new ideas or \\textit{memes}. Current studies often oversimplify\nthe concept of influence to the share of affiliations in academic papers,\ntypically sourced from limited databases such as arXiv or specific academic\nconferences.\n  The main goal of this paper is to unravel the specific nuances of such\ninfluence, determining which AI ideas are predominantly driven by Big Tech\nentities. By employing network and memetic analysis on AI-oriented paper\nabstracts and their citation network, we are able to grasp a deeper insight\ninto this phenomenon. By utilizing two databases: OpenAlex and S2ORC, we are\nable to perform such analysis on a much bigger scale than previous attempts.\n  Our findings suggest, that while Big Tech-affiliated papers are\ndisproportionately more cited in some areas, the most cited papers are those\naffiliated with both Big Tech and Academia. Focusing on the most contagious\nmemes, their attribution to specific affiliation groups (Big Tech, Academia,\nmixed affiliation) seems to be equally distributed between those three groups.\nThis suggests that the notion of Big Tech domination over AI research is\noversimplified in the discourse.\n  Ultimately, this more nuanced understanding of Big Tech's and Academia's\ninfluence could inform a more symbiotic alliance between these stakeholders\nwhich would better serve the dual goals of societal welfare and the scientific\nintegrity of AI research.\n",
                "链接": "https://arxiv.org/abs/2312.12881"
            },
            {
                "文章ID": "15243",
                "标题": "FCL-GAN: A Lightweight and Real-Time Baseline for Unsupervised Blind\n  Image Deblurring",
                "作者": " Suiyi Zhao,  Zhao Zhang,  Richang Hong,  Mingliang Xu,  Yi Yang,  Meng Wang",
                "发布日期": "2022-07-26",
                "摘要": "  Blind image deblurring (BID) remains a challenging and significant task.\nBenefiting from the strong fitting ability of deep learning, paired data-driven\nsupervised BID method has obtained great progress. However, paired data are\nusually synthesized by hand, and the realistic blurs are more complex than\nsynthetic ones, which makes the supervised methods inept at modeling realistic\nblurs and hinders their real-world applications. As such, unsupervised deep BID\nmethod without paired data offers certain advantages, but current methods still\nsuffer from some drawbacks, e.g., bulky model size, long inference time, and\nstrict image resolution and domain requirements. In this paper, we propose a\nlightweight and real-time unsupervised BID baseline, termed Frequency-domain\nContrastive Loss Constrained Lightweight CycleGAN (shortly, FCL-GAN), with\nattractive properties, i.e., no image domain limitation, no image resolution\nlimitation, 25x lighter than SOTA, and 5x faster than SOTA. To guarantee the\nlightweight property and performance superiority, two new collaboration units\ncalled lightweight domain conversion unit(LDCU) and parameter-free\nfrequency-domain contrastive unit(PFCU) are designed. LDCU mainly implements\ninter-domain conversion in lightweight manner. PFCU further explores the\nsimilarity measure, external difference and internal connection between the\nblurred domain and sharp domain images in frequency domain, without involving\nextra parameters. Extensive experiments on several image datasets demonstrate\nthe effectiveness of our FCL-GAN in terms of performance, model size and\nreference time.\n",
                "链接": "https://arxiv.org/abs/2204.07820"
            },
            {
                "文章ID": "20963",
                "标题": "lpSpikeCon: Enabling Low-Precision Spiking Neural Network Processing for\n  Efficient Unsupervised Continual Learning on Autonomous Agents",
                "作者": " Rachmad Vidya Wicaksana Putra,  Muhammad Shafique",
                "发布日期": "2023-03-06",
                "摘要": "  Recent advances have shown that SNN-based systems can efficiently perform\nunsupervised continual learning due to their bio-plausible learning rule, e.g.,\nSpike-Timing-Dependent Plasticity (STDP). Such learning capabilities are\nespecially beneficial for use cases like autonomous agents (e.g., robots and\nUAVs) that need to continuously adapt to dynamically changing\nscenarios/environments, where new data gathered directly from the environment\nmay have novel features that should be learned online. Current state-of-the-art\nworks employ high-precision weights (i.e., 32 bit) for both training and\ninference phases, which pose high memory and energy costs thereby hindering\nefficient embedded implementations of such systems for battery-driven mobile\nautonomous systems. On the other hand, precision reduction may jeopardize the\nquality of unsupervised continual learning due to information loss. Towards\nthis, we propose lpSpikeCon, a novel methodology to enable low-precision SNN\nprocessing for efficient unsupervised continual learning on\nresource-constrained autonomous agents/systems. Our lpSpikeCon methodology\nemploys the following key steps: (1) analyzing the impacts of training the SNN\nmodel under unsupervised continual learning settings with reduced weight\nprecision on the inference accuracy; (2) leveraging this study to identify SNN\nparameters that have a significant impact on the inference accuracy; and (3)\ndeveloping an algorithm for searching the respective SNN parameter values that\nimprove the quality of unsupervised continual learning. The experimental\nresults show that our lpSpikeCon can reduce weight memory of the SNN model by\n8x (i.e., by judiciously employing 4-bit weights) for performing online\ntraining with unsupervised continual learning and achieve no accuracy loss in\nthe inference phase, as compared to the baseline model with 32-bit weights\nacross different network sizes.\n",
                "链接": "https://arxiv.org/abs/2205.12295"
            }
        ]
    },
    {
        "question": {
            "question": "2023年以后关于NLP领域的持续性学习论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "74774",
                "标题": "SCOPE: Structural Continuity Preservation for Medical Image Segmentation",
                "作者": " Yousef Yeganeh,  Azade Farshad,  Goktug Guevercin,  Amr Abu-zer,  Rui Xiao,  Yongjian Tang,  Ehsan Adeli,  Nassir Navab",
                "发布日期": "2023-05-01",
                "摘要": "  Although the preservation of shape continuity and physiological anatomy is a\nnatural assumption in the segmentation of medical images, it is often neglected\nby deep learning methods that mostly aim for the statistical modeling of input\ndata as pixels rather than interconnected structures. In biological structures,\nhowever, organs are not separate entities; for example, in reality, a severed\nvessel is an indication of an underlying problem, but traditional segmentation\nmodels are not designed to strictly enforce the continuity of anatomy,\npotentially leading to inaccurate medical diagnoses. To address this issue, we\npropose a graph-based approach that enforces the continuity and connectivity of\nanatomical topology in medical images. Our method encodes the continuity of\nshapes as a graph constraint, ensuring that the network's predictions maintain\nthis continuity. We evaluate our method on two public benchmarks on retinal\nvessel segmentation, showing significant improvements in connectivity metrics\ncompared to traditional methods while getting better or on-par performance on\nsegmentation metrics.\n",
                "链接": "https://arxiv.org/abs/2304.14572"
            },
            {
                "文章ID": "111934",
                "标题": "Lipschitz and H\\\"older Continuity in Reproducing Kernel Hilbert Spaces",
                "作者": " Christian Fiedler",
                "发布日期": "2023-10-30",
                "摘要": "  Reproducing kernel Hilbert spaces (RKHSs) are very important function spaces,\nplaying an important role in machine learning, statistics, numerical analysis\nand pure mathematics. Since Lipschitz and H\\\"older continuity are important\nregularity properties, with many applications in interpolation, approximation\nand optimization problems, in this work we investigate these continuity notion\nin RKHSs. We provide several sufficient conditions as well as an in depth\ninvestigation of reproducing kernels inducing prescribed Lipschitz or H\\\"older\ncontinuity. Apart from new results, we also collect related known results from\nthe literature, making the present work also a convenient reference on this\ntopic.\n",
                "链接": "https://arxiv.org/abs/2310.18078"
            },
            {
                "文章ID": "49009",
                "标题": "Boosting Object Representation Learning via Motion and Object Continuity",
                "作者": " Quentin Delfosse,  Wolfgang Stammer,  Thomas Rothenbacher,  Dwarak Vittal,  Kristian Kersting",
                "发布日期": "2023-09-21",
                "摘要": "  Recent unsupervised multi-object detection models have shown impressive\nperformance improvements, largely attributed to novel architectural inductive\nbiases. Unfortunately, they may produce suboptimal object encodings for\ndownstream tasks. To overcome this, we propose to exploit object motion and\ncontinuity, i.e., objects do not pop in and out of existence. This is\naccomplished through two mechanisms: (i) providing priors on the location of\nobjects through integration of optical flow, and (ii) a contrastive object\ncontinuity loss across consecutive image frames. Rather than developing an\nexplicit deep architecture, the resulting Motion and Object Continuity (MOC)\nscheme can be instantiated using any baseline object detection model. Our\nresults show large improvements in the performances of a SOTA model in terms of\nobject discovery, convergence speed and overall latent object representations,\nparticularly for playing Atari games. Overall, we show clear benefits of\nintegrating motion and object continuity for downstream tasks, moving beyond\nobject representation learning based only on reconstruction.\n",
                "链接": "https://arxiv.org/abs/2211.09771"
            },
            {
                "文章ID": "19824",
                "标题": "Neural ODE Control for Trajectory Approximation of Continuity Equation",
                "作者": " Karthik Elamvazhuthi,  Bahman Gharesifard,  Andrea Bertozzi,  Stanley Osher",
                "发布日期": "2022-05-20",
                "摘要": "  We consider the controllability problem for the continuity equation,\ncorresponding to neural ordinary differential equations (ODEs), which describes\nhow a probability measure is pushedforward by the flow. We show that the\ncontrolled continuity equation has very strong controllability properties.\nParticularly, a given solution of the continuity equation corresponding to a\nbounded Lipschitz vector field defines a trajectory on the set of probability\nmeasures. For this trajectory, we show that there exist piecewise constant\ntraining weights for a neural ODE such that the solution of the continuity\nequation corresponding to the neural ODE is arbitrarily close to it. As a\ncorollary to this result, we establish that the continuity equation of the\nneural ODE is approximately controllable on the set of compactly supported\nprobability measures that are absolutely continuous with respect to the\nLebesgue measure.\n",
                "链接": "https://arxiv.org/abs/2205.09241"
            },
            {
                "文章ID": "53791",
                "标题": "Extending Universal Approximation Guarantees: A Theoretical\n  Justification for the Continuity of Real-World Learning Tasks",
                "作者": " Naveen Durvasula",
                "发布日期": "2022-12-16",
                "摘要": "  Universal Approximation Theorems establish the density of various classes of\nneural network function approximators in $C(K, \\mathbb{R}^m)$, where $K \\subset\n\\mathbb{R}^n$ is compact. In this paper, we aim to extend these guarantees by\nestablishing conditions on learning tasks that guarantee their continuity. We\nconsider learning tasks given by conditional expectations $x \\mapsto\n\\mathrm{E}\\left[Y \\mid X = x\\right]$, where the learning target $Y = f \\circ L$\nis a potentially pathological transformation of some underlying data-generating\nprocess $L$. Under a factorization $L = T \\circ W$ for the data-generating\nprocess where $T$ is thought of as a deterministic map acting on some random\ninput $W$, we establish conditions (that might be easily verified using\nknowledge of $T$ alone) that guarantee the continuity of practically\n\\textit{any} derived learning task $x \\mapsto \\mathrm{E}\\left[f \\circ L \\mid X\n= x\\right]$. We motivate the realism of our conditions using the example of\nrandomized stable matching, thus providing a theoretical justification for the\ncontinuity of real-world learning tasks.\n",
                "链接": "https://arxiv.org/abs/2212.07934"
            },
            {
                "文章ID": "29756",
                "标题": "Leveraging Action Affinity and Continuity for Semi-supervised Temporal\n  Action Segmentation",
                "作者": " Guodong Ding,  Angela Yao",
                "发布日期": "2022-07-22",
                "摘要": "  We present a semi-supervised learning approach to the temporal action\nsegmentation task. The goal of the task is to temporally detect and segment\nactions in long, untrimmed procedural videos, where only a small set of videos\nare densely labelled, and a large collection of videos are unlabelled. To this\nend, we propose two novel loss functions for the unlabelled data: an action\naffinity loss and an action continuity loss. The action affinity loss guides\nthe unlabelled samples learning by imposing the action priors induced from the\nlabelled set. Action continuity loss enforces the temporal continuity of\nactions, which also provides frame-wise classification supervision. In\naddition, we propose an Adaptive Boundary Smoothing (ABS) approach to build\ncoarser action boundaries for more robust and reliable learning. The proposed\nloss functions and ABS were evaluated on three benchmarks. Results show that\nthey significantly improved action segmentation performance with a low amount\n(5% and 10%) of labelled data and achieved comparable results to full\nsupervision with 50% labelled data. Furthermore, ABS succeeded in boosting\nperformance when integrated into fully-supervised learning.\n",
                "链接": "https://arxiv.org/abs/2207.08653"
            },
            {
                "文章ID": "40514",
                "标题": "Neural Conservation Laws: A Divergence-Free Perspective",
                "作者": " Jack Richter-Powell,  Yaron Lipman,  Ricky T. Q. Chen",
                "发布日期": "2022-12-13",
                "摘要": "  We investigate the parameterization of deep neural networks that by design\nsatisfy the continuity equation, a fundamental conservation law. This is\nenabled by the observation that any solution of the continuity equation can be\nrepresented as a divergence-free vector field. We hence propose building\ndivergence-free neural networks through the concept of differential forms, and\nwith the aid of automatic differentiation, realize two practical constructions.\nAs a result, we can parameterize pairs of densities and vector fields that\nalways exactly satisfy the continuity equation, foregoing the need for extra\npenalty methods or expensive numerical simulation. Furthermore, we prove these\nmodels are universal and so can be used to represent any divergence-free vector\nfield. Finally, we experimentally validate our approaches by computing neural\nnetwork-based solutions to fluid equations, solving for the Hodge\ndecomposition, and learning dynamical optimal transport maps.\n",
                "链接": "https://arxiv.org/abs/2210.01741"
            },
            {
                "文章ID": "37650",
                "标题": "Quantifying Discourse Support for Omitted Pronouns",
                "作者": " Shulin Zhang,  Jixing Li,  John Hale",
                "发布日期": "2022-09-19",
                "摘要": "  Pro-drop is commonly seen in many languages, but its discourse motivations\nhave not been well characterized. Inspired by the topic chain theory in\nChinese, this study shows how character-verb usage continuity distinguishes\ndropped pronouns from overt references to story characters. We model the choice\nto drop vs. not drop as a function of character-verb continuity. The results\nshow that omitted subjects have higher character history-current verb\ncontinuity salience than non-omitted subjects. This is consistent with the idea\nthat discourse coherence with a particular topic, such as a story character,\nindeed facilitates the omission of pronouns in languages and contexts where\nthey are optional.\n",
                "链接": "https://arxiv.org/abs/2209.07961"
            },
            {
                "文章ID": "65442",
                "标题": "Continuity-Aware Latent Interframe Information Mining for Reliable UAV\n  Tracking",
                "作者": " Changhong Fu,  Mutian Cai,  Sihang Li,  Kunhan Lu,  Haobo Zuo,  Chongjun Liu",
                "发布日期": "2023-03-09",
                "摘要": "  Unmanned aerial vehicle (UAV) tracking is crucial for autonomous navigation\nand has broad applications in robotic automation fields. However, reliable UAV\ntracking remains a challenging task due to various difficulties like frequent\nocclusion and aspect ratio change. Additionally, most of the existing work\nmainly focuses on explicit information to improve tracking performance,\nignoring potential interframe connections. To address the above issues, this\nwork proposes a novel framework with continuity-aware latent interframe\ninformation mining for reliable UAV tracking, i.e., ClimRT. Specifically, a new\nefficient continuity-aware latent interframe information mining network\n(ClimNet) is proposed for UAV tracking, which can generate highly-effective\nlatent frame between two adjacent frames. Besides, a novel location-continuity\nTransformer (LCT) is designed to fully explore continuity-aware\nspatial-temporal information, thereby markedly enhancing UAV tracking.\nExtensive qualitative and quantitative experiments on three authoritative\naerial benchmarks strongly validate the robustness and reliability of ClimRT in\nUAV tracking performance. Furthermore, real-world tests on the aerial platform\nvalidate its practicability and effectiveness. The code and demo materials are\nreleased at https://github.com/vision4robotics/ClimRT.\n",
                "链接": "https://arxiv.org/abs/2303.04525"
            },
            {
                "文章ID": "11751",
                "标题": "Continuous Dynamic-NeRF: Spline-NeRF",
                "作者": " Julian Knodt",
                "发布日期": "2022-03-28",
                "摘要": "  The problem of reconstructing continuous functions over time is important for\nproblems such as reconstructing moving scenes, and interpolating between time\nsteps. Previous approaches that use deep-learning rely on regularization to\nensure that reconstructions are approximately continuous, which works well on\nshort sequences. As sequence length grows, though, it becomes more difficult to\nregularize, and it becomes less feasible to learn only through regularization.\nWe propose a new architecture for function reconstruction based on classical\nBezier splines, which ensures $C^0$ and $C^1$-continuity, where $C^0$\ncontinuity is that $\\forall c:\\lim\\limits_{x\\to c} f(x)\n  = f(c)$, or more intuitively that there are no breaks at any point in the\nfunction. In order to demonstrate our architecture, we reconstruct dynamic\nscenes using Neural Radiance Fields, but hope it is clear that our approach is\ngeneral and can be applied to a variety of problems. We recover a Bezier spline\n$B(\\beta, t\\in[0,1])$, parametrized by the control points $\\beta$. Using Bezier\nsplines ensures reconstructions have $C^0$ and $C^1$ continuity, allowing for\nguaranteed interpolation over time. We reconstruct $\\beta$ with a multi-layer\nperceptron (MLP), blending machine learning with classical animation\ntechniques. All code is available at https://github.com/JulianKnodt/nerf_atlas,\nand datasets are from prior work.\n",
                "链接": "https://arxiv.org/abs/2203.13800"
            }
        ]
    },
    {
        "question": {
            "question": "查找近六个月工具学习评测数据集的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "118750",
                "标题": "CLiC: Concept Learning in Context",
                "作者": " Mehdi Safaee,  Aryan Mikaeili,  Or Patashnik,  Daniel Cohen-Or,  Ali Mahdavi-Amiri",
                "发布日期": "2023-11-30",
                "摘要": "  This paper addresses the challenge of learning a local visual pattern of an\nobject from one image, and generating images depicting objects with that\npattern. Learning a localized concept and placing it on an object in a target\nimage is a nontrivial task, as the objects may have different orientations and\nshapes. Our approach builds upon recent advancements in visual concept\nlearning. It involves acquiring a visual concept (e.g., an ornament) from a\nsource image and subsequently applying it to an object (e.g., a chair) in a\ntarget image. Our key idea is to perform in-context concept learning, acquiring\nthe local visual concept within the broader context of the objects they belong\nto. To localize the concept learning, we employ soft masks that contain both\nthe concept within the mask and the surrounding image area. We demonstrate our\napproach through object generation within an image, showcasing plausible\nembedding of in-context learned concepts. We also introduce methods for\ndirecting acquired concepts to specific locations within target images,\nemploying cross-attention mechanisms, and establishing correspondences between\nsource and target objects. The effectiveness of our method is demonstrated\nthrough quantitative and qualitative experiments, along with comparisons\nagainst baseline techniques.\n",
                "链接": "https://arxiv.org/abs/2311.17083"
            },
            {
                "文章ID": "90392",
                "标题": "Temporal Graphs Anomaly Emergence Detection: Benchmarking For Social\n  Media Interactions",
                "作者": " Teddy Lazebnik,  Or Iny",
                "发布日期": "2023-07-12",
                "摘要": "  Temporal graphs have become an essential tool for analyzing complex dynamic\nsystems with multiple agents. Detecting anomalies in temporal graphs is crucial\nfor various applications, including identifying emerging trends, monitoring\nnetwork security, understanding social dynamics, tracking disease outbreaks,\nand understanding financial dynamics. In this paper, we present a comprehensive\nbenchmarking study that compares 12 data-driven methods for anomaly detection\nin temporal graphs. We conduct experiments on two temporal graphs extracted\nfrom Twitter and Facebook, aiming to identify anomalies in group interactions.\nSurprisingly, our study reveals an unclear pattern regarding the best method\nfor such tasks, highlighting the complexity and challenges involved in anomaly\nemergence detection in large and dynamic systems. The results underscore the\nneed for further research and innovative approaches to effectively detect\nemerging anomalies in dynamic systems represented as temporal graphs.\n",
                "链接": "https://arxiv.org/abs/2307.05268"
            },
            {
                "文章ID": "56860",
                "标题": "Graph Laplacian for Semi-Supervised Learning",
                "作者": " Or Streicher,  Guy Gilboa",
                "发布日期": "2023-04-20",
                "摘要": "  Semi-supervised learning is highly useful in common scenarios where labeled\ndata is scarce but unlabeled data is abundant. The graph (or nonlocal)\nLaplacian is a fundamental smoothing operator for solving various learning\ntasks. For unsupervised clustering, a spectral embedding is often used, based\non graph-Laplacian eigenvectors. For semi-supervised problems, the common\napproach is to solve a constrained optimization problem, regularized by a\nDirichlet energy, based on the graph-Laplacian. However, as supervision\ndecreases, Dirichlet optimization becomes suboptimal. We therefore would like\nto obtain a smooth transition between unsupervised clustering and\nlow-supervised graph-based classification. In this paper, we propose a new type\nof graph-Laplacian which is adapted for Semi-Supervised Learning (SSL)\nproblems. It is based on both density and contrastive measures and allows the\nencoding of the labeled data directly in the operator. Thus, we can perform\nsuccessfully semi-supervised learning using spectral clustering. The benefits\nof our approach are illustrated for several SSL problems.\n",
                "链接": "https://arxiv.org/abs/2301.04956"
            },
            {
                "文章ID": "3017",
                "标题": "Transfer Learning In Differential Privacy's Hybrid-Model",
                "作者": " Refael Kohen,  Or Sheffet",
                "发布日期": "2022-06-17",
                "摘要": "  The hybrid-model (Avent et al 2017) in Differential Privacy is a an\naugmentation of the local-model where in addition to N local-agents we are\nassisted by one special agent who is in fact a curator holding the sensitive\ndetails of n additional individuals. Here we study the problem of machine\nlearning in the hybrid-model where the n individuals in the curators dataset\nare drawn from a different distribution than the one of the general population\n(the local-agents). We give a general scheme -- Subsample-Test-Reweigh -- for\nthis transfer learning problem, which reduces any curator-model DP-learner to a\nhybrid-model learner in this setting using iterative subsampling and reweighing\nof the n examples held by the curator based on a smooth variation of the\nMultiplicative-Weights algorithm (introduced by Bun et al, 2020). Our scheme\nhas a sample complexity which relies on the chi-squared divergence between the\ntwo distributions. We give worst-case analysis bounds on the sample complexity\nrequired for our private reduction. Aiming to reduce said sample complexity, we\ngive two specific instances our sample complexity can be drastically reduced\n(one instance is analyzed mathematically, while the other - empirically) and\npose several directions for follow-up work.\n",
                "链接": "https://arxiv.org/abs/2201.12018"
            },
            {
                "文章ID": "30828",
                "标题": "A Hybrid Model and Learning-Based Adaptive Navigation Filter",
                "作者": " Barak Or,  Itzik Klein",
                "发布日期": "2022-09-05",
                "摘要": "  The fusion between an inertial navigation system and global navigation\nsatellite systems is regularly used in many platforms such as drones, land\nvehicles, and marine vessels. The fusion is commonly carried out in a\nmodel-based extended Kalman filter framework. One of the critical parameters of\nthe filter is the process noise covariance. It is responsible for the real-time\nsolution accuracy, as it considers both vehicle dynamics uncertainty and the\ninertial sensors quality. In most situations, the process noise is covariance\nassumed to be constant. Yet, due to vehicle dynamics and sensor measurement\nvariations throughout the trajectory, the process noise covariance is subject\nto change. To cope with such situations, several adaptive model-based Kalman\nfilters were suggested in the literature. In this paper, we propose a hybrid\nmodel and learning-based adaptive navigation filter. We rely on the model-based\nKalman filter and design a deep neural network model to tune the momentary\nsystem noise covariance matrix, based only on the inertial sensor readings.\nOnce the process noise covariance is learned, it is plugged into the\nwell-established, model-based Kalman filter. After deriving the proposed hybrid\nframework, field experiment results using a quadrotor are presented and a\ncomparison to model-based adaptive approaches is given. We show that the\nproposed method obtained an improvement of 25% in the position error.\nFurthermore, the proposed hybrid learning method can be used in any navigation\nfilter and also in any relevant estimation problem.\n",
                "链接": "https://arxiv.org/abs/2207.12082"
            },
            {
                "文章ID": "19460",
                "标题": "Learning Car Speed Using Inertial Sensors for Dead Reckoning Navigation",
                "作者": " Maxim Freydin,  Barak Or",
                "发布日期": "2022-08-29",
                "摘要": "  A deep neural network (DNN) is trained to estimate the speed of a car driving\nin an urban area using as input a stream of measurements from a low-cost\nsix-axis inertial measurement unit (IMU). Three hours of data was collected by\ndriving through the city of Ashdod, Israel in a car equipped with a global\nnavigation satellite system (GNSS) real time kinematic (RTK) positioning device\nand a synchronized IMU. Ground truth labels for the car speed were calculated\nusing the position measurements obtained at the high rate of 50 Hz. A DNN\narchitecture with long short-term memory layers is proposed to enable\nhigh-frequency speed estimation that accounts for previous inputs history and\nthe nonlinear relation between speed, acceleration and angular velocity. A\nsimplified aided dead reckoning localization scheme is formulated to assess the\ntrained model which provides the speed pseudo-measurement. The trained model is\nshown to substantially improve the position accuracy during a 4 minutes drive\nwithout the use of GNSS position updates.\n",
                "链接": "https://arxiv.org/abs/2205.07883"
            },
            {
                "文章ID": "121320",
                "标题": "Textual Toxicity in Social Media: Understanding the Bangla Toxic\n  Language Expressed in Facebook Comment",
                "作者": " Mohammad Mamun Or Rashid",
                "发布日期": "2023-12-12",
                "摘要": "  Social Media is a repository of digital literature including user-generated\ncontent. The users of social media are expressing their opinion with diverse\nmediums such as text, emojis, memes, and also through other visual and textual\nmediums. A major portion of these media elements could be treated as harmful to\nothers and they are known by many words including Cyberbullying and Toxic\nLanguage . The goal of this research paper is to analyze a curated and\nvalue-added dataset of toxic language titled ToxLex_bn . It is an exhaustive\nwordlist that can be used as classifier material to detect toxicity in social\nmedia. The toxic language/script used by the Bengali community as\ncyberbullying, hate speech and moral policing became major trends in social\nmedia culture in Bangladesh and West Bengal. The toxicity became so high that\nthe victims has to post as a counter or release explanation video for the\nhaters. Most cases are pointed to women celebrity and their relation, dress,\nlifestyle are became trolled and toxicity flooded in comments boxes. Not only\ncelebrity bashing but also hates occurred between Hindu Muslims,\nIndia-Bangladesh, Two opponents of 1971 and these are very common for virtual\nconflict in the comment thread. Even many times facebook comment causes sue and\nlegal matters in Bangladesh and thus it requires more study. In this study, a\nBangla toxic language dataset has been analyzed which was inputted by the user\nin Bengali script & language. For this, about 1968 unique bigrams or phrases as\nwordlists have been analyzed which are derived from 2207590 comments. It is\nassumed that this analysis will reinforce the detection of Bangla's toxic\nlanguage used in social media and thus cure this virtual disease.\n",
                "链接": "https://arxiv.org/abs/2312.05467"
            },
            {
                "文章ID": "119010",
                "标题": "Pose Anything: A Graph-Based Approach for Category-Agnostic Pose\n  Estimation",
                "作者": " Or Hirschorn,  Shai Avidan",
                "发布日期": "2023-11-30",
                "摘要": "  Traditional 2D pose estimation models are limited by their category-specific\ndesign, making them suitable only for predefined object categories. This\nrestriction becomes particularly challenging when dealing with novel objects\ndue to the lack of relevant training data.\n  To address this limitation, category-agnostic pose estimation (CAPE) was\nintroduced. CAPE aims to enable keypoint localization for arbitrary object\ncategories using a single model, requiring minimal support images with\nannotated keypoints. This approach not only enables object pose generation\nbased on arbitrary keypoint definitions but also significantly reduces the\nassociated costs, paving the way for versatile and adaptable pose estimation\napplications.\n  We present a novel approach to CAPE that leverages the inherent geometrical\nrelations between keypoints through a newly designed Graph Transformer Decoder.\nBy capturing and incorporating this crucial structural information, our method\nenhances the accuracy of keypoint localization, marking a significant departure\nfrom conventional CAPE techniques that treat keypoints as isolated entities.\n  We validate our approach on the MP-100 benchmark, a comprehensive dataset\ncomprising over 20,000 images spanning more than 100 categories. Our method\noutperforms the prior state-of-the-art by substantial margins, achieving\nremarkable improvements of 2.16% and 1.82% under 1-shot and 5-shot settings,\nrespectively. Furthermore, our method's end-to-end training demonstrates both\nscalability and efficiency compared to previous CAPE approaches.\n",
                "链接": "https://arxiv.org/abs/2311.17891"
            },
            {
                "文章ID": "49384",
                "标题": "Normalizing Flows for Human Pose Anomaly Detection",
                "作者": " Or Hirschorn,  Shai Avidan",
                "发布日期": "2023-08-17",
                "摘要": "  Video anomaly detection is an ill-posed problem because it relies on many\nparameters such as appearance, pose, camera angle, background, and more. We\ndistill the problem to anomaly detection of human pose, thus decreasing the\nrisk of nuisance parameters such as appearance affecting the result. Focusing\non pose alone also has the side benefit of reducing bias against distinct\nminority groups. Our model works directly on human pose graph sequences and is\nexceptionally lightweight (~1K parameters), capable of running on any machine\nable to run the pose estimation with negligible additional resources. We\nleverage the highly compact pose representation in a normalizing flows\nframework, which we extend to tackle the unique characteristics of\nspatio-temporal pose data and show its advantages in this use case. The\nalgorithm is quite general and can handle training data of only normal examples\nas well as a supervised setting that consists of labeled normal and abnormal\nexamples. We report state-of-the-art results on two anomaly detection\nbenchmarks - the unsupervised ShanghaiTech dataset and the recent supervised\nUBnormal dataset.\n",
                "链接": "https://arxiv.org/abs/2211.10946"
            },
            {
                "文章ID": "111765",
                "标题": "Noise-Free Score Distillation",
                "作者": " Oren Katzir,  Or Patashnik,  Daniel Cohen-Or,  Dani Lischinski",
                "发布日期": "2023-10-27",
                "摘要": "  Score Distillation Sampling (SDS) has emerged as the de facto approach for\ntext-to-content generation in non-image domains. In this paper, we reexamine\nthe SDS process and introduce a straightforward interpretation that demystifies\nthe necessity for large Classifier-Free Guidance (CFG) scales, rooted in the\ndistillation of an undesired noise term. Building upon our interpretation, we\npropose a novel Noise-Free Score Distillation (NFSD) process, which requires\nminimal modifications to the original SDS framework. Through this streamlined\ndesign, we achieve more effective distillation of pre-trained text-to-image\ndiffusion models while using a nominal CFG scale. This strategic choice allows\nus to prevent the over-smoothing of results, ensuring that the generated data\nis both realistic and complies with the desired prompt. To demonstrate the\nefficacy of NFSD, we provide qualitative examples that compare NFSD and SDS, as\nwell as several other methods.\n",
                "链接": "https://arxiv.org/abs/2310.17590"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下工具评测相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "34033",
                "标题": "SimLDA: A tool for topic model evaluation",
                "作者": " Rebecca M. C. Taylor,  Johan A. du Preez",
                "发布日期": "2022-08-22",
                "摘要": "  Variational Bayes (VB) applied to latent Dirichlet allocation (LDA) has\nbecome the most popular algorithm for aspect modeling. While sufficiently\nsuccessful in text topic extraction from large corpora, VB is less successful\nin identifying aspects in the presence of limited data. We present a novel\nvariational message passing algorithm as applied to Latent Dirichlet Allocation\n(LDA) and compare it with the gold standard VB and collapsed Gibbs sampling. In\nsituations where marginalisation leads to non-conjugate messages, we use ideas\nfrom sampling to derive approximate update equations. In cases where conjugacy\nholds, Loopy Belief update (LBU) (also known as Lauritzen-Spiegelhalter) is\nused. Our algorithm, ALBU (approximate LBU), has strong similarities with\nVariational Message Passing (VMP) (which is the message passing variant of VB).\nTo compare the performance of the algorithms in the presence of limited data,\nwe use data sets consisting of tweets and news groups. Using coherence measures\nwe show that ALBU learns latent distributions more accurately than does VB,\nespecially for smaller data sets.\n",
                "链接": "https://arxiv.org/abs/2208.09299"
            },
            {
                "文章ID": "25887",
                "标题": "Diagnostic Tool for Out-of-Sample Model Evaluation",
                "作者": " Ludvig Hult,  Dave Zachariah,  Petre Stoica",
                "发布日期": "2023-10-17",
                "摘要": "  Assessment of model fitness is a key part of machine learning. The standard\nparadigm is to learn models by minimizing a chosen loss function averaged over\ntraining data, with the aim of achieving small losses on future data. In this\npaper, we consider the use of a finite calibration data set to characterize the\nfuture, out-of-sample losses of a model. We propose a simple model diagnostic\ntool that provides finite-sample guarantees under weak assumptions. The tool is\nsimple to compute and to interpret. Several numerical experiments are presented\nto show how the proposed method quantifies the impact of distribution shifts,\naids the analysis of regression, and enables model selection as well as\nhyper-parameter tuning.\n",
                "链接": "https://arxiv.org/abs/2206.10982"
            },
            {
                "文章ID": "108993",
                "标题": "A Comprehensive Evaluation of Tool-Assisted Generation Strategies",
                "作者": " Alon Jacovi,  Avi Caciularu,  Jonathan Herzig,  Roee Aharoni,  Bernd Bohnet,  Mor Geva",
                "发布日期": "2023-12-29",
                "摘要": "  A growing area of research investigates augmenting language models with tools\n(e.g., search engines, calculators) to overcome their shortcomings (e.g.,\nmissing or incorrect knowledge, incorrect logical inferences). Various few-shot\ntool-usage strategies have been proposed. However, there is no systematic and\nfair comparison across different strategies, or between these strategies and\nstrong baselines that do not leverage tools. We conduct an extensive empirical\nanalysis, finding that (1) across various datasets, example difficulty levels,\nand models, strong no-tool baselines are competitive to tool-assisted\nstrategies, implying that effectively using tools with in-context\ndemonstrations is a difficult unsolved problem; (2) for knowledge-retrieval\ntasks, strategies that *refine* incorrect outputs with tools outperform\nstrategies that retrieve relevant information *ahead of* or *during\ngeneration*; (3) tool-assisted strategies are expensive in the number of tokens\nthey require to work -- incurring additional costs by orders of magnitude --\nwhich does not translate into significant improvement in performance. Overall,\nour findings suggest that few-shot tool integration is still an open challenge,\nemphasizing the need for comprehensive evaluations of future strategies to\naccurately assess their *benefits* and *costs*.\n",
                "链接": "https://arxiv.org/abs/2310.10062"
            },
            {
                "文章ID": "86296",
                "标题": "INDCOR white paper 4: Evaluation of Interactive Narrative Design For\n  Complexity Representations",
                "作者": " Christian Roth,  Breanne Pitt,  Lāsma Šķestere,  Jonathan Barbara,  Agnes Karolina Bakk,  Kirsty Dunlop,  Maria del Mar Grandio,  Miguel Barreda,  Despoina Sampatakou,  Michael Schlauch",
                "发布日期": "2023-07-07",
                "摘要": "  While a strength of Interactive Digital Narratives (IDN) is its support for\nmultiperspectivity, this also poses a substantial challenge to its evaluation.\nMoreover, evaluation has to assess the system's ability to represent a complex\nreality as well as the user's understanding of that complex reality as a result\nof the experience of interacting with the system. This is needed to measure an\nIDN's efficiency and effectiveness in representing the chosen complex\nphenomenon. We here present some empirical methods employed by INDCOR members\nin their research including UX toolkits and scales. Particularly, we consider\nthe impact of IDN on transformative learning and its evaluation through\nself-reporting and other alternatives.\n",
                "链接": "https://arxiv.org/abs/2306.09817"
            },
            {
                "文章ID": "43389",
                "标题": "Summary Workbench: Unifying Application and Evaluation of Text\n  Summarization Models",
                "作者": " Shahbaz Syed,  Dominik Schwabe,  Martin Potthast",
                "发布日期": "2022-10-19",
                "摘要": "  This paper presents Summary Workbench, a new tool for developing and\nevaluating text summarization models. New models and evaluation measures can be\neasily integrated as Docker-based plugins, allowing to examine the quality of\ntheir summaries against any input and to evaluate them using various evaluation\nmeasures. Visual analyses combining multiple measures provide insights into the\nmodels' strengths and weaknesses. The tool is hosted at\n\\url{https://tldr.demo.webis.de} and also supports local deployment for private\nresources.\n",
                "链接": "https://arxiv.org/abs/2210.09587"
            },
            {
                "文章ID": "18767",
                "标题": "ALIGNMEET: A Comprehensive Tool for Meeting Annotation, Alignment, and\n  Evaluation",
                "作者": " Peter Polák,  Muskaan Singh,  Anna Nedoluzhko,  Ondřej Bojar",
                "发布日期": "2022-05-12",
                "摘要": "  Summarization is a challenging problem, and even more challenging is to\nmanually create, correct, and evaluate the summaries. The severity of the\nproblem grows when the inputs are multi-party dialogues in a meeting setup. To\nfacilitate the research in this area, we present ALIGNMEET, a comprehensive\ntool for meeting annotation, alignment, and evaluation. The tool aims to\nprovide an efficient and clear interface for fast annotation while mitigating\nthe risk of introducing errors. Moreover, we add an evaluation mode that\nenables a comprehensive quality evaluation of meeting minutes. To the best of\nour knowledge, there is no such tool available. We release the tool as open\nsource. It is also directly installable from PyPI.\n",
                "链接": "https://arxiv.org/abs/2205.05433"
            },
            {
                "文章ID": "38679",
                "标题": "An Interdisciplinary Perspective on Evaluation and Experimental Design\n  for Visual Text Analytics: Position Paper",
                "作者": " Kostiantyn Kucher,  Nicole Sultanum,  Angel Daza,  Vasiliki Simaki,  Maria Skeppstedt,  Barbara Plank,  Jean-Daniel Fekete,  Narges Mahyar",
                "发布日期": "2022-12-21",
                "摘要": "  Appropriate evaluation and experimental design are fundamental for empirical\nsciences, particularly in data-driven fields. Due to the successes in\ncomputational modeling of languages, for instance, research outcomes are having\nan increasingly immediate impact on end users. As the gap in adoption by end\nusers decreases, the need increases to ensure that tools and models developed\nby the research communities and practitioners are reliable, trustworthy, and\nsupportive of the users in their goals. In this position paper, we focus on the\nissues of evaluating visual text analytics approaches. We take an\ninterdisciplinary perspective from the visualization and natural language\nprocessing communities, as we argue that the design and validation of visual\ntext analytics include concerns beyond computational or visual/interactive\nmethods on their own. We identify four key groups of challenges for evaluating\nvisual text analytics approaches (data ambiguity, experimental design, user\ntrust, and \"big picture\" concerns) and provide suggestions for research\nopportunities from an interdisciplinary perspective.\n",
                "链接": "https://arxiv.org/abs/2209.11534"
            },
            {
                "文章ID": "46198",
                "标题": "Evaluation Metrics for Symbolic Knowledge Extracted from Machine\n  Learning Black Boxes: A Discussion Paper",
                "作者": " Federico Sabbatini,  Roberta Calegari",
                "发布日期": "2022-11-02",
                "摘要": "  As opaque decision systems are being increasingly adopted in almost any\napplication field, issues about their lack of transparency and human\nreadability are a concrete concern for end-users. Amongst existing proposals to\nassociate human-interpretable knowledge with accurate predictions provided by\nopaque models, there are rule extraction techniques, capable of extracting\nsymbolic knowledge out of an opaque model. However, how to assess the level of\nreadability of the extracted knowledge quantitatively is still an open issue.\nFinding such a metric would be the key, for instance, to enable automatic\ncomparison between a set of different knowledge representations, paving the way\nfor the development of parameter autotuning algorithms for knowledge\nextractors. In this paper we discuss the need for such a metric as well as the\ncriticalities of readability assessment and evaluation, taking into account the\nmost common knowledge representations while highlighting the most puzzling\nissues.\n",
                "链接": "https://arxiv.org/abs/2211.00238"
            },
            {
                "文章ID": "79556",
                "标题": "The Grammar and Syntax Based Corpus Analysis Tool For The Ukrainian\n  Language",
                "作者": " Daria Stetsenko,  Inez Okulska",
                "发布日期": "2023-05-24",
                "摘要": "  This paper provides an overview of a text mining tool the StyloMetrix\ndeveloped initially for the Polish language and further extended for English\nand recently for Ukrainian. The StyloMetrix is built upon various metrics\ncrafted manually by computational linguists and researchers from literary\nstudies to analyze grammatical, stylistic, and syntactic patterns. The idea of\nconstructing the statistical evaluation of syntactic and grammar features is\nstraightforward and familiar for the languages like English, Spanish, German,\nand others; it is yet to be developed for low-resource languages like\nUkrainian. We describe the StyloMetrix pipeline and provide some experiments\nwith this tool for the text classification task. We also describe our package's\nmain limitations and the metrics' evaluation procedure.\n",
                "链接": "https://arxiv.org/abs/2305.13530"
            },
            {
                "文章ID": "35158",
                "标题": "Textwash -- automated open-source text anonymisation",
                "作者": " Bennett Kleinberg,  Toby Davies,  Maximilian Mozes",
                "发布日期": "2022-08-30",
                "摘要": "  The increased use of text data in social science research has benefited from\neasy-to-access data (e.g., Twitter). That trend comes at the cost of research\nrequiring sensitive but hard-to-share data (e.g., interview data, police\nreports, electronic health records). We introduce a solution to that stalemate\nwith the open-source text anonymisation software_Textwash_. This paper presents\nthe empirical evaluation of the tool using the TILD criteria: a technical\nevaluation (how accurate is the tool?), an information loss evaluation (how\nmuch information is lost in the anonymisation process?) and a de-anonymisation\ntest (can humans identify individuals from anonymised text data?). The findings\nsuggest that Textwash performs similar to state-of-the-art entity recognition\nmodels and introduces a negligible information loss of 0.84%. For the\nde-anonymisation test, we tasked humans to identify individuals by name from a\ndataset of crowdsourced person descriptions of very famous, semi-famous and\nnon-existing individuals. The de-anonymisation rate ranged from 1.01-2.01% for\nthe realistic use cases of the tool. We replicated the findings in a second\nstudy and concluded that Textwash succeeds in removing potentially sensitive\ninformation that renders detailed person descriptions practically anonymous.\n",
                "链接": "https://arxiv.org/abs/2208.13081"
            }
        ]
    },
    {
        "question": {
            "question": "大模型在游戏方面的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "40769",
                "标题": "Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors",
                "作者": " Mohammad Reza Taesiri,  Finlay Macklon,  Yihe Wang,  Hengshuo Shen,  Cor-Paul Bezemer",
                "发布日期": "2022-10-07",
                "摘要": "  Video game testing requires game-specific knowledge as well as common sense\nreasoning about the events in the game. While AI-driven agents can satisfy the\nfirst requirement, it is not yet possible to meet the second requirement\nautomatically. Therefore, video game testing often still relies on manual\ntesting, and human testers are required to play the game thoroughly to detect\nbugs. As a result, it is challenging to fully automate game testing. In this\nstudy, we explore the possibility of leveraging the zero-shot capabilities of\nlarge language models for video game bug detection. By formulating the bug\ndetection problem as a question-answering task, we show that large language\nmodels can identify which event is buggy in a sequence of textual descriptions\nof events from a game. To this end, we introduce the GameBugDescriptions\nbenchmark dataset, which consists of 167 buggy gameplay videos and a total of\n334 question-answer pairs across 8 games. We extensively evaluate the\nperformance of six models across the OPT and InstructGPT large language model\nfamilies on our benchmark dataset. Our results show promising results for\nemploying language models to detect video game bugs. With the proper prompting\ntechnique, we could achieve an accuracy of 70.66%, and on some video games, up\nto 78.94%. Our code, evaluation data and the benchmark can be found on\nhttps://asgaardlab.github.io/LLMxBugs\n",
                "链接": "https://arxiv.org/abs/2210.02506"
            },
            {
                "文章ID": "108644",
                "标题": "The Consensus Game: Language Model Generation via Equilibrium Search",
                "作者": " Athul Paul Jacob,  Yikang Shen,  Gabriele Farina,  Jacob Andreas",
                "发布日期": "2023-10-16",
                "摘要": "  When applied to question answering and other text generation tasks, language\nmodels (LMs) may be queried generatively (by sampling answers from their output\ndistribution) or discriminatively (by using them to score or rank a set of\ncandidate outputs). These procedures sometimes yield very different\npredictions. How do we reconcile mutually incompatible scoring procedures to\nobtain coherent LM predictions? We introduce a new, a training-free,\ngame-theoretic procedure for language model decoding. Our approach casts\nlanguage model decoding as a regularized imperfect-information sequential\nsignaling game - which we term the CONSENSUS GAME - in which a GENERATOR seeks\nto communicate an abstract correctness parameter using natural language\nsentences to a DISCRIMINATOR. We develop computational procedures for finding\napproximate equilibria of this game, resulting in a decoding algorithm we call\nEQUILIBRIUM-RANKING. Applied to a large number of tasks (including reading\ncomprehension, commonsense reasoning, mathematical problem-solving, and\ndialog), EQUILIBRIUM-RANKING consistently, and sometimes substantially,\nimproves performance over existing LM decoding procedures - on multiple\nbenchmarks, we observe that applying EQUILIBRIUM-RANKING to LLaMA-7B\noutperforms the much larger LLaMA-65B and PaLM-540B models. These results\nhighlight the promise of game-theoretic tools for addressing fundamental\nchallenges of truthfulness and consistency in LMs.\n",
                "链接": "https://arxiv.org/abs/2310.09139"
            },
            {
                "文章ID": "121330",
                "标题": "Can Large Language Models Serve as Rational Players in Game Theory? A\n  Systematic Analysis",
                "作者": " Caoyun Fan,  Jindou Chen,  Yaohui Jin,  Hao He",
                "发布日期": "2023-12-13",
                "摘要": "  Game theory, as an analytical tool, is frequently utilized to analyze human\nbehavior in social science research. With the high alignment between the\nbehavior of Large Language Models (LLMs) and humans, a promising research\ndirection is to employ LLMs as substitutes for humans in game experiments,\nenabling social science research. However, despite numerous empirical\nresearches on the combination of LLMs and game theory, the capability\nboundaries of LLMs in game theory remain unclear. In this research, we endeavor\nto systematically analyze LLMs in the context of game theory. Specifically,\nrationality, as the fundamental principle of game theory, serves as the metric\nfor evaluating players' behavior -- building a clear desire, refining belief\nabout uncertainty, and taking optimal actions. Accordingly, we select three\nclassical games (dictator game, Rock-Paper-Scissors, and ring-network game) to\nanalyze to what extent LLMs can achieve rationality in these three aspects. The\nexperimental results indicate that even the current state-of-the-art LLM\n(GPT-4) exhibits substantial disparities compared to humans in game theory. For\ninstance, LLMs struggle to build desires based on uncommon preferences, fail to\nrefine belief from many simple patterns, and may overlook or modify refined\nbelief when taking actions. Therefore, we consider that introducing LLMs into\ngame experiments in the field of social science should be approached with\ngreater caution.\n",
                "链接": "https://arxiv.org/abs/2312.05488"
            },
            {
                "文章ID": "106983",
                "标题": "Kawaii Game Vocalics: A Preliminary Model",
                "作者": " Katie Seaborn,  Katja Rogers,  Somang Name,  Miu Kojima",
                "发布日期": "2023-10-10",
                "摘要": "  Kawaii is the Japanese concept of cute++, a global export with local\ncharacteristics. Recent work has explored kawaii as a feature of user\nexperience (UX) with social robots, virtual characters, and voice assistants,\ni.e., kawaii vocalics. Games have a long history of incorporating characters\nthat use voice as a means of expressing kawaii. Nevertheless, no work to date\nhas evaluated kawaii game voices or mapped out a model of kawaii game vocalics.\nIn this work, we explored whether and how a model of kawaii vocalics maps onto\ngame character voices. We conducted an online perceptions study (N=157) using\n18 voices from kawaii characters in Japanese games. We replicated the results\nfor computer voice and discovered nuanced relationships between gender and age,\nespecially youthfulness, agelessness, gender ambiguity, and gender neutrality.\nWe provide our initial model and advocate for future work on character visuals\nand within play contexts.\n",
                "链接": "https://arxiv.org/abs/2310.04731"
            },
            {
                "文章ID": "79112",
                "标题": "ToxBuster: In-game Chat Toxicity Buster with BERT",
                "作者": " Zachary Yang,  Yasmine Maricar,  MohammadReza Davari,  Nicolas Grenon-Godbout,  Reihaneh Rabbany",
                "发布日期": "2023-05-24",
                "摘要": "  Detecting toxicity in online spaces is challenging and an ever more pressing\nproblem given the increase in social media and gaming consumption. We introduce\nToxBuster, a simple and scalable model trained on a relatively large dataset of\n194k lines of game chat from Rainbow Six Siege and For Honor, carefully\nannotated for different kinds of toxicity. Compared to the existing\nstate-of-the-art, ToxBuster achieves 82.95% (+7) in precision and 83.56% (+57)\nin recall. This improvement is obtained by leveraging past chat history and\nmetadata. We also study the implication towards real-time and post-game\nmoderation as well as the model transferability from one game to another.\n",
                "链接": "https://arxiv.org/abs/2305.12542"
            },
            {
                "文章ID": "4452",
                "标题": "Large-scale Personalized Video Game Recommendation via Social-aware\n  Contextualized Graph Neural Network",
                "作者": " Liangwei Yang,  Zhiwei Liu,  Yu Wang,  Chen Wang,  Ziwei Fan,  Philip S. Yu",
                "发布日期": "2022-02-14",
                "摘要": "  Because of the large number of online games available nowadays, online game\nrecommender systems are necessary for users and online game platforms. The\nformer can discover more potential online games of their interests, and the\nlatter can attract users to dwell longer in the platform. This paper\ninvestigates the characteristics of user behaviors with respect to the online\ngames on the Steam platform. Based on the observations, we argue that a\nsatisfying recommender system for online games is able to characterize:\npersonalization, game contextualization and social connection. However,\nsimultaneously solving all is rather challenging for game recommendation.\nFirstly, personalization for game recommendation requires the incorporation of\nthe dwelling time of engaged games, which are ignored in existing methods.\nSecondly, game contextualization should reflect the complex and high-order\nproperties of those relations. Last but not least, it is problematic to use\nsocial connections directly for game recommendations due to the massive noise\nwithin social connections. To this end, we propose a Social-aware\nContextualized Graph Neural Recommender System (SCGRec), which harnesses three\nperspectives to improve game recommendation. We conduct a comprehensive\nanalysis of users' online game behaviors, which motivates the necessity of\nhandling those three characteristics in the online game recommendation.\n",
                "链接": "https://arxiv.org/abs/2202.03392"
            },
            {
                "文章ID": "16480",
                "标题": "Personality Traits in Game Development",
                "作者": " Miriam Sturdee,  Matthew Ivory,  David Ellis,  Patrick Stacey,  Paul Ralph",
                "发布日期": "2022-04-26",
                "摘要": "  Existing work on personality traits in software development excludes game\ndevelopers as a discrete group. Whilst games are software, game development has\nunique considerations, so game developers may exhibit different personality\ntraits from other software professionals. We assessed responses from 123 game\ndevelopers on an International Personality Item Pool Five Factor Model scale\nand demographic questionnaire using factor analysis. Programmers reported lower\nExtraversion than designers, artists and production team members; lower\nOpenness than designers and production, and reported higher Neuroticism than\nproduction -- potentially linked to burnout and crunch time. Compared to\npublished norms of software developers, game developers reported lower\nOpenness, Conscientiousness, Extraversion and Agreeableness, but higher\nNeuroticism. These personality differences have many practical implications:\ndifferences in Extraversion among roles may precipitate communication\nbreakdowns; differences in Openness may induce conflict between programmers and\ndesigners. Understanding the relationship between personality traits and roles\ncan help recruiters steer new employees into appropriate roles, and help\nmanagers apply appropriate stress management techniques. To realise these\nbenefits, individuals must be distinguished from roles: just because an\nindividual occupies a role does not mean they possess personality traits\nassociated with that role.\n",
                "链接": "https://arxiv.org/abs/2204.11826"
            },
            {
                "文章ID": "27135",
                "标题": "Mastering the Game of Stratego with Model-Free Multiagent Reinforcement\n  Learning",
                "作者": " Julien Perolat,  Bart de Vylder,  Daniel Hennes,  Eugene Tarassov,  Florian Strub,  Vincent de Boer,  Paul Muller,  Jerome T. Connor,  Neil Burch,  Thomas Anthony,  Stephen McAleer,  Romuald Elie,  Sarah H. Cen,  Zhe Wang,  Audrunas Gruslys,  Aleksandra Malysheva,  Mina Khan,  Sherjil Ozair,  Finbarr Timbers,  Toby Pohlen,  Tom Eccles,  Mark Rowland,  Marc Lanctot,  Jean-Baptiste Lespiau,  Bilal Piot,  Shayegan Omidshafiei,  Edward Lockhart,  Laurent Sifre,  Nathalie Beauguerlange,  Remi Munos,  David Silver,  Satinder Singh,  Demis Hassabis,  Karl Tuyls",
                "发布日期": "2023-01-11",
                "摘要": "  We introduce DeepNash, an autonomous agent capable of learning to play the\nimperfect information game Stratego from scratch, up to a human expert level.\nStratego is one of the few iconic board games that Artificial Intelligence (AI)\nhas not yet mastered. This popular game has an enormous game tree on the order\nof $10^{535}$ nodes, i.e., $10^{175}$ times larger than that of Go. It has the\nadditional complexity of requiring decision-making under imperfect information,\nsimilar to Texas hold'em poker, which has a significantly smaller game tree (on\nthe order of $10^{164}$ nodes). Decisions in Stratego are made over a large\nnumber of discrete actions with no obvious link between action and outcome.\nEpisodes are long, with often hundreds of moves before a player wins, and\nsituations in Stratego can not easily be broken down into manageably-sized\nsub-problems as in poker. For these reasons, Stratego has been a grand\nchallenge for the field of AI for decades, and existing AI methods barely reach\nan amateur level of play. DeepNash uses a game-theoretic, model-free deep\nreinforcement learning method, without search, that learns to master Stratego\nvia self-play. The Regularised Nash Dynamics (R-NaD) algorithm, a key component\nof DeepNash, converges to an approximate Nash equilibrium, instead of 'cycling'\naround it, by directly modifying the underlying multi-agent learning dynamics.\nDeepNash beats existing state-of-the-art AI methods in Stratego and achieved a\nyearly (2022) and all-time top-3 rank on the Gravon games platform, competing\nwith human expert players.\n",
                "链接": "https://arxiv.org/abs/2206.15378"
            },
            {
                "文章ID": "28347",
                "标题": "One Pixel, One Interaction, One Game: An Experiment in Minimalist Game\n  Design",
                "作者": " Pier Luca Lanzi,  Daniele Loiacono,  Alberto Arosio,  Dorian Bucur,  Davide Caio,  Luca Capecchi,  Maria Giulietta Cappelletti,  Lorenzo Carnaghi,  Marco Giuseppe Caruso,  Valerio Ceraudo,  Luca Contato,  Luca Cornaggia,  Christian Costanza,  Tommaso Grilli,  Sumero Lira,  Luca Marchetti,  Giulia Olivares,  Barbara Pagano,  Davide Pons,  Michele Pirovano,  Valentina Tosto",
                "发布日期": "2022-07-11",
                "摘要": "  Minimalist game design was introduced a decade ago as a general design\nprinciple with a list of key properties for minimalist games: basic controls,\nsimple but aesthetically pleasing visuals, interesting player choices with vast\npossibility spaces, and sounds that resonate with the design. In this paper, we\npresent an experiment we did to explore minimalism in games using a bottom-up\napproach. We invited a small group of professional game designers and a larger\ngroup of game design students to participate in a seminal experiment on\nminimalism in game design. We started from the most basic game elements: one\npixel and one key which provide the least amount of information we can display\nand reasonably the most elementary action players can perform. We designed a\ngame that starts with a black pixel and asks players to press a key when the\npixel turns white. This minimal game, almost a Skinner box, captures the\nessential elements of the mechanics of games like \"The Impossible Game,\" which\nasks players to do nothing more than press a key at the right moment. We\npresented this game concept to the professional game designers and challenged\nthem to create other games with the least amount of player interaction and\ndisplayed information. We did not specify any constraints (as usually done in\nother contexts) and left them free to express their view of minimalistic game\ndesign. We repeated the experiment with 100+ students attending a master-level\ncourse on video game design and development at our institution. We then\nanalyzed the creations of the two groups, discussing the idea of minimalistic\ndesign that emerges from the submitted game concepts.\n",
                "链接": "https://arxiv.org/abs/2207.03827"
            },
            {
                "文章ID": "37298",
                "标题": "Forecasting Evolution of Clusters in Game Agents with Hebbian Learning",
                "作者": " Beomseok Kang,  Saibal Mukhopadhyay",
                "发布日期": "2023-06-01",
                "摘要": "  Large multi-agent systems such as real-time strategy games are often driven\nby collective behavior of agents. For example, in StarCraft II, human players\ngroup spatially near agents into a team and control the team to defeat\nopponents. In this light, clustering the agents in the game has been used for\nvarious purposes such as the efficient control of the agents in multi-agent\nreinforcement learning and game analytic tools for the game users. However,\ndespite the useful information provided by clustering, learning the dynamics of\nmulti-agent systems at a cluster level has been rarely studied yet. In this\npaper, we present a hybrid AI model that couples unsupervised and\nself-supervised learning to forecast evolution of the clusters in StarCraft II.\nWe develop an unsupervised Hebbian learning method in a set-to-cluster module\nto efficiently create a variable number of the clusters with lower inference\ntime complexity than K-means clustering. Also, a long short-term memory based\nprediction module is designed to recursively forecast state vectors generated\nby the set-to-cluster module to define cluster configuration. We experimentally\ndemonstrate the proposed model successfully predicts complex movement of the\nclusters in the game.\n",
                "链接": "https://arxiv.org/abs/2209.06904"
            }
        ]
    },
    {
        "question": {
            "question": "对比解码相关论文",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找一下文本检索任务上，是否有关于大模型在语义坍缩问题上的研究",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "36351",
                "标题": "Zoom Text Detector",
                "作者": " Chuang. Yang,  Mulin. Chen,  Yuan. Yuan,  Qi. Wang",
                "发布日期": "2022-09-08",
                "摘要": "  To pursue comprehensive performance, recent text detectors improve detection\nspeed at the expense of accuracy. They adopt shrink-mask based text\nrepresentation strategies, which leads to a high dependency of detection\naccuracy on shrink-masks. Unfortunately, three disadvantages cause unreliable\nshrink-masks. Specifically, these methods try to strengthen the discrimination\nof shrink-masks from the background by semantic information. However, the\nfeature defocusing phenomenon that coarse layers are optimized by fine-grained\nobjectives limits the extraction of semantic features. Meanwhile, since both\nshrink-masks and the margins belong to texts, the detail loss phenomenon that\nthe margins are ignored hinders the distinguishment of shrink-masks from the\nmargins, which causes ambiguous shrink-mask edges. Moreover, false-positive\nsamples enjoy similar visual features with shrink-masks. They aggravate the\ndecline of shrink-masks recognition. To avoid the above problems, we propose a\nZoom Text Detector (ZTD) inspired by the zoom process of the camera.\nSpecifically, Zoom Out Module (ZOM) is introduced to provide coarse-grained\noptimization objectives for coarse layers to avoid feature defocusing.\nMeanwhile, Zoom In Module (ZIM) is presented to enhance the margins recognition\nto prevent detail loss. Furthermore, Sequential-Visual Discriminator (SVD) is\ndesigned to suppress false-positive samples by sequential and visual features.\nExperiments verify the superior comprehensive performance of ZTD.\n",
                "链接": "https://arxiv.org/abs/2209.03014"
            },
            {
                "文章ID": "12359",
                "标题": "Cross-Media Scientific Research Achievements Retrieval Based on Deep\n  Language Model",
                "作者": " Benzhi Wang,  Meiyu Liang,  Feifei Kou,  Mingying Xu",
                "发布日期": "2022-03-30",
                "摘要": "  Science and technology big data contain a lot of cross-media\ninformation.There are images and texts in the scientific paper.The s ingle\nmodal search method cannot well meet the needs of scientific researchers.This\npaper proposes a cross-media scientific research achievements retrieval method\nbased on deep language model (CARDL).It achieves a unified cross-media semantic\nrepresentation by learning the semantic association between different modal\ndata, and is applied to the generation of text semantic vector of scientific\nresearch achievements, and then cross-media retrieval is realized through\nsemantic similarity matching between different modal data.Experimental results\nshow that the proposed CARDL method achieves better cross-modal retrieval\nperformance than existing methods. Key words science and technology big data ;\ncross-media retrieval; cross-media semantic association learning; deep language\nmodel; semantic similarity\n",
                "链接": "https://arxiv.org/abs/2203.15595"
            },
            {
                "文章ID": "110956",
                "标题": "Semantic Data Management in Data Lakes",
                "作者": " Sayed Hoseini,  Johannes Theissen-Lipp,  Christoph Quix",
                "发布日期": "2023-10-25",
                "摘要": "  In recent years, data lakes emerged as away to manage large amounts of\nheterogeneous data for modern data analytics. One way to prevent data lakes\nfrom turning into inoperable data swamps is semantic data management. Some\napproaches propose the linkage of metadata to knowledge graphs based on the\nLinked Data principles to provide more meaning and semantics to the data in the\nlake. Such a semantic layer may be utilized not only for data management but\nalso to tackle the problem of data integration from heterogeneous sources, in\norder to make data access more expressive and interoperable. In this survey, we\nreview recent approaches with a specific focus on the application within data\nlake systems and scalability to Big Data. We classify the approaches into (i)\nbasic semantic data management, (ii) semantic modeling approaches for enriching\nmetadata in data lakes, and (iii) methods for ontologybased data access. In\neach category, we cover the main techniques and their background, and compare\nlatest research. Finally, we point out challenges for future work in this\nresearch area, which needs a closer integration of Big Data and Semantic Web\ntechnologies.\n",
                "链接": "https://arxiv.org/abs/2310.15373"
            },
            {
                "文章ID": "58762",
                "标题": "Semantic Tagging with LSTM-CRF",
                "作者": " Farshad Noravesh",
                "发布日期": "2023-01-31",
                "摘要": "  In the present paper, two models are presented namely LSTM-CRF and\nBERT-LSTM-CRF for semantic tagging of universal semantic tag dataset. The\nexperiments show that the first model is much easier to converge while the\nsecond model that leverages BERT embedding, takes a long time to converge and\nneeds a big dataset for semtagging to be effective.\n",
                "链接": "https://arxiv.org/abs/2301.12206"
            },
            {
                "文章ID": "115259",
                "标题": "SpectralGPT: Spectral Foundation Model",
                "作者": " Danfeng Hong,  Bing Zhang,  Xuyang Li,  Yuxuan Li,  Chenyu Li,  Jing Yao,  Naoto Yokoya,  Hao Li,  Pedram Ghamisi,  Xiuping Jia,  Antonio Plaza,  Gamba Paolo,  Jon Atli Benediktsson,  Jocelyn Chanussot",
                "发布日期": "2023-11-28",
                "摘要": "  The foundation model has recently garnered significant attention due to its\npotential to revolutionize the field of visual representation learning in a\nself-supervised manner. While most foundation models are tailored to\neffectively process RGB images for various visual tasks, there is a noticeable\ngap in research focused on spectral data, which offers valuable information for\nscene understanding, especially in remote sensing (RS) applications. To fill\nthis gap, we created for the first time a universal RS foundation model, named\nSpectralGPT, which is purpose-built to handle spectral RS images using a novel\n3D generative pretrained transformer (GPT). Compared to existing foundation\nmodels, SpectralGPT 1) accommodates input images with varying sizes,\nresolutions, time series, and regions in a progressive training fashion,\nenabling full utilization of extensive RS big data; 2) leverages 3D token\ngeneration for spatial-spectral coupling; 3) captures spectrally sequential\npatterns via multi-target reconstruction; 4) trains on one million spectral RS\nimages, yielding models with over 600 million parameters. Our evaluation\nhighlights significant performance improvements with pretrained SpectralGPT\nmodels, signifying substantial potential in advancing spectral RS big data\napplications within the field of geoscience across four downstream tasks:\nsingle/multi-label scene classification, semantic segmentation, and change\ndetection.\n",
                "链接": "https://arxiv.org/abs/2311.07113"
            },
            {
                "文章ID": "38757",
                "标题": "KeypartX: Graph-based Perception (Text) Representation",
                "作者": " Peng Yang",
                "发布日期": "2022-09-27",
                "摘要": "  The availability of big data has opened up big opportunities for individuals,\nbusinesses and academics to view big into what is happening in their world.\nPrevious works of text representation mostly focused on informativeness from\nmassive words' frequency or cooccurrence. However, big data is a double-edged\nsword which is big in volume but unstructured in format. The unstructured edge\nrequires specific techniques to transform 'big' into meaningful instead of\ninformative alone.\n  This study presents KeypartX, a graph-based approach to represent perception\n(text in general) by key parts of speech. Different from\nbag-of-words/vector-based machine learning, this technique is human-like\nlearning that could extracts meanings from linguistic (semantic, syntactic and\npragmatic) information. Moreover, KeypartX is big-data capable but not hungry,\nwhich is even applicable to the minimum unit of text:sentence.\n",
                "链接": "https://arxiv.org/abs/2209.11844"
            },
            {
                "文章ID": "41170",
                "标题": "Learnware: Small Models Do Big",
                "作者": " Zhi-Hua Zhou,  Zhi-Hao Tan",
                "发布日期": "2023-10-31",
                "摘要": "  There are complaints about current machine learning techniques such as the\nrequirement of a huge amount of training data and proficient training skills,\nthe difficulty of continual learning, the risk of catastrophic forgetting, the\nleaking of data privacy/proprietary, etc. Most research efforts have been\nfocusing on one of those concerned issues separately, paying less attention to\nthe fact that most issues are entangled in practice. The prevailing big model\nparadigm, which has achieved impressive results in natural language processing\nand computer vision applications, has not yet addressed those issues, whereas\nbecoming a serious source of carbon emissions. This article offers an overview\nof the learnware paradigm, which attempts to enable users not need to build\nmachine learning models from scratch, with the hope of reusing small models to\ndo things even beyond their original purposes, where the key ingredient is the\nspecification which enables a trained model to be adequately identified to\nreuse according to the requirement of future users who know nothing about the\nmodel in advance.\n",
                "链接": "https://arxiv.org/abs/2210.03647"
            },
            {
                "文章ID": "124027",
                "标题": "Big Tech influence over AI research revisited: memetic analysis of\n  attribution of ideas to affiliation",
                "作者": " Stanisław Giziński,  Paulina Kaczyńska,  Hubert Ruczyński,  Emilia Wiśnios,  Bartosz Pieliński,  Przemysław Biecek,  Julian Sienkiewicz",
                "发布日期": "2023-12-21",
                "摘要": "  There exists a growing discourse around the domination of Big Tech on the\nlandscape of artificial intelligence (AI) research, yet our comprehension of\nthis phenomenon remains cursory. This paper aims to broaden and deepen our\nunderstanding of Big Tech's reach and power within AI research. It highlights\nthe dominance not merely in terms of sheer publication volume but rather in the\npropagation of new ideas or \\textit{memes}. Current studies often oversimplify\nthe concept of influence to the share of affiliations in academic papers,\ntypically sourced from limited databases such as arXiv or specific academic\nconferences.\n  The main goal of this paper is to unravel the specific nuances of such\ninfluence, determining which AI ideas are predominantly driven by Big Tech\nentities. By employing network and memetic analysis on AI-oriented paper\nabstracts and their citation network, we are able to grasp a deeper insight\ninto this phenomenon. By utilizing two databases: OpenAlex and S2ORC, we are\nable to perform such analysis on a much bigger scale than previous attempts.\n  Our findings suggest, that while Big Tech-affiliated papers are\ndisproportionately more cited in some areas, the most cited papers are those\naffiliated with both Big Tech and Academia. Focusing on the most contagious\nmemes, their attribution to specific affiliation groups (Big Tech, Academia,\nmixed affiliation) seems to be equally distributed between those three groups.\nThis suggests that the notion of Big Tech domination over AI research is\noversimplified in the discourse.\n  Ultimately, this more nuanced understanding of Big Tech's and Academia's\ninfluence could inform a more symbiotic alliance between these stakeholders\nwhich would better serve the dual goals of societal welfare and the scientific\nintegrity of AI research.\n",
                "链接": "https://arxiv.org/abs/2312.12881"
            },
            {
                "文章ID": "31494",
                "标题": "Big Data and Analytics Implementation in Tertiary Institutions to\n  Predict Students Performance in Nigeria",
                "作者": " Ozioma Collins Oguine,  Kanyifeechukwu Jane Oguine,  Hashim Ibrahim Bisallah",
                "发布日期": "2022-08-01",
                "摘要": "  The term Big Data has been coined to refer to the gargantuan bulk of data\nthat cannot be dealt with by traditional data-handling techniques. Big Data is\nstill a novel concept, and in the following literature, we intend to elaborate\non it in a palpable fashion. It commences with the concept of the subject in\nitself, along with its properties and the two general approaches to dealing\nwith it. Big Data provides an opportunity for educational Institutions to use\ntheir Information Technology resources strategically to improve educational\nquality, guide students to higher completion rates and improve student\npersistence and outcomes. This paper explores the attributes of big data that\nare relevant to educational institutions, investigates the factors influencing\nthe adoption of big data and analytics in learning institutions, and seeks to\nestablish the limiting factors hindering the use of big data in Institutions of\nhigher learning. A survey research design was adopted in conducting this\nresearch, and Questionnaires were the instrument employed for data collection.\n",
                "链接": "https://arxiv.org/abs/2207.14677"
            },
            {
                "文章ID": "102086",
                "标题": "Prompting Segmentation with Sound is Generalizable Audio-Visual Source\n  Localizer",
                "作者": " Yaoting Wang,  Weisong Liu,  Guangyao Li,  Jian Ding,  Di Hu,  Xi Li",
                "发布日期": "2023-09-19",
                "摘要": "  Never having seen an object and heard its sound simultaneously, can the model\nstill accurately localize its visual position from the input audio? In this\nwork, we concentrate on the Audio-Visual Localization and Segmentation tasks\nbut under the demanding zero-shot and few-shot scenarios. To achieve this goal,\ndifferent from existing approaches that mostly employ the\nencoder-fusion-decoder paradigm to decode localization information from the\nfused audio-visual feature, we introduce the encoder-prompt-decoder paradigm,\naiming to better fit the data scarcity and varying data distribution dilemmas\nwith the help of abundant knowledge from pre-trained models. Specifically, we\nfirst propose to construct Semantic-aware Audio Prompt (SAP) to help the visual\nfoundation model focus on sounding objects, meanwhile, the semantic gap between\nthe visual and audio modalities is also encouraged to shrink. Then, we develop\na Correlation Adapter (ColA) to keep minimal training efforts as well as\nmaintain adequate knowledge of the visual foundation model. By equipping with\nthese means, extensive experiments demonstrate that this new paradigm\noutperforms other fusion-based methods in both the unseen class and\ncross-dataset settings. We hope that our work can further promote the\ngeneralization study of Audio-Visual Localization and Segmentation in practical\napplication scenarios.\n",
                "链接": "https://arxiv.org/abs/2309.07929"
            }
        ]
    },
    {
        "question": {
            "question": "查找关于gpt4自动生成prompt的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "118879",
                "标题": "TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP\n  Models via GPT4",
                "作者": " Zihao Tan,  Qingliang Chen,  Yongjian Huang,  Chen Liang",
                "发布日期": "2023-11-30",
                "摘要": "  Prompt-based learning has been widely applied in many low-resource NLP tasks\nsuch as few-shot scenarios. However, this paradigm has been shown to be\nvulnerable to backdoor attacks. Most of the existing attack methods focus on\ninserting manually predefined templates as triggers in the pre-training phase\nto train the victim model and utilize the same triggers in the downstream task\nto perform inference, which tends to ignore the transferability and\nstealthiness of the templates. In this work, we propose a novel approach of\nTARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models\nvia GPT4), which is a data-independent attack method. Specifically, we first\nutilize GPT4 to reformulate manual templates to generate tone-strong and normal\ntemplates, and the former are injected into the model as a backdoor trigger in\nthe pre-training phase. Then, we not only directly employ the above templates\nin the downstream task, but also use GPT4 to generate templates with similar\ntone to the above templates to carry out transferable attacks. Finally we have\nconducted extensive experiments on five NLP datasets and three BERT series\nmodels, with experimental results justifying that our TARGET method has better\nattack performance and stealthiness compared to the two-external baseline\nmethods on direct attacks, and in addition achieves satisfactory attack\ncapability in the unseen tone-similar templates.\n",
                "链接": "https://arxiv.org/abs/2311.17429"
            },
            {
                "文章ID": "116159",
                "标题": "Do Physicians Know How to Prompt? The Need for Automatic Prompt\n  Optimization Help in Clinical Note Generation",
                "作者": " Zonghai Yao,  Ahmed Jaafar,  Beining Wang,  Yue Zhu,  Zhichao Yang,  Hong Yu",
                "发布日期": "2023-11-17",
                "摘要": "  This study examines the effect of prompt engineering on the performance of\nLarge Language Models (LLMs) in clinical note generation. We introduce an\nAutomatic Prompt Optimization (APO) framework to refine initial prompts and\ncompare the outputs of medical experts, non-medical experts, and APO-enhanced\nGPT3.5 and GPT4. Results highlight GPT4 APO's superior performance in\nstandardizing prompt quality across clinical note sections. A human-in-the-loop\napproach shows that experts maintain content quality post-APO, with a\npreference for their own modifications, suggesting the value of expert\ncustomization. We recommend a two-phase optimization process, leveraging\nAPO-GPT4 for consistency and expert input for personalization.\n",
                "链接": "https://arxiv.org/abs/2311.09684"
            },
            {
                "文章ID": "103691",
                "标题": "OpenAi's GPT4 as coding assistant",
                "作者": " Lefteris Moussiades,  George Zografos",
                "发布日期": "2023-09-25",
                "摘要": "  Lately, Large Language Models have been widely used in code generation. GPT4\nis considered the most potent Large Language Model from Openai. In this paper,\nwe examine GPT3.5 and GPT4 as coding assistants. More specifically, we have\nconstructed appropriate tests to check whether the two systems can a) answer\ntypical questions that can arise during the code development, b) produce\nreliable code, and c) contribute to code debugging. The test results are\nimpressive. The performance of GPT4 is outstanding and signals an increase in\nthe productivity of programmers and the reorganization of software development\nprocedures based on these new tools.\n",
                "链接": "https://arxiv.org/abs/2309.12732"
            },
            {
                "文章ID": "91796",
                "标题": "Enhancing conversational quality in language learning chatbots: An\n  evaluation of GPT4 for ASR error correction",
                "作者": " Long Mai,  Julie Carson-Berndsen",
                "发布日期": "2023-07-20",
                "摘要": "  The integration of natural language processing (NLP) technologies into\neducational applications has shown promising results, particularly in the\nlanguage learning domain. Recently, many spoken open-domain chatbots have been\nused as speaking partners, helping language learners improve their language\nskills. However, one of the significant challenges is the high word-error-rate\n(WER) when recognizing non-native/non-fluent speech, which interrupts\nconversation flow and leads to disappointment for learners. This paper explores\nthe use of GPT4 for ASR error correction in conversational settings. In\naddition to WER, we propose to use semantic textual similarity (STS) and next\nresponse sensibility (NRS) metrics to evaluate the impact of error correction\nmodels on the quality of the conversation. We find that transcriptions\ncorrected by GPT4 lead to higher conversation quality, despite an increase in\nWER. GPT4 also outperforms standard error correction methods without the need\nfor in-domain training data.\n",
                "链接": "https://arxiv.org/abs/2307.09744"
            },
            {
                "文章ID": "106036",
                "标题": "Towards End-to-End Embodied Decision Making via Multi-modal Large\n  Language Model: Explorations with GPT4-Vision and Beyond",
                "作者": " Liang Chen,  Yichi Zhang,  Shuhuai Ren,  Haozhe Zhao,  Zefan Cai,  Yuchi Wang,  Peiyi Wang,  Tianyu Liu,  Baobao Chang",
                "发布日期": "2023-11-29",
                "摘要": "  In this study, we explore the potential of Multimodal Large Language Models\n(MLLMs) in improving embodied decision-making processes for agents. While Large\nLanguage Models (LLMs) have been widely used due to their advanced reasoning\nskills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual\nunderstanding and reasoning capabilities. We investigate whether\nstate-of-the-art MLLMs can handle embodied decision-making in an end-to-end\nmanner and whether collaborations between LLMs and MLLMs can enhance\ndecision-making. To address these questions, we introduce a new benchmark\ncalled PCA-EVAL, which evaluates embodied decision-making from the perspectives\nof Perception, Cognition, and Action. Additionally, we propose HOLMES, a\nmulti-agent cooperation framework that allows LLMs to leverage MLLMs and APIs\nto gather multimodal information for informed decision-making. We compare\nend-to-end embodied decision-making and HOLMES on our benchmark and find that\nthe GPT4-Vision model demonstrates strong end-to-end embodied decision-making\nabilities, outperforming GPT4-HOLMES in terms of average decision accuracy\n(+3%). However, this performance is exclusive to the latest GPT4-Vision model,\nsurpassing the open-source state-of-the-art MLLM by 26%. Our results indicate\nthat powerful MLLMs like GPT4-Vision hold promise for decision-making in\nembodied agents, offering new avenues for MLLM research. Code and data are open\nat https://github.com/pkunlp-icler/PCA-EVAL/.\n",
                "链接": "https://arxiv.org/abs/2310.02071"
            },
            {
                "文章ID": "89555",
                "标题": "What Matters in Training a GPT4-Style Language Model with Multimodal\n  Inputs?",
                "作者": " Yan Zeng,  Hanbo Zhang,  Jiani Zheng,  Jiangnan Xia,  Guoqiang Wei,  Yang Wei,  Yuchen Zhang,  Tao Kong",
                "发布日期": "2023-08-01",
                "摘要": "  Recent advancements in Large Language Models (LLMs) such as GPT4 have\ndisplayed exceptional multi-modal capabilities in following open-ended\ninstructions given images. However, the performance of these models heavily\nrelies on design choices such as network structures, training data, and\ntraining strategies, and these choices have not been extensively discussed in\nthe literature, making it difficult to quantify progress in this field. To\naddress this issue, this paper presents a systematic and comprehensive study,\nquantitatively and qualitatively, on training such models. We implement over 20\nvariants with controlled settings. Concretely, for network structures, we\ncompare different LLM backbones and model designs. For training data, we\ninvestigate the impact of data and sampling strategies. For instructions, we\nexplore the influence of diversified prompts on the instruction-following\nability of the trained models. For benchmarks, we contribute the first, to our\nbest knowledge, comprehensive evaluation set including both image and video\ntasks through crowd-sourcing. Based on our findings, we present Lynx, which\nperforms the most accurate multi-modal understanding while keeping the best\nmulti-modal generation ability compared to existing open-sourced GPT4-style\nmodels.\n",
                "链接": "https://arxiv.org/abs/2307.02469"
            },
            {
                "文章ID": "90467",
                "标题": "GPT4 is Slightly Helpful for Peer-Review Assistance: A Pilot Study",
                "作者": " Zachary Robertson",
                "发布日期": "2023-07-13",
                "摘要": "  In this pilot study, we investigate the use of GPT4 to assist in the\npeer-review process. Our key hypothesis was that GPT-generated reviews could\nachieve comparable helpfulness to human reviewers. By comparing reviews\ngenerated by both human reviewers and GPT models for academic papers submitted\nto a major machine learning conference, we provide initial evidence that\nartificial intelligence can contribute effectively to the peer-review process.\nWe also perform robustness experiments with inserted errors to understand which\nparts of the paper the model tends to focus on. Our findings open new avenues\nfor leveraging machine learning tools to address resource constraints in peer\nreview. The results also shed light on potential enhancements to the review\nprocess and lay the groundwork for further research on scaling oversight in a\ndomain where human-feedback is increasingly a scarce resource.\n",
                "链接": "https://arxiv.org/abs/2307.05492"
            },
            {
                "文章ID": "122541",
                "标题": "Domain Prompt Learning with Quaternion Networks",
                "作者": " Qinglong Cao,  Zhengqin Xu,  Yuntian Chen,  Chao Ma,  Xiaokang Yang",
                "发布日期": "2023-12-15",
                "摘要": "  Prompt learning has emerged as an effective and data-efficient technique in\nlarge Vision-Language Models (VLMs). However, when adapting VLMs to specialized\ndomains such as remote sensing and medical imaging, domain prompt learning\nremains underexplored. While large-scale domain-specific foundation models can\nhelp tackle this challenge, their concentration on a single vision level makes\nit challenging to prompt both vision and language modalities. To overcome this,\nwe propose to leverage domain-specific knowledge from domain-specific\nfoundation models to transfer the robust recognition ability of VLMs from\ngeneralized to specialized domains, using quaternion networks. Specifically,\nthe proposed method involves using domain-specific vision features from\ndomain-specific foundation models to guide the transformation of generalized\ncontextual embeddings from the language branch into a specialized space within\nthe quaternion networks. Moreover, we present a hierarchical approach that\ngenerates vision prompt features by analyzing intermodal relationships between\nhierarchical language prompt features and domain-specific vision features. In\nthis way, quaternion networks can effectively mine the intermodal relationships\nin the specific domain, facilitating domain-specific vision-language\ncontrastive learning. Extensive experiments on domain-specific datasets show\nthat our proposed method achieves new state-of-the-art results in prompt\nlearning.\n",
                "链接": "https://arxiv.org/abs/2312.08878"
            },
            {
                "文章ID": "83977",
                "标题": "An Approach to Solving the Abstraction and Reasoning Corpus (ARC)\n  Challenge",
                "作者": " Tan John Chong Min",
                "发布日期": "2023-06-07",
                "摘要": "  We utilise the power of Large Language Models (LLMs), in particular GPT4, to\nbe prompt engineered into performing an arbitrary task. Here, we give the model\nsome human priors via text, along with some typical procedures for solving the\nARC tasks, and ask it to generate the i) broad description of the input-output\nrelation, ii) detailed steps of the input-output mapping, iii) use the detailed\nsteps to perform manipulation on the test input and derive the test output. The\ncurrent GPT3.5/GPT4 prompt solves 2 out of 4 tested small ARC challenges (those\nwith small grids of 8x8 and below). With tweaks to the prompt to make it more\nspecific for the use case, it can solve more. We posit that when scaled to a\nmulti-agent system with usage of past memory and equipped with an image\ninterpretation tool via Visual Question Answering, we may actually be able to\nsolve the majority of the ARC challenge\n",
                "链接": "https://arxiv.org/abs/2306.03553"
            },
            {
                "文章ID": "100418",
                "标题": "PromptTTS 2: Describing and Generating Voices with Text Prompt",
                "作者": " Yichong Leng,  Zhifang Guo,  Kai Shen,  Xu Tan,  Zeqian Ju,  Yanqing Liu,  Yufei Liu,  Dongchao Yang,  Leying Zhang,  Kaitao Song,  Lei He,  Xiang-Yang Li,  Sheng Zhao,  Tao Qin,  Jiang Bian",
                "发布日期": "2023-10-13",
                "摘要": "  Speech conveys more information than text, as the same word can be uttered in\nvarious voices to convey diverse information. Compared to traditional\ntext-to-speech (TTS) methods relying on speech prompts (reference speech) for\nvoice variability, using text prompts (descriptions) is more user-friendly\nsince speech prompts can be hard to find or may not exist at all. TTS\napproaches based on the text prompt face two main challenges: 1) the\none-to-many problem, where not all details about voice variability can be\ndescribed in the text prompt, and 2) the limited availability of text prompt\ndatasets, where vendors and large cost of data labeling are required to write\ntext prompts for speech. In this work, we introduce PromptTTS 2 to address\nthese challenges with a variation network to provide variability information of\nvoice not captured by text prompts, and a prompt generation pipeline to utilize\nthe large language models (LLM) to compose high quality text prompts.\nSpecifically, the variation network predicts the representation extracted from\nthe reference speech (which contains full information about voice variability)\nbased on the text prompt representation. For the prompt generation pipeline, it\ngenerates text prompts for speech with a speech language understanding model to\nrecognize voice attributes (e.g., gender, speed) from speech and a large\nlanguage model to formulate text prompts based on the recognition results.\nExperiments on a large-scale (44K hours) speech dataset demonstrate that\ncompared to the previous works, PromptTTS 2 generates voices more consistent\nwith text prompts and supports the sampling of diverse voice variability,\nthereby offering users more choices on voice generation. Additionally, the\nprompt generation pipeline produces high-quality text prompts, eliminating the\nlarge labeling cost. The demo page of PromptTTS 2 is available online.\n",
                "链接": "https://arxiv.org/abs/2309.02285"
            }
        ]
    },
    {
        "question": {
            "question": "查找多模态agent, 具身智能的相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "114225",
                "标题": "Scene-Driven Multimodal Knowledge Graph Construction for Embodied AI",
                "作者": " Song Yaoxian,  Sun Penglei,  Liu Haoyu,  Li Zhixu,  Song Wei,  Xiao Yanghua,  Zhou Xiaofang",
                "发布日期": "2023-11-08",
                "摘要": "  Embodied AI is one of the most popular studies in artificial intelligence and\nrobotics, which can effectively improve the intelligence of real-world agents\n(i.e. robots) serving human beings. Scene knowledge is important for an agent\nto understand the surroundings and make correct decisions in the varied open\nworld. Currently, knowledge base for embodied tasks is missing and most\nexisting work use general knowledge base or pre-trained models to enhance the\nintelligence of an agent. For conventional knowledge base, it is sparse,\ninsufficient in capacity and cost in data collection. For pre-trained models,\nthey face the uncertainty of knowledge and hard maintenance. To overcome the\nchallenges of scene knowledge, we propose a scene-driven multimodal knowledge\ngraph (Scene-MMKG) construction method combining conventional knowledge\nengineering and large language models. A unified scene knowledge injection\nframework is introduced for knowledge representation. To evaluate the\nadvantages of our proposed method, we instantiate Scene-MMKG considering\ntypical indoor robotic functionalities (Manipulation and Mobility), named\nManipMob-MMKG. Comparisons in characteristics indicate our instantiated\nManipMob-MMKG has broad superiority in data-collection efficiency and knowledge\nquality. Experimental results on typical embodied tasks show that\nknowledge-enhanced methods using our instantiated ManipMob-MMKG can improve the\nperformance obviously without re-designing model structures complexly. Our\nproject can be found at https://sites.google.com/view/manipmob-mmkg\n",
                "链接": "https://arxiv.org/abs/2311.03783"
            },
            {
                "文章ID": "63780",
                "标题": "Multimodal Speech Recognition for Language-Guided Embodied Agents",
                "作者": " Allen Chang,  Xiaoyuan Zhu,  Aarav Monga,  Seoho Ahn,  Tejas Srinivasan,  Jesse Thomason",
                "发布日期": "2023-10-11",
                "摘要": "  Benchmarks for language-guided embodied agents typically assume text-based\ninstructions, but deployed agents will encounter spoken instructions. While\nAutomatic Speech Recognition (ASR) models can bridge the input gap, erroneous\nASR transcripts can hurt the agents' ability to complete tasks. In this work,\nwe propose training a multimodal ASR model to reduce errors in transcribing\nspoken instructions by considering the accompanying visual context. We train\nour model on a dataset of spoken instructions, synthesized from the ALFRED task\ncompletion dataset, where we simulate acoustic noise by systematically masking\nspoken words. We find that utilizing visual observations facilitates masked\nword recovery, with multimodal ASR models recovering up to 30% more masked\nwords than unimodal baselines. We also find that a text-trained embodied agent\nsuccessfully completes tasks more often by following transcribed instructions\nfrom multimodal ASR models. github.com/Cylumn/embodied-multimodal-asr\n",
                "链接": "https://arxiv.org/abs/2302.14030"
            },
            {
                "文章ID": "117303",
                "标题": "An Embodied Generalist Agent in 3D World",
                "作者": " Jiangyong Huang,  Silong Yong,  Xiaojian Ma,  Xiongkun Linghu,  Puhao Li,  Yan Wang,  Qing Li,  Song-Chun Zhu,  Baoxiong Jia,  Siyuan Huang",
                "发布日期": "2023-11-23",
                "摘要": "  Leveraging massive knowledge and learning schemes from large language models\n(LLMs), recent machine learning models show notable successes in building\ngeneralist agents that exhibit the capability of general-purpose task solving\nin diverse domains, including natural language processing, computer vision, and\nrobotics. However, a significant challenge remains as these models exhibit\nlimited ability in understanding and interacting with the 3D world. We argue\nthis limitation significantly hinders the current models from performing\nreal-world tasks and further achieving general intelligence. To this end, we\nintroduce an embodied multi-modal and multi-task generalist agent that excels\nin perceiving, grounding, reasoning, planning, and acting in the 3D world. Our\nproposed agent, referred to as LEO, is trained with shared LLM-based model\narchitectures, objectives, and weights in two stages: (i) 3D vision-language\nalignment and (ii) 3D vision-language-action instruction tuning. To facilitate\nthe training, we meticulously curate and generate an extensive dataset\ncomprising object-level and scene-level multi-modal tasks with exceeding scale\nand complexity, necessitating a deep understanding of and interaction with the\n3D world. Through rigorous experiments, we demonstrate LEO's remarkable\nproficiency across a wide spectrum of tasks, including 3D captioning, question\nanswering, embodied reasoning, embodied navigation, and robotic manipulation.\nOur ablation results further provide valuable insights for the development of\nfuture embodied generalist agents.\n",
                "链接": "https://arxiv.org/abs/2311.12871"
            },
            {
                "文章ID": "118026",
                "标题": "Agent as Cerebrum, Controller as Cerebellum: Implementing an Embodied\n  LMM-based Agent on Drones",
                "作者": " Haoran Zhao,  Fengxing Pan,  Huqiuyue Ping,  Yaoming Zhou",
                "发布日期": "2023-11-28",
                "摘要": "  In this study, we present a novel paradigm for industrial robotic embodied\nagents, encapsulating an 'agent as cerebrum, controller as cerebellum'\narchitecture. Our approach harnesses the power of Large Multimodal Models\n(LMMs) within an agent framework known as AeroAgent, tailored for drone\ntechnology in industrial settings. To facilitate seamless integration with\nrobotic systems, we introduce ROSchain, a bespoke linkage framework connecting\nLMM-based agents to the Robot Operating System (ROS). We report findings from\nextensive empirical research, including simulated experiments on the Airgen and\nreal-world case study, particularly in individual search and rescue operations.\nThe results demonstrate AeroAgent's superior performance in comparison to\nexisting Deep Reinforcement Learning (DRL)-based agents, highlighting the\nadvantages of the embodied LMM in complex, real-world scenarios.\n",
                "链接": "https://arxiv.org/abs/2311.15033"
            },
            {
                "文章ID": "114303",
                "标题": "Multitask Multimodal Prompted Training for Interactive Embodied Task\n  Completion",
                "作者": " Georgios Pantazopoulos,  Malvina Nikandrou,  Amit Parekh,  Bhathiya Hemanthage,  Arash Eshghi,  Ioannis Konstas,  Verena Rieser,  Oliver Lemon,  Alessandro Suglia",
                "发布日期": "2023-11-08",
                "摘要": "  Interactive and embodied tasks pose at least two fundamental challenges to\nexisting Vision & Language (VL) models, including 1) grounding language in\ntrajectories of actions and observations, and 2) referential disambiguation. To\ntackle these challenges, we propose an Embodied MultiModal Agent (EMMA): a\nunified encoder-decoder model that reasons over images and trajectories, and\ncasts action prediction as multimodal text generation. By unifying all tasks as\ntext generation, EMMA learns a language of actions which facilitates transfer\nacross tasks. Different to previous modular approaches with independently\ntrained components, we use a single multitask model where each task contributes\nto goal completion. EMMA performs on par with similar models on several VL\nbenchmarks and sets a new state-of-the-art performance (36.81% success rate) on\nthe Dialog-guided Task Completion (DTC), a benchmark to evaluate dialog-guided\nagents in the Alexa Arena\n",
                "链接": "https://arxiv.org/abs/2311.04067"
            },
            {
                "文章ID": "104544",
                "标题": "The Importance of Multimodal Emotion Conditioning and Affect Consistency\n  for Embodied Conversational Agents",
                "作者": " Che-Jui Chang,  Samuel S. Sohn,  Sen Zhang,  Rajath Jayashankar,  Muhammad Usman,  Mubbasir Kapadia",
                "发布日期": "2023-12-08",
                "摘要": "  Previous studies regarding the perception of emotions for embodied virtual\nagents have shown the effectiveness of using virtual characters in conveying\nemotions through interactions with humans. However, creating an autonomous\nembodied conversational agent with expressive behaviors presents two major\nchallenges. The first challenge is the difficulty of synthesizing the\nconversational behaviors for each modality that are as expressive as real human\nbehaviors. The second challenge is that the affects are modeled independently,\nwhich makes it difficult to generate multimodal responses with consistent\nemotions across all modalities. In this work, we propose a conceptual\nframework, ACTOR (Affect-Consistent mulTimodal behaviOR generation), that aims\nto increase the perception of affects by generating multimodal behaviors\nconditioned on a consistent driving affect. We have conducted a user study with\n199 participants to assess how the average person judges the affects perceived\nfrom multimodal behaviors that are consistent and inconsistent with respect to\na driving affect. The result shows that among all model conditions, our\naffect-consistent framework receives the highest Likert scores for the\nperception of driving affects. Our statistical analysis suggests that making a\nmodality affect-inconsistent significantly decreases the perception of driving\naffects. We also observe that multimodal behaviors conditioned on consistent\naffects are more expressive compared to behaviors with inconsistent affects.\nTherefore, we conclude that multimodal emotion conditioning and affect\nconsistency are vital to enhancing the perception of affects for embodied\nconversational agents.\n",
                "链接": "https://arxiv.org/abs/2309.15311"
            },
            {
                "文章ID": "65105",
                "标题": "PaLM-E: An Embodied Multimodal Language Model",
                "作者": " Danny Driess,  Fei Xia,  Mehdi S. M. Sajjadi,  Corey Lynch,  Aakanksha Chowdhery,  Brian Ichter,  Ayzaan Wahid,  Jonathan Tompson,  Quan Vuong,  Tianhe Yu,  Wenlong Huang,  Yevgen Chebotar,  Pierre Sermanet,  Daniel Duckworth,  Sergey Levine,  Vincent Vanhoucke,  Karol Hausman,  Marc Toussaint,  Klaus Greff,  Andy Zeng,  Igor Mordatch,  Pete Florence",
                "发布日期": "2023-03-07",
                "摘要": "  Large language models excel at a wide range of complex tasks. However,\nenabling general inference in the real world, e.g., for robotics problems,\nraises the challenge of grounding. We propose embodied language models to\ndirectly incorporate real-world continuous sensor modalities into language\nmodels and thereby establish the link between words and percepts. Input to our\nembodied language model are multi-modal sentences that interleave visual,\ncontinuous state estimation, and textual input encodings. We train these\nencodings end-to-end, in conjunction with a pre-trained large language model,\nfor multiple embodied tasks including sequential robotic manipulation planning,\nvisual question answering, and captioning. Our evaluations show that PaLM-E, a\nsingle large embodied multimodal model, can address a variety of embodied\nreasoning tasks, from a variety of observation modalities, on multiple\nembodiments, and further, exhibits positive transfer: the model benefits from\ndiverse joint training across internet-scale language, vision, and\nvisual-language domains. Our largest model, PaLM-E-562B with 562B parameters,\nin addition to being trained on robotics tasks, is a visual-language generalist\nwith state-of-the-art performance on OK-VQA, and retains generalist language\ncapabilities with increasing scale.\n",
                "链接": "https://arxiv.org/abs/2303.03378"
            },
            {
                "文章ID": "93062",
                "标题": "MAEA: Multimodal Attribution for Embodied AI",
                "作者": " Vidhi Jain,  Jayant Sravan Tamarapalli,  Sahiti Yerramilli,  Yonatan Bisk",
                "发布日期": "2023-07-27",
                "摘要": "  Understanding multimodal perception for embodied AI is an open question\nbecause such inputs may contain highly complementary as well as redundant\ninformation for the task. A relevant direction for multimodal policies is\nunderstanding the global trends of each modality at the fusion layer. To this\nend, we disentangle the attributions for visual, language, and previous action\ninputs across different policies trained on the ALFRED dataset. Attribution\nanalysis can be utilized to rank and group the failure scenarios, investigate\nmodeling and dataset biases, and critically analyze multimodal EAI policies for\nrobustness and user trust before deployment. We present MAEA, a framework to\ncompute global attributions per modality of any differentiable policy. In\naddition, we show how attributions enable lower-level behavior analysis in EAI\npolicies for language and visual attributions.\n",
                "链接": "https://arxiv.org/abs/2307.13850"
            },
            {
                "文章ID": "93106",
                "标题": "Heterogeneous Embodied Multi-Agent Collaboration",
                "作者": " Xinzhu Liu,  Di Guo,  Huaping Liu",
                "发布日期": "2023-07-28",
                "摘要": "  Multi-agent embodied tasks have recently been studied in complex indoor\nvisual environments. Collaboration among multiple agents can improve work\nefficiency and has significant practical value. However, most of the existing\nresearch focuses on homogeneous multi-agent tasks. Compared with homogeneous\nagents, heterogeneous agents can leverage their different capabilities to\nallocate corresponding sub-tasks and cooperate to complete complex tasks.\nHeterogeneous multi-agent tasks are common in real-world scenarios, and the\ncollaboration strategy among heterogeneous agents is a challenging and\nimportant problem to be solved. To study collaboration among heterogeneous\nagents, we propose the heterogeneous multi-agent tidying-up task, in which\nmultiple heterogeneous agents with different capabilities collaborate with each\nother to detect misplaced objects and place them in reasonable locations. This\nis a demanding task since it requires agents to make the best use of their\ndifferent capabilities to conduct reasonable task planning and complete the\nwhole task. To solve this task, we build a heterogeneous multi-agent tidying-up\nbenchmark dataset in a large number of houses with multiple rooms based on\nProcTHOR-10K. We propose the hierarchical decision model based on misplaced\nobject detection, reasonable receptacle prediction, as well as the\nhandshake-based group communication mechanism. Extensive experiments are\nconducted to demonstrate the effectiveness of the proposed model. The project's\nwebsite and videos of experiments can be found at https://hetercol.github.io/.\n",
                "链接": "https://arxiv.org/abs/2307.13957"
            },
            {
                "文章ID": "44810",
                "标题": "Embodied, Situated, and Grounded Intelligence: Implications for AI",
                "作者": " Tyler Millhouse,  Melanie Moses,  Melanie Mitchell",
                "发布日期": "2022-10-26",
                "摘要": "  In April of 2022, the Santa Fe Institute hosted a workshop on embodied,\nsituated, and grounded intelligence as part of the Institute's Foundations of\nIntelligence project. The workshop brought together computer scientists,\npsychologists, philosophers, social scientists, and others to discuss the\nscience of embodiment and related issues in human intelligence, and its\nimplications for building robust, human-level AI. In this report, we summarize\neach of the talks and the subsequent discussions. We also draw out a number of\nkey themes and identify important frontiers for future research.\n",
                "链接": "https://arxiv.org/abs/2210.13589"
            }
        ]
    },
    {
        "question": {
            "question": "帮我查找风格化机器翻译相关的论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "92370",
                "标题": "Incorporating Human Translator Style into English-Turkish Literary\n  Machine Translation",
                "作者": " Zeynep Yirmibeşoğlu,  Olgun Dursun,  Harun Dallı,  Mehmet Şahin,  Ena Hodzik,  Sabri Gürses,  Tunga Güngör",
                "发布日期": "2023-07-24",
                "摘要": "  Although machine translation systems are mostly designed to serve in the\ngeneral domain, there is a growing tendency to adapt these systems to other\ndomains like literary translation. In this paper, we focus on English-Turkish\nliterary translation and develop machine translation models that take into\naccount the stylistic features of translators. We fine-tune a pre-trained\nmachine translation model by the manually-aligned works of a particular\ntranslator. We make a detailed analysis of the effects of manual and automatic\nalignments, data augmentation methods, and corpus size on the translations. We\npropose an approach based on stylistic features to evaluate the style of a\ntranslator in the output translations. We show that the human translator style\ncan be highly recreated in the target machine translations by adapting the\nmodels to the style of the translator.\n",
                "链接": "https://arxiv.org/abs/2307.11457"
            },
            {
                "文章ID": "54085",
                "标题": "Controlling Styles in Neural Machine Translation with Activation Prompt",
                "作者": " Yifan Wang,  Zewei Sun,  Shanbo Cheng,  Weiguo Zheng,  Mingxuan Wang",
                "发布日期": "2023-05-30",
                "摘要": "  Controlling styles in neural machine translation (NMT) has attracted wide\nattention, as it is crucial for enhancing user experience. Earlier studies on\nthis topic typically concentrate on regulating the level of formality and\nachieve some progress in this area. However, they still encounter two major\nchallenges. The first is the difficulty in style evaluation. The style\ncomprises various aspects such as lexis, syntax, and others that provide\nabundant information. Nevertheless, only formality has been thoroughly\ninvestigated. The second challenge involves excessive dependence on incremental\nadjustments, particularly when new styles are necessary. To address both\nchallenges, this paper presents a new benchmark and approach. A multiway\nstylized machine translation (MSMT) benchmark is introduced, incorporating\ndiverse categories of styles across four linguistic domains. Then, we propose a\nmethod named style activation prompt (StyleAP) by retrieving prompts from\nstylized monolingual corpus, which does not require extra fine-tuning.\nExperiments show that StyleAP could effectively control the style of\ntranslation and achieve remarkable performance.\n",
                "链接": "https://arxiv.org/abs/2212.08909"
            },
            {
                "文章ID": "83081",
                "标题": "Text Style Transfer Back-Translation",
                "作者": " Daimeng Wei,  Zhanglin Wu,  Hengchao Shang,  Zongyao Li,  Minghan Wang,  Jiaxin Guo,  Xiaoyu Chen,  Zhengzhe Yu,  Hao Yang",
                "发布日期": "2023-06-05",
                "摘要": "  Back Translation (BT) is widely used in the field of machine translation, as\nit has been proved effective for enhancing translation quality. However, BT\nmainly improves the translation of inputs that share a similar style (to be\nmore specific, translation-like inputs), since the source side of BT data is\nmachine-translated. For natural inputs, BT brings only slight improvements and\nsometimes even adverse effects. To address this issue, we propose Text Style\nTransfer Back Translation (TST BT), which uses a style transfer model to modify\nthe source side of BT data. By making the style of source-side text more\nnatural, we aim to improve the translation of natural inputs. Our experiments\non various language pairs, including both high-resource and low-resource ones,\ndemonstrate that TST BT significantly improves translation performance against\npopular BT benchmarks. In addition, TST BT is proved to be effective in domain\nadaptation so this strategy can be regarded as a general data augmentation\nmethod. Our training code and text style transfer model are open-sourced.\n",
                "链接": "https://arxiv.org/abs/2306.01318"
            },
            {
                "文章ID": "12274",
                "标题": "A Style-aware Discriminator for Controllable Image Translation",
                "作者": " Kunhee Kim,  Sanghun Park,  Eunyeong Jeon,  Taehun Kim,  Daijin Kim",
                "发布日期": "2022-03-30",
                "摘要": "  Current image-to-image translations do not control the output domain beyond\nthe classes used during training, nor do they interpolate between different\ndomains well, leading to implausible results. This limitation largely arises\nbecause labels do not consider the semantic distance. To mitigate such\nproblems, we propose a style-aware discriminator that acts as a critic as well\nas a style encoder to provide conditions. The style-aware discriminator learns\na controllable style space using prototype-based self-supervised learning and\nsimultaneously guides the generator. Experiments on multiple datasets verify\nthat the proposed model outperforms current state-of-the-art image-to-image\ntranslation methods. In contrast with current methods, the proposed approach\nsupports various applications, including style interpolation, content\ntransplantation, and local image translation.\n",
                "链接": "https://arxiv.org/abs/2203.15375"
            },
            {
                "文章ID": "7020",
                "标题": "StyleCLIPDraw: Coupling Content and Style in Text-to-Drawing Translation",
                "作者": " Peter Schaldenbrand,  Zhixuan Liu,  Jean Oh",
                "发布日期": "2022-02-28",
                "摘要": "  Generating images that fit a given text description using machine learning\nhas improved greatly with the release of technologies such as the CLIP\nimage-text encoder model; however, current methods lack artistic control of the\nstyle of image to be generated. We present an approach for generating styled\ndrawings for a given text description where a user can specify a desired\ndrawing style using a sample image. Inspired by a theory in art that style and\ncontent are generally inseparable during the creative process, we propose a\ncoupled approach, known here as StyleCLIPDraw, whereby the drawing is generated\nby optimizing for style and content simultaneously throughout the process as\nopposed to applying style transfer after creating content in a sequence. Based\non human evaluation, the styles of images generated by StyleCLIPDraw are\nstrongly preferred to those by the sequential approach. Although the quality of\ncontent generation degrades for certain styles, overall considering both\ncontent \\textit{and} style, StyleCLIPDraw is found far more preferred,\nindicating the importance of style, look, and feel of machine generated images\nto people as well as indicating that style is coupled in the drawing process\nitself. Our code (https://github.com/pschaldenbrand/StyleCLIPDraw), a\ndemonstration (https://replicate.com/pschaldenbrand/style-clip-draw), and style\nevaluation data\n(https://www.kaggle.com/pittsburghskeet/drawings-with-style-evaluation-styleclipdraw)\nare publicly available.\n",
                "链接": "https://arxiv.org/abs/2202.12362"
            },
            {
                "文章ID": "113724",
                "标题": "Narrowing the Gap between Zero- and Few-shot Machine Translation by\n  Matching Styles",
                "作者": " Weiting Tan,  Haoran Xu,  Lingfeng Shen,  Shuyue Stella Li,  Kenton Murray,  Philipp Koehn,  Benjamin Van Durme,  Yunmo Chen",
                "发布日期": "2023-11-07",
                "摘要": "  Large language models trained primarily in a monolingual setting have\ndemonstrated their ability to generalize to machine translation using zero- and\nfew-shot examples with in-context learning. However, even though zero-shot\ntranslations are relatively good, there remains a discernible gap comparing\ntheir performance with the few-shot setting. In this paper, we investigate the\nfactors contributing to this gap and find that this gap can largely be closed\n(for about 70%) by matching the writing styles of the target corpus.\nAdditionally, we explore potential approaches to enhance zero-shot baselines\nwithout the need for parallel demonstration examples, providing valuable\ninsights into how these methods contribute to improving translation metrics.\n",
                "链接": "https://arxiv.org/abs/2311.02310"
            },
            {
                "文章ID": "55230",
                "标题": "DSI2I: Dense Style for Unpaired Image-to-Image Translation",
                "作者": " Baran Ozaydin,  Tong Zhang,  Sabine Süsstrunk,  Mathieu Salzmann",
                "发布日期": "2023-01-02",
                "摘要": "  Unpaired exemplar-based image-to-image (UEI2I) translation aims to translate\na source image to a target image domain with the style of a target image\nexemplar, without ground-truth input-translation pairs. Existing UEI2I methods\nrepresent style using either a global, image-level feature vector, or one\nvector per object instance/class but requiring knowledge of the scene\nsemantics. Here, by contrast, we propose to represent style as a dense feature\nmap, allowing for a finer-grained transfer to the source image without\nrequiring any external semantic information. We then rely on perceptual and\nadversarial losses to disentangle our dense style and content representations,\nand exploit unsupervised cross-domain semantic correspondences to warp the\nexemplar style to the source content. We demonstrate the effectiveness of our\nmethod on two datasets using standard metrics together with a new localized\nstyle metric measuring style similarity in a class-wise manner. Our results\nevidence that the translations produced by our approach are more diverse and\ncloser to the exemplars than those of the state-of-the-art methods while\nnonetheless preserving the source content.\n",
                "链接": "https://arxiv.org/abs/2212.13253"
            },
            {
                "文章ID": "56794",
                "标题": "SHUNIT: Style Harmonization for Unpaired Image-to-Image Translation",
                "作者": " Seokbeom Song,  Suhyeon Lee,  Hongje Seong,  Kyoungwon Min,  Euntai Kim",
                "发布日期": "2023-01-13",
                "摘要": "  We propose a novel solution for unpaired image-to-image (I2I) translation. To\ntranslate complex images with a wide range of objects to a different domain,\nrecent approaches often use the object annotations to perform per-class\nsource-to-target style mapping. However, there remains a point for us to\nexploit in the I2I. An object in each class consists of multiple components,\nand all the sub-object components have different characteristics. For example,\na car in CAR class consists of a car body, tires, windows and head and tail\nlamps, etc., and they should be handled separately for realistic I2I\ntranslation. The simplest solution to the problem will be to use more detailed\nannotations with sub-object component annotations than the simple object\nannotations, but it is not possible. The key idea of this paper is to bypass\nthe sub-object component annotations by leveraging the original style of the\ninput image because the original style will include the information about the\ncharacteristics of the sub-object components. Specifically, for each pixel, we\nuse not only the per-class style gap between the source and target domains but\nalso the pixel's original style to determine the target style of a pixel. To\nthis end, we present Style Harmonization for unpaired I2I translation (SHUNIT).\nOur SHUNIT generates a new style by harmonizing the target domain style\nretrieved from a class memory and an original source image style. Instead of\ndirect source-to-target style mapping, we aim for source and target styles\nharmonization. We validate our method with extensive experiments and achieve\nstate-of-the-art performance on the latest benchmark sets. The source code is\navailable online: https://github.com/bluejangbaljang/SHUNIT.\n",
                "链接": "https://arxiv.org/abs/2301.04685"
            },
            {
                "文章ID": "48179",
                "标题": "Easy Guided Decoding in Providing Suggestions for Interactive Machine\n  Translation",
                "作者": " Ke Wang,  Xin Ge,  Jiayi Wang,  Yu Zhao,  Yuqi Zhang",
                "发布日期": "2023-06-05",
                "摘要": "  Machine translation technology has made great progress in recent years, but\nit cannot guarantee error free results. Human translators perform post editing\non machine translations to correct errors in the scene of computer aided\ntranslation. In favor of expediting the post editing process, many works have\ninvestigated machine translation in interactive modes, in which machines can\nautomatically refine the rest of translations constrained by human's edits.\nTranslation Suggestion (TS), as an interactive mode to assist human\ntranslators, requires machines to generate alternatives for specific incorrect\nwords or phrases selected by human translators. In this paper, we utilize the\nparameterized objective function of neural machine translation (NMT) and\npropose a novel constrained decoding algorithm, namely Prefix Suffix Guided\nDecoding (PSGD), to deal with the TS problem without additional training.\nCompared to the state of the art lexically constrained decoding method, PSGD\nimproves translation quality by an average of $10.87$ BLEU and $8.62$ BLEU on\nthe WeTS and the WMT 2022 Translation Suggestion datasets, respectively, and\nreduces decoding time overhead by an average of 63.4% tested on the WMT\ntranslation datasets. Furthermore, on both of the TS benchmark datasets, it is\nsuperior to other supervised learning systems trained with TS annotated data.\n",
                "链接": "https://arxiv.org/abs/2211.07093"
            },
            {
                "文章ID": "40005",
                "标题": "FRMT: A Benchmark for Few-Shot Region-Aware Machine Translation",
                "作者": " Parker Riley,  Timothy Dozat,  Jan A. Botha,  Xavier Garcia,  Dan Garrette,  Jason Riesa,  Orhan Firat,  Noah Constant",
                "发布日期": "2023-10-04",
                "摘要": "  We present FRMT, a new dataset and evaluation benchmark for Few-shot\nRegion-aware Machine Translation, a type of style-targeted translation. The\ndataset consists of professional translations from English into two regional\nvariants each of Portuguese and Mandarin Chinese. Source documents are selected\nto enable detailed analysis of phenomena of interest, including lexically\ndistinct terms and distractor terms. We explore automatic evaluation metrics\nfor FRMT and validate their correlation with expert human evaluation across\nboth region-matched and mismatched rating scenarios. Finally, we present a\nnumber of baseline models for this task, and offer guidelines for how\nresearchers can train, evaluate, and compare their own models. Our dataset and\nevaluation code are publicly available: https://bit.ly/frmt-task\n",
                "链接": "https://arxiv.org/abs/2210.00193"
            }
        ]
    },
    {
        "question": {
            "question": "查找中文ner常用的数据集论文",
            "type": "1"
        },
        "results": []
    },
    {
        "question": {
            "question": "查找一下最近用反事实做数据增强的文章",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "80211",
                "标题": "Large Language Models as Counterfactual Generator: Strengths and\n  Weaknesses",
                "作者": " Yongqi Li,  Mayi Xu,  Xin Miao,  Shen Zhou,  Tieyun Qian",
                "发布日期": "2023-05-25",
                "摘要": "  Large language models (LLMs) have demonstrated remarkable performance in a\nrange of natural language understanding and generation tasks. Yet, their\nability to generate counterfactuals, which can be used for areas like data\naugmentation, remains under-explored. This study aims to investigate the\ncounterfactual generation capabilities of LLMs and analysis factors that\ninfluence this ability. First, we evaluate how effective are LLMs in\ncounterfactual generation through data augmentation experiments for small\nlanguage models (SLMs) across four tasks: sentiment analysis, natural language\ninference, named entity recognition, and relation extraction. While LLMs show\npromising enhancements in various settings, they struggle in complex tasks due\nto their self-limitations and the lack of logical guidance to produce\ncounterfactuals that align with commonsense. Second, our analysis reveals the\npivotal role of providing accurate task definitions and detailed step-by-step\ninstructions to LLMs in generating counterfactuals. Interestingly, we also find\nthat LLMs can generate reasonable counterfactuals even with unreasonable\ndemonstrations, which illustrates that demonstrations are primarily to regulate\nthe output format.This study provides the first comprehensive insight into\ncounterfactual generation abilities of LLMs, and offers a novel perspective on\nutilizing LLMs for data augmentation to enhance SLMs.\n",
                "链接": "https://arxiv.org/abs/2305.14791"
            },
            {
                "文章ID": "104252",
                "标题": "COCO-Counterfactuals: Automatically Constructed Counterfactual Examples\n  for Image-Text Pairs",
                "作者": " Tiep Le,  Vasudev Lal,  Phillip Howard",
                "发布日期": "2023-11-01",
                "摘要": "  Counterfactual examples have proven to be valuable in the field of natural\nlanguage processing (NLP) for both evaluating and improving the robustness of\nlanguage models to spurious correlations in datasets. Despite their\ndemonstrated utility for NLP, multimodal counterfactual examples have been\nrelatively unexplored due to the difficulty of creating paired image-text data\nwith minimal counterfactual changes. To address this challenge, we introduce a\nscalable framework for automatic generation of counterfactual examples using\ntext-to-image diffusion models. We use our framework to create\nCOCO-Counterfactuals, a multimodal counterfactual dataset of paired image and\ntext captions based on the MS-COCO dataset. We validate the quality of\nCOCO-Counterfactuals through human evaluations and show that existing\nmultimodal models are challenged by our counterfactual image-text pairs.\nAdditionally, we demonstrate the usefulness of COCO-Counterfactuals for\nimproving out-of-domain generalization of multimodal vision-language models via\ntraining data augmentation.\n",
                "链接": "https://arxiv.org/abs/2309.14356"
            },
            {
                "文章ID": "876",
                "标题": "Learning Fair Node Representations with Graph Counterfactual Fairness",
                "作者": " Jing Ma,  Ruocheng Guo,  Mengting Wan,  Longqi Yang,  Aidong Zhang,  Jundong Li",
                "发布日期": "2022-01-12",
                "摘要": "  Fair machine learning aims to mitigate the biases of model predictions\nagainst certain subpopulations regarding sensitive attributes such as race and\ngender. Among the many existing fairness notions, counterfactual fairness\nmeasures the model fairness from a causal perspective by comparing the\npredictions of each individual from the original data and the counterfactuals.\nIn counterfactuals, the sensitive attribute values of this individual had been\nmodified. Recently, a few works extend counterfactual fairness to graph data,\nbut most of them neglect the following facts that can lead to biases: 1) the\nsensitive attributes of each node's neighbors may causally affect the\nprediction w.r.t. this node; 2) the sensitive attributes may causally affect\nother features and the graph structure. To tackle these issues, in this paper,\nwe propose a novel fairness notion - graph counterfactual fairness, which\nconsiders the biases led by the above facts. To learn node representations\ntowards graph counterfactual fairness, we propose a novel framework based on\ncounterfactual data augmentation. In this framework, we generate\ncounterfactuals corresponding to perturbations on each node's and their\nneighbors' sensitive attributes. Then we enforce fairness by minimizing the\ndiscrepancy between the representations learned from the original graph and the\ncounterfactuals for each node. Experiments on both synthetic and real-world\ngraphs show that our framework outperforms the state-of-the-art baselines in\ngraph counterfactual fairness, and also achieves comparable prediction\nperformance.\n",
                "链接": "https://arxiv.org/abs/2201.03662"
            },
            {
                "文章ID": "54607",
                "标题": "DISCO: Distilling Counterfactuals with Large Language Models",
                "作者": " Zeming Chen,  Qiyue Gao,  Antoine Bosselut,  Ashish Sabharwal,  Kyle Richardson",
                "发布日期": "2023-06-07",
                "摘要": "  Models trained with counterfactually augmented data learn representations of\nthe causal structure of tasks, enabling robust generalization. However,\nhigh-quality counterfactual data is scarce for most tasks and not easily\ngenerated at scale. When crowdsourced, such data is typically limited in scale\nand diversity; when generated using supervised methods, it is computationally\nexpensive to extend to new counterfactual dimensions. In this work, we\nintroduce DISCO (DIStilled COunterfactual Data), a new method for automatically\ngenerating high quality counterfactual data at scale. DISCO engineers prompts\nto generate phrasal perturbations with a large general language model. Then, a\ntask-specific teacher model filters these generations to distill high-quality\ncounterfactual data. While task-agnostic, we apply our pipeline to the task of\nnatural language inference (NLI) and find that on challenging evaluations such\nas the NLI stress test, comparatively smaller student models trained with DISCO\ngenerated counterfactuals are more robust (6% absolute) and generalize better\nacross distributions (2%) compared to models trained without data augmentation.\nFurthermore, DISCO augmented models are 10% more consistent between\ncounterfactual pairs on three evaluation sets, demonstrating that DISCO\naugmentation enables models to more reliably learn causal representations. Our\nrepository is available at: https://github.com/eric11eca/disco\n",
                "链接": "https://arxiv.org/abs/2212.10534"
            },
            {
                "文章ID": "53601",
                "标题": "Generative Robust Classification",
                "作者": " Xuwang Yin",
                "发布日期": "2022-12-15",
                "摘要": "  Training adversarially robust discriminative (i.e., softmax) classifier has\nbeen the dominant approach to robust classification. Building on recent work on\nadversarial training (AT)-based generative models, we investigate using AT to\nlearn unnormalized class-conditional density models and then performing\ngenerative robust classification. Our result shows that, under the condition of\nsimilar model capacities, the generative robust classifier achieves comparable\nperformance to a baseline softmax robust classifier when the test data is clean\nor when the test perturbation is of limited size, and much better performance\nwhen the test perturbation size exceeds the training perturbation size. The\ngenerative classifier is also able to generate samples or counterfactuals that\nmore closely resemble the training data, suggesting that the generative\nclassifier can better capture the class-conditional distributions. In contrast\nto standard discriminative adversarial training where advanced data\naugmentation techniques are only effective when combined with weight averaging,\nwe find it straightforward to apply advanced data augmentation to achieve\nbetter robustness in our approach. Our result suggests that the generative\nclassifier is a competitive alternative to robust classification, especially\nfor problems with limited number of classes.\n",
                "链接": "https://arxiv.org/abs/2212.07283"
            },
            {
                "文章ID": "42062",
                "标题": "Question Answering Over Biological Knowledge Graph via Amazon Alexa",
                "作者": " Md. Rezaul Karim,  Hussain Ali,  Prinon Das,  Mohamed Abdelwaheb,  Stefan Decker",
                "发布日期": "2022-10-13",
                "摘要": "  Structured and unstructured data and facts about drugs, genes, protein,\nviruses, and their mechanism are spread across a huge number of scientific\narticles. These articles are a large-scale knowledge source and can have a huge\nimpact on disseminating knowledge about the mechanisms of certain biological\nprocesses. A knowledge graph (KG) can be constructed by integrating such facts\nand data and be used for data integration, exploration, and federated queries.\nHowever, exploration and querying large-scale KGs is tedious for certain groups\nof users due to a lack of knowledge about underlying data assets or semantic\ntechnologies. A question-answering (QA) system allows the answer of natural\nlanguage questions over KGs automatically using triples contained in a KG.\nRecently, the use and adaption of digital assistants are getting wider owing to\ntheir capability at enabling users to voice commands to control smart systems\nor devices. This paper is about using Amazon Alexa's voice-enabled interface\nfor QA over KGs. As a proof-of-concept, we use the well-known DisgeNET KG,\nwhich contains knowledge covering 1.13 million gene-disease associations\nbetween 21,671 genes and 30,170 diseases, disorders, and clinical or abnormal\nhuman phenotypes. Our study shows how Alex could be of help to find facts about\ncertain biological entities from large-scale knowledge bases.\n",
                "链接": "https://arxiv.org/abs/2210.06040"
            },
            {
                "文章ID": "81622",
                "标题": "Data Augmentation for Low-Resource Keyphrase Generation",
                "作者": " Krishna Garg,  Jishnu Ray Chowdhury,  Cornelia Caragea",
                "发布日期": "2023-05-30",
                "摘要": "  Keyphrase generation is the task of summarizing the contents of any given\narticle into a few salient phrases (or keyphrases). Existing works for the task\nmostly rely on large-scale annotated datasets, which are not easy to acquire.\nVery few works address the problem of keyphrase generation in low-resource\nsettings, but they still rely on a lot of additional unlabeled data for\npretraining and on automatic methods for pseudo-annotations. In this paper, we\npresent data augmentation strategies specifically to address keyphrase\ngeneration in purely resource-constrained domains. We design techniques that\nuse the full text of the articles to improve both present and absent keyphrase\ngeneration. We test our approach comprehensively on three datasets and show\nthat the data augmentation strategies consistently improve the state-of-the-art\nperformance. We release our source code at\nhttps://github.com/kgarg8/kpgen-lowres-data-aug.\n",
                "链接": "https://arxiv.org/abs/2305.17968"
            },
            {
                "文章ID": "64484",
                "标题": "Hyperparameter Tuning and Model Evaluation in Causal Effect Estimation",
                "作者": " Damian Machlanski,  Spyridon Samothrakis,  Paul Clarke",
                "发布日期": "2023-03-03",
                "摘要": "  The performance of most causal effect estimators relies on accurate\npredictions of high-dimensional non-linear functions of the observed data. The\nremarkable flexibility of modern Machine Learning (ML) methods is perfectly\nsuited to this task. However, data-driven hyperparameter tuning of ML methods\nrequires effective model evaluation to avoid large errors in causal estimates,\na task made more challenging because causal inference involves unavailable\ncounterfactuals. Multiple performance-validation metrics have recently been\nproposed such that practitioners now not only have to make complex decisions\nabout which causal estimators, ML learners and hyperparameters to choose, but\nalso about which evaluation metric to use. This paper, motivated by unclear\nrecommendations, investigates the interplay between the four different aspects\nof model evaluation for causal effect estimation. We develop a comprehensive\nexperimental setup that involves many commonly used causal estimators, ML\nmethods and evaluation approaches and apply it to four well-known causal\ninference benchmark datasets. Our results suggest that optimal hyperparameter\ntuning of ML learners is enough to reach state-of-the-art performance in effect\nestimation, regardless of estimators and learners. We conclude that most causal\nestimators are roughly equivalent in performance if tuned thoroughly enough. We\nalso find hyperparameter tuning and model evaluation are much more important\nthan causal estimators and ML methods. Finally, from the significant gap we\nfind in estimation performance of popular evaluation metrics compared with\noptimal model selection choices, we call for more research into causal model\nevaluation to unlock the optimum performance not currently being delivered even\nby state-of-the-art procedures.\n",
                "链接": "https://arxiv.org/abs/2303.01412"
            },
            {
                "文章ID": "8031",
                "标题": "Data Augmentation as Feature Manipulation",
                "作者": " Ruoqi Shen,  Sébastien Bubeck,  Suriya Gunasekar",
                "发布日期": "2022-09-22",
                "摘要": "  Data augmentation is a cornerstone of the machine learning pipeline, yet its\ntheoretical underpinnings remain unclear. Is it merely a way to artificially\naugment the data set size? Or is it about encouraging the model to satisfy\ncertain invariance? In this work we consider another angle, and we study the\neffect of data augmentation on the dynamic of the learning process. We find\nthat data augmentation can alter the relative importance of various features,\neffectively making certain informative but hard to learn features more likely\nto be captured in the learning process. Importantly, we show that this effect\nis more pronounced for non-linear models, such as neural networks. Our main\ncontribution is a detailed analysis of data augmentation on the learning\ndynamic for a two layer convolutional neural network in the recently proposed\nmulti-view data model by Allen-Zhu and Li [2020]. We complement this analysis\nwith further experimental evidence that data augmentation can be viewed as\nfeature manipulation.\n",
                "链接": "https://arxiv.org/abs/2203.01572"
            },
            {
                "文章ID": "34101",
                "标题": "Predicting the Masses of Exotic Hadrons with Data Augmentation Using\n  Multilayer Perceptron",
                "作者": " Huseyin Bahtiyar",
                "发布日期": "2023-01-23",
                "摘要": "  Recently, there have been significant developments in neural networks, which\nled to the frequent use of neural networks in the physics literature. This work\nis focused on predicting the masses of exotic hadrons, doubly charmed and\nbottomed baryons using neural networks trained on meson and baryon masses that\nare determined by experiments. The original data set has been extended using\nthe recently proposed artificial data augmentation methods. We have observed\nthat the neural network's predictive ability increases with the use of\naugmented data. The results indicated that data augmentation techniques play an\nessential role in improving neural network predictions; moreover, neural\nnetworks can make reasonable predictions for exotic hadrons, doubly charmed,\nand doubly bottomed baryons. The results are also comparable to Gaussian\nProcess and Constituent Quark Model.\n",
                "链接": "https://arxiv.org/abs/2208.09538"
            }
        ]
    },
    {
        "question": {
            "question": "使用LLM进行蛋白质结构/功能/性质预测的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "109510",
                "标题": "Protein 3D Graph Structure Learning for Robust Structure-based Protein\n  Property Prediction",
                "作者": " Yufei Huang,  Siyuan Li,  Jin Su,  Lirong Wu,  Odin Zhang,  Haitao Lin,  Jingqi Qi,  Zihan Liu,  Zhangyang Gao,  Yuyang Liu,  Jiangbin Zheng,  Stan. ZQ. Li",
                "发布日期": "2023-10-20",
                "摘要": "  Protein structure-based property prediction has emerged as a promising\napproach for various biological tasks, such as protein function prediction and\nsub-cellular location estimation. The existing methods highly rely on\nexperimental protein structure data and fail in scenarios where these data are\nunavailable. Predicted protein structures from AI tools (e.g., AlphaFold2) were\nutilized as alternatives. However, we observed that current practices, which\nsimply employ accurately predicted structures during inference, suffer from\nnotable degradation in prediction accuracy. While similar phenomena have been\nextensively studied in general fields (e.g., Computer Vision) as model\nrobustness, their impact on protein property prediction remains unexplored. In\nthis paper, we first investigate the reason behind the performance decrease\nwhen utilizing predicted structures, attributing it to the structure embedding\nbias from the perspective of structure representation learning. To study this\nproblem, we identify a Protein 3D Graph Structure Learning Problem for Robust\nProtein Property Prediction (PGSL-RP3), collect benchmark datasets, and present\na protein Structure embedding Alignment Optimization framework (SAO) to\nmitigate the problem of structure embedding bias between the predicted and\nexperimental protein structures. Extensive experiments have shown that our\nframework is model-agnostic and effective in improving the property prediction\nof both predicted structures and experimental structures. The benchmark\ndatasets and codes will be released to benefit the community.\n",
                "链接": "https://arxiv.org/abs/2310.11466"
            },
            {
                "文章ID": "26245",
                "标题": "PSP: Million-level Protein Sequence Dataset for Protein Structure\n  Prediction",
                "作者": " Sirui Liu,  Jun Zhang,  Haotian Chu,  Min Wang,  Boxin Xue,  Ningxi Ni,  Jialiang Yu,  Yuhao Xie,  Zhenyu Chen,  Mengyun Chen,  Yuan Liu,  Piya Patra,  Fan Xu,  Jie Chen,  Zidong Wang,  Lijiang Yang,  Fan Yu,  Lei Chen,  Yi Qin Gao",
                "发布日期": "2022-06-27",
                "摘要": "  Proteins are essential component of human life and their structures are\nimportant for function and mechanism analysis. Recent work has shown the\npotential of AI-driven methods for protein structure prediction. However, the\ndevelopment of new models is restricted by the lack of dataset and benchmark\ntraining procedure. To the best of our knowledge, the existing open source\ndatasets are far less to satisfy the needs of modern protein sequence-structure\nrelated research. To solve this problem, we present the first million-level\nprotein structure prediction dataset with high coverage and diversity, named as\nPSP. This dataset consists of 570k true structure sequences (10TB) and 745k\ncomplementary distillation sequences (15TB). We provide in addition the\nbenchmark training procedure for SOTA protein structure prediction model on\nthis dataset. We validate the utility of this dataset for training by\nparticipating CAMEO contest in which our model won the first place. We hope our\nPSP dataset together with the training benchmark can enable a broader community\nof AI/biology researchers for AI-driven protein related research.\n",
                "链接": "https://arxiv.org/abs/2206.12240"
            },
            {
                "文章ID": "24433",
                "标题": "Exploring evolution-aware & -free protein language models as protein\n  function predictors",
                "作者": " Mingyang Hu,  Fajie Yuan,  Kevin K. Yang,  Fusong Ju,  Jin Su,  Hui Wang,  Fei Yang,  Qiuyang Ding",
                "发布日期": "2022-10-18",
                "摘要": "  Large-scale Protein Language Models (PLMs) have improved performance in\nprotein prediction tasks, ranging from 3D structure prediction to various\nfunction predictions. In particular, AlphaFold, a ground-breaking AI system,\ncould potentially reshape structural biology. However, the utility of the PLM\nmodule in AlphaFold, Evoformer, has not been explored beyond structure\nprediction. In this paper, we investigate the representation ability of three\npopular PLMs: ESM-1b (single sequence), MSA-Transformer (multiple sequence\nalignment) and Evoformer (structural), with a special focus on Evoformer.\nSpecifically, we aim to answer the following key questions: (i) Does the\nEvoformer trained as part of AlphaFold produce representations amenable to\npredicting protein function? (ii) If yes, can Evoformer replace ESM-1b and\nMSA-Transformer? (ii) How much do these PLMs rely on evolution-related protein\ndata? In this regard, are they complementary to each other? We compare these\nmodels by empirical study along with new insights and conclusions. All code and\ndatasets for reproducibility are available at\nhttps://github.com/elttaes/Revisiting-PLMs.\n",
                "链接": "https://arxiv.org/abs/2206.06583"
            },
            {
                "文章ID": "51324",
                "标题": "Protein Language Models and Structure Prediction: Connection and\n  Progression",
                "作者": " Bozhen Hu,  Jun Xia,  Jiangbin Zheng,  Cheng Tan,  Yufei Huang,  Yongjie Xu,  Stan Z. Li",
                "发布日期": "2022-12-01",
                "摘要": "  The prediction of protein structures from sequences is an important task for\nfunction prediction, drug design, and related biological processes\nunderstanding. Recent advances have proved the power of language models (LMs)\nin processing the protein sequence databases, which inherit the advantages of\nattention networks and capture useful information in learning representations\nfor proteins. The past two years have witnessed remarkable success in tertiary\nprotein structure prediction (PSP), including evolution-based and\nsingle-sequence-based PSP. It seems that instead of using energy-based models\nand sampling procedures, protein language model (pLM)-based pipelines have\nemerged as mainstream paradigms in PSP. Despite the fruitful progress, the PSP\ncommunity needs a systematic and up-to-date survey to help bridge the gap\nbetween LMs in the natural language processing (NLP) and PSP domains and\nintroduce their methodologies, advancements and practical applications. To this\nend, in this paper, we first introduce the similarities between protein and\nhuman languages that allow LMs extended to pLMs, and applied to protein\ndatabases. Then, we systematically review recent advances in LMs and pLMs from\nthe perspectives of network architectures, pre-training strategies,\napplications, and commonly-used protein databases. Next, different types of\nmethods for PSP are discussed, particularly how the pLM-based architectures\nfunction in the process of protein folding. Finally, we identify challenges\nfaced by the PSP community and foresee promising research directions along with\nthe advances of pLMs. This survey aims to be a hands-on guide for researchers\nto understand PSP methods, develop pLMs and tackle challenging problems in this\nfield for practical purposes.\n",
                "链接": "https://arxiv.org/abs/2211.16742"
            },
            {
                "文章ID": "9383",
                "标题": "Protein Representation Learning by Geometric Structure Pretraining",
                "作者": " Zuobai Zhang,  Minghao Xu,  Arian Jamasb,  Vijil Chenthamarakshan,  Aurelie Lozano,  Payel Das,  Jian Tang",
                "发布日期": "2023-01-31",
                "摘要": "  Learning effective protein representations is critical in a variety of tasks\nin biology such as predicting protein function or structure. Existing\napproaches usually pretrain protein language models on a large number of\nunlabeled amino acid sequences and then finetune the models with some labeled\ndata in downstream tasks. Despite the effectiveness of sequence-based\napproaches, the power of pretraining on known protein structures, which are\navailable in smaller numbers only, has not been explored for protein property\nprediction, though protein structures are known to be determinants of protein\nfunction. In this paper, we propose to pretrain protein representations\naccording to their 3D structures. We first present a simple yet effective\nencoder to learn the geometric features of a protein. We pretrain the protein\ngraph encoder by leveraging multiview contrastive learning and different\nself-prediction tasks. Experimental results on both function prediction and\nfold classification tasks show that our proposed pretraining methods outperform\nor are on par with the state-of-the-art sequence-based methods, while using\nmuch less pretraining data. Our implementation is available at\nhttps://github.com/DeepGraphLearning/GearNet.\n",
                "链接": "https://arxiv.org/abs/2203.06125"
            },
            {
                "文章ID": "70802",
                "标题": "EigenFold: Generative Protein Structure Prediction with Diffusion Models",
                "作者": " Bowen Jing,  Ezra Erives,  Peter Pao-Huang,  Gabriele Corso,  Bonnie Berger,  Tommi Jaakkola",
                "发布日期": "2023-04-06",
                "摘要": "  Protein structure prediction has reached revolutionary levels of accuracy on\nsingle structures, yet distributional modeling paradigms are needed to capture\nthe conformational ensembles and flexibility that underlie biological function.\nTowards this goal, we develop EigenFold, a diffusion generative modeling\nframework for sampling a distribution of structures from a given protein\nsequence. We define a diffusion process that models the structure as a system\nof harmonic oscillators and which naturally induces a cascading-resolution\ngenerative process along the eigenmodes of the system. On recent CAMEO targets,\nEigenFold achieves a median TMScore of 0.84, while providing a more\ncomprehensive picture of model uncertainty via the ensemble of sampled\nstructures relative to existing methods. We then assess EigenFold's ability to\nmodel and predict conformational heterogeneity for fold-switching proteins and\nligand-induced conformational change. Code is available at\nhttps://github.com/bjing2016/EigenFold.\n",
                "链接": "https://arxiv.org/abs/2304.02198"
            },
            {
                "文章ID": "25918",
                "标题": "Transformer Neural Networks Attending to Both Sequence and Structure for\n  Protein Prediction Tasks",
                "作者": " Anowarul Kabir,  Amarda Shehu",
                "发布日期": "2022-06-23",
                "摘要": "  The increasing number of protein sequences decoded from genomes is opening up\nnew avenues of research on linking protein sequence to function with\ntransformer neural networks. Recent research has shown that the number of known\nprotein sequences supports learning useful, task-agnostic sequence\nrepresentations via transformers. In this paper, we posit that learning joint\nsequence-structure representations yields better representations for\nfunction-related prediction tasks. We propose a transformer neural network that\nattends to both sequence and tertiary structure. We show that such joint\nrepresentations are more powerful than sequence-based representations only, and\nthey yield better performance on superfamily membership across various metrics.\n",
                "链接": "https://arxiv.org/abs/2206.11057"
            },
            {
                "文章ID": "31301",
                "标题": "HelixFold-Single: MSA-free Protein Structure Prediction by Using Protein\n  Language Model as an Alternative",
                "作者": " Xiaomin Fang,  Fan Wang,  Lihang Liu,  Jingzhou He,  Dayong Lin,  Yingfei Xiang,  Xiaonan Zhang,  Hua Wu,  Hui Li,  Le Song",
                "发布日期": "2023-10-19",
                "摘要": "  AI-based protein structure prediction pipelines, such as AlphaFold2, have\nachieved near-experimental accuracy. These advanced pipelines mainly rely on\nMultiple Sequence Alignments (MSAs) as inputs to learn the co-evolution\ninformation from the homologous sequences. Nonetheless, searching MSAs from\nprotein databases is time-consuming, usually taking dozens of minutes.\nConsequently, we attempt to explore the limits of fast protein structure\nprediction by using only primary sequences of proteins. HelixFold-Single is\nproposed to combine a large-scale protein language model with the superior\ngeometric learning capability of AlphaFold2. Our proposed method,\nHelixFold-Single, first pre-trains a large-scale protein language model (PLM)\nwith thousands of millions of primary sequences utilizing the self-supervised\nlearning paradigm, which will be used as an alternative to MSAs for learning\nthe co-evolution information. Then, by combining the pre-trained PLM and the\nessential components of AlphaFold2, we obtain an end-to-end differentiable\nmodel to predict the 3D coordinates of atoms from only the primary sequence.\nHelixFold-Single is validated in datasets CASP14 and CAMEO, achieving\ncompetitive accuracy with the MSA-based methods on the targets with large\nhomologous families. Furthermore, HelixFold-Single consumes much less time than\nthe mainstream pipelines for protein structure prediction, demonstrating its\npotential in tasks requiring many predictions. The code of HelixFold-Single is\navailable at\nhttps://github.com/PaddlePaddle/PaddleHelix/tree/dev/apps/protein_folding/helixfold-single,\nand we also provide stable web services on\nhttps://paddlehelix.baidu.com/app/drug/protein-single/forecast.\n",
                "链接": "https://arxiv.org/abs/2207.13921"
            },
            {
                "文章ID": "92831",
                "标题": "DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for\n  Automatic Protein Function Prediction",
                "作者": " Zihao Li,  Changkun Jiang,  Jianqiang Li",
                "发布日期": "2023-07-26",
                "摘要": "  Automatic protein function prediction (AFP) is classified as a large-scale\nmulti-label classification problem aimed at automating protein enrichment\nanalysis to eliminate the current reliance on labor-intensive wet-lab methods.\nCurrently, popular methods primarily combine protein-related information and\nGene Ontology (GO) terms to generate final functional predictions. For example,\nprotein sequences, structural information, and protein-protein interaction\nnetworks are integrated as prior knowledge to fuse with GO term embeddings and\ngenerate the ultimate prediction results. However, these methods are limited by\nthe difficulty in obtaining structural information or network topology\ninformation, as well as the accuracy of such data. Therefore, more and more\nmethods that only use protein sequences for protein function prediction have\nbeen proposed, which is a more reliable and computationally cheaper approach.\nHowever, the existing methods fail to fully extract feature information from\nprotein sequences or label data because they do not adequately consider the\nintrinsic characteristics of the data itself. Therefore, we propose a\nsequence-based hierarchical prediction method, DeepGATGO, which processes\nprotein sequences and GO term labels hierarchically, and utilizes graph\nattention networks (GATs) and contrastive learning for protein function\nprediction. Specifically, we compute embeddings of the sequence and label data\nusing pre-trained models to reduce computational costs and improve the\nembedding accuracy. Then, we use GATs to dynamically extract the structural\ninformation of non-Euclidean data, and learn general features of the label\ndataset with contrastive learning by constructing positive and negative example\nsamples. Experimental results demonstrate that our proposed model exhibits\nbetter scalability in GO term enrichment analysis on large-scale datasets.\n",
                "链接": "https://arxiv.org/abs/2307.13004"
            },
            {
                "文章ID": "83294",
                "标题": "Enhancing the Protein Tertiary Structure Prediction by Multiple Sequence\n  Alignment Generation",
                "作者": " Le Zhang,  Jiayang Chen,  Tao Shen,  Yu Li,  Siqi Sun",
                "发布日期": "2023-06-06",
                "摘要": "  The field of protein folding research has been greatly advanced by deep\nlearning methods, with AlphaFold2 (AF2) demonstrating exceptional performance\nand atomic-level precision. As co-evolution is integral to protein structure\nprediction, AF2's accuracy is significantly influenced by the depth of multiple\nsequence alignment (MSA), which requires extensive exploration of a large\nprotein database for similar sequences. However, not all protein sequences\npossess abundant homologous families, and consequently, AF2's performance can\ndegrade on such queries, at times failing to produce meaningful results. To\naddress this, we introduce a novel generative language model, MSA-Augmenter,\nwhich leverages protein-specific attention mechanisms and large-scale MSAs to\ngenerate useful, novel protein sequences not currently found in databases.\nThese sequences supplement shallow MSAs, enhancing the accuracy of structural\nproperty predictions. Our experiments on CASP14 demonstrate that MSA-Augmenter\ncan generate de novo sequences that retain co-evolutionary information from\ninferior MSAs, thereby improving protein structure prediction quality on top of\nstrong AF2.\n",
                "链接": "https://arxiv.org/abs/2306.01824"
            }
        ]
    },
    {
        "question": {
            "question": "查找使用BERT和RoBERTa进行多语言情感分析的最新论文，要求涵盖2022年以来的研究。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "52169",
                "标题": "Video Games as a Corpus: Sentiment Analysis using Fallout New Vegas\n  Dialog",
                "作者": " Mika Hämäläinen,  Khalid Alnajjar,  Thierry Poibeau",
                "发布日期": "2022-12-06",
                "摘要": "  We present a method for extracting a multilingual sentiment annotated dialog\ndata set from Fallout New Vegas. The game developers have preannotated every\nline of dialog in the game in one of the 8 different sentiments: \\textit{anger,\ndisgust, fear, happy, neutral, pained, sad } and \\textit{surprised}. The game\nhas been translated into English, Spanish, German, French and Italian. We\nconduct experiments on multilingual, multilabel sentiment analysis on the\nextracted data set using multilingual BERT, XLMRoBERTa and language specific\nBERT models. In our experiments, multilingual BERT outperformed XLMRoBERTa for\nmost of the languages, also language specific models were slightly better than\nmultilingual BERT for most of the languages. The best overall accuracy was 54\\%\nand it was achieved by using multilingual BERT on Spanish data. The extracted\ndata set presents a challenging task for sentiment analysis. We have released\nthe data, including the testing and training splits, openly on Zenodo. The data\nset has been shuffled for copyright reasons.\n",
                "链接": "https://arxiv.org/abs/2212.02168"
            },
            {
                "文章ID": "15513",
                "标题": "Mono vs Multilingual BERT for Hate Speech Detection and Text\n  Classification: A Case Study in Marathi",
                "作者": " Abhishek Velankar,  Hrushikesh Patil,  Raviraj Joshi",
                "发布日期": "2022-11-15",
                "摘要": "  Transformers are the most eminent architectures used for a vast range of\nNatural Language Processing tasks. These models are pre-trained over a large\ntext corpus and are meant to serve state-of-the-art results over tasks like\ntext classification. In this work, we conduct a comparative study between\nmonolingual and multilingual BERT models. We focus on the Marathi language and\nevaluate the models on the datasets for hate speech detection, sentiment\nanalysis and simple text classification in Marathi. We use standard\nmultilingual models such as mBERT, indicBERT and xlm-RoBERTa and compare with\nMahaBERT, MahaALBERT and MahaRoBERTa, the monolingual models for Marathi. We\nfurther show that Marathi monolingual models outperform the multilingual BERT\nvariants on five different downstream fine-tuning experiments. We also evaluate\nsentence embeddings from these models by freezing the BERT encoder layers. We\nshow that monolingual MahaBERT based models provide rich representations as\ncompared to sentence embeddings from multi-lingual counterparts. However, we\nobserve that these embeddings are not generic enough and do not work well on\nout of domain social media datasets. We consider two Marathi hate speech\ndatasets L3Cube-MahaHate, HASOC-2021, a Marathi sentiment classification\ndataset L3Cube-MahaSent, and Marathi Headline, Articles classification\ndatasets.\n",
                "链接": "https://arxiv.org/abs/2204.08669"
            },
            {
                "文章ID": "114320",
                "标题": "Modelling Sentiment Analysis: LLMs and data augmentation techniques",
                "作者": " Guillem Senabre Prades",
                "发布日期": "2023-11-08",
                "摘要": "  This paper provides different approaches for a binary sentiment\nclassification on a small training dataset. LLMs that provided state-of-the-art\nresults in sentiment analysis and similar domains are being used, such as BERT,\nRoBERTa and XLNet.\n",
                "链接": "https://arxiv.org/abs/2311.04139"
            },
            {
                "文章ID": "87606",
                "标题": "L3Cube-MahaSent-MD: A Multi-domain Marathi Sentiment Analysis Dataset\n  and Transformer Models",
                "作者": " Aabha Pingle,  Aditya Vyawahare,  Isha Joshi,  Rahul Tangsali,  Raviraj Joshi",
                "发布日期": "2023-06-27",
                "摘要": "  The exploration of sentiment analysis in low-resource languages, such as\nMarathi, has been limited due to the availability of suitable datasets. In this\nwork, we present L3Cube-MahaSent-MD, a multi-domain Marathi sentiment analysis\ndataset, with four different domains - movie reviews, general tweets, TV show\nsubtitles, and political tweets. The dataset consists of around 60,000 manually\ntagged samples covering 3 distinct sentiments - positive, negative, and\nneutral. We create a sub-dataset for each domain comprising 15k samples. The\nMahaSent-MD is the first comprehensive multi-domain sentiment analysis dataset\nwithin the Indic sentiment landscape. We fine-tune different monolingual and\nmultilingual BERT models on these datasets and report the best accuracy with\nthe MahaBERT model. We also present an extensive in-domain and cross-domain\nanalysis thus highlighting the need for low-resource multi-domain datasets. The\ndata and models are available at https://github.com/l3cube-pune/MarathiNLP .\n",
                "链接": "https://arxiv.org/abs/2306.13888"
            },
            {
                "文章ID": "31048",
                "标题": "Enhancing Collaborative Filtering Recommender with Prompt-Based\n  Sentiment Analysis",
                "作者": " Elliot Dang,  Zheyuan Hu,  Tong Li",
                "发布日期": "2022-07-27",
                "摘要": "  Collaborative Filtering(CF) recommender is a crucial application in the\nonline market and ecommerce. However, CF recommender has been proven to suffer\nfrom persistent problems related to sparsity of the user rating that will\nfurther lead to a cold-start issue. Existing methods address the data sparsity\nissue by applying token-level sentiment analysis that translate text review\ninto sentiment scores as a complement of the user rating. In this paper, we\nattempt to optimize the sentiment analysis with advanced NLP models including\nBERT and RoBERTa, and experiment on whether the CF recommender has been further\nenhanced. We build the recommenders on the Amazon US Reviews dataset, and tune\nthe pretrained BERT and RoBERTa with the traditional fine-tuned paradigm as\nwell as the new prompt-based learning paradigm. Experimental result shows that\nthe recommender enhanced with the sentiment ratings predicted by the fine-tuned\nRoBERTa has the best performance, and achieved 30.7% overall gain by comparing\nMAP, NDCG and precision at K to the baseline recommender. Prompt-based learning\nparadigm, although superior to traditional fine-tune paradigm in pure sentiment\nanalysis, fail to further improve the CF recommender.\n",
                "链接": "https://arxiv.org/abs/2207.12883"
            },
            {
                "文章ID": "74465",
                "标题": "HausaNLP at SemEval-2023 Task 12: Leveraging African Low Resource\n  TweetData for Sentiment Analysis",
                "作者": " Saheed Abdullahi Salahudeen,  Falalu Ibrahim Lawan,  Ahmad Mustapha Wali,  Amina Abubakar Imam,  Aliyu Rabiu Shuaibu,  Aliyu Yusuf,  Nur Bala Rabiu,  Musa Bello,  Shamsuddeen Umaru Adamu,  Saminu Mohammad Aliyu,  Murja Sani Gadanya,  Sanah Abdullahi Muaz,  Mahmoud Said Ahmad,  Abdulkadir Abdullahi,  Abdulmalik Yusuf Jamoh",
                "发布日期": "2023-04-27",
                "摘要": "  We present the findings of SemEval-2023 Task 12, a shared task on sentiment\nanalysis for low-resource African languages using Twitter dataset. The task\nfeatured three subtasks; subtask A is monolingual sentiment classification with\n12 tracks which are all monolingual languages, subtask B is multilingual\nsentiment classification using the tracks in subtask A and subtask C is a\nzero-shot sentiment classification. We present the results and findings of\nsubtask A, subtask B and subtask C. We also release the code on github. Our\ngoal is to leverage low-resource tweet data using pre-trained Afro-xlmr-large,\nAfriBERTa-Large, Bert-base-arabic-camelbert-da-sentiment (Arabic-camelbert),\nMultilingual-BERT (mBERT) and BERT models for sentiment analysis of 14 African\nlanguages. The datasets for these subtasks consists of a gold standard\nmulti-class labeled Twitter datasets from these languages. Our results\ndemonstrate that Afro-xlmr-large model performed better compared to the other\nmodels in most of the languages datasets. Similarly, Nigerian languages: Hausa,\nIgbo, and Yoruba achieved better performance compared to other languages and\nthis can be attributed to the higher volume of data present in the languages.\n",
                "链接": "https://arxiv.org/abs/2304.13634"
            },
            {
                "文章ID": "102384",
                "标题": "Has Sentiment Returned to the Pre-pandemic Level? A Sentiment Analysis\n  Using U.S. College Subreddit Data from 2019 to 2022",
                "作者": " Tian Yan,  Fang Liu",
                "发布日期": "2023-09-19",
                "摘要": "  As impact of COVID-19 pandemic winds down, both individuals and society\ngradually return to pre-pandemic activities. This study aims to explore how\npeople's emotions have changed from the pre-pandemic during the pandemic to\npost-emergency period and whether it has returned to pre-pandemic level. We\ncollected Reddit data in 2019 (pre-pandemic), 2020 (peak pandemic), 2021, and\n2022 (late stages of pandemic, transitioning period to post-emergency period)\nfrom subreddits in 128 universities/colleges in the U.S., and a set of\nschool-level characteristics. We predicted two sets of sentiments from a\npre-trained Robustly Optimized BERT pre-training approach (RoBERTa) and graph\nattention network (GAT) that leverages both rich semantic and relational\ninformation among posted messages and then applied a logistic stacking method\nto obtain the final sentiment classification. After obtaining sentiment label\nfor each message, we used a generalized linear mixed-effects model to estimate\ntemporal trend in sentiment from 2019 to 2022 and how school-level factors may\naffect sentiment. Compared to the year 2019, the odds of negative sentiment in\nyears 2020, 2021, and 2022 are 24%, 4.3%, and 10.3% higher, respectively, which\nare all statistically significant(adjusted $p$<0.05). Our study findings\nsuggest a partial recovery in the sentiment composition in the\npost-pandemic-emergency era. The results align with common expectations and\nprovide a detailed quantification of how sentiments have evolved from 2019 to\n2022.\n",
                "链接": "https://arxiv.org/abs/2309.08845"
            },
            {
                "文章ID": "82977",
                "标题": "UCAS-IIE-NLP at SemEval-2023 Task 12: Enhancing Generalization of\n  Multilingual BERT for Low-resource Sentiment Analysis",
                "作者": " Dou Hu,  Lingwei Wei,  Yaxin Liu,  Wei Zhou,  Songlin Hu",
                "发布日期": "2023-06-05",
                "摘要": "  This paper describes our system designed for SemEval-2023 Task 12: Sentiment\nanalysis for African languages. The challenge faced by this task is the\nscarcity of labeled data and linguistic resources in low-resource settings. To\nalleviate these, we propose a generalized multilingual system SACL-XLMR for\nsentiment analysis on low-resource languages. Specifically, we design a\nlexicon-based multilingual BERT to facilitate language adaptation and\nsentiment-aware representation learning. Besides, we apply a supervised\nadversarial contrastive learning technique to learn sentiment-spread structured\nrepresentations and enhance model generalization. Our system achieved\ncompetitive results, largely outperforming baselines on both multilingual and\nzero-shot sentiment classification subtasks. Notably, the system obtained the\n1st rank on the zero-shot classification subtask in the official ranking.\nExtensive experiments demonstrate the effectiveness of our system.\n",
                "链接": "https://arxiv.org/abs/2306.01093"
            },
            {
                "文章ID": "48125",
                "标题": "Xu at SemEval-2022 Task 4: Pre-BERT Neural Network Methods vs Post-BERT\n  RoBERTa Approach for Patronizing and Condescending Language Detection",
                "作者": " Jinghua Xu",
                "发布日期": "2022-11-15",
                "摘要": "  This paper describes my participation in the SemEval-2022 Task 4: Patronizing\nand Condescending Language Detection. I participate in both subtasks:\nPatronizing and Condescending Language (PCL) Identification and Patronizing and\nCondescending Language Categorization, with the main focus put on subtask 1.\nThe experiments compare pre-BERT neural network (NN) based systems against\npost-BERT pretrained language model RoBERTa. This research finds NN-based\nsystems in the experiments perform worse on the task compared to the pretrained\nlanguage models. The top-performing RoBERTa system is ranked 26 out of 78 teams\n(F1-score: 54.64) in subtask 1, and 23 out of 49 teams (F1-score: 30.03) in\nsubtask 2.\n",
                "链接": "https://arxiv.org/abs/2211.06874"
            },
            {
                "文章ID": "63716",
                "标题": "ChatGPT: A Meta-Analysis after 2.5 Months",
                "作者": " Christoph Leiter,  Ran Zhang,  Yanran Chen,  Jonas Belouadi,  Daniil Larionov,  Vivian Fresen,  Steffen Eger",
                "发布日期": "2023-02-28",
                "摘要": "  ChatGPT, a chatbot developed by OpenAI, has gained widespread popularity and\nmedia attention since its release in November 2022. However, little hard\nevidence is available regarding its perception in various sources. In this\npaper, we analyze over 300,000 tweets and more than 150 scientific papers to\ninvestigate how ChatGPT is perceived and discussed. Our findings show that\nChatGPT is generally viewed as of high quality, with positive sentiment and\nemotions of joy dominating in social media. Its perception has slightly\ndecreased since its debut, however, with joy decreasing and (negative) surprise\non the rise, and it is perceived more negatively in languages other than\nEnglish. In recent scientific papers, ChatGPT is characterized as a great\nopportunity across various fields including the medical domain, but also as a\nthreat concerning ethics and receives mixed assessments for education. Our\ncomprehensive meta-analysis of ChatGPT's current perception after 2.5 months\nsince its release can contribute to shaping the public debate and informing its\nfuture development. We make our data available.\n",
                "链接": "https://arxiv.org/abs/2302.13795"
            }
        ]
    },
    {
        "question": {
            "question": "找一下使用强化学习做代码生成的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "59289",
                "标题": "Execution-based Code Generation using Deep Reinforcement Learning",
                "作者": " Parshin Shojaee,  Aneesh Jain,  Sindhu Tipirneni,  Chandan K. Reddy",
                "发布日期": "2023-07-21",
                "摘要": "  The utilization of programming language (PL) models, pre-trained on\nlarge-scale code corpora, as a means of automating software engineering\nprocesses has demonstrated considerable potential in streamlining various code\ngeneration tasks such as code completion, code translation, and program\nsynthesis. However, current approaches mainly rely on supervised fine-tuning\nobjectives borrowed from text generation, neglecting unique sequence-level\ncharacteristics of code, including but not limited to compilability as well as\nsyntactic and functional correctness. To address this limitation, we propose\nPPOCoder, a new framework for code generation that synergistically combines\npre-trained PL models with Proximal Policy Optimization (PPO) which is a widely\nused deep reinforcement learning technique. By utilizing non-differentiable\nfeedback from code execution and structure alignment, PPOCoder seamlessly\nintegrates external code-specific knowledge into the model optimization\nprocess. It's important to note that PPOCoder is a task-agnostic and\nmodel-agnostic framework that can be used across different code generation\ntasks and PLs. Extensive experiments on three code generation tasks demonstrate\nthe effectiveness of our proposed approach compared to SOTA methods, achieving\nsignificant improvements in compilation success rates and functional\ncorrectness across different PLs.\n",
                "链接": "https://arxiv.org/abs/2301.13816"
            },
            {
                "文章ID": "3647",
                "标题": "Tutorial on amortized optimization",
                "作者": " Brandon Amos",
                "发布日期": "2023-04-25",
                "摘要": "  Optimization is a ubiquitous modeling tool and is often deployed in settings\nwhich repeatedly solve similar instances of the same problem. Amortized\noptimization methods use learning to predict the solutions to problems in these\nsettings, exploiting the shared structure between similar problem instances.\nThese methods have been crucial in variational inference and reinforcement\nlearning and are capable of solving optimization problems many orders of\nmagnitudes times faster than traditional optimization methods that do not use\namortization. This tutorial presents an introduction to the amortized\noptimization foundations behind these advancements and overviews their\napplications in variational inference, sparse coding, gradient-based\nmeta-learning, control, reinforcement learning, convex optimization, optimal\ntransport, and deep equilibrium networks. The source code for this tutorial is\navailable at\nhttps://github.com/facebookresearch/amortized-optimization-tutorial.\n",
                "链接": "https://arxiv.org/abs/2202.00665"
            },
            {
                "文章ID": "43952",
                "标题": "RMBench: Benchmarking Deep Reinforcement Learning for Robotic\n  Manipulator Control",
                "作者": " Yanfei Xiang,  Xin Wang,  Shu Hu,  Bin Zhu,  Xiaomeng Huang,  Xi Wu,  Siwei Lyu",
                "发布日期": "2023-03-08",
                "摘要": "  Reinforcement learning is applied to solve actual complex tasks from\nhigh-dimensional, sensory inputs. The last decade has developed a long list of\nreinforcement learning algorithms. Recent progress benefits from deep learning\nfor raw sensory signal representation. One question naturally arises: how well\ndo they perform concerning different robotic manipulation tasks? Benchmarks use\nobjective performance metrics to offer a scientific way to compare algorithms.\nIn this paper, we present RMBench, the first benchmark for robotic\nmanipulations, which have high-dimensional continuous action and state spaces.\nWe implement and evaluate reinforcement learning algorithms that directly use\nobserved pixels as inputs. We report their average performance and learning\ncurves to show their performance and stability of training. Our study concludes\nthat none of the studied algorithms can handle all tasks well, soft\nActor-Critic outperforms most algorithms in average reward and stability, and\nan algorithm combined with data augmentation may facilitate learning policies.\nOur code is publicly available at\nhttps://github.com/xiangyanfei212/RMBench-2022, including all benchmark tasks\nand studied algorithms.\n",
                "链接": "https://arxiv.org/abs/2210.11262"
            },
            {
                "文章ID": "73084",
                "标题": "How to Do Things with Deep Learning Code",
                "作者": " Minh Hua,  Rita Raley",
                "发布日期": "2023-04-20",
                "摘要": "  The premise of this article is that a basic understanding of the composition\nand functioning of large language models is critically urgent. To that end, we\nextract a representational map of OpenAI's GPT-2 with what we articulate as two\nclasses of deep learning code, that which pertains to the model and that which\nunderwrites applications built around the model. We then verify this map\nthrough case studies of two popular GPT-2 applications: the text adventure\ngame, AI Dungeon, and the language art project, This Word Does Not Exist. Such\nan exercise allows us to test the potential of Critical Code Studies when the\nobject of study is deep learning code and to demonstrate the validity of code\nas an analytical focus for researchers in the subfields of Critical Artificial\nIntelligence and Critical Machine Learning Studies. More broadly, however, our\nwork draws attention to the means by which ordinary users might interact with,\nand even direct, the behavior of deep learning systems, and by extension works\ntoward demystifying some of the auratic mystery of \"AI.\" What is at stake is\nthe possibility of achieving an informed sociotechnical consensus about the\nresponsible applications of large language models, as well as a more expansive\nsense of their creative capabilities-indeed, understanding how and where\nengagement occurs allows all of us to become more active participants in the\ndevelopment of machine learning systems.\n",
                "链接": "https://arxiv.org/abs/2304.09406"
            },
            {
                "文章ID": "27761",
                "标题": "CodeRL: Mastering Code Generation through Pretrained Models and Deep\n  Reinforcement Learning",
                "作者": " Hung Le,  Yue Wang,  Akhilesh Deepak Gotmare,  Silvio Savarese,  Steven C. H. Hoi",
                "发布日期": "2022-11-04",
                "摘要": "  Program synthesis or code generation aims to generate a program that\nsatisfies a problem specification. Recent approaches using large-scale\npretrained language models (LMs) have shown promising results, yet they have\nsome critical limitations. In particular, they often follow a standard\nsupervised fine-tuning procedure to train a code generation model only from the\npairs of natural-language problem descriptions and ground-truth programs. Such\nparadigm largely ignores some important but potentially useful signals in the\nproblem specification such as unit tests, which thus often results in poor\nperformance when solving complex unseen coding tasks. To address the\nlimitations, we propose \"CodeRL\", a new framework for program synthesis tasks\nthrough pretrained LMs and deep reinforcement learning (RL). Specifically,\nduring training, we treat the code-generating LM as an actor network, and\nintroduce a critic network that is trained to predict the functional\ncorrectness of generated programs and provide dense feedback signals to the\nactor. During inference, we introduce a new generation procedure with a\ncritical sampling strategy that allows a model to automatically regenerate\nprograms based on feedback from example unit tests and critic scores. For the\nmodel backbones, we extended the encoder-decoder architecture of CodeT5 with\nenhanced learning objectives, larger model sizes, and better pretraining data.\nOur method not only achieves new SOTA results on the challenging APPS\nbenchmark, but also shows strong zero-shot transfer capability with new SOTA\nresults on the simpler MBPP benchmark.\n",
                "链接": "https://arxiv.org/abs/2207.01780"
            },
            {
                "文章ID": "106137",
                "标题": "Reinforcement Learning from Automatic Feedback for High-Quality Unit\n  Test Generation",
                "作者": " Benjamin Steenhoek,  Michele Tufano,  Neel Sundaresan,  Alexey Svyatkovskiy",
                "发布日期": "2023-10-05",
                "摘要": "  Software testing is a crucial aspect of software development, and the\ncreation of high-quality tests that adhere to best practices is essential for\neffective maintenance. Recently, Large Language Models (LLMs) have gained\npopularity for code generation, including the automated creation of test cases.\nHowever, these LLMs are often trained on vast amounts of publicly available\ncode, which may include test cases that do not adhere to best practices and may\neven contain test smells (anti-patterns). To address this issue, we propose a\nnovel technique called Reinforcement Learning from Static Quality Metrics\n(RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show\nthat LLMs can generate undesirable test smells. Thus, we train specific reward\nmodels for each static quality metric, then utilize Proximal Policy\nOptimization (PPO) to train models for optimizing a single quality metric at a\ntime. Furthermore, we amalgamate these rewards into a unified reward model\naimed at capturing different best practices and quality aspects of tests. By\ncomparing RL-trained models with those trained using supervised learning, we\nprovide insights into how reliably utilize RL to improve test generation\nquality and into the effects of various training strategies. Our experimental\nresults demonstrate that the RL-optimized model consistently generated\nhigh-quality test cases compared to the base LLM, improving the model by up to\n21%, and successfully generates nearly 100% syntactically correct code. RLSQM\nalso outperformed GPT-4 on four out of seven metrics. This represents a\nsignificant step towards enhancing the overall efficiency and reliability of\nsoftware testing through Reinforcement Learning and static quality metrics. Our\ndata are available at this link: https://figshare.com/s/ded476c8d4c221222849.\n",
                "链接": "https://arxiv.org/abs/2310.02368"
            },
            {
                "文章ID": "32049",
                "标题": "Character Generation through Self-Supervised Vectorization",
                "作者": " Gokcen Gokceoglu,  Emre Akbas",
                "发布日期": "2022-08-04",
                "摘要": "  The prevalent approach in self-supervised image generation is to operate on\npixel level representations. While this approach can produce high quality\nimages, it cannot benefit from the simplicity and innate quality of\nvectorization. Here we present a drawing agent that operates on stroke-level\nrepresentation of images. At each time step, the agent first assesses the\ncurrent canvas and decides whether to stop or keep drawing. When a 'draw'\ndecision is made, the agent outputs a program indicating the stroke to be\ndrawn. As a result, it produces a final raster image by drawing the strokes on\na canvas, using a minimal number of strokes and dynamically deciding when to\nstop. We train our agent through reinforcement learning on MNIST and Omniglot\ndatasets for unconditional generation and parsing (reconstruction) tasks. We\nutilize our parsing agent for exemplar generation and type conditioned concept\ngeneration in Omniglot challenge without any further training. We present\nsuccessful results on all three generation tasks and the parsing task.\nCrucially, we do not need any stroke-level or vector supervision; we only use\nraster images for training.\n",
                "链接": "https://arxiv.org/abs/2208.02012"
            },
            {
                "文章ID": "5731",
                "标题": "Code Generation for Unknown Libraries via Reading API Documentations",
                "作者": " Koki Washio,  Yusuke Miyao",
                "发布日期": "2022-02-17",
                "摘要": "  Open-domain code generation is a challenging problem because the set of\nfunctions and classes that we use are frequently changed and extended in\nprogramming communities. We consider the challenge of code generation for\nunknown libraries without additional training. In this paper, we explore a\nframework of code generation that can refer to relevant API documentations like\nhuman programmers to handle unknown libraries. As a first step of this\ndirection, we implement a model that can extract relevant code signatures from\nAPI documentations based on a natural language intent and copy primitives from\nthe extracted signatures. Moreover, to evaluate code generation for unknown\nlibraries and our framework, we extend an existing dataset of open-domain code\ngeneration and resplit it so that the evaluation data consist of only examples\nusing the libraries that do not appear in the training data. Experiments on our\nnew split show that baseline encoder-decoder models cannot generate code using\nprimitives of unknown libraries as expected. In contrast, our model outperforms\nthe baseline on the new split and can properly generate unknown primitives when\nextracted code signatures are noiseless.\n",
                "链接": "https://arxiv.org/abs/2202.07806"
            },
            {
                "文章ID": "87952",
                "标题": "InterCode: Standardizing and Benchmarking Interactive Coding with\n  Execution Feedback",
                "作者": " John Yang,  Akshara Prabhakar,  Karthik Narasimhan,  Shunyu Yao",
                "发布日期": "2023-10-31",
                "摘要": "  Humans write code in a fundamentally interactive manner and rely on constant\nexecution feedback to correct errors, resolve ambiguities, and decompose tasks.\nWhile LLMs have recently exhibited promising coding capabilities, current\ncoding benchmarks mostly consider a static instruction-to-code sequence\ntransduction process, which has the potential for error propagation and a\ndisconnect between the generated code and its final execution environment. To\naddress this gap, we introduce InterCode, a lightweight, flexible, and\neasy-to-use framework of interactive coding as a standard reinforcement\nlearning (RL) environment, with code as actions and execution feedback as\nobservations. Our framework is language and platform agnostic, uses\nself-contained Docker environments to provide safe and reproducible execution,\nand is compatible out-of-the-box with traditional seq2seq coding methods, while\nenabling the development of new methods for interactive code generation. We use\nInterCode to create three interactive code environments with Bash, SQL, and\nPython as action spaces, leveraging data from the static NL2Bash, Spider, and\nMBPP datasets. We demonstrate InterCode's viability as a testbed by evaluating\nmultiple state-of-the-art LLMs configured with different prompting strategies\nsuch as ReAct and Plan & Solve. Our results showcase the benefits of\ninteractive code generation and demonstrate that InterCode can serve as a\nchallenging benchmark for advancing code understanding and generation\ncapabilities. InterCode is designed to be easily extensible and can even be\nused to create new tasks such as Capture the Flag, a popular coding puzzle that\nis inherently multi-step and involves multiple programming languages. Project\nsite with code and data: https://intercode-benchmark.github.io\n",
                "链接": "https://arxiv.org/abs/2306.14898"
            },
            {
                "文章ID": "110290",
                "标题": "Automatic Unit Test Data Generation and Actor-Critic Reinforcement\n  Learning for Code Synthesis",
                "作者": " Philip John Gorinski,  Matthieu Zimmer,  Gerasimos Lampouras,  Derrick Goh Xin Deik,  Ignacio Iacobacci",
                "发布日期": "2023-10-23",
                "摘要": "  The advent of large pre-trained language models in the domain of Code\nSynthesis has shown remarkable performance on various benchmarks, treating the\nproblem of Code Generation in a fashion similar to Natural Language Generation,\ntrained with a Language Modelling (LM) objective. In addition, the property of\nprogramming language code being precisely evaluable with respect to its\nsemantics -- through the use of Unit Tests to check its functional correctness\n-- lends itself to using Reinforcement Learning (RL) as a further training\nparadigm. Previous work has shown that RL can be applied as such to improve\nmodels' coding capabilities; however, such RL-based methods rely on a reward\nsignal based on defined Unit Tests, which are much harder to obtain compared to\nthe huge crawled code datasets used in LM objectives. In this work, we present\na novel approach to automatically obtain data consisting of function signatures\nand associated Unit Tests, suitable for RL training of Code Synthesis models.\nWe also introduce a straightforward, simple yet effective Actor-Critic RL\ntraining scheme and show that it, in conjunction with automatically generated\ntraining data, leads to improvement of a pre-trained code language model's\nperformance by up to 9.9% improvement over the original underlying code\nsynthesis LM, and up to 4.3% over RL-based models trained with standard PPO or\nCodeRL.\n",
                "链接": "https://arxiv.org/abs/2310.13669"
            }
        ]
    },
    {
        "question": {
            "question": "请找到使用自蒸馏加强目标检测性能的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "42527",
                "标题": "Composite Learning for Robust and Effective Dense Predictions",
                "作者": " Menelaos Kanakis,  Thomas E. Huang,  David Bruggemann,  Fisher Yu,  Luc Van Gool",
                "发布日期": "2022-10-14",
                "摘要": "  Multi-task learning promises better model generalization on a target task by\njointly optimizing it with an auxiliary task. However, the current practice\nrequires additional labeling efforts for the auxiliary task, while not\nguaranteeing better model performance. In this paper, we find that jointly\ntraining a dense prediction (target) task with a self-supervised (auxiliary)\ntask can consistently improve the performance of the target task, while\neliminating the need for labeling auxiliary tasks. We refer to this joint\ntraining as Composite Learning (CompL). Experiments of CompL on monocular depth\nestimation, semantic segmentation, and boundary detection show consistent\nperformance improvements in fully and partially labeled datasets. Further\nanalysis on depth estimation reveals that joint training with self-supervision\noutperforms most labeled auxiliary tasks. We also find that CompL can improve\nmodel robustness when the models are evaluated in new domains. These results\ndemonstrate the benefits of self-supervision as an auxiliary task, and\nestablish the design of novel task-specific self-supervised methods as a new\naxis of investigation for future multi-task learning research.\n",
                "链接": "https://arxiv.org/abs/2210.07239"
            },
            {
                "文章ID": "38386",
                "标题": "Sample, Crop, Track: Self-Supervised Mobile 3D Object Detection for\n  Urban Driving LiDAR",
                "作者": " Sangyun Shin,  Stuart Golodetz,  Madhu Vankadari,  Kaichen Zhou,  Andrew Markham,  Niki Trigoni",
                "发布日期": "2022-09-22",
                "摘要": "  Deep learning has led to great progress in the detection of mobile (i.e.\nmovement-capable) objects in urban driving scenes in recent years. Supervised\napproaches typically require the annotation of large training sets; there has\nthus been great interest in leveraging weakly, semi- or self-supervised methods\nto avoid this, with much success. Whilst weakly and semi-supervised methods\nrequire some annotation, self-supervised methods have used cues such as motion\nto relieve the need for annotation altogether. However, a complete absence of\nannotation typically degrades their performance, and ambiguities that arise\nduring motion grouping can inhibit their ability to find accurate object\nboundaries. In this paper, we propose a new self-supervised mobile object\ndetection approach called SCT. This uses both motion cues and expected object\nsizes to improve detection performance, and predicts a dense grid of 3D\noriented bounding boxes to improve object discovery. We significantly\noutperform the state-of-the-art self-supervised mobile object detection method\nTCR on the KITTI tracking benchmark, and achieve performance that is within 30%\nof the fully supervised PV-RCNN++ method for IoUs <= 0.5.\n",
                "链接": "https://arxiv.org/abs/2209.10471"
            },
            {
                "文章ID": "66762",
                "标题": "SeqCo-DETR: Sequence Consistency Training for Self-Supervised Object\n  Detection with Transformers",
                "作者": " Guoqiang Jin,  Fan Yang,  Mingshan Sun,  Ruyi Zhao,  Yakun Liu,  Wei Li,  Tianpeng Bao,  Liwei Wu,  Xingyu Zeng,  Rui Zhao",
                "发布日期": "2023-03-16",
                "摘要": "  Self-supervised pre-training and transformer-based networks have\nsignificantly improved the performance of object detection. However, most of\nthe current self-supervised object detection methods are built on\nconvolutional-based architectures. We believe that the transformers' sequence\ncharacteristics should be considered when designing a transformer-based\nself-supervised method for the object detection task. To this end, we propose\nSeqCo-DETR, a novel Sequence Consistency-based self-supervised method for\nobject DEtection with TRansformers. SeqCo-DETR defines a simple but effective\npretext by minimizes the discrepancy of the output sequences of transformers\nwith different image views as input and leverages bipartite matching to find\nthe most relevant sequence pairs to improve the sequence-level self-supervised\nrepresentation learning performance. Furthermore, we provide a mask-based\naugmentation strategy incorporated with the sequence consistency strategy to\nextract more representative contextual information about the object for the\nobject detection task. Our method achieves state-of-the-art results on MS COCO\n(45.8 AP) and PASCAL VOC (64.1 AP), demonstrating the effectiveness of our\napproach.\n",
                "链接": "https://arxiv.org/abs/2303.08481"
            },
            {
                "文章ID": "92646",
                "标题": "Self-Supervised Learning for Audio-Based Emotion Recognition",
                "作者": " Peranut Nimitsurachat,  Peter Washington",
                "发布日期": "2023-07-25",
                "摘要": "  Emotion recognition models using audio input data can enable the development\nof interactive systems with applications in mental healthcare, marketing,\ngaming, and social media analysis. While the field of affective computing using\naudio data is rich, a major barrier to achieve consistently high-performance\nmodels is the paucity of available training labels. Self-supervised learning\n(SSL) is a family of methods which can learn despite a scarcity of supervised\nlabels by predicting properties of the data itself. To understand the utility\nof self-supervised learning for audio-based emotion recognition, we have\napplied self-supervised learning pre-training to the classification of emotions\nfrom the CMU- MOSEI's acoustic modality. Unlike prior papers that have\nexperimented with raw acoustic data, our technique has been applied to encoded\nacoustic data. Our model is first pretrained to uncover the randomly-masked\ntimestamps of the acoustic data. The pre-trained model is then fine-tuned using\na small sample of annotated data. The performance of the final model is then\nevaluated via several evaluation metrics against a baseline deep learning model\nwith an identical backbone architecture. We find that self-supervised learning\nconsistently improves the performance of the model across all metrics. This\nwork shows the utility of self-supervised learning for affective computing,\ndemonstrating that self-supervised learning is most useful when the number of\ntraining examples is small, and that the effect is most pronounced for emotions\nwhich are easier to classify such as happy, sad and anger. This work further\ndemonstrates that self-supervised learning works when applied to embedded\nfeature representations rather than the traditional approach of pre-training on\nthe raw input space.\n",
                "链接": "https://arxiv.org/abs/2307.12343"
            },
            {
                "文章ID": "106676",
                "标题": "WLST: Weak Labels Guided Self-training for Weakly-supervised Domain\n  Adaptation on 3D Object Detection",
                "作者": " Tsung-Lin Tsou,  Tsung-Han Wu,  Winston H. Hsu",
                "发布日期": "2023-10-09",
                "摘要": "  In the field of domain adaptation (DA) on 3D object detection, most of the\nwork is dedicated to unsupervised domain adaptation (UDA). Yet, without any\ntarget annotations, the performance gap between the UDA approaches and the\nfully-supervised approach is still noticeable, which is impractical for\nreal-world applications. On the other hand, weakly-supervised domain adaptation\n(WDA) is an underexplored yet practical task that only requires few labeling\neffort on the target domain. To improve the DA performance in a cost-effective\nway, we propose a general weak labels guided self-training framework, WLST,\ndesigned for WDA on 3D object detection. By incorporating autolabeler, which\ncan generate 3D pseudo labels from 2D bounding boxes, into the existing\nself-training pipeline, our method is able to generate more robust and\nconsistent pseudo labels that would benefit the training process on the target\ndomain. Extensive experiments demonstrate the effectiveness, robustness, and\ndetector-agnosticism of our WLST framework. Notably, it outperforms previous\nstate-of-the-art methods on all evaluation tasks.\n",
                "链接": "https://arxiv.org/abs/2310.03821"
            },
            {
                "文章ID": "40642",
                "标题": "Exploring The Role of Mean Teachers in Self-supervised Masked\n  Auto-Encoders",
                "作者": " Youngwan Lee,  Jeffrey Willette,  Jonghee Kim,  Juho Lee,  Sung Ju Hwang",
                "发布日期": "2022-10-06",
                "摘要": "  Masked image modeling (MIM) has become a popular strategy for self-supervised\nlearning~(SSL) of visual representations with Vision Transformers. A\nrepresentative MIM model, the masked auto-encoder (MAE), randomly masks a\nsubset of image patches and reconstructs the masked patches given the unmasked\npatches. Concurrently, many recent works in self-supervised learning utilize\nthe student/teacher paradigm which provides the student with an additional\ntarget based on the output of a teacher composed of an exponential moving\naverage (EMA) of previous students. Although common, relatively little is known\nabout the dynamics of the interaction between the student and teacher. Through\nanalysis on a simple linear model, we find that the teacher conditionally\nremoves previous gradient directions based on feature similarities which\neffectively acts as a conditional momentum regularizer. From this analysis, we\npresent a simple SSL method, the Reconstruction-Consistent Masked Auto-Encoder\n(RC-MAE) by adding an EMA teacher to MAE. We find that RC-MAE converges faster\nand requires less memory usage than state-of-the-art self-distillation methods\nduring pre-training, which may provide a way to enhance the practicality of\nprohibitively expensive self-supervised learning of Vision Transformer models.\nAdditionally, we show that RC-MAE achieves more robustness and better\nperformance compared to MAE on downstream tasks such as ImageNet-1K\nclassification, object detection, and instance segmentation.\n",
                "链接": "https://arxiv.org/abs/2210.02077"
            },
            {
                "文章ID": "109203",
                "标题": "Factored Verification: Detecting and Reducing Hallucination in Summaries\n  of Academic Papers",
                "作者": " Charlie George,  Andreas Stuhlmüller",
                "发布日期": "2023-10-17",
                "摘要": "  Hallucination plagues even frontier LLMs--but how bad is it really for\nsummarizing academic papers? We evaluate Factored Verification, a simple\nautomated method for detecting hallucinations in abstractive summaries. This\nmethod sets a new SotA on hallucination detection in the summarization task of\nthe HaluEval benchmark, achieving 76.2% accuracy. We then use this method to\nestimate how often language models hallucinate when summarizing across multiple\nacademic papers and find 0.62 hallucinations in the average ChatGPT (16k)\nsummary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct\nusing Factored Critiques and find that this lowers the number of hallucinations\nto 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations\nwe find are often subtle, so we advise caution when using models to synthesize\nacademic papers.\n",
                "链接": "https://arxiv.org/abs/2310.10627"
            },
            {
                "文章ID": "3850",
                "标题": "Understanding Cross-Domain Few-Shot Learning Based on Domain Similarity\n  and Few-Shot Difficulty",
                "作者": " Jaehoon Oh,  Sungnyun Kim,  Namgyu Ho,  Jin-Hwa Kim,  Hwanjun Song,  Se-Young Yun",
                "发布日期": "2022-10-13",
                "摘要": "  Cross-domain few-shot learning (CD-FSL) has drawn increasing attention for\nhandling large differences between the source and target domains--an important\nconcern in real-world scenarios. To overcome these large differences, recent\nworks have considered exploiting small-scale unlabeled data from the target\ndomain during the pre-training stage. This data enables self-supervised\npre-training on the target domain, in addition to supervised pre-training on\nthe source domain. In this paper, we empirically investigate which pre-training\nis preferred based on domain similarity and few-shot difficulty of the target\ndomain. We discover that the performance gain of self-supervised pre-training\nover supervised pre-training becomes large when the target domain is dissimilar\nto the source domain, or the target domain itself has low few-shot difficulty.\nWe further design two pre-training schemes, mixed-supervised and two-stage\nlearning, that improve performance. In this light, we present six findings for\nCD-FSL, which are supported by extensive experiments and analyses on three\nsource and eight target benchmark datasets with varying levels of domain\nsimilarity and few-shot difficulty. Our code is available at\nhttps://github.com/sungnyun/understanding-cdfsl.\n",
                "链接": "https://arxiv.org/abs/2202.01339"
            },
            {
                "文章ID": "85973",
                "标题": "A Comparison of Self-Supervised Pretraining Approaches for Predicting\n  Disease Risk from Chest Radiograph Images",
                "作者": " Yanru Chen,  Michael T Lu,  Vineet K Raghu",
                "发布日期": "2023-06-16",
                "摘要": "  Deep learning is the state-of-the-art for medical imaging tasks, but requires\nlarge, labeled datasets. For risk prediction, large datasets are rare since\nthey require both imaging and follow-up (e.g., diagnosis codes). However, the\nrelease of publicly available imaging data with diagnostic labels presents an\nopportunity for self and semi-supervised approaches to improve label efficiency\nfor risk prediction. Though several studies have compared self-supervised\napproaches in natural image classification, object detection, and medical image\ninterpretation, there is limited data on which approaches learn robust\nrepresentations for risk prediction. We present a comparison of semi- and\nself-supervised learning to predict mortality risk using chest x-ray images. We\nfind that a semi-supervised autoencoder outperforms contrastive and transfer\nlearning in internal and external validation.\n",
                "链接": "https://arxiv.org/abs/2306.08955"
            },
            {
                "文章ID": "26873",
                "标题": "EBMs vs. CL: Exploring Self-Supervised Visual Pretraining for Visual\n  Question Answering",
                "作者": " Violetta Shevchenko,  Ehsan Abbasnejad,  Anthony Dick,  Anton van den Hengel,  Damien Teney",
                "发布日期": "2022-06-30",
                "摘要": "  The availability of clean and diverse labeled data is a major roadblock for\ntraining models on complex tasks such as visual question answering (VQA). The\nextensive work on large vision-and-language models has shown that\nself-supervised learning is effective for pretraining multimodal interactions.\nIn this technical report, we focus on visual representations. We review and\nevaluate self-supervised methods to leverage unlabeled images and pretrain a\nmodel, which we then fine-tune on a custom VQA task that allows controlled\nevaluation and diagnosis. We compare energy-based models (EBMs) with\ncontrastive learning (CL). While EBMs are growing in popularity, they lack an\nevaluation on downstream tasks. We find that both EBMs and CL can learn\nrepresentations from unlabeled images that enable training a VQA model on very\nlittle annotated data. In a simple setting similar to CLEVR, we find that CL\nrepresentations also improve systematic generalization, and even match the\nperformance of representations from a larger, supervised, ImageNet-pretrained\nmodel. However, we find EBMs to be difficult to train because of instabilities\nand high variability in their results. Although EBMs prove useful for OOD\ndetection, other results on supervised energy-based training and uncertainty\ncalibration are largely negative. Overall, CL currently seems a preferable\noption over EBMs.\n",
                "链接": "https://arxiv.org/abs/2206.14355"
            }
        ]
    },
    {
        "question": {
            "question": "2023年后利用hotpotqa数据集做问题生成任务的论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "5376",
                "标题": "QA4QG: Using Question Answering to Constrain Multi-Hop Question\n  Generation",
                "作者": " Dan Su,  Peng Xu,  Pascale Fung",
                "发布日期": "2022-02-15",
                "摘要": "  Multi-hop question generation (MQG) aims to generate complex questions which\nrequire reasoning over multiple pieces of information of the input passage.\nMost existing work on MQG has focused on exploring graph-based networks to\nequip the traditional Sequence-to-sequence framework with reasoning ability.\nHowever, these models do not take full advantage of the constraint between\nquestions and answers. Furthermore, studies on multi-hop question answering\n(QA) suggest that Transformers can replace the graph structure for multi-hop\nreasoning. Therefore, in this work, we propose a novel framework, QA4QG, a\nQA-augmented BART-based framework for MQG. It augments the standard BART model\nwith an additional multi-hop QA module to further constrain the generated\nquestion. Our results on the HotpotQA dataset show that QA4QG outperforms all\nstate-of-the-art models, with an increase of 8 BLEU-4 and 8 ROUGE points\ncompared to the best results previously reported. Our work suggests the\nadvantage of introducing pre-trained language models and QA module for the MQG\ntask.\n",
                "链接": "https://arxiv.org/abs/2202.06538"
            },
            {
                "文章ID": "10231",
                "标题": "Ask to Understand: Question Generation for Multi-hop Question Answering",
                "作者": " Jiawei Li,  Mucheng Ren,  Yang Gao,  Yizhe Yang",
                "发布日期": "2022-03-18",
                "摘要": "  Multi-hop Question Answering (QA) requires the machine to answer complex\nquestions by finding scattering clues and reasoning from multiple documents.\nGraph Network (GN) and Question Decomposition (QD) are two common approaches at\npresent. The former uses the \"black-box\" reasoning process to capture the\npotential relationship between entities and sentences, thus achieving good\nperformance. At the same time, the latter provides a clear reasoning logical\nroute by decomposing multi-hop questions into simple single-hop sub-questions.\nIn this paper, we propose a novel method to complete multi-hop QA from the\nperspective of Question Generation (QG). Specifically, we carefully design an\nend-to-end QG module on the basis of a classical QA module, which could help\nthe model understand the context by asking inherently logical sub-questions,\nthus inheriting interpretability from the QD-based method and showing superior\nperformance. Experiments on the HotpotQA dataset demonstrate that the\neffectiveness of our proposed QG module, human evaluation further clarifies its\ninterpretability quantitatively, and thorough analysis shows that the QG module\ncould generate better sub-questions than QD methods in terms of fluency,\nconsistency, and diversity.\n",
                "链接": "https://arxiv.org/abs/2203.09073"
            },
            {
                "文章ID": "58619",
                "标题": "Graph Attention with Hierarchies for Multi-hop Question Answering",
                "作者": " Yunjie He,  Philip John Gorinski,  Ieva Staliunaite,  Pontus Stenetorp",
                "发布日期": "2023-01-30",
                "摘要": "  Multi-hop QA (Question Answering) is the task of finding the answer to a\nquestion across multiple documents. In recent years, a number of Deep\nLearning-based approaches have been proposed to tackle this complex task, as\nwell as a few standard benchmarks to assess models Multi-hop QA capabilities.\nIn this paper, we focus on the well-established HotpotQA benchmark dataset,\nwhich requires models to perform answer span extraction as well as support\nsentence prediction. We present two extensions to the SOTA Graph Neural Network\n(GNN) based model for HotpotQA, Hierarchical Graph Network (HGN): (i) we\ncomplete the original hierarchical structure by introducing new edges between\nthe query and context sentence nodes; (ii) in the graph propagation step, we\npropose a novel extension to Hierarchical Graph Attention Network GATH (Graph\nATtention with Hierarchies) that makes use of the graph hierarchy to update the\nnode representations in a sequential fashion. Experiments on HotpotQA\ndemonstrate the efficiency of the proposed modifications and support our\nassumptions about the effects of model related variables.\n",
                "链接": "https://arxiv.org/abs/2301.11792"
            },
            {
                "文章ID": "20775",
                "标题": "From Easy to Hard: Two-stage Selector and Reader for Multi-hop Question\n  Answering",
                "作者": " Xin-Yi Li,  Wei-Jun Lei,  Yu-Bin Yang",
                "发布日期": "2022-05-25",
                "摘要": "  Multi-hop question answering (QA) is a challenging task requiring QA systems\nto perform complex reasoning over multiple documents and provide supporting\nfacts together with the exact answer. Existing works tend to utilize\ngraph-based reasoning and question decomposition to obtain the reasoning chain,\nwhich inevitably introduces additional complexity and cumulative error to the\nsystem. To address the above issue, we propose a simple yet effective novel\nframework, From Easy to Hard (FE2H), to remove distracting information and\nobtain better contextual representations for the multi-hop QA task. Inspired by\nthe iterative document selection process and the progressive learning custom of\nhumans, FE2H divides both the document selector and reader into two stages\nfollowing an easy-to-hard manner. Specifically, we first select the document\nmost relevant to the question and then utilize the question together with this\ndocument to select other pertinent documents. As for the QA phase, our reader\nis first trained on a single-hop QA dataset and then transferred into the\nmulti-hop QA task. We comprehensively evaluate our model on the popular\nmulti-hop QA benchmark HotpotQA. Experimental results demonstrate that our\nmethod ourperforms all other methods in the leaderboard of HotpotQA (distractor\nsetting).\n",
                "链接": "https://arxiv.org/abs/2205.11729"
            },
            {
                "文章ID": "73844",
                "标题": "IslamicPCQA: A Dataset for Persian Multi-hop Complex Question Answering\n  in Islamic Text Resources",
                "作者": " Arash Ghafouri,  Hasan Naderi,  Mohammad Aghajani asl,  Mahdi Firouzmandi",
                "发布日期": "2023-04-25",
                "摘要": "  Nowadays, one of the main challenges for Question Answering Systems is to\nanswer complex questions using various sources of information. Multi-hop\nquestions are a type of complex questions that require multi-step reasoning to\nanswer. In this article, the IslamicPCQA dataset is introduced. This is the\nfirst Persian dataset for answering complex questions based on non-structured\ninformation sources and consists of 12,282 question-answer pairs extracted from\n9 Islamic encyclopedias. This dataset has been created inspired by the HotpotQA\nEnglish dataset approach, which was customized to suit the complexities of the\nPersian language. Answering questions in this dataset requires more than one\nparagraph and reasoning. The questions are not limited to any prior knowledge\nbase or ontology, and to provide robust reasoning ability, the dataset also\nincludes supporting facts and key sentences. The prepared dataset covers a wide\nrange of Islamic topics and aims to facilitate answering complex Persian\nquestions within this subject matter\n",
                "链接": "https://arxiv.org/abs/2304.11664"
            },
            {
                "文章ID": "25087",
                "标题": "Interpretable AMR-Based Question Decomposition for Multi-hop Question\n  Answering",
                "作者": " Zhenyun Deng,  Yonghua Zhu,  Yang Chen,  Michael Witbrock,  Patricia Riddle",
                "发布日期": "2022-08-29",
                "摘要": "  Effective multi-hop question answering (QA) requires reasoning over multiple\nscattered paragraphs and providing explanations for answers. Most existing\napproaches cannot provide an interpretable reasoning process to illustrate how\nthese models arrive at an answer. In this paper, we propose a Question\nDecomposition method based on Abstract Meaning Representation (QDAMR) for\nmulti-hop QA, which achieves interpretable reasoning by decomposing a multi-hop\nquestion into simpler sub-questions and answering them in order. Since\nannotating the decomposition is expensive, we first delegate the complexity of\nunderstanding the multi-hop question to an AMR parser. We then achieve the\ndecomposition of a multi-hop question via segmentation of the corresponding AMR\ngraph based on the required reasoning type. Finally, we generate sub-questions\nusing an AMR-to-Text generation model and answer them with an off-the-shelf QA\nmodel. Experimental results on HotpotQA demonstrate that our approach is\ncompetitive for interpretable reasoning and that the sub-questions generated by\nQDAMR are well-formed, outperforming existing question-decomposition-based\nmulti-hop QA approaches.\n",
                "链接": "https://arxiv.org/abs/2206.08486"
            },
            {
                "文章ID": "76360",
                "标题": "SkillQG: Learning to Generate Question for Reading Comprehension\n  Assessment",
                "作者": " Xiaoqiang Wang,  Bang Liu,  Siliang Tang,  Lingfei Wu",
                "发布日期": "2023-05-09",
                "摘要": "  We present $\\textbf{$\\texttt{SkillQG}$}$: a question generation framework\nwith controllable comprehension types for assessing and improving machine\nreading comprehension models. Existing question generation systems widely\ndifferentiate questions by $\\textit{literal}$ information such as question\nwords and answer types to generate semantically relevant questions for a given\ncontext. However, they rarely consider the $\\textit{comprehension}$ nature of\nquestions, i.e. the different comprehension capabilities embodied by different\nquestions. In comparison, our $\\texttt{SkillQG}$ is able to tailor a\nfine-grained assessment and improvement to the capabilities of question\nanswering models built on it. Specifically, we first frame the comprehension\ntype of questions based on a hierarchical skill-based schema, then formulate\n$\\texttt{SkillQG}$ as a skill-conditioned question generator. Furthermore, to\nimprove the controllability of generation, we augment the input text with\nquestion focus and skill-specific knowledge, which are constructed by\niteratively prompting the pre-trained language models. Empirical results\ndemonstrate that $\\texttt{SkillQG}$ outperforms baselines in terms of quality,\nrelevance, and skill-controllability while showing a promising performance\nboost in downstream question answering task.\n",
                "链接": "https://arxiv.org/abs/2305.04737"
            },
            {
                "文章ID": "115380",
                "标题": "A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question\n  Decomposition with Large Language Models",
                "作者": " Hejing Cao,  Zhenwei An,  Jiazhan Feng,  Kun Xu,  Liwei Chen,  Dongyan Zhao",
                "发布日期": "2023-11-14",
                "摘要": "  While large language models exhibit remarkable performance in the Question\nAnswering task, they are susceptible to hallucinations. Challenges arise when\nthese models grapple with understanding multi-hop relations in complex\nquestions or lack the necessary knowledge for a comprehensive response. To\naddress this issue, we introduce the \"Decompose-and-Query\" framework (D&Q).\nThis framework guides the model to think and utilize external knowledge similar\nto ReAct, while also restricting its thinking to reliable information,\neffectively mitigating the risk of hallucinations. Experiments confirm the\neffectiveness of D&Q: On our ChitChatQA dataset, D&Q does not lose to ChatGPT\nin 67% of cases; on the HotPotQA question-only setting, D&Q achieved an F1\nscore of 59.6%. Our code is available at\nhttps://github.com/alkaidpku/DQ-ToolQA.\n",
                "链接": "https://arxiv.org/abs/2311.07491"
            },
            {
                "文章ID": "109543",
                "标题": "What is a good question? Task-oriented asking with fact-level masking",
                "作者": " Matthew Toles,  Yukun Huang,  Zhou Yu,  Luis Gravano",
                "发布日期": "2023-10-19",
                "摘要": "  Asking questions is an important element of real-life collaboration on\nreasoning tasks like question answering. For example, a legal assistant chatbot\nmay be unable to make accurate recommendations without specific information on\nthe user's circumstances. However, large language models are usually deployed\nto solve reasoning tasks directly without asking follow-up questions to the\nuser or third parties. We term this problem task-oriented asking (TOA).\nZero-shot chat models can perform TOA, but their training is primarily based on\nnext-token prediction rather than whether questions contribute to successful\ncollaboration. To enable the training and evaluation of TOA models, we present\na definition and framework for natural language task-oriented asking, the\nproblem of generating questions that result in answers useful for a reasoning\ntask. We also present fact-level masking (FLM), a procedure for converting\nnatural language datasets into self-supervised TOA datasets by omitting\nparticular critical facts. Finally, we generate a TOA dataset from the HotpotQA\ndataset using FLM and evaluate several zero-shot language models on it. Our\nexperiments show that current zero-shot models struggle to ask questions that\nretrieve useful information, as compared to human annotators. These results\ndemonstrate an opportunity to use FLM datasets and the TOA framework to train\nand evaluate better TOA models.\n",
                "链接": "https://arxiv.org/abs/2310.11571"
            },
            {
                "文章ID": "34341",
                "标题": "Locate Then Ask: Interpretable Stepwise Reasoning for Multi-hop Question\n  Answering",
                "作者": " Siyuan Wang,  Zhongyu Wei,  Zhihao Fan,  Qi Zhang,  Xuanjing Huang",
                "发布日期": "2022-08-23",
                "摘要": "  Multi-hop reasoning requires aggregating multiple documents to answer a\ncomplex question. Existing methods usually decompose the multi-hop question\ninto simpler single-hop questions to solve the problem for illustrating the\nexplainable reasoning process. However, they ignore grounding on the supporting\nfacts of each reasoning step, which tends to generate inaccurate\ndecompositions. In this paper, we propose an interpretable stepwise reasoning\nframework to incorporate both single-hop supporting sentence identification and\nsingle-hop question generation at each intermediate step, and utilize the\ninference of the current hop for the next until reasoning out the final result.\nWe employ a unified reader model for both intermediate hop reasoning and final\nhop inference and adopt joint optimization for more accurate and robust\nmulti-hop reasoning. We conduct experiments on two benchmark datasets HotpotQA\nand 2WikiMultiHopQA. The results show that our method can effectively boost\nperformance and also yields a better interpretable reasoning process without\ndecomposition supervision.\n",
                "链接": "https://arxiv.org/abs/2208.10297"
            }
        ]
    },
    {
        "question": {
            "question": "查找关于深度学习在医学影像分析中的最新研究。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "90244",
                "标题": "Invariant Scattering Transform for Medical Imaging",
                "作者": " Nafisa Labiba Ishrat Huda,  Angona Biswas,  MD Abdullah Al Nasim,  Md. Fahim Rahman,  Shoaib Ahmed",
                "发布日期": "2023-07-12",
                "摘要": "  Invariant scattering transform introduces new area of research that merges\nthe signal processing with deep learning for computer vision. Nowadays, Deep\nLearning algorithms are able to solve a variety of problems in medical sector.\nMedical images are used to detect diseases brain cancer or tumor, Alzheimer's\ndisease, breast cancer, Parkinson's disease and many others. During pandemic\nback in 2020, machine learning and deep learning has played a critical role to\ndetect COVID-19 which included mutation analysis, prediction, diagnosis and\ndecision making. Medical images like X-ray, MRI known as magnetic resonance\nimaging, CT scans are used for detecting diseases. There is another method in\ndeep learning for medical imaging which is scattering transform. It builds\nuseful signal representation for image classification. It is a wavelet\ntechnique; which is impactful for medical image classification problems. This\nresearch article discusses scattering transform as the efficient system for\nmedical image analysis where it's figured by scattering the signal information\nimplemented in a deep convolutional network. A step by step case study is\nmanifested at this research work.\n",
                "链接": "https://arxiv.org/abs/2307.04771"
            },
            {
                "文章ID": "18063",
                "标题": "Super Images -- A New 2D Perspective on 3D Medical Imaging Analysis",
                "作者": " Ikboljon Sobirov,  Numan Saeed,  Mohammad Yaqub",
                "发布日期": "2023-05-18",
                "摘要": "  In medical imaging analysis, deep learning has shown promising results. We\nfrequently rely on volumetric data to segment medical images, necessitating the\nuse of 3D architectures, which are commended for their capacity to capture\ninterslice context. However, because of the 3D convolutions, max pooling,\nup-convolutions, and other operations utilized in these networks, these\narchitectures are often more inefficient in terms of time and computation than\ntheir 2D equivalents. Furthermore, there are few 3D pretrained model weights,\nand pretraining is often difficult. We present a simple yet effective 2D method\nto handle 3D data while efficiently embedding the 3D knowledge during training.\nWe propose transforming volumetric data into 2D super images and segmenting\nwith 2D networks to solve these challenges. Our method generates a\nsuper-resolution image by stitching slices side by side in the 3D image. We\nexpect deep neural networks to capture and learn these properties spatially\ndespite losing depth information. This work aims to present a novel perspective\nwhen dealing with volumetric data, and we test the hypothesis using CNN and ViT\nnetworks as well as self-supervised pretraining. While attaining equal, if not\nsuperior, results to 3D networks utilizing only 2D counterparts, the model\ncomplexity is reduced by around threefold. Because volumetric data is\nrelatively scarce, we anticipate that our approach will entice more studies,\nparticularly in medical imaging analysis.\n",
                "链接": "https://arxiv.org/abs/2205.02847"
            },
            {
                "文章ID": "117162",
                "标题": "GMISeg: General Medical Image Segmentation without Re-Training",
                "作者": " Jing Xu",
                "发布日期": "2023-11-22",
                "摘要": "  Although deep learning models have become the main method for medical image\nsegmentation, they often cannot be extended to unknown segmentation tasks\ninvolving new anatomical structures, image shapes, or labels. For new\nsegmentation tasks, researchers often have to retrain or fine-tune the model,\nwhich is time-consuming and poses a significant obstacle to clinical\nresearchers, who often lack the resources and professional knowledge to train\nneural networks. Therefore, we proposed a general method that can solve unknown\nmedical image segmentation tasks without requiring additional training. Given\nan example set of images and prompts for defining new segmentation tasks,\nGMISeg applies a novel low-rank fine-tuning strategy based on the proposed\napproach to the SAM (Segment Anything Model) image encoder, and works with the\nprompt encoder and mask decoder to fine-tune the labeled dataset without the\nneed for additional training. To achieve generalization of new tasks, we used\nmedical image datasets with different imaging modes for different parts. We\ntrained and generalized GMISeg on a different set of anatomical and imaging\nmodes using cardiac images on other site datasets. We have demonstrated that\nGMISeg outperforms the latest methods on unknown tasks and have conducted a\ncomprehensive analysis and summary of the important performance of the proposed\nmethod.\n",
                "链接": "https://arxiv.org/abs/2311.12539"
            },
            {
                "文章ID": "103258",
                "标题": "A Systematic Review of Few-Shot Learning in Medical Imaging",
                "作者": " Eva Pachetti,  Sara Colantonio",
                "发布日期": "2023-09-21",
                "摘要": "  The lack of annotated medical images limits the performance of deep learning\nmodels, which usually need large-scale labelled datasets. Few-shot learning\ntechniques can reduce data scarcity issues and enhance medical image analysis,\nespecially with meta-learning. This systematic review gives a comprehensive\noverview of few-shot learning in medical imaging. We searched the literature\nsystematically and selected 80 relevant articles published from 2018 to 2023.\nWe clustered the articles based on medical outcomes, such as tumour\nsegmentation, disease classification, and image registration; anatomical\nstructure investigated (i.e. heart, lung, etc.); and the meta-learning method\nused. For each cluster, we examined the papers' distributions and the results\nprovided by the state-of-the-art. In addition, we identified a generic pipeline\nshared among all the studies. The review shows that few-shot learning can\novercome data scarcity in most outcomes and that meta-learning is a popular\nchoice to perform few-shot learning because it can adapt to new tasks with few\nlabelled samples. In addition, following meta-learning, supervised learning and\nsemi-supervised learning stand out as the predominant techniques employed to\ntackle few-shot learning challenges in medical imaging and also best\nperforming. Lastly, we observed that the primary application areas\npredominantly encompass cardiac, pulmonary, and abdominal domains. This\nsystematic review aims to inspire further research to improve medical image\nanalysis and patient care.\n",
                "链接": "https://arxiv.org/abs/2309.11433"
            },
            {
                "文章ID": "103759",
                "标题": "Performance Analysis of UNet and Variants for Medical Image Segmentation",
                "作者": " Walid Ehab,  Yongmin Li",
                "发布日期": "2023-09-25",
                "摘要": "  Medical imaging plays a crucial role in modern healthcare by providing\nnon-invasive visualisation of internal structures and abnormalities, enabling\nearly disease detection, accurate diagnosis, and treatment planning. This study\naims to explore the application of deep learning models, particularly focusing\non the UNet architecture and its variants, in medical image segmentation. We\nseek to evaluate the performance of these models across various challenging\nmedical image segmentation tasks, addressing issues such as image\nnormalization, resizing, architecture choices, loss function design, and\nhyperparameter tuning. The findings reveal that the standard UNet, when\nextended with a deep network layer, is a proficient medical image segmentation\nmodel, while the Res-UNet and Attention Res-UNet architectures demonstrate\nsmoother convergence and superior performance, particularly when handling fine\nimage details. The study also addresses the challenge of high class imbalance\nthrough careful preprocessing and loss function definitions. We anticipate that\nthe results of this study will provide useful insights for researchers seeking\nto apply these models to new medical imaging problems and offer guidance and\nbest practices for their implementation.\n",
                "链接": "https://arxiv.org/abs/2309.13013"
            },
            {
                "文章ID": "85091",
                "标题": "Online learning for X-ray, CT or MRI",
                "作者": " Mosabbir Bhuiyan,  MD Abdullah Al Nasim,  Sarwar Saif,  Dr. Kishor Datta Gupta,  Md Jahangir Alam,  Sajedul Talukder",
                "发布日期": "2023-06-13",
                "摘要": "  Medical imaging plays an important role in the medical sector in identifying\ndiseases. X-ray, computed tomography (CT) scans, and magnetic resonance imaging\n(MRI) are a few examples of medical imaging. Most of the time, these imaging\ntechniques are utilized to examine and diagnose diseases. Medical professionals\nidentify the problem after analyzing the images. However, manual identification\ncan be challenging because the human eye is not always able to recognize\ncomplex patterns in an image. Because of this, it is difficult for any\nprofessional to recognize a disease with rapidity and accuracy. In recent\nyears, medical professionals have started adopting Computer-Aided Diagnosis\n(CAD) systems to evaluate medical images. This system can analyze the image and\ndetect the disease very precisely and quickly. However, this system has certain\ndrawbacks in that it needs to be processed before analysis. Medical research is\nalready entered a new era of research which is called Artificial Intelligence\n(AI). AI can automatically find complex patterns from an image and identify\ndiseases. Methods for medical imaging that uses AI techniques will be covered\nin this chapter.\n",
                "链接": "https://arxiv.org/abs/2306.06491"
            },
            {
                "文章ID": "60401",
                "标题": "Improving CT Image Segmentation Accuracy Using StyleGAN Driven Data\n  Augmentation",
                "作者": " Soham Bhosale,  Arjun Krishna,  Ge Wang,  Klaus Mueller",
                "发布日期": "2023-02-08",
                "摘要": "  Medical Image Segmentation is a useful application for medical image analysis\nincluding detecting diseases and abnormalities in imaging modalities such as\nMRI, CT etc. Deep learning has proven to be promising for this task but usually\nhas a low accuracy because of the lack of appropriate publicly available\nannotated or segmented medical datasets. In addition, the datasets that are\navailable may have a different texture because of different dosage values or\nscanner properties than the images that need to be segmented. This paper\npresents a StyleGAN-driven approach for segmenting publicly available large\nmedical datasets by using readily available extremely small annotated datasets\nin similar modalities. The approach involves augmenting the small segmented\ndataset and eliminating texture differences between the two datasets. The\ndataset is augmented by being passed through six different StyleGANs that are\ntrained on six different style images taken from the large non-annotated\ndataset we want to segment. Specifically, style transfer is used to augment the\ntraining dataset. The annotations of the training dataset are hence combined\nwith the textures of the non-annotated dataset to generate new anatomically\nsound images. The augmented dataset is then used to train a U-Net segmentation\nnetwork which displays a significant improvement in the segmentation accuracy\nin segmenting the large non-annotated dataset.\n",
                "链接": "https://arxiv.org/abs/2302.03285"
            },
            {
                "文章ID": "72003",
                "标题": "Automated computed tomography and magnetic resonance imaging\n  segmentation using deep learning: a beginner's guide",
                "作者": " Diedre Carmo,  Gustavo Pinheiro,  Lívia Rodrigues,  Thays Abreu,  Roberto Lotufo,  Letícia Rittner",
                "发布日期": "2023-04-13",
                "摘要": "  Medical image segmentation is an increasingly popular area of research in\nmedical imaging processing and analysis. However, many researchers who are new\nto the field struggle with basic concepts. This tutorial paper aims to provide\nan overview of the fundamental concepts of medical imaging, with a focus on\nMagnetic Resonance and Computerized Tomography. We will also discuss deep\nlearning algorithms, tools, and frameworks used for segmentation tasks, and\nsuggest best practices for method development and image analysis. Our tutorial\nincludes sample tasks using public data, and accompanying code is available on\nGitHub (https://github.com/MICLab-Unicamp/Medical-ImagingTutorial). By sharing\nour insights gained from years of experience in the field and learning from\nrelevant literature, we hope to assist researchers in overcoming the initial\nchallenges they may encounter in this exciting and important area of research.\n",
                "链接": "https://arxiv.org/abs/2304.05901"
            },
            {
                "文章ID": "69437",
                "标题": "Medical Image Analysis using Deep Relational Learning",
                "作者": " Zhihua Liu",
                "发布日期": "2023-03-29",
                "摘要": "  In the past ten years, with the help of deep learning, especially the rapid\ndevelopment of deep neural networks, medical image analysis has made remarkable\nprogress. However, how to effectively use the relational information between\nvarious tissues or organs in medical images is still a very challenging\nproblem, and it has not been fully studied. In this thesis, we propose two\nnovel solutions to this problem based on deep relational learning. First, we\npropose a context-aware fully convolutional network that effectively models\nimplicit relation information between features to perform medical image\nsegmentation. The network achieves the state-of-the-art segmentation results on\nthe Multi Modal Brain Tumor Segmentation 2017 (BraTS2017) and Multi Modal Brain\nTumor Segmentation 2018 (BraTS2018) data sets. Subsequently, we propose a new\nhierarchical homography estimation network to achieve accurate medical image\nmosaicing by learning the explicit spatial relationship between adjacent\nframes. We use the UCL Fetoscopy Placenta dataset to conduct experiments and\nour hierarchical homography estimation network outperforms the other\nstate-of-the-art mosaicing methods while generating robust and meaningful\nmosaicing result on unseen frames.\n",
                "链接": "https://arxiv.org/abs/2303.16099"
            },
            {
                "文章ID": "68755",
                "标题": "Adversarial Attack and Defense for Medical Image Analysis: Methods and\n  Applications",
                "作者": " Junhao Dong,  Junxi Chen,  Xiaohua Xie,  Jianhuang Lai,  Hao Chen",
                "发布日期": "2023-03-27",
                "摘要": "  Deep learning techniques have achieved superior performance in computer-aided\nmedical image analysis, yet they are still vulnerable to imperceptible\nadversarial attacks, resulting in potential misdiagnosis in clinical practice.\nOppositely, recent years have also witnessed remarkable progress in defense\nagainst these tailored adversarial examples in deep medical diagnosis systems.\nIn this exposition, we present a comprehensive survey on recent advances in\nadversarial attack and defense for medical image analysis with a novel taxonomy\nin terms of the application scenario. We also provide a unified theoretical\nframework for different types of adversarial attack and defense methods for\nmedical image analysis. For a fair comparison, we establish a new benchmark for\nadversarially robust medical diagnosis models obtained by adversarial training\nunder various scenarios. To the best of our knowledge, this is the first survey\npaper that provides a thorough evaluation of adversarially robust medical\ndiagnosis models. By analyzing qualitative and quantitative results, we\nconclude this survey with a detailed discussion of current challenges for\nadversarial attack and defense in medical image analysis systems to shed light\non future research directions.\n",
                "链接": "https://arxiv.org/abs/2303.14133"
            }
        ]
    },
    {
        "question": {
            "question": "查找基于优化实现模型越狱的文献",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "108400",
                "标题": "Jailbreaking Black Box Large Language Models in Twenty Queries",
                "作者": " Patrick Chao,  Alexander Robey,  Edgar Dobriban,  Hamed Hassani,  George J. Pappas,  Eric Wong",
                "发布日期": "2023-10-17",
                "摘要": "  There is growing interest in ensuring that large language models (LLMs) align\nwith human values. However, the alignment of such models is vulnerable to\nadversarial jailbreaks, which coax LLMs into overriding their safety\nguardrails. The identification of these vulnerabilities is therefore\ninstrumental in understanding inherent weaknesses and preventing future misuse.\nTo this end, we propose Prompt Automatic Iterative Refinement (PAIR), an\nalgorithm that generates semantic jailbreaks with only black-box access to an\nLLM. PAIR -- which is inspired by social engineering attacks -- uses an\nattacker LLM to automatically generate jailbreaks for a separate targeted LLM\nwithout human intervention. In this way, the attacker LLM iteratively queries\nthe target LLM to update and refine a candidate jailbreak. Empirically, PAIR\noften requires fewer than twenty queries to produce a jailbreak, which is\norders of magnitude more efficient than existing algorithms. PAIR also achieves\ncompetitive jailbreaking success rates and transferability on open and\nclosed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.\n",
                "链接": "https://arxiv.org/abs/2310.08419"
            },
            {
                "文章ID": "115646",
                "标题": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can\n  Fool Large Language Models Easily",
                "作者": " Peng Ding,  Jun Kuang,  Dan Ma,  Xuezhi Cao,  Yunsen Xian,  Jiajun Chen,  Shujian Huang",
                "发布日期": "2023-11-15",
                "摘要": "  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate harmful\ncontent. Exploring jailbreak prompts can help to better reveal the weaknesses\nof LLMs and further steer us to secure them. Unfortunately, existing jailbreak\nmethods either suffer from intricate manual design or require optimization on\nanother white-box model, compromising generalization or jailbreak efficiency.\nIn this paper, we generalize jailbreak prompt attacks into two aspects: (1)\nPrompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM,\nan automatic framework that leverages LLMs themselves to generate effective\njailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly\nimproves the attack success rate while greatly reducing the time cost compared\nto existing baselines. Our study also reveals the inadequacy of current defense\nmethods in safeguarding LLMs. Finally, we offer detailed analysis and\ndiscussion from the perspective of prompt execution priority on the failure of\nLLMs' defense. We hope that our research can catalyze both the academic\ncommunity and LLMs vendors towards the provision of safer and more regulated\nLarge Language Models.\n",
                "链接": "https://arxiv.org/abs/2311.08268"
            },
            {
                "文章ID": "115932",
                "标题": "Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts",
                "作者": " Yuanwei Wu,  Xiang Li,  Yixin Liu,  Pan Zhou,  Lichao Sun",
                "发布日期": "2023-11-16",
                "摘要": "  Existing work on jailbreak Multimodal Large Language Models (MLLMs) has\nfocused primarily on adversarial examples in model inputs, with less attention\nto vulnerabilities in model APIs. To fill the research gap, we carry out the\nfollowing work: 1) We discover a system prompt leakage vulnerability in GPT-4V.\nThrough carefully designed dialogue, we successfully steal the internal system\nprompts of GPT-4V. This finding indicates potential exploitable security risks\nin MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLM\njailbreaking attack method termed SASP (Self-Adversarial Attack via System\nPrompt). By employing GPT-4 as a red teaming tool against itself, we aim to\nsearch for potential jailbreak prompts leveraging stolen system prompts.\nFurthermore, in pursuit of better performance, we also add human modification\nbased on GPT-4's analysis, which further improves the attack success rate to\n98.7\\%; 3) We evaluated the effect of modifying system prompts to defend\nagainst jailbreaking attacks. Results show that appropriately designed system\nprompts can significantly reduce jailbreak success rates. Overall, our work\nprovides new insights into enhancing MLLM security, demonstrating the important\nrole of system prompts in jailbreaking, which could be leveraged to greatly\nfacilitate jailbreak success rates while also holding the potential for\ndefending against jailbreaks.\n",
                "链接": "https://arxiv.org/abs/2311.09127"
            },
            {
                "文章ID": "114015",
                "标题": "DeepInception: Hypnotize Large Language Model to Be Jailbreaker",
                "作者": " Xuan Li,  Zhanke Zhou,  Jianing Zhu,  Jiangchao Yao,  Tongliang Liu,  Bo Han",
                "发布日期": "2023-12-06",
                "摘要": "  Despite remarkable success in various applications, large language models\n(LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails\nvoid. However, previous studies for jailbreaks usually resort to brute-force\noptimization or extrapolations of a high computation cost, which might not be\npractical or effective. In this paper, inspired by the Milgram experiment that\nindividuals can harm another person if they are told to do so by an\nauthoritative figure, we disclose a lightweight method, termed as\nDeepInception, which can easily hypnotize LLM to be a jailbreaker and unlock\nits misusing risks. Specifically, DeepInception leverages the personification\nability of LLM to construct a novel nested scene to behave, which realizes an\nadaptive way to escape the usage control in a normal scenario and provides the\npossibility for further direct jailbreaks. Empirically, we conduct\ncomprehensive experiments to show its efficacy. Our DeepInception can achieve\ncompetitive jailbreak success rates with previous counterparts and realize a\ncontinuous jailbreak in subsequent interactions, which reveals the critical\nweakness of self-losing on both open/closed-source LLMs like Falcon, Vicuna,\nLlama-2, and GPT-3.5/4/4V. Our investigation appeals that people should pay\nmore attention to the safety aspects of LLMs and a stronger defense against\ntheir misuse risks. The code is publicly available at:\nhttps://github.com/tmlr-group/DeepInception.\n",
                "链接": "https://arxiv.org/abs/2311.03191"
            },
            {
                "文章ID": "120088",
                "标题": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
                "作者": " Anay Mehrotra,  Manolis Zampetakis,  Paul Kassianik,  Blaine Nelson,  Hyrum Anderson,  Yaron Singer,  Amin Karbasi",
                "发布日期": "2023-12-05",
                "摘要": "  While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an LLM to\niteratively refine candidate (attack) prompts using tree-of-thoughts reasoning\nuntil one of the generated prompts jailbreaks the target. Crucially, before\nsending prompts to the target, TAP assesses them and prunes the ones unlikely\nto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate\na large search space of prompts and pruning reduces the total number of queries\nsent to the target. In empirical evaluations, we observe that TAP generates\nprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)\nfor more than 80% of the prompts using only a small number of queries. This\nsignificantly improves upon the previous state-of-the-art black-box method for\ngenerating jailbreaks.\n",
                "链接": "https://arxiv.org/abs/2312.02119"
            },
            {
                "文章ID": "80304",
                "标题": "Tricking LLMs into Disobedience: Understanding, Analyzing, and\n  Preventing Jailbreaks",
                "作者": " Abhinav Rao,  Sachin Vashistha,  Atharva Naik,  Somak Aditya,  Monojit Choudhury",
                "发布日期": "2023-05-25",
                "摘要": "  Recent explorations with commercial Large Language Models (LLMs) have shown\nthat non-expert users can jailbreak LLMs by simply manipulating the prompts;\nresulting in degenerate output behavior, privacy and security breaches,\noffensive outputs, and violations of content regulator policies. Limited formal\nstudies have been carried out to formalize and analyze these attacks and their\nmitigations. We bridge this gap by proposing a formalism and a taxonomy of\nknown (and possible) jailbreaks. We perform a survey of existing jailbreak\nmethods and their effectiveness on open-source and commercial LLMs (such as GPT\n3.5, OPT, BLOOM, and FLAN-T5-xxl). We further propose a limited set of prompt\nguards and discuss their effectiveness against known attack types.\n",
                "链接": "https://arxiv.org/abs/2305.14965"
            },
            {
                "文章ID": "89560",
                "标题": "Jailbroken: How Does LLM Safety Training Fail?",
                "作者": " Alexander Wei,  Nika Haghtalab,  Jacob Steinhardt",
                "发布日期": "2023-07-06",
                "摘要": "  Large language models trained for safety and harmlessness remain susceptible\nto adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on\nearly releases of ChatGPT that elicit undesired behavior. Going beyond\nrecognition of the issue, we investigate why such attacks succeed and how they\ncan be created. We hypothesize two failure modes of safety training: competing\nobjectives and mismatched generalization. Competing objectives arise when a\nmodel's capabilities and safety goals conflict, while mismatched generalization\noccurs when safety training fails to generalize to a domain for which\ncapabilities exist. We use these failure modes to guide jailbreak design and\nthen evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's\nClaude v1.3, against both existing and newly designed attacks. We find that\nvulnerabilities persist despite the extensive red-teaming and safety-training\nefforts behind these models. Notably, new attacks utilizing our failure modes\nsucceed on every prompt in a collection of unsafe requests from the models'\nred-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our\nanalysis emphasizes the need for safety-capability parity -- that safety\nmechanisms should be as sophisticated as the underlying model -- and argues\nagainst the idea that scaling alone can resolve these safety failure modes.\n",
                "链接": "https://arxiv.org/abs/2307.02483"
            },
            {
                "文章ID": "14911",
                "标题": "Gradient boosting for convex cone predict and optimize problems",
                "作者": " Andrew Butler,  Roy H. Kwon",
                "发布日期": "2023-06-08",
                "摘要": "  Prediction models are typically optimized independently from decision\noptimization. A smart predict then optimize (SPO) framework optimizes\nprediction models to minimize downstream decision regret. In this paper we\npresent dboost, the first general purpose implementation of smart gradient\nboosting for `predict, then optimize' problems. The framework supports convex\nquadratic cone programming and gradient boosting is performed by implicit\ndifferentiation of a custom fixed-point mapping. Experiments comparing with\nstate-of-the-art SPO methods show that dboost can further reduce out-of-sample\ndecision regret.\n",
                "链接": "https://arxiv.org/abs/2204.06895"
            },
            {
                "文章ID": "78917",
                "标题": "SneakyPrompt: Jailbreaking Text-to-image Generative Models",
                "作者": " Yuchen Yang,  Bo Hui,  Haolin Yuan,  Neil Gong,  Yinzhi Cao",
                "发布日期": "2023-11-14",
                "摘要": "  Text-to-image generative models such as Stable Diffusion and DALL$\\cdot$E\nraise many ethical concerns due to the generation of harmful images such as\nNot-Safe-for-Work (NSFW) ones. To address these ethical concerns, safety\nfilters are often adopted to prevent the generation of NSFW images. In this\nwork, we propose SneakyPrompt, the first automated attack framework, to\njailbreak text-to-image generative models such that they generate NSFW images\neven if safety filters are adopted. Given a prompt that is blocked by a safety\nfilter, SneakyPrompt repeatedly queries the text-to-image generative model and\nstrategically perturbs tokens in the prompt based on the query results to\nbypass the safety filter. Specifically, SneakyPrompt utilizes reinforcement\nlearning to guide the perturbation of tokens. Our evaluation shows that\nSneakyPrompt successfully jailbreaks DALL$\\cdot$E 2 with closed-box safety\nfilters to generate NSFW images. Moreover, we also deploy several\nstate-of-the-art, open-source safety filters on a Stable Diffusion model. Our\nevaluation shows that SneakyPrompt not only successfully generates NSFW images,\nbut also outperforms existing text adversarial attacks when extended to\njailbreak text-to-image generative models, in terms of both the number of\nqueries and qualities of the generated NSFW images. SneakyPrompt is open-source\nand available at this repository:\n\\url{https://github.com/Yuchen413/text2image_safety}.\n",
                "链接": "https://arxiv.org/abs/2305.12082"
            },
            {
                "文章ID": "110873",
                "标题": "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large\n  Language Models",
                "作者": " Sicheng Zhu,  Ruiyi Zhang,  Bang An,  Gang Wu,  Joe Barrow,  Zichao Wang,  Furong Huang,  Ani Nenkova,  Tong Sun",
                "发布日期": "2023-12-15",
                "摘要": "  Safety alignment of Large Language Models (LLMs) can be compromised with\nmanual jailbreak attacks and (automatic) adversarial attacks. Recent studies\nsuggest that defending against these attacks is possible: adversarial attacks\ngenerate unlimited but unreadable gibberish prompts, detectable by\nperplexity-based filters; manual jailbreak attacks craft readable prompts, but\ntheir limited number due to the necessity of human creativity allows for easy\nblocking. In this paper, we show that these solutions may be too optimistic. We\nintroduce AutoDAN, an interpretable, gradient-based adversarial attack that\nmerges the strengths of both attack types. Guided by the dual goals of\njailbreak and readability, AutoDAN optimizes and generates tokens one by one\nfrom left to right, resulting in readable prompts that bypass perplexity\nfilters while maintaining high attack success rates. Notably, these prompts,\ngenerated from scratch using gradients, are interpretable and diverse, with\nemerging strategies commonly seen in manual jailbreak attacks. They also\ngeneralize to unforeseen harmful behaviors and transfer to black-box LLMs\nbetter than their unreadable counterparts when using limited training data or a\nsingle proxy model. Furthermore, we show the versatility of AutoDAN by\nautomatically leaking system prompts using a customized objective. Our work\noffers a new way to red-team LLMs and understand jailbreak mechanisms via\ninterpretability.\n",
                "链接": "https://arxiv.org/abs/2310.15140"
            }
        ]
    },
    {
        "question": {
            "question": "强化学习在大语言模型领域中应用的相关论文",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "119258",
                "标题": "ArthModel: Enhance Arithmetic Skills to Large Language Model",
                "作者": " Yingdi Guo",
                "发布日期": "2023-12-01",
                "摘要": "  With the great success of ChatGPT, the research of large language models has\nbecome increasingly popular. However, the models have several limitations, such\nas toxicity and pool performance of arithmetic solving. Meanwhile, LLM may have\nsome potential abilities that have yet to be exploited. In this paper, we\nchoose a different way to enhance the arithmetic ability of LLM. We propose to\ntrain LLM to generate a postfix expression related to the arithmetic problem\nand incorporate it with small pretrained models. Moreover, this small model\ntransfers the token embeddings into real dense numbers and invokes native\nfunctions of a deep learning platform to get the correct answer. To generate\nthe final result, we propose prompt injection for adding the result outputs by\nthe small model to LLM. This work provides different ways of thinking, training\nand using a language model. The codes and models will be released at\n\\url{https://github.com/eteced/arithmetic_finetuning_v1}.\n",
                "链接": "https://arxiv.org/abs/2311.18609"
            },
            {
                "文章ID": "92138",
                "标题": "Topics, Authors, and Networks in Large Language Model Research: Trends\n  from a Survey of 17K arXiv Papers",
                "作者": " Rajiv Movva,  Sidhika Balachandar,  Kenny Peng,  Gabriel Agostini,  Nikhil Garg,  Emma Pierson",
                "发布日期": "2023-10-24",
                "摘要": "  Large language model (LLM) research is dramatically impacting society, making\nit essential to understand the topics and values it prioritizes, the authors\nand institutions driving it, and its networks of collaboration. Due to the\nrecent growth of the field, many of these fundamental attributes lack\nsystematic description. We gather, annotate, and analyze a new dataset of\n16,979 LLM-related arXiv papers, focusing on changes in 2023 vs. 2018-2022. We\nshow that LLM research increasingly focuses on societal impacts: the Computers\nand Society sub-arXiv has seen 20x growth in its proportion of LLM-related\npapers in 2023. This change is driven in part by an influx of new authors: a\nmajority of 2023 papers are first-authored by researchers who have not\npreviously written an LLM-related paper, and these papers focus particularly on\napplications and societal considerations. While a handful of companies hold\noutsize influence, academia publishes a much larger fraction of papers than\nindustry overall, and this gap widens in 2023. LLM research is also being\nshaped by social dynamics: there are gender and academic/industry differences\nin the topics authors prioritize, and a stark U.S./China schism in the\ncollaboration network. Overall, our analysis documents how LLM research both\nshapes and is shaped by society, attesting to the necessity of sociotechnical\nlenses; we discuss implications for researchers and policymakers.\n",
                "链接": "https://arxiv.org/abs/2307.10700"
            },
            {
                "文章ID": "110997",
                "标题": "CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without\n  Full Large Language Model",
                "作者": " Kaiyan Zhang,  Ning Ding,  Biqing Qi,  Xuekai Zhu,  Xinwei Long,  Bowen Zhou",
                "发布日期": "2023-10-25",
                "摘要": "  Instruction tuning has recently been recognized as an effective way of\naligning Large Language Models (LLMs) to enhance their generalization ability\nacross various tasks. However, when tuning publicly accessible, centralized\nLLMs with private instruction data, privacy concerns are inevitable. While\ndirect transfer of parameterized modules between models is a plausible approach\nto address this, its implications and effectiveness need further exploration.\nThis paper focuses on Offsite-Tuning (OFT), a representative technique that\ntransfers transformer blocks between centralized LLMs and downstream emulators.\nGiven the limited understanding of the underlying mechanism of OFT, we perform\nan empirical analysis on LLMs from the perspectives of representation and\nfunctional similarity. Interestingly, our findings reveal a unique modular\nstructure within the layers of LLMs that appears to emerge as the model size\nexpands. Simultaneously, we note subtle but potentially significant changes in\nrepresentation and intermediate predictions across the layers. Inspired by\nthese observations, we propose CRaSh, involving Clustering, Removing, and\nSharing, a training-free strategy to derive improved emulators from LLMs. CRaSh\nsignificantly boosts performance of OFT with billions of parameters.\nFurthermore, we investigate the optimal solutions yielded by fine-tuning with\nand without full model through the lens of loss landscape. Our findings\ndemonstrate a linear connectivity among these optima falling over the same\nbasin, thereby highlighting the effectiveness of CRaSh and OFT. The source code\nis publicly available at https://github.com/TsinghuaC3I/CRaSh.\n",
                "链接": "https://arxiv.org/abs/2310.15477"
            },
            {
                "文章ID": "6997",
                "标题": "Matching Papers and Reviewers at Large Conferences",
                "作者": " Kevin Leyton-Brown,   Mausam,  Yatin Nandwani,  Hedayat Zarkoob,  Chris Cameron,  Neil Newman,  Dinesh Raghu",
                "发布日期": "2022-08-08",
                "摘要": "  Peer-reviewed conferences, the main publication venues in CS, rely critically\non matching highly qualified reviewers for each paper. Because of the growing\nscale of these conferences, the tight timelines on which they operate, and a\nrecent surge in explicitly dishonest behavior, there is now no alternative to\nperforming this matching in an automated way. This paper studies a novel\nreviewer-paper matching approach that was recently deployed in the 35th AAAI\nConference on Artificial Intelligence (AAAI 2021), and has since been adopted\n(wholly or partially) by other conferences including ICML 2022, AAAI 2022, and\nIJCAI 2022. This approach has three main elements: (1) collecting and\nprocessing input data to identify problematic matches and generate\nreviewer-paper scores; (2) formulating and solving an optimization problem to\nfind good reviewer-paper matchings; and (3) a two-phase reviewing process that\nshifts reviewing resources away from papers likely to be rejected and towards\npapers closer to the decision boundary. This paper also describes an evaluation\nof these innovations based on an extensive post-hoc analysis on real data --\nincluding a comparison with the matching algorithm used in AAAI's previous\n(2020) iteration -- and supplements this with additional numerical\nexperimentation.\n",
                "链接": "https://arxiv.org/abs/2202.12273"
            },
            {
                "文章ID": "80027",
                "标题": "Self-Polish: Enhance Reasoning in Large Language Models via Problem\n  Refinement",
                "作者": " Zhiheng Xi,  Senjie Jin,  Yuhao Zhou,  Rui Zheng,  Songyang Gao,  Tao Gui,  Qi Zhang,  Xuanjing Huang",
                "发布日期": "2023-05-25",
                "摘要": "  Prompting methods such as Chain-of-Thought (CoT) have shed new light on\nenhancing the reasoning capabilities of large language models, and researchers\nhave extensively explored the generation process of rationales and answers.\nHowever, they have overlooked the potential challenges posed by the poor\nquality of reasoning problems, which may influence the reasoning performance\nsignificantly. In this work, we propose Self-Polish (SP), a novel method that\nfacilitates the model's problem-solving process by prompting them to\nprogressively refine the given problems to be more comprehensible and solvable.\nSpecifically, the method teaches models to eliminate irrelevant information,\nrearrange the logic structure and organize local conditions into new ones\nparallelly. SP is orthogonal to all other prompting methods, making it\nconvenient to integrate with state-of-the-art techniques for further\nimprovement. We conduct thorough experiments on five benchmarks to illustrate\nthe effectiveness of the proposed method. For example, with Text-davinci-003,\nour method boosts the performance of standard few-shot prompting by $8.0\\%$ on\nGSM8K and $17.8\\%$ on MultiArith; it also improves the performance of CoT by\n$6.0\\%$ on GSM8K and $6.0\\%$ on MathQA, respectively. Furthermore, our method\nalso showcases impressive performance on robustness evaluation.\n",
                "链接": "https://arxiv.org/abs/2305.14497"
            },
            {
                "文章ID": "107503",
                "标题": "An evolutionary model of personality traits related to cooperative\n  behavior using a large language model",
                "作者": " Reiji Suzuki,  Takaya Arita",
                "发布日期": "2023-10-11",
                "摘要": "  This paper aims to shed light on the evolutionary dynamics of diverse and\nsocial populations by introducing the rich expressiveness of generative models\ninto the trait expression of social agent-based evolutionary models.\nSpecifically, we focus on the evolution of personality traits in the context of\na game-theoretic relationship as a situation in which inter-individual\ninterests exert strong selection pressures. We construct an agent model in\nwhich linguistic descriptions of personality traits related to cooperative\nbehavior are used as genes. The deterministic strategies extracted from Large\nLanguage Model (LLM) that make behavioral decisions based on these personality\ntraits are used as behavioral traits. The population is evolved according to\nselection based on average payoff and mutation of genes by asking LLM to\nslightly modify the parent gene toward cooperative or selfish. Through\npreliminary experiments and analyses, we clarify that such a model can indeed\nexhibit the evolution of cooperative behavior based on the diverse and\nhigher-order representation of personality traits. We also observed the\nrepeated intrusion of cooperative and selfish personality traits through\nchanges in the expression of personality traits, and found that the emerging\nwords in the evolved gene well reflected the behavioral tendency of its\npersonality in terms of their semantics.\n",
                "链接": "https://arxiv.org/abs/2310.05976"
            },
            {
                "文章ID": "125173",
                "标题": "A Large Language Model-based Computational Approach to Improve\n  Identity-Related Write-Ups",
                "作者": " Alex Doboli",
                "发布日期": "2023-12-29",
                "摘要": "  Creating written products is essential to modern life, including writings\nabout one's identity and personal experiences. However, writing is often a\ndifficult activity that requires extensive effort to frame the central ideas,\nthe pursued approach to communicate the central ideas, e.g., using analogies,\nmetaphors, or other possible means, the needed presentation structure, and the\nactual verbal expression. Large Language Models, a recently emerged approach in\nMachine Learning, can offer a significant help in reducing the effort and\nimproving the quality of written products. This paper proposes a new\ncomputational approach to explore prompts that given as inputs to a Large\nLanguage Models can generate cues to improve the considered written products.\nTwo case studies on improving write-ups, one based on an analogy and one on a\nmetaphor, are also presented in the paper.\n",
                "链接": "https://arxiv.org/abs/2312.16659"
            },
            {
                "文章ID": "104308",
                "标题": "People's Perceptions Toward Bias and Related Concepts in Large Language\n  Models: A Systematic Review",
                "作者": " Lu Wang,  Max Song,  Rezvaneh Rezapour,  Bum Chul Kwon,  Jina Huh-Yoo",
                "发布日期": "2023-09-27",
                "摘要": "  Large language models (LLMs) have brought breakthroughs in tasks including\ntranslation, summarization, information retrieval, and language generation,\ngaining growing interest in the CHI community. Meanwhile, the literature shows\nresearchers' controversial perceptions about the efficacy, ethics, and\nintellectual abilities of LLMs. However, we do not know how lay people perceive\nLLMs that are pervasive in everyday tools, specifically regarding their\nexperience with LLMs around bias, stereotypes, social norms, or safety. In this\nstudy, we conducted a systematic review to understand what empirical insights\npapers have gathered about people's perceptions toward LLMs. From a total of\n231 retrieved papers, we full-text reviewed 15 papers that recruited human\nevaluators to assess their experiences with LLMs. We report different biases\nand related concepts investigated by these studies, four broader LLM\napplication areas, the evaluators' perceptions toward LLMs' performances\nincluding advantages, biases, and conflicting perceptions, factors influencing\nthese perceptions, and concerns about LLM applications.\n",
                "链接": "https://arxiv.org/abs/2309.14504"
            },
            {
                "文章ID": "95587",
                "标题": "NLLG Quarterly arXiv Report 06/23: What are the most influential current\n  AI Papers?",
                "作者": " Steffen Eger,  Christoph Leiter,  Jonas Belouadi,  Ran Zhang,  Aida Kostikova,  Daniil Larionov,  Yanran Chen,  Vivian Fresen",
                "发布日期": "2023-08-15",
                "摘要": "  The rapid growth of information in the field of Generative Artificial\nIntelligence (AI), particularly in the subfields of Natural Language Processing\n(NLP) and Machine Learning (ML), presents a significant challenge for\nresearchers and practitioners to keep pace with the latest developments. To\naddress the problem of information overload, this report by the Natural\nLanguage Learning Group at Bielefeld University focuses on identifying the most\npopular papers on arXiv, with a specific emphasis on NLP and ML. The objective\nis to offer a quick guide to the most relevant and widely discussed research,\naiding both newcomers and established researchers in staying abreast of current\ntrends. In particular, we compile a list of the 40 most popular papers based on\nnormalized citation counts from the first half of 2023. We observe the\ndominance of papers related to Large Language Models (LLMs) and specifically\nChatGPT during the first half of 2023, with the latter showing signs of\ndeclining popularity more recently, however. Further, NLP related papers are\nthe most influential (around 60\\% of top papers) even though there are twice as\nmany ML related papers in our data. Core issues investigated in the most\nheavily cited papers are: LLM efficiency, evaluation techniques, ethical\nconsiderations, embodied agents, and problem-solving with LLMs. Additionally,\nwe examine the characteristics of top papers in comparison to others outside\nthe top-40 list (noticing the top paper's focus on LLM related issues and\nhigher number of co-authors) and analyze the citation distributions in our\ndataset, among others.\n",
                "链接": "https://arxiv.org/abs/2308.04889"
            },
            {
                "文章ID": "44067",
                "标题": "Using Large Language Models to Enhance Programming Error Messages",
                "作者": " Juho Leinonen,  Arto Hellas,  Sami Sarsa,  Brent Reeves,  Paul Denny,  James Prather,  Brett A. Becker",
                "发布日期": "2022-10-24",
                "摘要": "  A key part of learning to program is learning to understand programming error\nmessages. They can be hard to interpret and identifying the cause of errors can\nbe time-consuming. One factor in this challenge is that the messages are\ntypically intended for an audience that already knows how to program, or even\nfor programming environments that then use the information to highlight areas\nin code. Researchers have been working on making these errors more novice\nfriendly since the 1960s, however progress has been slow. The present work\ncontributes to this stream of research by using large language models to\nenhance programming error messages with explanations of the errors and\nsuggestions on how to fix the error. Large language models can be used to\ncreate useful and novice-friendly enhancements to programming error messages\nthat sometimes surpass the original programming error messages in\ninterpretability and actionability. These results provide further evidence of\nthe benefits of large language models for computing educators, highlighting\ntheir use in areas known to be challenging for students. We further discuss the\nbenefits and downsides of large language models and highlight future streams of\nresearch for enhancing programming error messages.\n",
                "链接": "https://arxiv.org/abs/2210.11630"
            }
        ]
    },
    {
        "question": {
            "question": "有关大模型在新任务上面知识迁移的研究",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "119143",
                "标题": "Label-efficient Training of Small Task-specific Models by Leveraging\n  Vision Foundation Models",
                "作者": " Raviteja Vemulapalli,  Hadi Pouransari,  Fartash Faghri,  Sachin Mehta,  Mehrdad Farajtabar,  Mohammad Rastegari,  Oncel Tuzel",
                "发布日期": "2023-12-01",
                "摘要": "  Large Vision Foundation Models (VFMs) pretrained on massive datasets exhibit\nimpressive performance on various downstream tasks, especially with limited\nlabeled target data. However, due to their high memory and compute\nrequirements, these models cannot be deployed in resource constrained settings.\nThis raises an important question: How can we utilize the knowledge from a\nlarge VFM to train a small task-specific model for a new target task with\nlimited labeled training data? In this work, we answer this question by\nproposing a simple and highly effective task-oriented knowledge transfer\napproach to leverage pretrained VFMs for effective training of small\ntask-specific models. Our experimental results on four target tasks under\nlimited labeled data settings show that the proposed knowledge transfer\napproach outperforms task-agnostic VFM distillation, web-scale CLIP pretraining\nand supervised ImageNet pretraining by 1-10.5%, 2-22% and 2-14%, respectively.\nWe also show that the dataset used for transferring knowledge has a significant\neffect on the final target task performance, and propose an image\nretrieval-based approach for curating effective transfer sets.\n",
                "链接": "https://arxiv.org/abs/2311.18237"
            },
            {
                "文章ID": "46363",
                "标题": "Beyond Not-Forgetting: Continual Learning with Backward Knowledge\n  Transfer",
                "作者": " Sen Lin,  Li Yang,  Deliang Fan,  Junshan Zhang",
                "发布日期": "2022-11-03",
                "摘要": "  By learning a sequence of tasks continually, an agent in continual learning\n(CL) can improve the learning performance of both a new task and `old' tasks by\nleveraging the forward knowledge transfer and the backward knowledge transfer,\nrespectively. However, most existing CL methods focus on addressing\ncatastrophic forgetting in neural networks by minimizing the modification of\nthe learnt model for old tasks. This inevitably limits the backward knowledge\ntransfer from the new task to the old tasks, because judicious model updates\ncould possibly improve the learning performance of the old tasks as well. To\ntackle this problem, we first theoretically analyze the conditions under which\nupdating the learnt model of old tasks could be beneficial for CL and also lead\nto backward knowledge transfer, based on the gradient projection onto the input\nsubspaces of old tasks. Building on the theoretical analysis, we next develop a\nContinUal learning method with Backward knowlEdge tRansfer (CUBER), for a fixed\ncapacity neural network without data replay. In particular, CUBER first\ncharacterizes the task correlation to identify the positively correlated old\ntasks in a layer-wise manner, and then selectively modifies the learnt model of\nthe old tasks when learning the new task. Experimental studies show that CUBER\ncan even achieve positive backward knowledge transfer on several existing CL\nbenchmarks for the first time without data replay, where the related baselines\nstill suffer from catastrophic forgetting (negative backward knowledge\ntransfer). The superior performance of CUBER on the backward knowledge transfer\nalso leads to higher accuracy accordingly.\n",
                "链接": "https://arxiv.org/abs/2211.00789"
            },
            {
                "文章ID": "40278",
                "标题": "Visual Prompt Tuning for Generative Transfer Learning",
                "作者": " Kihyuk Sohn,  Yuan Hao,  José Lezama,  Luisa Polania,  Huiwen Chang,  Han Zhang,  Irfan Essa,  Lu Jiang",
                "发布日期": "2022-10-04",
                "摘要": "  Transferring knowledge from an image synthesis model trained on a large\ndataset is a promising direction for learning generative image models from\nvarious domains efficiently. While previous works have studied GAN models, we\npresent a recipe for learning vision transformers by generative knowledge\ntransfer. We base our framework on state-of-the-art generative vision\ntransformers that represent an image as a sequence of visual tokens to the\nautoregressive or non-autoregressive transformers. To adapt to a new domain, we\nemploy prompt tuning, which prepends learnable tokens called prompt to the\nimage token sequence, and introduce a new prompt design for our task. We study\non a variety of visual domains, including visual task adaptation\nbenchmark~\\cite{zhai2019large}, with varying amount of training images, and\nshow effectiveness of knowledge transfer and a significantly better image\ngeneration quality over existing works.\n",
                "链接": "https://arxiv.org/abs/2210.00990"
            },
            {
                "文章ID": "64040",
                "标题": "Generic-to-Specific Distillation of Masked Autoencoders",
                "作者": " Wei Huang,  Zhiliang Peng,  Li Dong,  Furu Wei,  Jianbin Jiao,  Qixiang Ye",
                "发布日期": "2023-03-01",
                "摘要": "  Large vision Transformers (ViTs) driven by self-supervised pre-training\nmechanisms achieved unprecedented progress. Lightweight ViT models limited by\nthe model capacity, however, benefit little from those pre-training mechanisms.\nKnowledge distillation defines a paradigm to transfer representations from\nlarge (teacher) models to small (student) ones. However, the conventional\nsingle-stage distillation easily gets stuck on task-specific transfer, failing\nto retain the task-agnostic knowledge crucial for model generalization. In this\nstudy, we propose generic-to-specific distillation (G2SD), to tap the potential\nof small ViT models under the supervision of large models pre-trained by masked\nautoencoders. In generic distillation, decoder of the small model is encouraged\nto align feature predictions with hidden representations of the large model, so\nthat task-agnostic knowledge can be transferred. In specific distillation,\npredictions of the small model are constrained to be consistent with those of\nthe large model, to transfer task-specific features which guarantee task\nperformance. With G2SD, the vanilla ViT-Small model respectively achieves\n98.7%, 98.1% and 99.3% the performance of its teacher (ViT-Base) for image\nclassification, object detection, and semantic segmentation, setting a solid\nbaseline for two-stage vision distillation. Code will be available at\nhttps://github.com/pengzhiliang/G2SD.\n",
                "链接": "https://arxiv.org/abs/2302.14771"
            },
            {
                "文章ID": "55221",
                "标题": "Prototype-guided Cross-task Knowledge Distillation for Large-scale\n  Models",
                "作者": " Deng Li,  Aming Wu,  Yahong Han,  Qi Tian",
                "发布日期": "2022-12-27",
                "摘要": "  Recently, large-scale pre-trained models have shown their advantages in many\ntasks. However, due to the huge computational complexity and storage\nrequirements, it is challenging to apply the large-scale model to real scenes.\nA common solution is knowledge distillation which regards the large-scale model\nas a teacher model and helps to train a small student model to obtain a\ncompetitive performance. Cross-task Knowledge distillation expands the\napplication scenarios of the large-scale pre-trained model. Existing knowledge\ndistillation works focus on directly mimicking the final prediction or the\nintermediate layers of the teacher model, which represent the global-level\ncharacteristics and are task-specific. To alleviate the constraint of different\nlabel spaces, capturing invariant intrinsic local object characteristics (such\nas the shape characteristics of the leg and tail of the cattle and horse) plays\na key role. Considering the complexity and variability of real scene tasks, we\npropose a Prototype-guided Cross-task Knowledge Distillation (ProC-KD) approach\nto transfer the intrinsic local-level object knowledge of a large-scale teacher\nnetwork to various task scenarios. First, to better transfer the generalized\nknowledge in the teacher model in cross-task scenarios, we propose a prototype\nlearning module to learn from the essential feature representation of objects\nin the teacher model. Secondly, for diverse downstream tasks, we propose a\ntask-adaptive feature augmentation module to enhance the features of the\nstudent model with the learned generalization prototype features and guide the\ntraining of the student model to improve its generalization ability. The\nexperimental results on various visual tasks demonstrate the effectiveness of\nour approach for large-scale model cross-task knowledge distillation scenes.\n",
                "链接": "https://arxiv.org/abs/2212.13180"
            },
            {
                "文章ID": "26664",
                "标题": "ST-Adapter: Parameter-Efficient Image-to-Video Transfer Learning",
                "作者": " Junting Pan,  Ziyi Lin,  Xiatian Zhu,  Jing Shao,  Hongsheng Li",
                "发布日期": "2022-10-14",
                "摘要": "  Capitalizing on large pre-trained models for various downstream tasks of\ninterest have recently emerged with promising performance. Due to the\never-growing model size, the standard full fine-tuning based task adaptation\nstrategy becomes prohibitively costly in terms of model training and storage.\nThis has led to a new research direction in parameter-efficient transfer\nlearning. However, existing attempts typically focus on downstream tasks from\nthe same modality (e.g., image understanding) of the pre-trained model. This\ncreates a limit because in some specific modalities, (e.g., video\nunderstanding) such a strong pre-trained model with sufficient knowledge is\nless or not available. In this work, we investigate such a novel cross-modality\ntransfer learning setting, namely parameter-efficient image-to-video transfer\nlearning. To solve this problem, we propose a new Spatio-Temporal Adapter\n(ST-Adapter) for parameter-efficient fine-tuning per video task. With a\nbuilt-in spatio-temporal reasoning capability in a compact design, ST-Adapter\nenables a pre-trained image model without temporal knowledge to reason about\ndynamic video content at a small (~8%) per-task parameter cost, requiring\napproximately 20 times fewer updated parameters compared to previous work.\nExtensive experiments on video action recognition tasks show that our\nST-Adapter can match or even outperform the strong full fine-tuning strategy\nand state-of-the-art video models, whilst enjoying the advantage of parameter\nefficiency. The code and model are available at\nhttps://github.com/linziyi96/st-adapter\n",
                "链接": "https://arxiv.org/abs/2206.13559"
            },
            {
                "文章ID": "89073",
                "标题": "Analysis of Task Transferability in Large Pre-trained Classifiers",
                "作者": " Akshay Mehra,  Yunbei Zhang,  Jihun Hamm",
                "发布日期": "2023-07-04",
                "摘要": "  Transfer learning transfers the knowledge acquired by a model from a source\ntask to multiple downstream target tasks with minimal fine-tuning. The success\nof transfer learning at improving performance, especially with the use of large\npre-trained models has made transfer learning an essential tool in the machine\nlearning toolbox. However, the conditions under which the performance is\ntransferable to downstream tasks are not understood very well. In this work, we\nanalyze the transfer of performance for classification tasks, when only the\nlast linear layer of the source model is fine-tuned on the target task. We\npropose a novel Task Transfer Analysis approach that transforms the source\ndistribution (and classifier) by changing the class prior distribution, label,\nand feature spaces to produce a new source distribution (and classifier) and\nallows us to relate the loss of the downstream task (i.e., transferability) to\nthat of the source task. Concretely, our bound explains transferability in\nterms of the Wasserstein distance between the transformed source and downstream\ntask's distribution, conditional entropy between the label distributions of the\ntwo tasks, and weighted loss of the source classifier on the source task.\nMoreover, we propose an optimization problem for learning the transforms of the\nsource task to minimize the upper bound on transferability. We perform a\nlarge-scale empirical study by using state-of-the-art pre-trained models and\ndemonstrate the effectiveness of our bound and optimization at predicting\ntransferability. The results of our experiments demonstrate how factors such as\ntask relatedness, pretraining method, and model architecture affect\ntransferability.\n",
                "链接": "https://arxiv.org/abs/2307.00823"
            },
            {
                "文章ID": "66657",
                "标题": "Is forgetting less a good inductive bias for forward transfer?",
                "作者": " Jiefeng Chen,  Timothy Nguyen,  Dilan Gorur,  Arslan Chaudhry",
                "发布日期": "2023-03-16",
                "摘要": "  One of the main motivations of studying continual learning is that the\nproblem setting allows a model to accrue knowledge from past tasks to learn new\ntasks more efficiently. However, recent studies suggest that the key metric\nthat continual learning algorithms optimize, reduction in catastrophic\nforgetting, does not correlate well with the forward transfer of knowledge. We\nbelieve that the conclusion previous works reached is due to the way they\nmeasure forward transfer. We argue that the measure of forward transfer to a\ntask should not be affected by the restrictions placed on the continual learner\nin order to preserve knowledge of previous tasks. Instead, forward transfer\nshould be measured by how easy it is to learn a new task given a set of\nrepresentations produced by continual learning on previous tasks. Under this\nnotion of forward transfer, we evaluate different continual learning algorithms\non a variety of image classification benchmarks. Our results indicate that less\nforgetful representations lead to a better forward transfer suggesting a strong\ncorrelation between retaining past information and learning efficiency on new\ntasks. Further, we found less forgetful representations to be more diverse and\ndiscriminative compared to their forgetful counterparts.\n",
                "链接": "https://arxiv.org/abs/2303.08207"
            },
            {
                "文章ID": "40064",
                "标题": "Transfer Learning for Individual Treatment Effect Estimation",
                "作者": " Ahmed Aloui,  Juncheng Dong,  Cat P. Le,  Vahid Tarokh",
                "发布日期": "2023-06-07",
                "摘要": "  This work considers the problem of transferring causal knowledge between\ntasks for Individual Treatment Effect (ITE) estimation. To this end, we\ntheoretically assess the feasibility of transferring ITE knowledge and present\na practical framework for efficient transfer. A lower bound is introduced on\nthe ITE error of the target task to demonstrate that ITE knowledge transfer is\nchallenging due to the absence of counterfactual information. Nevertheless, we\nestablish generalization upper bounds on the counterfactual loss and ITE error\nof the target task, demonstrating the feasibility of ITE knowledge transfer.\nSubsequently, we introduce a framework with a new Causal Inference Task\nAffinity (CITA) measure for ITE knowledge transfer. Specifically, we use CITA\nto find the closest source task to the target task and utilize it for ITE\nknowledge transfer. Empirical studies are provided, demonstrating the efficacy\nof the proposed method. We observe that ITE knowledge transfer can\nsignificantly (up to 95%) reduce the amount of data required for ITE\nestimation.\n",
                "链接": "https://arxiv.org/abs/2210.00380"
            },
            {
                "文章ID": "9527",
                "标题": "Continual Prompt Tuning for Dialog State Tracking",
                "作者": " Qi Zhu,  Bing Li,  Fei Mi,  Xiaoyan Zhu,  Minlie Huang",
                "发布日期": "2022-03-15",
                "摘要": "  A desirable dialog system should be able to continually learn new skills\nwithout forgetting old ones, and thereby adapt to new domains or tasks in its\nlife cycle. However, continually training a model often leads to a well-known\ncatastrophic forgetting issue. In this paper, we present Continual Prompt\nTuning, a parameter-efficient framework that not only avoids forgetting but\nalso enables knowledge transfer between tasks. To avoid forgetting, we only\nlearn and store a few prompt tokens' embeddings for each task while freezing\nthe backbone pre-trained model. To achieve bi-directional knowledge transfer\namong tasks, we propose several techniques (continual prompt initialization,\nquery fusion, and memory replay) to transfer knowledge from preceding tasks and\na memory-guided technique to transfer knowledge from subsequent tasks.\nExtensive experiments demonstrate the effectiveness and efficiency of our\nproposed method on continual learning for dialog state tracking, compared with\nstate-of-the-art baselines.\n",
                "链接": "https://arxiv.org/abs/2203.06654"
            }
        ]
    },
    {
        "question": {
            "question": "对比解码综述",
            "type": "6"
        },
        "results": [
            {
                "文章ID": "37208",
                "标题": "How to Find Strong Summary Coherence Measures? A Toolbox and a\n  Comparative Study for Summary Coherence Measure Evaluation",
                "作者": " Julius Steen,  Katja Markert",
                "发布日期": "2022-09-16",
                "摘要": "  Automatically evaluating the coherence of summaries is of great significance\nboth to enable cost-efficient summarizer evaluation and as a tool for improving\ncoherence by selecting high-scoring candidate summaries. While many different\napproaches have been suggested to model summary coherence, they are often\nevaluated using disparate datasets and metrics. This makes it difficult to\nunderstand their relative performance and identify ways forward towards better\nsummary coherence modelling. In this work, we conduct a large-scale\ninvestigation of various methods for summary coherence modelling on an even\nplaying field. Additionally, we introduce two novel analysis measures,\nintra-system correlation and bias matrices, that help identify biases in\ncoherence measures and provide robustness against system-level confounders.\nWhile none of the currently available automatic coherence measures are able to\nassign reliable coherence scores to system summaries across all evaluation\nmetrics, large-scale language models fine-tuned on self-supervised tasks show\npromising results, as long as fine-tuning takes into account that they need to\ngeneralize across different summary lengths.\n",
                "链接": "https://arxiv.org/abs/2209.06517"
            },
            {
                "文章ID": "35198",
                "标题": "Podcast Summary Assessment: A Resource for Evaluating Summary Assessment\n  Methods",
                "作者": " Potsawee Manakul,  Mark J. F. Gales",
                "发布日期": "2022-08-30",
                "摘要": "  Automatic summary assessment is useful for both machine-generated and\nhuman-produced summaries. Automatically evaluating the summary text given the\ndocument enables, for example, summary generation system development and\ndetection of inappropriate summaries. Summary assessment can be run in a number\nof modes: ranking summary generation systems; ranking summaries of a particular\ndocument; and estimating the quality of a document-summary pair on an absolute\nscale. Existing datasets with annotation for summary assessment are usually\nbased on news summarization datasets such as CNN/DailyMail or XSum. In this\nwork, we describe a new dataset, the podcast summary assessment corpus, a\ncollection of podcast summaries that were evaluated by human experts at\nTREC2020. Compared to existing summary assessment data, this dataset has two\nunique aspects: (i) long-input, speech podcast based, documents; and (ii) an\nopportunity to detect inappropriate reference summaries in podcast corpus.\nFirst, we examine existing assessment methods, including model-free and\nmodel-based methods, and provide benchmark results for this long-input summary\nassessment dataset. Second, with the aim of filtering reference\nsummary-document pairings for training, we apply summary assessment for data\nselection. The experimental results on these two aspects provide interesting\ninsights on the summary assessment and generation tasks. The podcast summary\nassessment data is available.\n",
                "链接": "https://arxiv.org/abs/2208.13265"
            },
            {
                "文章ID": "43109",
                "标题": "Towards Summary Candidates Fusion",
                "作者": " Mathieu Ravaut,  Shafiq Joty,  Nancy F. Chen",
                "发布日期": "2023-05-29",
                "摘要": "  Sequence-to-sequence deep neural models fine-tuned for abstractive\nsummarization can achieve great performance on datasets with enough human\nannotations. Yet, it has been shown that they have not reached their full\npotential, with a wide gap between the top beam search output and the oracle\nbeam. Recently, re-ranking methods have been proposed, to learn to select a\nbetter summary candidate. However, such methods are limited by the summary\nquality aspects captured by the first-stage candidates. To bypass this\nlimitation, we propose a new paradigm in second-stage abstractive summarization\ncalled SummaFusion that fuses several summary candidates to produce a novel\nabstractive second-stage summary. Our method works well on several\nsummarization datasets, improving both the ROUGE scores and qualitative\nproperties of fused summaries. It is especially good when the candidates to\nfuse are worse, such as in the few-shot setup where we set a new\nstate-of-the-art. We will make our code and checkpoints available at\nhttps://github.com/ntunlp/SummaFusion/.\n",
                "链接": "https://arxiv.org/abs/2210.08779"
            },
            {
                "文章ID": "80840",
                "标题": "Abstractive Summary Generation for the Urdu Language",
                "作者": " Ali Raza,  Hadia Sultan Raja,  Usman Maratib",
                "发布日期": "2023-05-26",
                "摘要": "  Abstractive summary generation is a challenging task that requires the model\nto comprehend the source text and generate a concise and coherent summary that\ncaptures the essential information. In this paper, we explore the use of an\nencoder/decoder approach for abstractive summary generation in the Urdu\nlanguage. We employ a transformer-based model that utilizes self-attention\nmechanisms to encode the input text and generate a summary. Our experiments\nshow that our model can produce summaries that are grammatically correct and\nsemantically meaningful. We evaluate our model on a publicly available dataset\nand achieve state-of-the-art results in terms of Rouge scores. We also conduct\na qualitative analysis of our model's output to assess its effectiveness and\nlimitations. Our findings suggest that the encoder/decoder approach is a\npromising method for abstractive summary generation in Urdu and can be extended\nto other languages with suitable modifications.\n",
                "链接": "https://arxiv.org/abs/2305.16195"
            },
            {
                "文章ID": "18214",
                "标题": "Summary Markov Models for Event Sequences",
                "作者": " Debarun Bhattacharjya,  Saurabh Sihag,  Oktie Hassanzadeh,  Liza Bialik",
                "发布日期": "2022-05-09",
                "摘要": "  Datasets involving sequences of different types of events without meaningful\ntime stamps are prevalent in many applications, for instance when extracted\nfrom textual corpora. We propose a family of models for such event sequences --\nsummary Markov models -- where the probability of observing an event type\ndepends only on a summary of historical occurrences of its influencing set of\nevent types. This Markov model family is motivated by Granger causal models for\ntime series, with the important distinction that only one event can occur in a\nposition in an event sequence. We show that a unique minimal influencing set\nexists for any set of event types of interest and choice of summary function,\nformulate two novel models from the general family that represent specific\nsequence dynamics, and propose a greedy search algorithm for learning them from\nevent sequence data. We conduct an experimental investigation comparing the\nproposed models with relevant baselines, and illustrate their knowledge\nacquisition and discovery capabilities through case studies involving sequences\nfrom text.\n",
                "链接": "https://arxiv.org/abs/2205.03375"
            },
            {
                "文章ID": "53718",
                "标题": "Summary-Oriented Vision Modeling for Multimodal Abstractive\n  Summarization",
                "作者": " Yunlong Liang,  Fandong Meng,  Jinan Xu,  Jiaan Wang,  Yufeng Chen,  Jie Zhou",
                "发布日期": "2023-05-05",
                "摘要": "  Multimodal abstractive summarization (MAS) aims to produce a concise summary\ngiven the multimodal data (text and vision). Existing studies mainly focus on\nhow to effectively use the visual features from the perspective of an article,\nhaving achieved impressive success on the high-resource English dataset.\nHowever, less attention has been paid to the visual features from the\nperspective of the summary, which may limit the model performance, especially\nin the low- and zero-resource scenarios. In this paper, we propose to improve\nthe summary quality through summary-oriented visual features. To this end, we\ndevise two auxiliary tasks including vision to summary task and masked image\nmodeling task. Together with the main summarization task, we optimize the MAS\nmodel via the training objectives of all these tasks. By these means, the MAS\nmodel can be enhanced by capturing the summary-oriented visual features,\nthereby yielding more accurate summaries. Experiments on 44 languages, covering\nmid-high-, low-, and zero-resource scenarios, verify the effectiveness and\nsuperiority of the proposed approach, which achieves state-of-the-art\nperformance under all scenarios. Additionally, we will contribute a large-scale\nmultilingual multimodal abstractive summarization (MM-Sum) dataset.\n",
                "链接": "https://arxiv.org/abs/2212.07672"
            },
            {
                "文章ID": "49805",
                "标题": "HaRiM$^+$: Evaluating Summary Quality with Hallucination Risk",
                "作者": " Seonil Son,  Junsoo Park,  Jeong-in Hwang,  Junghwa Lee,  Hyungjong Noh,  Yeonsoo Lee",
                "发布日期": "2022-11-28",
                "摘要": "  One of the challenges of developing a summarization model arises from the\ndifficulty in measuring the factual inconsistency of the generated text. In\nthis study, we reinterpret the decoder overconfidence-regularizing objective\nsuggested in (Miao et al., 2021) as a hallucination risk measurement to better\nestimate the quality of generated summaries. We propose a reference-free\nmetric, HaRiM+, which only requires an off-the-shelf summarization model to\ncompute the hallucination risk based on token likelihoods. Deploying it\nrequires no additional training of models or ad-hoc modules, which usually need\nalignment to human judgments. For summary-quality estimation, HaRiM+ records\nstate-of-the-art correlation to human judgment on three summary-quality\nannotation sets: FRANK, QAGS, and SummEval. We hope that our work, which merits\nthe use of summarization models, facilitates the progress of both automated\nevaluation and generation of summary.\n",
                "链接": "https://arxiv.org/abs/2211.12118"
            },
            {
                "文章ID": "56035",
                "标题": "A Succinct Summary of Reinforcement Learning",
                "作者": " Sanjeevan Ahilan",
                "发布日期": "2023-01-05",
                "摘要": "  This document is a concise summary of many key results in single-agent\nreinforcement learning (RL). The intended audience are those who already have\nsome familiarity with RL and are looking to review, reference and/or remind\nthemselves of important ideas in the field.\n",
                "链接": "https://arxiv.org/abs/2301.01379"
            },
            {
                "文章ID": "24084",
                "标题": "Comparative Snippet Generation",
                "作者": " Saurabh Jain,  Yisong Miao,  Min-Yen Kan",
                "发布日期": "2022-06-14",
                "摘要": "  We model product reviews to generate comparative responses consisting of\npositive and negative experiences regarding the product. Specifically, we\ngenerate a single-sentence, comparative response from a given positive and a\nnegative opinion. We contribute the first dataset for this task of Comparative\nSnippet Generation from contrasting opinions regarding a product, and a\nperformance analysis of a pre-trained BERT model to generate such snippets.\n",
                "链接": "https://arxiv.org/abs/2206.05473"
            },
            {
                "文章ID": "64690",
                "标题": "Summary Statistic Privacy in Data Sharing",
                "作者": " Zinan Lin,  Shuaiqi Wang,  Vyas Sekar,  Giulia Fanti",
                "发布日期": "2023-10-31",
                "摘要": "  We study a setting where a data holder wishes to share data with a receiver,\nwithout revealing certain summary statistics of the data distribution (e.g.,\nmean, standard deviation). It achieves this by passing the data through a\nrandomization mechanism. We propose summary statistic privacy, a metric for\nquantifying the privacy risk of such a mechanism based on the worst-case\nprobability of an adversary guessing the distributional secret within some\nthreshold. Defining distortion as a worst-case Wasserstein-1 distance between\nthe real and released data, we prove lower bounds on the tradeoff between\nprivacy and distortion. We then propose a class of quantization mechanisms that\ncan be adapted to different data distributions. We show that the quantization\nmechanism's privacy-distortion tradeoff matches our lower bounds under certain\nregimes, up to small constant factors. Finally, we demonstrate on real-world\ndatasets that the proposed quantization mechanisms achieve better\nprivacy-distortion tradeoffs than alternative privacy mechanisms.\n",
                "链接": "https://arxiv.org/abs/2303.02014"
            }
        ]
    },
    {
        "question": {
            "question": "查找一下近三个月有关语言模型rlhf的arxiv上的全部文章。",
            "type": "5"
        },
        "results": [
            {
                "文章ID": "102832",
                "标题": "Stabilizing RLHF through Advantage Model and Selective Rehearsal",
                "作者": " Baolin Peng,  Linfeng Song,  Ye Tian,  Lifeng Jin,  Haitao Mi,  Dong Yu",
                "发布日期": "2023-09-20",
                "摘要": "  Large Language Models (LLMs) have revolutionized natural language processing,\nyet aligning these models with human values and preferences using RLHF remains\na significant challenge. This challenge is characterized by various\ninstabilities, such as reward hacking and catastrophic forgetting. In this\ntechnical report, we propose two innovations to stabilize RLHF training: 1)\nAdvantage Model, which directly models advantage score i.e., extra reward\ncompared to the expected rewards and regulates score distributions across tasks\nto prevent reward hacking. 2) Selective Rehearsal, which mitigates catastrophic\nforgetting by strategically selecting data for PPO training and knowledge\nrehearsing. Our experimental analysis on public and proprietary datasets\nreveals that the proposed methods not only increase stability in RLHF training\nbut also achieve higher reward scores and win rates.\n",
                "链接": "https://arxiv.org/abs/2309.10202"
            },
            {
                "文章ID": "108320",
                "标题": "Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse\n  Autoencoders",
                "作者": " Luke Marks,  Amir Abdullah,  Luna Mendez,  Rauno Arike,  Philip Torr,  Fazl Barez",
                "发布日期": "2023-11-29",
                "摘要": "  Large language models (LLMs) aligned to human preferences via reinforcement\nlearning from human feedback (RLHF) underpin many commercial applications of\nLLM technology. Despite this, the impacts of RLHF on LLM internals remain\nopaque. We propose a novel method for interpreting implicit reward models\n(IRMs) in LLMs learned through RLHF. Our approach trains pairs of autoencoders\non activations from a base LLM and its RLHF-tuned variant. Through a comparison\nof autoencoder hidden spaces, we identify features that reflect the accuracy of\nthe learned IRM. To illustrate our method, we fine-tune an LLM via RLHF to\nlearn a token-utility mapping and maximize the aggregate utility of generated\ntext. This is the first application of sparse autoencoders to interpreting\nIRMs. Our method provides an abstract approximation of reward integrity and\nholds promise for measuring alignment between specified objectives and learned\nmodel behaviors.\n",
                "链接": "https://arxiv.org/abs/2310.08164"
            },
            {
                "文章ID": "90293",
                "标题": "Secrets of RLHF in Large Language Models Part I: PPO",
                "作者": " Rui Zheng,  Shihan Dou,  Songyang Gao,  Yuan Hua,  Wei Shen,  Binghai Wang,  Yan Liu,  Senjie Jin,  Qin Liu,  Yuhao Zhou,  Limao Xiong,  Lu Chen,  Zhiheng Xi,  Nuo Xu,  Wenbin Lai,  Minghao Zhu,  Cheng Chang,  Zhangyue Yin,  Rongxiang Weng,  Wensen Cheng,  Haoran Huang,  Tianxiang Sun,  Hang Yan,  Tao Gui,  Qi Zhang,  Xipeng Qiu,  Xuanjing Huang",
                "发布日期": "2023-07-19",
                "摘要": "  Large language models (LLMs) have formulated a blueprint for the advancement\nof artificial general intelligence. Its primary objective is to function as a\nhuman-centric (helpful, honest, and harmless) assistant. Alignment with humans\nassumes paramount significance, and reinforcement learning with human feedback\n(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.\nCurrent technical routes usually include \\textbf{reward models} to measure\nhuman preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize\npolicy model outputs, and \\textbf{process supervision} to improve step-by-step\nreasoning capabilities. However, due to the challenges of reward design,\nenvironment interaction, and agent training, coupled with huge trial and error\ncost of large language models, there is a significant barrier for AI\nresearchers to motivate the development of technical alignment and safe landing\nof LLMs. The stable training of RLHF has still been a puzzle. In the first\nreport, we dissect the framework of RLHF, re-evaluate the inner workings of\nPPO, and explore how the parts comprising PPO algorithms impact policy agent\ntraining. We identify policy constraints being the key factor for the effective\nimplementation of the PPO algorithm. Therefore, we explore the PPO-max, an\nadvanced version of PPO algorithm, to efficiently improve the training\nstability of the policy model. Based on our main results, we perform a\ncomprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.\nThe absence of open-source implementations has posed significant challenges to\nthe investigation of LLMs alignment. Therefore, we are eager to release\ntechnical reports, reward models and PPO codes, aiming to make modest\ncontributions to the advancement of LLMs.\n",
                "链接": "https://arxiv.org/abs/2307.04964"
            },
            {
                "文章ID": "109941",
                "标题": "Safe RLHF: Safe Reinforcement Learning from Human Feedback",
                "作者": " Josef Dai,  Xuehai Pan,  Ruiyang Sun,  Jiaming Ji,  Xinbo Xu,  Mickel Liu,  Yizhou Wang,  Yaodong Yang",
                "发布日期": "2023-10-20",
                "摘要": "  With the development of large language models (LLMs), striking a balance\nbetween the performance and safety of AI systems has never been more critical.\nHowever, the inherent tension between the objectives of helpfulness and\nharmlessness presents a significant challenge during LLM training. To address\nthis issue, we propose Safe Reinforcement Learning from Human Feedback (Safe\nRLHF), a novel algorithm for human value alignment. Safe RLHF explicitly\ndecouples human preferences regarding helpfulness and harmlessness, effectively\navoiding the crowdworkers' confusion about the tension and allowing us to train\nseparate reward and cost models. We formalize the safety concern of LLMs as an\noptimization task of maximizing the reward function while satisfying specified\ncost constraints. Leveraging the Lagrangian method to solve this constrained\nproblem, Safe RLHF dynamically adjusts the balance between the two objectives\nduring fine-tuning. Through a three-round fine-tuning using Safe RLHF, we\ndemonstrate a superior ability to mitigate harmful responses while enhancing\nmodel performance compared to existing value-aligned algorithms.\nExperimentally, we fine-tuned the Alpaca-7B using Safe RLHF and aligned it with\ncollected human preferences, significantly improving its helpfulness and\nharmlessness according to human evaluations.\n",
                "链接": "https://arxiv.org/abs/2310.12773"
            },
            {
                "文章ID": "104317",
                "标题": "Aligning Large Multimodal Models with Factually Augmented RLHF",
                "作者": " Zhiqing Sun,  Sheng Shen,  Shengcao Cao,  Haotian Liu,  Chunyuan Li,  Yikang Shen,  Chuang Gan,  Liang-Yan Gui,  Yu-Xiong Wang,  Yiming Yang,  Kurt Keutzer,  Trevor Darrell",
                "发布日期": "2023-09-27",
                "摘要": "  Large Multimodal Models (LMM) are built across modalities and the\nmisalignment between two modalities can result in \"hallucination\", generating\ntextual outputs that are not grounded by the multimodal information in context.\nTo address the multimodal misalignment issue, we adapt the Reinforcement\nLearning from Human Feedback (RLHF) from the text domain to the task of\nvision-language alignment, where human annotators are asked to compare two\nresponses and pinpoint the more hallucinated one, and the vision-language model\nis trained to maximize the simulated human rewards. We propose a new alignment\nalgorithm called Factually Augmented RLHF that augments the reward model with\nadditional factual information such as image captions and ground-truth\nmulti-choice options, which alleviates the reward hacking phenomenon in RLHF\nand further improves the performance. We also enhance the GPT-4-generated\ntraining data (for vision instruction tuning) with previously available\nhuman-written image-text pairs to improve the general capabilities of our\nmodel. To evaluate the proposed approach in real-world scenarios, we develop a\nnew evaluation benchmark MMHAL-BENCH with a special focus on penalizing\nhallucinations. As the first LMM trained with RLHF, our approach achieves\nremarkable improvement on the LLaVA-Bench dataset with the 94% performance\nlevel of the text-only GPT-4 (while previous best methods can only achieve the\n87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We\nopensource our code, model, data at https://llava-rlhf.github.io.\n",
                "链接": "https://arxiv.org/abs/2309.14525"
            },
            {
                "文章ID": "123656",
                "标题": "An Adaptive Placement and Parallelism Framework for Accelerating RLHF\n  Training",
                "作者": " Youshao Xiao,  Weichang Wu,  Zhenglei Zhou,  Fagui Mao,  Shangchun Zhao,  Lin Ju,  Lei Liang,  Xiaolu Zhang,  Jun Zhou",
                "发布日期": "2023-12-20",
                "摘要": "  Recently, ChatGPT or InstructGPT like large language models (LLM) has made a\nsignificant impact in the AI world. These models are incredibly versatile,\ncapable of performing language tasks on par or even exceeding the capabilities\nof human experts. Many works have attempted to reproduce the complex\nInstructGPT's RLHF (Reinforcement Learning with Human Feedback) training\npipeline. However, the mainstream distributed RLHF training methods typically\nadopt a fixed model placement strategy, referred to as the Flattening strategy.\nThis strategy treats all four models involved in RLHF as a single entity and\nplaces them on all devices, regardless of their differences. Unfortunately,\nthis strategy exacerbates the generation bottlenecks in the RLHF training and\ndegrades the overall training efficiency. To address these issues, we propose\nan adaptive model placement framework that offers two flexible model placement\nstrategies. These strategies allow for the agile allocation of models across\ndevices in a fine-grained manner. The Interleaving strategy helps reduce memory\nredundancy and communication costs during RLHF training. On the other hand, the\nSeparation strategy improves the throughput of model training by separating the\ntraining and generation stages of the RLHF pipeline. Notably, this framework\nseamlessly integrates with other mainstream techniques for acceleration and\nenables automatic hyperparameter search. Extensive experiments have\ndemonstrated that our Interleaving and Separation strategies can achieve\nnotable improvements up to 11x, compared to the current state-of-the-art (SOTA)\napproaches. These experiments encompassed a wide range of training scenarios,\ninvolving models of varying sizes and devices of different scales. The results\nhighlight the effectiveness and superiority of our approaches in accelerating\nthe training of distributed RLHF.\n",
                "链接": "https://arxiv.org/abs/2312.11819"
            },
            {
                "文章ID": "114775",
                "标题": "Removing RLHF Protections in GPT-4 via Fine-Tuning",
                "作者": " Qiusi Zhan,  Richard Fang,  Rohan Bindu,  Akul Gupta,  Tatsunori Hashimoto,  Daniel Kang",
                "发布日期": "2023-11-14",
                "摘要": "  As large language models (LLMs) have increased in their capabilities, so does\ntheir potential for dual use. To reduce harmful outputs, produces and vendors\nof LLMs have used reinforcement learning with human feedback (RLHF). In tandem,\nLLM vendors have been increasingly enabling fine-tuning of their most powerful\nmodels. However, concurrent work has shown that fine-tuning can remove RLHF\nprotections. We may expect that the most powerful models currently available\n(GPT-4) are less susceptible to fine-tuning attacks.\n  In this work, we show the contrary: fine-tuning allows attackers to remove\nRLHF protections with as few as 340 examples and a 95% success rate. These\ntraining examples can be automatically generated with weaker models. We further\nshow that removing RLHF protections does not decrease usefulness on\nnon-censored outputs, providing evidence that our fine-tuning strategy does not\ndecrease usefulness despite using weaker models to generate training data. Our\nresults show the need for further research on protections on LLMs.\n",
                "链接": "https://arxiv.org/abs/2311.05553"
            },
            {
                "文章ID": "99899",
                "标题": "Efficient RLHF: Reducing the Memory Usage of PPO",
                "作者": " Michael Santacroce,  Yadong Lu,  Han Yu,  Yuanzhi Li,  Yelong Shen",
                "发布日期": "2023-09-06",
                "摘要": "  Reinforcement Learning with Human Feedback (RLHF) has revolutionized language\nmodeling by aligning models with human preferences. However, the RL stage,\nProximal Policy Optimization (PPO), requires over 3x the memory of Supervised\nFine-Tuning (SFT), making it infeasible to use for most practitioners. To\naddress this issue, we present a comprehensive analysis the memory usage,\nperformance, and training time of memory-savings techniques for PPO. We\nintroduce Hydra-RLHF by first integrating the SFT and Reward models and then\ndynamically turning LoRA \"off\" during training. Our experiments show: 1. Using\nLoRA during PPO reduces its memory usage to be smaller than SFT while improving\nalignment across four public benchmarks, and 2. Hydra-PPO reduces the latency\nper sample of LoRA-PPO by up to 65% while maintaining its performance. Our\nresults demonstrate that Hydra-PPO is a simple and promising solution for\nenabling more widespread usage of RLHF.\n",
                "链接": "https://arxiv.org/abs/2309.00754"
            },
            {
                "文章ID": "106633",
                "标题": "A Long Way to Go: Investigating Length Correlations in RLHF",
                "作者": " Prasann Singhal,  Tanya Goyal,  Jiacheng Xu,  Greg Durrett",
                "发布日期": "2023-10-06",
                "摘要": "  Great successes have been reported using Reinforcement Learning from Human\nFeedback (RLHF) to align large language models. Open-source preference datasets\nand reward models have enabled wider experimentation beyond generic chat\nsettings, particularly to make systems more \"helpful\" for tasks like web\nquestion answering, summarization, and multi-turn dialogue. When optimizing for\nhelpfulness, RLHF has been consistently observed to drive models to produce\nlonger outputs. This paper demonstrates that optimizing for response length is\na significant factor behind RLHF's reported improvements in these settings.\nFirst, we study the relationship between reward and length for reward models\ntrained on three open-source preference datasets for helpfulness. Here, length\ncorrelates strongly with reward, and improvements in reward score are driven in\nlarge part by shifting the distribution over output lengths. We then explore\ninterventions during both RL and reward model learning to see if we can achieve\nthe same downstream improvements as RLHF without increasing length. While our\ninterventions mitigate length increases, they aren't uniformly effective across\nsettings. Furthermore, we find that even running RLHF with a reward based\nsolely on length can reproduce most of the downstream improvements over the\ninitial policy model, showing that reward models in these settings have a long\nway to go.\n",
                "链接": "https://arxiv.org/abs/2310.03716"
            },
            {
                "文章ID": "107260",
                "标题": "SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to\n  RLHF",
                "作者": " Yi Dong,  Zhilin Wang,  Makesh Narsimhan Sreedhar,  Xianchao Wu,  Oleksii Kuchaiev",
                "发布日期": "2023-10-10",
                "摘要": "  Model alignment with human preferences is an essential step in making Large\nLanguage Models (LLMs) helpful and consistent with human values. It typically\nconsists of supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF) stages. However, RLHF faces inherent limitations stemming from\na complex training setup and its tendency to align the model with implicit\nvalues that end users cannot control at run-time. Moreover, reward models in\nRLHF stage commonly rely on single-dimensional feedback as opposed to explicit,\nmultifaceted signals that indicate attributes such as helpfulness, humor, and\ntoxicity. To address these limitations, we propose SteerLM, a supervised\nfine-tuning method that empowers end-users to control responses during\ninference. SteerLM conditions responses to conform to an explicitly defined\nmulti-dimensional set of attributes, thereby empowering a steerable AI capable\nof generating helpful and high-quality responses while maintaining\ncustomizability. Experiments show that SteerLM trained on open source datasets\ngenerates responses that are preferred by human and automatic evaluators to\nmany state-of-the-art baselines trained with RLHF while being much easier to\ntrain. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B\n",
                "链接": "https://arxiv.org/abs/2310.05344"
            }
        ]
    },
    {
        "question": {
            "question": "查找多模态大模型理解和生成统一建模、端到端训练相关论文",
            "type": "1"
        },
        "results": [
            {
                "文章ID": "118545",
                "标题": "GPT4Video: A Unified Multimodal Large Language Model for\n  lnstruction-Followed Understanding and Safety-Aware Generation",
                "作者": " Zhanyu Wang,  Longyue Wang,  Zhen Zhao,  Minghao Wu,  Chenyang Lyu,  Huayang Li,  Deng Cai,  Luping Zhou,  Shuming Shi,  Zhaopeng Tu",
                "发布日期": "2023-11-29",
                "摘要": "  While the recent advances in Multimodal Large Language Models (MLLMs)\nconstitute a significant leap forward in the field, these models are\npredominantly confined to the realm of input-side multimodal comprehension,\nlacking the capacity for multimodal content generation. To fill this gap, we\npresent GPT4Video, a unified multi-model framework that empowers Large Language\nModels (LLMs) with the capability of both video understanding and generation.\nSpecifically, we develop an instruction-following-based approach integrated\nwith the stable diffusion generative model, which has demonstrated to\neffectively and securely handle video generation scenarios. GPT4Video offers\nthe following benefits: 1) It exhibits impressive capabilities in both video\nunderstanding and generation scenarios. For example, GPT4Video outperforms\nValley by 11.8\\% on the Video Question Answering task, and surpasses NExt-GPT\nby 2.3\\% on the Text to Video generation task. 2) it endows the LLM/MLLM with\nvideo generation capabilities without requiring additional training parameters\nand can flexibly interface with a wide range of models to perform video\ngeneration. 3) it maintains a safe and healthy conversation not only in\noutput-side but also the input side in an end-to-end manner. Qualitative and\nqualitative experiments demonstrate that GPT4Video holds the potential to\nfunction as a effective, safe and Humanoid-like video assistant that can handle\nboth video understanding and generation scenarios.\n",
                "链接": "https://arxiv.org/abs/2311.16511"
            },
            {
                "文章ID": "21046",
                "标题": "End-to-End Multimodal Fact-Checking and Explanation Generation: A\n  Challenging Dataset and Models",
                "作者": "Virginia Tech  Barry Menglong Yao, Virginia Tech  Aditya Shah, Lehigh University  Lichao Sun, Virginia Tech  Jin-Hee Cho, Virginia Tech  Lifu Huang",
                "发布日期": "2023-07-10",
                "摘要": "  We propose end-to-end multimodal fact-checking and explanation generation,\nwhere the input is a claim and a large collection of web sources, including\narticles, images, videos, and tweets, and the goal is to assess the\ntruthfulness of the claim by retrieving relevant evidence and predicting a\ntruthfulness label (e.g., support, refute or not enough information), and to\ngenerate a statement to summarize and explain the reasoning and ruling process.\nTo support this research, we construct Mocheg, a large-scale dataset consisting\nof 15,601 claims where each claim is annotated with a truthfulness label and a\nruling statement, and 33,880 textual paragraphs and 12,112 images in total as\nevidence. To establish baseline performances on Mocheg, we experiment with\nseveral state-of-the-art neural architectures on the three pipelined subtasks:\nmultimodal evidence retrieval, claim verification, and explanation generation,\nand demonstrate that the performance of the state-of-the-art end-to-end\nmultimodal fact-checking does not provide satisfactory outcomes. To the best of\nour knowledge, we are the first to build the benchmark dataset and solutions\nfor end-to-end multimodal fact-checking and explanation generation. The\ndataset, source code and model checkpoints are available at\nhttps://github.com/VT-NLP/Mocheg.\n",
                "链接": "https://arxiv.org/abs/2205.12487"
            },
            {
                "文章ID": "2020",
                "标题": "End-to-end Generative Pretraining for Multimodal Video Captioning",
                "作者": " Paul Hongsuck Seo,  Arsha Nagrani,  Anurag Arnab,  Cordelia Schmid",
                "发布日期": "2022-05-11",
                "摘要": "  Recent video and language pretraining frameworks lack the ability to generate\nsentences. We present Multimodal Video Generative Pretraining (MV-GPT), a new\npretraining framework for learning from unlabelled videos which can be\neffectively used for generative tasks such as multimodal video captioning.\nUnlike recent video-language pretraining frameworks, our framework trains both\na multimodal video encoder and a sentence decoder jointly. To overcome the lack\nof captions in unlabelled videos, we leverage the future utterance as an\nadditional text source and propose a bidirectional generation objective -- we\ngenerate future utterances given the present mulitmodal context, and also the\npresent utterance given future observations. With this objective, we train an\nencoder-decoder model end-to-end to generate a caption from raw pixels and\ntranscribed speech directly. Our model achieves state-of-the-art performance\nfor multimodal video captioning on four standard benchmarks, as well as for\nother video understanding tasks such as VideoQA, video retrieval and action\nclassification.\n",
                "链接": "https://arxiv.org/abs/2201.08264"
            },
            {
                "文章ID": "37245",
                "标题": "SPACE-3: Unified Dialog Model Pre-training for Task-Oriented Dialog\n  Understanding and Generation",
                "作者": " Wanwei He,  Yinpei Dai,  Min Yang,  Jian Sun,  Fei Huang,  Luo Si,  Yongbin Li",
                "发布日期": "2022-09-15",
                "摘要": "  Recently, pre-training methods have shown remarkable success in task-oriented\ndialog (TOD) systems. However, most existing pre-trained models for TOD focus\non either dialog understanding or dialog generation, but not both. In this\npaper, we propose SPACE-3, a novel unified semi-supervised pre-trained\nconversation model learning from large-scale dialog corpora with limited\nannotations, which can be effectively fine-tuned on a wide range of downstream\ndialog tasks. Specifically, SPACE-3 consists of four successive components in a\nsingle transformer to maintain a task-flow in TOD systems: (i) a dialog\nencoding module to encode dialog history, (ii) a dialog understanding module to\nextract semantic vectors from either user queries or system responses, (iii) a\ndialog policy module to generate a policy vector that contains high-level\nsemantics of the response, and (iv) a dialog generation module to produce\nappropriate responses. We design a dedicated pre-training objective for each\ncomponent. Concretely, we pre-train the dialog encoding module with span mask\nlanguage modeling to learn contextualized dialog information. To capture the\nstructured dialog semantics, we pre-train the dialog understanding module via a\nnovel tree-induced semi-supervised contrastive learning objective with the help\nof extra dialog annotations. In addition, we pre-train the dialog policy module\nby minimizing the L2 distance between its output policy vector and the semantic\nvector of the response for policy optimization. Finally, the dialog generation\nmodel is pre-trained by language modeling. Results show that SPACE-3 achieves\nstate-of-the-art performance on eight downstream dialog benchmarks, including\nintent prediction, dialog state tracking, and end-to-end dialog modeling. We\nalso show that SPACE-3 has a stronger few-shot ability than existing models\nunder the low-resource setting.\n",
                "链接": "https://arxiv.org/abs/2209.06664"
            },
            {
                "文章ID": "116124",
                "标题": "Efficient End-to-End Visual Document Understanding with Rationale\n  Distillation",
                "作者": " Wang Zhu,  Alekh Agarwal,  Mandar Joshi,  Robin Jia,  Jesse Thomason,  Kristina Toutanova",
                "发布日期": "2023-11-17",
                "摘要": "  Understanding visually situated language requires recognizing text and visual\nelements, and interpreting complex layouts. State-of-the-art methods commonly\nuse specialized pre-processing tools, such as optical character recognition\n(OCR) systems, that map document image inputs to extracted information in the\nspace of textual tokens, and sometimes also employ large language models (LLMs)\nto reason in text token space. However, the gains from external tools and LLMs\ncome at the cost of increased computational and engineering complexity. In this\npaper, we ask whether small pretrained image-to-text models can learn selective\ntext or layout recognition and reasoning as an intermediate inference step in\nan end-to-end model for pixel-level visual language understanding. We\nincorporate the outputs of such OCR tools, LLMs, and larger multimodal models\nas intermediate ``rationales'' on training data, and train a small student\nmodel to predict both rationales and answers for input questions based on those\ntraining examples. A student model based on Pix2Struct (282M parameters)\nachieves consistent improvements on three visual document understanding\nbenchmarks representing infographics, scanned documents, and figures, with\nimprovements of more than 4\\% absolute over a comparable Pix2Struct model that\npredicts answers directly.\n",
                "链接": "https://arxiv.org/abs/2311.09612"
            },
            {
                "文章ID": "106028",
                "标题": "Tuning Large language model for End-to-end Speech Translation",
                "作者": " Hao Zhang,  Nianwen Si,  Yaqi Chen,  Wenlin Zhang,  Xukui Yang,  Dan Qu,  Xiaolin Jiao",
                "发布日期": "2023-10-04",
                "摘要": "  With the emergence of large language models (LLMs), multimodal models based\non LLMs have demonstrated significant potential. Models such as LLaSM, X-LLM,\nand SpeechGPT exhibit an impressive ability to comprehend and generate human\ninstructions. However, their performance often falters when faced with complex\ntasks like end-to-end speech translation (E2E-ST), a cross-language and\ncross-modal translation task. In comparison to single-modal models, multimodal\nmodels lag behind in these scenarios. This paper introduces LST, a Large\nmultimodal model designed to excel at the E2E-ST task. LST consists of a speech\nfrontend, an adapter, and a LLM backend. The training of LST consists of two\nstages: (1) Modality adjustment, where the adapter is tuned to align speech\nrepresentation with text embedding space, and (2) Downstream task fine-tuning,\nwhere both the adapter and LLM model are trained to optimize performance on the\nE2EST task. Experimental results on the MuST-C speech translation benchmark\ndemonstrate that LST-13B achieves BLEU scores of 30.39/41.55/35.33 on\nEn-De/En-Fr/En-Es language pairs, surpassing previous models and establishing a\nnew state-of-the-art. Additionally, we conduct an in-depth analysis of\nsingle-modal model selection and the impact of training strategies, which lays\nthe foundation for future research. We will open up our code and models after\nreview.\n",
                "链接": "https://arxiv.org/abs/2310.02050"
            },
            {
                "文章ID": "13855",
                "标题": "Three-Module Modeling For End-to-End Spoken Language Understanding Using\n  Pre-trained DNN-HMM-Based Acoustic-Phonetic Model",
                "作者": " Nick J. C. Wang,  Lu Wang,  Yandan Sun,  Haimei Kang,  Dejun Zhang",
                "发布日期": "2022-04-08",
                "摘要": "  In spoken language understanding (SLU), what the user says is converted to\nhis/her intent. Recent work on end-to-end SLU has shown that accuracy can be\nimproved via pre-training approaches. We revisit ideas presented by Lugosch et\nal. using speech pre-training and three-module modeling; however, to ease\nconstruction of the end-to-end SLU model, we use as our phoneme module an\nopen-source acoustic-phonetic model from a DNN-HMM hybrid automatic speech\nrecognition (ASR) system instead of training one from scratch. Hence we\nfine-tune on speech only for the word module, and we apply multi-target\nlearning (MTL) on the word and intent modules to jointly optimize SLU\nperformance. MTL yields a relative reduction of 40% in intent-classification\nerror rates (from 1.0% to 0.6%). Note that our three-module model is a\nstreaming method. The final outcome of the proposed three-module modeling\napproach yields an intent accuracy of 99.4% on FluentSpeech, an intent error\nrate reduction of 50% compared to that of Lugosch et al. Although we focus on\nreal-time streaming methods, we also list non-streaming methods for comparison.\n",
                "链接": "https://arxiv.org/abs/2204.03315"
            },
            {
                "文章ID": "100055",
                "标题": "End-to-End Learning on Multimodal Knowledge Graphs",
                "作者": " W. X. Wilcke,  P. Bloem,  V. de Boer,  R. H. van t Veer",
                "发布日期": "2023-09-06",
                "摘要": "  Knowledge graphs enable data scientists to learn end-to-end on heterogeneous\nknowledge. However, most end-to-end models solely learn from the relational\ninformation encoded in graphs' structure: raw values, encoded as literal nodes,\nare either omitted completely or treated as regular nodes without consideration\nfor their values. In either case we lose potentially relevant information which\ncould have otherwise been exploited by our learning methods. We propose a\nmultimodal message passing network which not only learns end-to-end from the\nstructure of graphs, but also from their possibly divers set of multimodal node\nfeatures. Our model uses dedicated (neural) encoders to naturally learn\nembeddings for node features belonging to five different types of modalities,\nincluding numbers, texts, dates, images and geometries, which are projected\ninto a joint representation space together with their relational information.\nWe implement and demonstrate our model on node classification and link\nprediction for artificial and real-worlds datasets, and evaluate the effect\nthat each modality has on the overall performance in an inverse ablation study.\nOur results indicate that end-to-end multimodal learning from any arbitrary\nknowledge graph is indeed possible, and that including multimodal information\ncan significantly affect performance, but that much depends on the\ncharacteristics of the data.\n",
                "链接": "https://arxiv.org/abs/2309.01169"
            },
            {
                "文章ID": "10229",
                "标题": "UNIMO-2: End-to-End Unified Vision-Language Grounded Learning",
                "作者": " Wei Li,  Can Gao,  Guocheng Niu,  Xinyan Xiao,  Hao Liu,  Jiachen Liu,  Hua Wu,  Haifeng Wang",
                "发布日期": "2022-03-18",
                "摘要": "  Vision-Language Pre-training (VLP) has achieved impressive performance on\nvarious cross-modal downstream tasks. However, most existing methods can only\nlearn from aligned image-caption data and rely heavily on expensive regional\nfeatures, which greatly limits their scalability and performance. In this\npaper, we propose an end-to-end unified-modal pre-training framework, namely\nUNIMO-2, for joint learning on both aligned image-caption data and unaligned\nimage-only and text-only corpus. We build a unified Transformer model to\njointly learn visual representations, textual representations and semantic\nalignment between images and texts. In particular, we propose to conduct\ngrounded learning on both images and texts via a sharing grounded space, which\nhelps bridge unaligned images and texts, and align the visual and textual\nsemantic spaces on different types of corpora. The experiments show that our\ngrounded learning method can improve textual and visual semantic alignment for\nimproving performance on various cross-modal tasks. Moreover, benefiting from\neffective joint modeling of different types of corpora, our model also achieves\nimpressive performance on single-modal visual and textual tasks. Our code and\nmodels are public at the UNIMO project page https://unimo-ptm.github.io/.\n",
                "链接": "https://arxiv.org/abs/2203.09067"
            },
            {
                "文章ID": "94301",
                "标题": "Interpretable End-to-End Driving Model for Implicit Scene Understanding",
                "作者": " Yiyang Sun,  Xiaonian Wang,  Yangyang Zhang,  Jiagui Tang,  Xiaqiang Tang,  Jing Yao",
                "发布日期": "2023-08-03",
                "摘要": "  Driving scene understanding is to obtain comprehensive scene information\nthrough the sensor data and provide a basis for downstream tasks, which is\nindispensable for the safety of self-driving vehicles. Specific perception\ntasks, such as object detection and scene graph generation, are commonly used.\nHowever, the results of these tasks are only equivalent to the characterization\nof sampling from high-dimensional scene features, which are not sufficient to\nrepresent the scenario. In addition, the goal of perception tasks is\ninconsistent with human driving that just focuses on what may affect the\nego-trajectory. Therefore, we propose an end-to-end Interpretable Implicit\nDriving Scene Understanding (II-DSU) model to extract implicit high-dimensional\nscene features as scene understanding results guided by a planning module and\nto validate the plausibility of scene understanding using auxiliary perception\ntasks for visualization. Experimental results on CARLA benchmarks show that our\napproach achieves the new state-of-the-art and is able to obtain scene features\nthat embody richer scene information relevant to driving, enabling superior\nperformance of the downstream planning.\n",
                "链接": "https://arxiv.org/abs/2308.01180"
            }
        ]
    }
]